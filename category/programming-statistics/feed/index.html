<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title> 程式與統計 &#8211; 果醬珍珍•JamJam</title>
	<atom:link href="/category/programming-statistics/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>健忘女孩Jam的學習筆記和生活雜記</description>
	<lastBuildDate>Mon, 12 Oct 2020 01:30:37 +0000</lastBuildDate>
	<language>zh-TW</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Gradient Boosting Machines GBM &#124; gbm, xgboost, h2o &#124; R語言</title>
		<link>/gradient-boosting-machines-gbm/</link>
					<comments>/gradient-boosting-machines-gbm/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Sun, 14 Apr 2019 14:54:16 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<guid isPermaLink="false">/?p=2875</guid>

					<description><![CDATA[<p>Gradient Boosting Machines 是一個超級受歡迎的機器學習法，在許多領域上都有非常成功的表現，也是Kaggle競賽時常勝出的主要演算法之一 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/gradient-boosting-machines-gbm/">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Gradient Boosting Machines 是一個超級受歡迎的機器學習法，在許多領域上都有非常成功的表現，也是Kaggle競賽時常勝出的主要演算法之一。有別於<a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">隨機森林</a>集成眾多深且獨立的樹模型，GBM則是集成諸多淺且弱連續的樹模型，每個樹模型會以之前的樹模型為基礎去學習和精進，結果通常是難以擊敗的。</p>
<h3>套件與資料準備</h3>
<p></p><pre class="crayon-plain-tag">library(rsample)      # data splitting 
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
library(vtreat)</pre><p>使用AmesHousing套件中的Ames Housing數據，並將數據切分為7:3的訓練測試比例。</p><pre class="crayon-plain-tag"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</pre><p>tree-based演算法往往可以將未處理資料配適的很好(即不需要特別將資料進行normalize, center,scale)，所以在以下筆記中將聚焦在如何使用多種不同套件執行GBMs。而雖然在這邊沒有去進行資料前處理，但我們仍可花時間透過處理變數特徵使得模型成效更佳。</p>
<h3>Gradient Boosting Machines&#8217; Advantage &amp; Disadvantages</h3>
<h4>優勢</h4>
<ol>
<li>產生的預測精準度通常無人能打敗</li>
<li>擁有許多彈性 &#8211; 可以針對不同的Loss Function來進行優化(*Loss Function即所有需要被「最小化」的目標函式，一個最常用來尋找使目標函式最小值的資料點的方法即為gradient descent)，且有hyperparameters的參數選項可以tuning，好讓目標函式配適的很好。</li>
<li>不需要資料前處理 &#8211; 通常可以很好的處理類別和數值型變數。</li>
<li>可以處理遺失值 &#8211; 不需要空值填補。</li>
</ol>
<h4>劣勢</h4>
<ol>
<li>GBMs會持續優化模型來最小化誤差。這樣可能會過度擬和極端值的部分而造成過度配適。因此必須綜合使用cross validation來銷除這個情況。</li>
<li>計算上成本非常高 &#8211; GBMs通常會包含許多樹(&gt;1000)，會佔用許多時間和記憶體。</li>
<li>模型彈性度高，導致許多參數會影響grid search的流程(比如說迭代次數，樹的深度，參數正規化等等)，在tuning模型的時候會需要大型的grid search過程。</li>
<li>可解釋程度稍稍少了一點，但有許多輔助工具如變數重要性、partial dependence plots, LIME等。</li>
</ol>
<h3>Gradient Boosting Machines (GBMs) 概念</h3>
<p>許多監督式機器學習模型都是建立在單一預測模的基礎上（比如說linear regression, penalized models or regularized regression, naive Bayes, support vector machines)。而則有像是Bagging和Random Forests這種集成學習演算法(ensemble learning)，集成眾多預測模型並單純平均每個模型的預測結果得出預測值。另外一種則是Boosting系列，是基於不一樣的建設性策略來進行集成學習。</p>
<p>Boosting的主要概念就是將新模型「有順序、循序漸進」的加入集成學習。在每一次迭代中，會新增一個新的(new)、弱的(weak)、base-learner模型，針對到目前為止集成學習的誤差進行訓練。</p>
<p>以下進一步解釋boosting models的特徵關鍵字：</p>
<ol>
<li>Base-learning models: Boosting是一個迭代改善任何弱學習模型的框架。許多Gradienet Boosting應用可允使用者任意加入眾多類型的weak learners模型。然而在實務上，boosted algorithm幾乎總是使用Decision Trees當作base-learner。也因此本學習筆記會主要討論boosting regression trees的應用。</li>
<li>Training weak models: 所謂的「弱模型(weak model)」指的是模型的錯誤率只有稍稍比隨機猜測好一點點的模型。Boosting背後的概念就是每一個順序模型都會建立一個僅稍稍改善殘餘誤差的弱模型(weak model)。以決策樹來說，較淺的樹就是弱模型的代表。一般來說，淺的決策樹模型切割數約在1~6之間。綜合多個弱模型會有以下好處(將較於綜合多個強模型)：
<ul>
<li>速率: 建構弱模型在計算成本上是便宜的。</li>
<li>精準度的改善: weak model允許演算法「慢慢學習」；即在模型表現不好的新領域進行些微調整。一般來說，慢慢學習的統計方法通常表現不錯。</li>
<li>避免overfitting: 由於在集成學習過程中，每一次訓練模型僅稍稍貢獻一點點額外的成效改善，使得我們有辦法即時在偵測到overfitting時即停止學習(使用的cross validation)。</li>
</ul>
</li>
<li>針對集成學習的殘餘誤差有順序性的訓練: Boosted trees是有順序性的；每一個樹模型都是依據前面的樹模型所得的資訊而訓練而成。基本的boosted regression trees演算法可被一般化為以下步驟(其中x代表features而y代表目標回應變數)：
<ul>
<li>依據原始資料配適一棵樹模型: \(F_{1}(x) = y\)</li>
<li>並依據先前的殘餘誤差訓練下一棵樹模型: \(h_{1}(x) = y &#8211; F_{1}(x)\)</li>
<li>將新的樹新增到演算法：\(F_{2}(x) = F_{1}(x) + h_{1}(x)\)</li>
<li>再一次依據殘餘誤差訓練下一棵樹模型 = \(h_{2}(x) = y &#8211; F_{2}(x)\)</li>
<li>將新的樹新增到演算法：\(F_{3}(x) = F_{2}(x) + h_{2}(x)\)</li>
<li>持續這個過程直到一些機制(如cross validation)告訴演算法可以停止。</li>
</ul>
</li>
</ol>
<p>boosted regression trees的基本演算法概念可以一般化為以下，即最終模型是b個獨立的回歸樹模型階段性相加的結果：<br />
\[ f(x) = \sum_{b=1}^B f^b(x) \]</p>
<h3>Gradient descent</h3>
<p>許多演算法，包括Decision trees，都聚焦在最小化殘差，也因此皆強調「MSE Loss Function」的目標函式。上面所討論到的演算法摘要了boosting法如何循序漸進地利用有順序新的弱回歸樹模型來配飾真實資料趨勢並最小化誤差。這個就是gradient boosting用來最小化Meas Squared Error (MSE) Loss Funciton的方法。雖然有時候我們會想要聚焦在其他Loss function上，如Mean Absolute Error(MAE)，或是遇到分類問題時所使用的deviance。而<strong><em>gradient</em></strong> boosting machines的命名則是取自於這個方法可以擴張至除了MSE以外的Loss function。</p>
<p>Gradient Boosting被認為是一個<strong><em>gradient descent</em></strong>(梯度下降)的演算法。Gradient Descent是一個非常通用的最適化演算法，能夠找到解決各種問題的最佳解。Gradient Descent的概念就是迭代的微調整參數來來最小化損失函數Loss Funciton。Gradient Descent會在給定的一組參數\(\theta\)區間，衡量局部Loss(cost) function的gradient，並沿著gradient下降的方向。直到gradient為0時，則找到局部最小值。</p>
<p><strong><em>Gradient descent</em></strong>可以被應用在任意可微分的loss function上，所以讓GBMs可以針對感興趣的loss function來尋找最適解。在gradient descent中一個重要的參數就是由_learning rate_s所決定的每次的變動率(size of steps)。較小的變動率會使得模型訓練過程中會迭代多次來尋找最小值，而過高的變動率則可能會讓模型錯過最小值和遠遠偏離初始值。</p>
<p>此外，並不是所有的loss function都是凸形的(convex)(如碗型)。可能會有局部的最小值、平坦高原或不規則地形等，使得尋找global minimum變得困難。<strong><em>Stochastic gradient descent</em></strong>(隨機梯度下降)則可處理這樣的問題，透過抽樣一部分比例的觀察值（通常不重複），並使用此子集合建立下一個模型。這使得演算法變得更快一些，但是隨機抽樣的隨機性亦造成下降的loss function gradient的隨機性。雖然這個隨機性使得演算法無法找到absolute global minimum(全域最小值)，但隨機性確實能讓演算法跳脫local minimum(局部最小值)、平坦高原等局部解，並接近全域的最小值。</p>
<p>我們在下一個階段即能看到如何透過多種hyperparameters參數選項來調整如何處理loss function的gradient descent。</p>
<h3>Tuning</h3>
<p>GBMs的好處與壞處就是該演算法提供多個調整參數。好處是GBMs在執行上非常具彈性，但壞處就是在調整與尋找最適參數組合上會很耗時。以下為幾個GBMs最常見的調整hyperparameters參數：</p>
<ul>
<li>決策樹模型的數量：總共要配適的決策樹模型數量。GBMs通常會需要很多很多樹，但不像random Forests，GBMs是可以過度擬和(overfit)的，所以他演算的目標是尋找最適決策樹數量使得感興趣的loss function最小化。</li>
<li>決策樹的深度：d,每棵樹模型的切割(split)數。用來控制boosted集成模型的複雜度。通常\(d=1\)的效果不錯，即此弱模型是由一次分割所得的樹模型所組成。而更常見的split數可能介在\(1&lt;d&lt;10\)之間。</li>
<li>Learning rate: 決定演算法計算gradient descent的速率。較慢的learning rate速率，可以避免overfitting的機會，但同時也會增加尋找最適解的時間。learning rate也被稱作shrinkage。</li>
<li>subsampling: 控制是否要使用原始訓練資料部分比例的抽樣子集合。使用少於100%的訓練資料表示你將使用stochastic gradient descent，這將有助於避免overfitting以及陷在loss function gradient的局部最小最大值。</li>
</ul>
<p>而在本學習筆記中，亦會介紹到專屬於特定package的調整hyperparameters參數，用來改善模型成效以及模型訓練與調整的效率。</p>
<h3>使用R實作Gradient Boosting Machines</h3>
<p>有很多執行GBMs和GBM變種的套件。而本學習筆記會cover到的幾個最受歡迎的套件，包括：</p>
<ul>
<li>gbm: 最原始的執行GBMs的套件。</li>
<li>xgboost: 一個更快速且有效的gradient boosting架構（後端為c++）。</li>
<li>h2o: 強大的java-based的介面，提供平行分散演算法和高銷率的生產。</li>
</ul>
<h3>gbm</h3>
<p>gbm套件是R裡面最原始執行GBM的套件。是來自於Freund &amp; Schapire’s <a href="http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf" target="_blank" rel="noopener noreferrer">AdaBoost algorithm</a>的Friedman’s <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank" rel="noopener noreferrer">gradient boosting machine</a>延伸。由Mark Landry撰寫的GBM套件簡報可參考<a href="https://www.slideshare.net/mark_landry/gbm-package-in-r" target="_blank" rel="noopener noreferrer">此連結</a>。</p>
<p>gbm套件幾個功能與特色包括：</p>
<ul>
<li>Stochastic GBM(隨機 GBM)</li>
<li>可支援到1024個factor levels</li>
<li>支援「分類」和「回歸」樹</li>
<li>包括許多loss functions</li>
<li>提供Out-of-bag 估計法來尋找最適的迭代次數</li>
<li>容易overfitting &#8211; 因為套件中沒有自動使用提早煞車功能來偵測overfitting</li>
<li>如果內部使用cross-validation，這可以被平行分散到所有機器核心</li>
<li>目前gbm套件正在進行重新建構與重寫(並已持續一段時間)。</li>
</ul>
<h4>基本的gbm實作</h4>
<p>gbm套件中有兩個主要的訓練用函數：gbm::gbm跟gbm::fit。</p>
<ul>
<li>gbm::gbm &#8211; 使用「formula介面」來設定模型。</li>
<li>gbm::fit &#8211; 使用「x &amp; y矩陣」來設定模型。</li>
</ul>
<p><span style="color: #9f6ad4;">當變數量很大的時候，使用「x &amp; y 矩陣」會比「formula」介面來的更有效率。</span></p>
<p>gbm()函數預設的幾個參數值如下：</p>
<ul>
<li>learning rate(shrinkage):0.1。學習步伐，通常越小的學習步伐會需要越多模型數(n.tree)來找到最小的MSE。而預設n.tree為100，是相當足夠的。</li>
<li>number of trees(n.tree): 100。總迭代次數(新增模型數)。</li>
<li>depth of tree(interaction.depth): 1。最淺的樹（最弱的模型）。</li>
<li>bag.fraction: 0.5。訓練資料集有多少比例會被抽樣做為下一個樹模型的基礎。用來替模型注入隨機性。</li>
<li>train.fraction: 1。模型首次使用訓練資料的比例，剩餘的觀測資料則最為OOB sample用來估計loss function用。</li>
<li>cv.folds: 0。如果使用&gt;1的cross validation folds，除了會回傳該參數組合下的模型配適結果，也會估計cv.error。</li>
<li>verbose: 預設為FALSE。決定是否要印出程序和成效指標。</li>
<li>n.cores: 使用的CPU核心數。由於在使用cross validation的時候，loop會將不同CV folds分配到不同核心。沒特別設定的話會使用偵測機器核心數函數來處理parallele::detectCores。</li>
</ul>
<p>以下我們來建立一個學習步伐為0.001且模型數量為10000的GBMs。並使用5 folds的交叉驗證計算cross-validated error。</p><pre class="crayon-plain-tag"># 使抽樣結果可以重複
set.seed(123)

# train GBM model
system.time(
  gbm.fit &lt;- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 10000, # 總迭代次數
  interaction.depth = 1, # 弱模型的切割數
  shrinkage = 0.001, # 學習步伐
  cv.folds = 5, # cross validation folds
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  23.348   0.332  80.086</pre><p>GBMs模型約花80秒(約1分多鐘)。</p>
<p>將模型結果印出。結果包括文字資訊以及每一次迭代次數所對應的loss function(squared error loss)變化。</p><pre class="crayon-plain-tag">print(gbm.fit)</pre><p></p><pre class="crayon-plain-tag">## gbm(formula = Sale_Price ~ ., distribution = &quot;gaussian&quot;, data = ames_train, 
##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001, 
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 10000 iterations were performed.
## The best cross-validation iteration was 10000.
## There were 80 predictors of which 45 had non-zero influence.</pre><p>模型結果資訊是由list所儲存。可以使用索引的方式取出。</p>
<p>比如說我們來看最小的CV RMSE值。</p><pre class="crayon-plain-tag">sqrt(min(gbm.fit$cv.error))</pre><p></p><pre class="crayon-plain-tag">## [1] 29133.33</pre><p>表示平均來說模型估計值離真實Sale_Price差了約30K。</p>
<p>我們也可以透過以下方式將GBMs找尋最佳迭代數的過程繪出：(其中黑線的為訓練誤差(train.error)，綠線為cv.error, 若method使用“test&#8221;，則會有紅線表示valid.error)</p><pre class="crayon-plain-tag">gbm.perf(object = gbm.fit, plot.it = TRUE,method = "cv")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-6-1-1.png" alt="gradient boosting machines, GBM" /></p><pre class="crayon-plain-tag">## [1] 10000</pre><p>可以發現以此小學習步伐(0.001)，會需要很多模型來接近最小的loss function(使cv.error最小化)，最佳迭代數為10000。</p>
<h4>Tuning</h4>
<p>「手動tuning」</p>
<p>假設我們將學習步伐加大為0.1，迭代模型數降低為5000，且模型複雜度增加到3 splits。</p><pre class="crayon-plain-tag"># for reproducibility
set.seed(123)

# train GBM model
system.time(
gbm.fit2 &lt;- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 5000,
  interaction.depth = 3,
  shrinkage = 0.1,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  ) 
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  35.555   1.314 151.695</pre><p>調大步伐後(0.1)，花的時間變多為151秒(2.5分鐘)，GBMs模型最小cv.error變得更低(23K)。<br />
(v.s.小步伐(0.001)的cv.error: 29K)</p><pre class="crayon-plain-tag">sqrt(min(gbm.fit2$cv.error))</pre><p></p><pre class="crayon-plain-tag">## [1] 23112.1</pre><p>將模型結果印出。最佳迭代數(所需模型數)為1260。</p><pre class="crayon-plain-tag">gbm.perf(object = gbm.fit2, method = "cv")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-9-1-1.png" alt="gradient boosting machines, GBM" /></p><pre class="crayon-plain-tag">## [1] 1260</pre><p>「grid search自動化tuning」</p>
<p>因為手動調整參數是沒效率的，我們來建立hyperparameters grid並自動套用grid search。</p><pre class="crayon-plain-tag"># create hyperparameter grid
hyper_grid &lt;- expand.grid(
  shrinkage = c(.01, .1, .3), # 學習步伐
  interaction.depth = c(1, 3, 5), # 模型切割數
  n.minobsinnode = c(5, 10, 15), # 節點最小觀測值個數
  bag.fraction = c(.65, .8, 1), # 使用隨機梯度下降(&lt;1)
  optimal_trees = 0,               # 儲存最適模型樹的欄位
  min_RMSE = 0                     # 儲存最小均方差的欄位
)

# total number of combinations
nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 81</pre><p>我們一一測試以上81種超參數排列組合的效果，並指定使用5000個樹模型。<br />
另外，為了降低執行的時間，有別於使用cross validation，我們改使用75%的訓練資料，使用剩下的25%的資料來進行OOB評估效果。需要特別注意的是，當使用train.fraction參數時，模型會直接使用前XX %的資料來使用，因此需要確保說資料是隨機排列的。</p>
<p>先將資料進行隨機排列處理。</p><pre class="crayon-plain-tag"># randomize data
random_index &lt;- sample(1:nrow(ames_train), nrow(ames_train))
random_ames_train &lt;- ames_train[random_index, ]</pre><p>開始執行grid search</p><pre class="crayon-plain-tag"># grid search 
for(i in 1:nrow(hyper_grid)) {

  # reproducibility
  set.seed(123)

  # train model
  gbm.tune &lt;- gbm(
    formula = Sale_Price ~ .,
    distribution = "gaussian",
    data = random_ames_train,
    n.trees = 5000, # 使用5000個樹模型
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75, # 使用75%的訓練資料，並用剩餘資料做OOB成效評估/驗證
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )

  # 將每個GBM模型最小的模型代號和對應的驗證均方誤差(valid RMSE)回傳到
  hyper_grid$optimal_trees[i] &lt;- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] &lt;- sqrt(min(gbm.tune$valid.error))
}</pre><p>將每種參數組合的結果，依照RMSE由小到大排列，並取出排名前10的模型，查看參數組合細節。</p><pre class="crayon-plain-tag">hyper_grid %&gt;% 
  dplyr::arrange(min_RMSE) %&gt;%
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees
## 1       0.01                 5              5         0.65          3867
## 2       0.01                 5              5         0.80          4209
## 3       0.01                 5              5         1.00          4281
## 4       0.10                 3             10         0.80           489
## 5       0.01                 3              5         0.80          4777
## 6       0.01                 3             10         0.80          4919
## 7       0.01                 3              5         0.65          4997
## 8       0.01                 5             10         0.80          4123
## 9       0.01                 5             10         0.65          4850
## 10      0.01                 3             10         1.00          4794
##    min_RMSE
## 1  16647.87
## 2  16960.78
## 3  17084.29
## 4  17093.77
## 5  17121.26
## 6  17139.59
## 7  17139.88
## 8  17162.60
## 9  17247.72
## 10 17353.36</pre><p>我們可以看到以下幾點：</p>
<ol>
<li>最佳模型的最小RMSE(17K)，較先前的RMSE(23K)降低了有6K左右。</li>
<li>前十的模型的學習步伐都小於0.3，表示較小的學習步伐在尋找最小誤差的模型效果是不錯的。</li>
<li>前十的模型都選用&gt;1切割數的樹模型，。</li>
<li>十個模型有8個模型都使用bag.fraction &lt; 1的隨機梯度下降(即使用&lt;100%的訓練資料集進行每個模型的訓練)，這將有助於避免overfitting以及陷在loss function gradient的局部最小最大值。</li>
<li>前十的模型中，也沒有使用採用節點觀測數大於等於15者。因為較小觀測數的節點較能捕捉到更多特徵。</li>
<li>部分參數組合所使用的最佳迭代數（總樹模型數）都很接近5000個。下次執行grid search時或許可以考慮增大樹模型數。</li>
</ol>
<p>根據以上測試，我們已更接近最適的參數組合區間，我們此時在此聚焦範圍內再一次執行81種超參數組合的最佳模型搜尋。</p><pre class="crayon-plain-tag"># 根據上一部的結果，調整參數區間與數值
hyper_grid_2 &lt;- expand.grid(
  shrinkage = c(.01, .05, .1), # 聚焦更小的學習步伐
  interaction.depth = c(3, 5, 7), #聚焦&gt;1的切割數
  n.minobsinnode = c(5, 7, 10), # 聚焦更小的節點觀測值數量
  bag.fraction = c(.65, .8, 1), # 不變
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid_2)</pre><p></p><pre class="crayon-plain-tag">## [1] 81</pre><p>我們再一次的用for loop迴圈執行以上81種超參數組合的模型，找出每一次最適的模型與對應的最小誤差。</p><pre class="crayon-plain-tag"># grid search 
for(i in 1:nrow(hyper_grid_2)) {

  # reproducibility
  set.seed(123)

  # train model
  gbm.tune &lt;- gbm(
    formula = Sale_Price ~ .,
    distribution = "gaussian",
    data = random_ames_train,
    n.trees = 6000,
    interaction.depth = hyper_grid_2$interaction.depth[i],
    shrinkage = hyper_grid_2$shrinkage[i],
    n.minobsinnode = hyper_grid_2$n.minobsinnode[i],
    bag.fraction = hyper_grid_2$bag.fraction[i],
    train.fraction = .75, # 使用剩餘的25%資料估計OOB誤差
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )

  # add min training error and trees to grid
  hyper_grid_2$optimal_trees[i] &lt;- which.min(gbm.tune$valid.error)
  hyper_grid_2$min_RMSE[i] &lt;- sqrt(min(gbm.tune$valid.error))
}</pre><p>檢視結果</p><pre class="crayon-plain-tag">hyper_grid_2 %&gt;% 
  dplyr::arrange(min_RMSE) %&gt;%
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    shrinkage interaction.depth n.minobsinnode bag.fraction
## 1       0.01                 5              5         0.65
## 2       0.01                 7              5         0.65
## 3       0.05                 5              5         0.65
## 4       0.01                 7              5         0.80
## 5       0.01                 7              7         0.65
## 6       0.05                 5             10         0.80
## 7       0.01                 5              5         0.80
## 8       0.01                 3              7         0.80
## 9       0.01                 5              7         0.65
## 10      0.01                 5              7         0.80
##    optimal_trees min_RMSE
## 1           3867 16647.87
## 2           2905 16782.26
## 3            640 16783.13
## 4           4193 16833.74
## 5           3275 16906.53
## 6            958 16933.12
## 7           4209 16960.78
## 8           5813 16998.79
## 9           4798 17003.94
## 10          4753 17004.20</pre><p>本次結果與上一次結果十分類似。最佳模型和上一次選出的最佳模型是一樣的(相同的參數排列組合)，RMSE約是17K。</p>
<p>一旦找到最佳模型，我們則可使用該參數組合來train一個模型。也因為最佳模型約收斂在僅1634個樹模型，我們可以 訓練一個由1634個樹模型所組成的cross validation模型(使用CV來提供更穩健的誤差估計值)。</p><pre class="crayon-plain-tag"># for reproducibility
set.seed(123)

system.time(
# train GBM model
gbm.fit.final &lt;- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 1634,
  interaction.depth = 5,
  shrinkage = 0.05,
  n.minobsinnode = 7,
  bag.fraction = .8, 
  train.fraction = 1, # 如果使用&lt;1(xx%)的訓練比例，就會用剩餘的(1-XX%)資料估計OOB誤差
  cv.folds = 4, # 有別於使用OOB估計誤差，我們估計更穩健的CV誤差
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  18.394   0.617  49.558</pre><p>最佳模型的cv誤差如下(22K)。</p><pre class="crayon-plain-tag">sqrt(min(gbm.fit.final$cv.error)) # 必須是模型參數cv.folds &gt; 1 才會回傳cv.error</pre><p></p><pre class="crayon-plain-tag">## [1] 22165.11</pre><p></p>
<h4>視覺化</h4>
<h5>variable importance</h5>
<p>執行完最後的最佳模型後(gbm.fit.final)，我們會想要看對目標變數sale price來說最有影響力的解釋變數有哪些，以捕捉模型的「可解釋性」。我們可以使用summary()函數來回傳gmb模型中最具影響力的解釋變數清單(data frame &amp; plot)。並可以使用gbm模型summary函式中的cBars參數來指定要顯示的解釋變數清單數(根據影響力排名)。預設使用相對影響力來計算變數重要性。以下說明計算變數重要性的兩個方法：</p>
<ol>
<li>method = relative.influence: 每棵樹在進行節點分割時，gbm會計算每個變數作為切點，切割後對模型誤差所帶來的改善(回歸模型的話就是MSE)。gbm於是會平均每個變數在不同樹模型的誤差改善值，當作變數影響力。具有越高平均誤差降低值得變數即被視作最具影響力的變數。</li>
<li>method = permutation.test.gbm: 模型會隨機置換（一次一個）不同預測變數，來計算個別變數的對預測性能的改善（使用所有training data），並平均每個變數在不同樹模型對正確率造成的改變量。具有越高正確率改變量的預測變數越具重要性。</li>
</ol>
<p></p><pre class="crayon-plain-tag">par(mar = c(5, 8, 1, 1))
# S3 method for class 'gbm'
summary(
  gbm.fit.final, # gbm object
  cBars = 10, # the number of bars to draw. length(object$var.names)
  plotit = TRUE, # an indicator as to whether the plot is generated.defult TRUE.
  method = relative.influence, # The function used to compute the relative influence. 亦可使用permutation.test.gbm
  las = 2
  )</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-19-1-1.png" alt="gradient boosting machines, GBM" /></p><pre class="crayon-plain-tag">##                                   var      rel.inf
## Overall_Qual             Overall_Qual 4.199240e+01
## Gr_Liv_Area               Gr_Liv_Area 1.343609e+01
## Neighborhood             Neighborhood 1.111899e+01
## Garage_Cars               Garage_Cars 5.374303e+00
## Total_Bsmt_SF           Total_Bsmt_SF 5.031406e+00
## First_Flr_SF             First_Flr_SF 3.641743e+00
## Bsmt_Qual                   Bsmt_Qual 2.365967e+00
## Exter_Qual                 Exter_Qual 1.559647e+00
## Kitchen_Qual             Kitchen_Qual 1.210399e+00
## Year_Remod_Add         Year_Remod_Add 9.800489e-01
## MS_SubClass               MS_SubClass 9.168515e-01
## Fireplace_Qu             Fireplace_Qu 7.900484e-01
## Mas_Vnr_Area             Mas_Vnr_Area 7.480704e-01
## Lot_Area                     Lot_Area 7.181146e-01
## Bsmt_Unf_SF               Bsmt_Unf_SF 7.036017e-01
## Second_Flr_SF           Second_Flr_SF 6.564021e-01
## Garage_Area               Garage_Area 5.812072e-01
## Screen_Porch             Screen_Porch 5.697864e-01
## Bsmt_Exposure           Bsmt_Exposure 5.447030e-01
## Overall_Cond             Overall_Cond 5.149690e-01
## BsmtFin_Type_1         BsmtFin_Type_1 5.136834e-01
## Full_Bath                   Full_Bath 4.517156e-01
## Lot_Frontage             Lot_Frontage 4.308841e-01
## Latitude                     Latitude 4.021943e-01
## Sale_Condition         Sale_Condition 3.736402e-01
## Garage_Type               Garage_Type 3.400862e-01
## Bsmt_Full_Bath         Bsmt_Full_Bath 2.830678e-01
## Garage_Finish           Garage_Finish 2.794122e-01
## Open_Porch_SF           Open_Porch_SF 2.731140e-01
## Exterior_1st             Exterior_1st 2.449858e-01
## Fireplaces                 Fireplaces 2.446628e-01
## Sale_Type                   Sale_Type 2.083020e-01
## Year_Built                 Year_Built 2.060348e-01
## Central_Air               Central_Air 1.895762e-01
## Exterior_2nd             Exterior_2nd 1.791393e-01
## TotRms_AbvGrd           TotRms_AbvGrd 1.624508e-01
## Wood_Deck_SF             Wood_Deck_SF 1.619419e-01
## Mo_Sold                       Mo_Sold 1.556240e-01
## Condition_1               Condition_1 1.555862e-01
## Garage_Cond               Garage_Cond 1.433484e-01
## Functional                 Functional 1.195756e-01
## Land_Contour             Land_Contour 1.176260e-01
## Longitude                   Longitude 1.099157e-01
## Heating_QC                 Heating_QC 7.996284e-02
## Year_Sold                   Year_Sold 7.732664e-02
## Roof_Matl                   Roof_Matl 6.581906e-02
## House_Style               House_Style 5.653741e-02
## Mas_Vnr_Type             Mas_Vnr_Type 4.764729e-02
## Bedroom_AbvGr           Bedroom_AbvGr 4.542366e-02
## Lot_Config                 Lot_Config 3.928406e-02
## Paved_Drive               Paved_Drive 3.815510e-02
## BsmtFin_Type_2         BsmtFin_Type_2 3.573880e-02
## Roof_Style                 Roof_Style 3.492513e-02
## Bsmt_Cond                   Bsmt_Cond 3.320548e-02
## Land_Slope                 Land_Slope 3.191664e-02
## Exter_Cond                 Exter_Cond 3.036215e-02
## MS_Zoning                   MS_Zoning 3.016913e-02
## Enclosed_Porch         Enclosed_Porch 2.361207e-02
## Alley                           Alley 1.860542e-02
## Condition_2               Condition_2 1.829088e-02
## Half_Bath                   Half_Bath 1.622231e-02
## Three_season_porch Three_season_porch 1.091252e-02
## Foundation                 Foundation 1.039571e-02
## Low_Qual_Fin_SF       Low_Qual_Fin_SF 8.975948e-03
## BsmtFin_SF_2             BsmtFin_SF_2 8.747153e-03
## Fence                           Fence 7.868905e-03
## Lot_Shape                   Lot_Shape 5.645516e-03
## Electrical                 Electrical 4.850525e-03
## Heating                       Heating 4.405392e-03
## Garage_Qual               Garage_Qual 3.845332e-03
## BsmtFin_SF_1             BsmtFin_SF_1 2.902816e-03
## Misc_Val                     Misc_Val 2.321802e-03
## Bsmt_Half_Bath         Bsmt_Half_Bath 2.263392e-03
## Street                         Street 1.157860e-03
## Bldg_Type                   Bldg_Type 4.970819e-04
## Misc_Feature             Misc_Feature 4.236315e-04
## Pool_QC                       Pool_QC 1.575743e-04
## Pool_Area                   Pool_Area 9.720817e-05
## Utilities                   Utilities 0.000000e+00
## Kitchen_AbvGr           Kitchen_AbvGr 0.000000e+00</pre><p>另外一個方式，就是使用vip套件(variable importance plot)的vip函式，會回傳ggplot形式的重要變數圖表。 為兩個解釋集成樹的兩大重要解釋指標)，是許多機器學習模型常用的變數重要性繪圖框架。</p><pre class="crayon-plain-tag"># install.packages('vip')
vip::vip(gbm.fit.final)</pre><p></p>
<h5><img loading="lazy" class="alignnone size-full wp-image-2879" src="/wp-content/uploads/2019/04/Rplot01.png" alt="gradient boosting machines, GBM" width="631" height="441" srcset="/wp-content/uploads/2019/04/Rplot01.png 631w, /wp-content/uploads/2019/04/Rplot01-300x210.png 300w, /wp-content/uploads/2019/04/Rplot01-230x161.png 230w, /wp-content/uploads/2019/04/Rplot01-350x245.png 350w, /wp-content/uploads/2019/04/Rplot01-480x335.png 480w" sizes="(max-width: 631px) 100vw, 631px" /></h5>
<h5>Partial dependence plots</h5>
<p>一旦識別出最重要的幾個變數後，下一步就是去了解當解釋變數變動時，目標變數是如何變動的(即marginal effects，邊際效果，每變動一單位解釋變數時，對目標變數的影響)。我們可以使用partial dependence plots(PDPs)和individual conditional expectation(ICE)曲線。</p>
<ul>
<li>PDPs: 繪製特定變數邊際變動造成的平均目標預測值的變動。比如說，下圖繪製了預測變數Gr_Liv_Area邊際變動(控制其他變數不變的情況下)對平均目標預測銷售金額(avg. sale price_的影響。下圖PDPs描述隨著房屋基底的面積邊際增加，平均銷售價格增加的變化。</li>
</ul>
<p></p><pre class="crayon-plain-tag">gbm.fit.final %&gt;%
  partial(object = .,# A fitted model object of appropriate class (e.g., "gbm", "lm", "randomForest", "train", etc.).
          pred.var = "Gr_Liv_Area", 
          n.trees = gbm.fit.final$n.trees, # 如果是gbm的話，需指定模型所使用樹個數
          grid.resolution = 100) %&gt;%
  # The autplot function can be used to produce graphics based on ggplot2
  autoplot(rug = TRUE, train = ames_train) + # plotPartial()不支援gbm
  scale_y_continuous(labels = scales::dollar) # 因為是使用ggplot基礎繪圖，故可以使用ggplot相關圖層來調整</pre><p><img loading="lazy" class="alignnone size-full wp-image-2880" src="/wp-content/uploads/2019/04/Rplot02.png" alt="gradient boosting machines, GBM" width="606" height="439" srcset="/wp-content/uploads/2019/04/Rplot02.png 606w, /wp-content/uploads/2019/04/Rplot02-300x217.png 300w, /wp-content/uploads/2019/04/Rplot02-230x167.png 230w, /wp-content/uploads/2019/04/Rplot02-350x254.png 350w, /wp-content/uploads/2019/04/Rplot02-480x348.png 480w" sizes="(max-width: 606px) 100vw, 606px" /></p>
<p>拆解步驟1: 先檢視沒有繪圖(plot = FALSE)的所回傳的data.frame。在一些例子中，使用partial根據object來擷取training data是困難的，此時便會出現錯誤訊息要求使用者透過train參數提供訓練資料集。但絕大部分的時候，partial會預設擷取當下環境下訓練object所使用的訓練資料集，所以很重要的事，在執行partial之前不行改變到訓練資料集的變數內容，而此問題可透過明確指定train參數所對應的訓練資料集而解決。</p><pre class="crayon-plain-tag">partialDf &lt;-partial(
        object = gbm.fit.final,pred.var = "Gr_Liv_Area", 
        n.trees = gbm.fit.final$n.trees, 
        grid.resolution = 100,
        train = ames_train)
partialDf</pre><p></p><pre class="crayon-plain-tag">##     Gr_Liv_Area     yhat
## 1           334 156956.9
## 2           387 156956.9
## 3           441 156956.9
## 4           494 156956.9
## 5           548 156956.9
## 6           602 156956.9
## 7           655 156956.9
## 8           709 156660.6
## 9           762 156755.5
## 10          816 156871.1
## 11          870 156871.1
## 12          923 158251.7
## 13          977 161056.8
## 14         1031 163112.1
## 15         1084 163138.2
## 16         1138 163214.8
## 17         1191 166534.4
## 18         1245 166987.2
## 19         1299 169077.6
## 20         1352 170758.6
## 21         1406 172776.0
## 22         1459 174681.8
## 23         1513 181046.8
## 24         1567 182290.3
## 25         1620 185044.8
## 26         1674 185035.8
## 27         1728 186329.2
## 28         1781 189560.9
## 29         1835 192374.4
## 30         1888 193609.3
## 31         1942 194775.0
## 32         1996 200759.0
## 33         2049 202826.8
## 34         2103 206586.7
## 35         2156 207502.2
## 36         2210 207502.2
## 37         2264 209486.0
## 38         2317 212637.8
## 39         2371 214445.4
## 40         2425 214863.0
## 41         2478 215367.1
## 42         2532 215404.3
## 43         2585 215404.3
## 44         2639 215571.6
## 45         2693 226433.3
## 46         2746 225632.2
## 47         2800 225849.6
## 48         2853 223628.9
## 49         2907 219924.6
## 50         2961 218770.4
## 51         3014 218770.4
## 52         3068 218770.4
## 53         3122 227327.1
## 54         3175 229537.3
## 55         3229 235917.9
## 56         3282 241038.0
## 57         3336 240301.2
## 58         3390 243415.0
## 59         3443 243127.7
## 60         3497 246930.9
## 61         3550 246930.9
## 62         3604 246930.9
## 63         3658 246930.9
## 64         3711 246930.9
## 65         3765 246930.9
## 66         3819 246930.9
## 67         3872 246930.9
## 68         3926 246930.9
## 69         3979 246930.9
## 70         4033 246930.9
## 71         4087 246930.9
## 72         4140 246930.9
## 73         4194 246930.9
## 74         4247 246930.9
## 75         4301 246930.9
## 76         4355 246930.9
## 77         4408 246930.9
## 78         4462 246930.9
## 79         4516 246930.9
## 80         4569 246930.9
## 81         4623 246930.9
## 82         4676 246930.9
## 83         4730 246930.9
## 84         4784 246930.9
## 85         4837 246930.9
## 86         4891 246930.9
## 87         4944 246930.9
## 88         4998 246930.9
## 89         5052 246930.9
## 90         5105 246930.9
## 91         5159 246930.9
## 92         5213 246930.9
## 93         5266 246930.9
## 94         5320 246930.9
## 95         5373 246930.9
## 96         5427 246930.9
## 97         5481 246930.9
## 98         5534 246930.9
## 99         5588 246930.9
## 100        5642 246930.9</pre><p>亦可使用plot = TRUE將以上結果繪出。(不是ggplot2)<br />
但其實比較推薦保留plot = FALSE，將partial回傳結果先儲存。這樣的好處在於繪圖上會更有彈性，當預設繪圖結果不足夠時，不需要重新執行partial()。</p><pre class="crayon-plain-tag">partial(gbm.fit.final, train = ames_train, pred.var = 'Gr_Liv_Area',n.trees = gbm.fit.final$n.trees, plot = TRUE)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-23-1-1.png" alt="gradient boosting machines, GBM" /></p>
<ul>
<li>ICE curves: 是PDPs圖的延伸。與PDPs圖不同的地方在於，PDPs是繪製每個解釋變數邊際變動所造成的「平均」目標數值的變化(平均所有觀測值)，而ICE curves則是繪製「每個」解釋變數邊際變動對所有觀測值的目標數值的變動。下面分別呈現了regular ICE曲線圖(左)和centered ICE曲線圖(右)。當曲線有很寬廣的截距且彼此疊在一起，很難辨別感興趣的預測變數邊際變動所造成的回應變數變動量的異質性。而centered ICE曲線，則能強調結果中的異質性。以下regular ICE圖顯示，當Gr_Liv_Area增加時，大部分的觀測值的目標變數變化具有共通趨勢，而centered ICE圖則凸顯部分與共通趨勢不一致的觀察值變化圖。</li>
</ul>
<p></p><pre class="crayon-plain-tag">ice1 &lt;- gbm.fit.final %&gt;%
  partial(
    pred.var = "Gr_Liv_Area", 
    n.trees = gbm.fit.final$n.trees, 
    grid.resolution = 100,
    ice = TRUE # 當ice = TRUE或給定pred.fun參數，會回傳使用newdata替每個觀測值預測的結果
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) + # alpha參數只在ICE curves繪製上有效
  ggtitle("Non-centered") +
  scale_y_continuous(labels = scales::dollar)</pre><p></p><pre class="crayon-plain-tag">ice2 &lt;- gbm.fit.final %&gt;%
  partial(
    pred.var = "Gr_Liv_Area", 
    n.trees = gbm.fit.final$n.trees, 
    grid.resolution = 100,
    ice = TRUE
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  ggtitle("Centered") +
  scale_y_continuous(labels = scales::dollar)</pre><p></p><pre class="crayon-plain-tag">gridExtra::grid.arrange(ice1, ice2, nrow = 1)</pre><p><img loading="lazy" class="alignnone size-full wp-image-2881" src="/wp-content/uploads/2019/04/Rplot03.png" alt="gradient boosting machines, GBM" width="1042" height="530" srcset="/wp-content/uploads/2019/04/Rplot03.png 1042w, /wp-content/uploads/2019/04/Rplot03-300x153.png 300w, /wp-content/uploads/2019/04/Rplot03-768x391.png 768w, /wp-content/uploads/2019/04/Rplot03-1024x521.png 1024w, /wp-content/uploads/2019/04/Rplot03-830x422.png 830w, /wp-content/uploads/2019/04/Rplot03-230x117.png 230w, /wp-content/uploads/2019/04/Rplot03-350x178.png 350w, /wp-content/uploads/2019/04/Rplot03-480x244.png 480w" sizes="(max-width: 1042px) 100vw, 1042px" /></p>
<h5>LIME</h5>
<p>LIME是一種新程序幫助我們了解，單一觀察值的預測目標值是如何產生的。對gbm物件使用lime套件，我們需要定義模型類型model type和預測方法prediction methods。</p><pre class="crayon-plain-tag">model_type.gbm &lt;- function(x, ...) {
  return("regression")
}

predict_model.gbm &lt;- function(x, newdata, ...) {
  pred &lt;- predict(x, newdata, n.trees = x$n.trees)
  return(as.data.frame(pred))
}</pre><p>以下我們便挑選兩個觀測值來檢視其預測目標值是如何產生的。<br />
結果包括預測目標值（分別為case1: 127K case2: 159K）、局部模型配適（兩者的局部配飾都太好）、以及對不同觀測值來說，對目標變數最具影響力的特徵變數。</p><pre class="crayon-plain-tag"># get a few observations to perform local interpretation on
local_obs &lt;- ames_test[1:2, ]

# apply LIME
explainer &lt;- lime(
  x=ames_train, # The training data used for training the model that should be explained.
  model = gbm.fit.final # The model whose output should be explained
  )
# 一旦使用lime()創建好了explainer，則可將explainer用作解釋模型作用在新觀察值的結果
explanation &lt;- explain(x = local_obs, # New observations to explain
                       explainer = explainer,
                       n_features = 5 # The number of features to use for each explanation.
                       )</pre><p></p><pre class="crayon-plain-tag">plot_features(explanation = explanation)</pre><p><img loading="lazy" class="alignnone size-full wp-image-2882" src="/wp-content/uploads/2019/04/Rplot04.png" alt="gradient boosting machines, GBM" width="1033" height="525" srcset="/wp-content/uploads/2019/04/Rplot04.png 1033w, /wp-content/uploads/2019/04/Rplot04-300x152.png 300w, /wp-content/uploads/2019/04/Rplot04-768x390.png 768w, /wp-content/uploads/2019/04/Rplot04-1024x520.png 1024w, /wp-content/uploads/2019/04/Rplot04-830x422.png 830w, /wp-content/uploads/2019/04/Rplot04-230x117.png 230w, /wp-content/uploads/2019/04/Rplot04-350x178.png 350w, /wp-content/uploads/2019/04/Rplot04-480x244.png 480w" sizes="(max-width: 1033px) 100vw, 1033px" /></p>
<h4>Predicting</h4>
<p>一旦決定好最佳的模型後，便使用模型來預測新的資料集(ames_test)。跟大部分模型一樣，我們可以使用predict()函數，只不過我們需要指定所需要的樹模型個數(可參考?predict.gbm之說明)。我們可以觀察到，測試資料集所得到的RMSE跟我們得到的最佳gbm模型的RMSE(22K)是差不多的。</p><pre class="crayon-plain-tag"># predict values for test data
pred &lt;- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, ames_test)

# results
caret::RMSE(pred, ames_test$Sale_Price)</pre><p></p><pre class="crayon-plain-tag">## [1] 19686.42</pre><p></p>
<h3>xgboost</h3>
<p>xgboost套件提供一個Extreme Gradient Boosting的R API，可以具效率地去執行gradient boosting framework(約比gbm套件快上10倍)。<a href="https://github.com/dmlc/xgboost/tree/master/demo" target="_blank" rel="noopener noreferrer">xgboost</a>文件的知識庫有豐富的資訊。亦可參考非常完整的參數tuning<a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="noopener noreferrer">教學文章</a>。xgboost套件一直以來在kaggle data mining競賽上是滿受歡迎且成功的演算法套件。</p>
<p>xgboost幾個特色包括：</p>
<ul>
<li>提供內建的k-fold cross validation</li>
<li>隨機GBM，兼具column和row的抽樣(per split and per tree)，以達到更好的一般性(避免過度擬合)。</li>
<li>包括高效的線性模型求解器和樹學習演算法。</li>
<li>單一機器上的平行運算。</li>
<li>支援多個目標函數，包括回歸regression，分類classification，和排序ranking。</li>
<li>該套件被設計為有延展性的(extensible)，因此使用者可以自己定義自己的目標函式(objectives)。</li>
<li>Apache 2.0 License</li>
</ul>
<h4>基本的xgboost實作</h4>
<p>xgboost 只能在都是「數值變數的矩陣」下運作(由於在空間中表示點的位置，所有特徵值須為數值)。因此，我們必須先將數據進行編碼轉換。</p>
<p>一般來說編碼類別變數有兩種方式，分別為Label Encoding和one-hot encoding，通常前者會衍伸出編碼後的數值變成有順序意義在的問題，以及在空間維度中代表不同距離的意義之問題，故通常最常使用one-hot encoding法是最適合的，來使類別變數中三個屬性在空間中距離原點的距離是相同的(*但必須注意的是，one-hot encoding只適用在類別種類少的情況，如果種類過多，過多展開的維度會衍伸出其他問題)。</p>
<p>在R中有幾個執行one-hot encoding的方式，包括Matrix::sparse.model.matrix， caret::dummyVars，但我們這邊會使用vtreat套件。vtreat是一個強大的資料前處理套件，且有助於處理因遺失值、或新資料中才新冒出的類別資料級別（原本不為訓練資料類別變數中的選項）等因素而造成的問題。然而vtreat在使用上不是很直覺。本篇學習筆記在此不會說明太多vtreat的功能，如需要了解可先參考<a href="https://arxiv.org/abs/1611.09477" target="_blank" rel="noopener noreferrer">連結1</a>，<a href="https://www.r-bloggers.com/a-demonstration-of-vtreat-data-preparation/" target="_blank" rel="noopener noreferrer">連結2</a>和<a href="https://github.com/WinVector/vtreat" target="_blank" rel="noopener noreferrer">連結3</a>。</p>
<p>以下使用vtreat將training &amp; testing資料即重新one-hot encoding編碼。</p><pre class="crayon-plain-tag"># variable names
features &lt;- setdiff(names(ames_train), "Sale_Price")

# Create the treatment plan from the training data
treatplan &lt;- vtreat::designTreatmentsZ(ames_train, features, verbose = FALSE)

# Get the "clean" variable names from the scoreFrame
new_vars &lt;- treatplan %&gt;%
  magrittr::use_series(scoreFrame) %&gt;%        
  dplyr::filter(code %in% c("clean", "lev")) %&gt;% 
  magrittr::use_series(varName)     

# Prepare the training data
features_train &lt;- vtreat::prepare(treatplan, ames_train, varRestriction = new_vars) %&gt;% as.matrix()
response_train &lt;- ames_train$Sale_Price

# Prepare the test data
features_test &lt;- vtreat::prepare(treatplan, ames_test, varRestriction = new_vars) %&gt;% as.matrix()
response_test &lt;- ames_test$Sale_Price</pre><p>編碼後的維度個數</p><pre class="crayon-plain-tag">dim(features_train)</pre><p></p><pre class="crayon-plain-tag">## [1] 2051  348</pre><p></p><pre class="crayon-plain-tag">dim(features_test)</pre><p></p><pre class="crayon-plain-tag">## [1] 879 348</pre><p>xgboost提供不同的訓練函數(像是xgb.train,xgb.cv)。而我們這邊會使用能夠進行cross-validation的xgb.cv。以下我們訓練一個使用5-fold cv且使用1000棵樹的xgb模型。xgb.cv中有許多可調整的參數(基本的參數都跟xgb.train是一樣的)，幾個比較常出現的參數（與其預設值）分別為以下：</p>
<ul>
<li>data: 只允許數值型矩陣，如xgb.DMatrix, matrix, dgCMartirx類型。</li>
<li>nrounds: 模型所使用的樹個數(迭代數)。</li>
<li>nfold: 將投入的data參數值（在此處為訓練資料集），隨機切割(partition)為n等分的子樣本。</li>
<li>params: list()，參數list。常用的參數包括：
<ul>
<li>objective : 目標函數(亦可使用params = list()進行參數指定)。常用的目標函數包括：</li>
<li>reg:linear : 線性迴歸</li>
<li>binary:logistic : 分類用的羅吉斯迴歸</li>
<li>eta: learning rate,學習步伐(default為0.3)</li>
<li>max_depth: tree depth, 樹模型的深度(default為6)</li>
<li>min_child_weight: minimum node size,最小節點個數值(default為1)</li>
<li>subsample: percentage of training data to sample for each tree (就如同gbm套件中的bag.fraction參數),每棵樹模型將抽樣使用多少比例的訓練資料集(default: 100% -&gt; 沒有OOB sample)，用來避免overfitting，亦可加快運算(分析較少的資料)。</li>
</ul>
</li>
</ul>
<p>其中cross-validation會執行nrounds次，每次迭代中，nfold個子樣本都會輪流作為驗證資料集，來驗證nfold-1子集合所訓練出的模型。</p><pre class="crayon-plain-tag"># reproducibility
set.seed(123)

system.time(
  xgb.fit1 &lt;- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0               # silent,不要顯示詳細資訊
)
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
## 234.376   2.540 244.823</pre><p>檢視每次迭代的cross-validation的結果。分別會有訓練資料集的平均RMSE，和測試資料集的平均RMSE，希望兩者越接近越好。</p><pre class="crayon-plain-tag">print(xgb.fit1,verbose = TRUE)</pre><p></p><pre class="crayon-plain-tag">## ##### xgb.cv 5-folds
## call:
##   xgb.cv(data = features_train, nrounds = 1000, nfold = 5, label = response_train, 
##     verbose = 0, objective = &quot;reg:linear&quot;)
## params (as set within xgb.cv):
##   objective = &quot;reg:linear&quot;, silent = &quot;1&quot;
## callbacks:
##   cb.evaluation.log()
## niter: 1000
## evaluation_log:
##     iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std
##        1    1.420236e+05   850.15595494      142612.98      4504.553
##        2    1.023304e+05   645.18845214      104198.60      4628.932
##        3    7.443451e+04   535.78249514       77533.40      4635.993
##        4    5.484559e+04   468.37246662       59185.94      4420.339
##        5    4.118994e+04   386.94999310       47545.03      4452.632
## ---                                                                 
##      996    4.831320e-02     0.01131165       27373.21      3243.812
##      997    4.831320e-02     0.01131165       27373.21      3243.812
##      998    4.831320e-02     0.01131165       27373.21      3243.812
##      999    4.831320e-02     0.01131165       27373.21      3243.811
##     1000    4.831320e-02     0.01131165       27373.21      3243.812</pre><p>檢視回傳xbg.fit1物件的屬性(attributes)。</p><pre class="crayon-plain-tag">attributes(xgb.fit1)</pre><p></p><pre class="crayon-plain-tag">## $names
## [1] &quot;call&quot;           &quot;params&quot;         &quot;callbacks&quot;     
## [4] &quot;evaluation_log&quot; &quot;niter&quot;          &quot;nfeatures&quot;     
## [7] &quot;folds&quot;         
## 
## $class
## [1] &quot;xgb.cv.synchronous&quot;</pre><p>回傳的xgb.fit1物件包含很多重要資訊。特別像是我們可以擷取xgb.fit1$evaluation_log來觀察發生在訓練資料集和測試資料集的最小的RMSE和最適的樹數量(跟print(xgb.fit1)效果一樣)，以及cross-validation error。</p><pre class="crayon-plain-tag">xgb.fit1$evaluation_log</pre><p></p><pre class="crayon-plain-tag">##       iter train_rmse_mean train_rmse_std test_rmse_mean
##    1:    1    1.420236e+05   850.15595494      142612.98
##    2:    2    1.023304e+05   645.18845214      104198.60
##    3:    3    7.443451e+04   535.78249514       77533.40
##    4:    4    5.484559e+04   468.37246662       59185.94
##    5:    5    4.118994e+04   386.94999310       47545.03
##   ---                                                   
##  996:  996    4.831320e-02     0.01131165       27373.21
##  997:  997    4.831320e-02     0.01131165       27373.21
##  998:  998    4.831320e-02     0.01131165       27373.21
##  999:  999    4.831320e-02     0.01131165       27373.21
## 1000: 1000    4.831320e-02     0.01131165       27373.21
##       test_rmse_std
##    1:      4504.553
##    2:      4628.932
##    3:      4635.993
##    4:      4420.339
##    5:      4452.632
##   ---              
##  996:      3243.812
##  997:      3243.812
##  998:      3243.812
##  999:      3243.811
## 1000:      3243.812</pre><p>我們找出使得訓練和測試誤差最小的迭代數(模型所使用的樹個數)，以及所對應的RMSE。由下表所示，訓練誤差持續下降，並約在924棵樹時逼近為零(0.048)。然而，交叉驗證誤差約在60棵樹左右達到最小RMSE(約27K)。</p><pre class="crayon-plain-tag"># get number of trees that minimize error
xgb.fit1$evaluation_log %&gt;%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean)
  )</pre><p></p><pre class="crayon-plain-tag">##   ntrees.train rmse.train ntrees.test rmse.test
## 1          924  0.0483002          60  27337.79</pre><p></p><pre class="crayon-plain-tag">##   ntrees.train rmse.train ntrees.test rmse.test
## 1          965  0.5022836          60  27572.31</pre><p>將詳細xgboost模型迭代資訊繪出如下，紅色線代表訓練誤差，藍色線表示交叉驗證誤差。</p><pre class="crayon-plain-tag"># plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-36-1-1.png" alt="gradient boosting machines, GBM" /></p>
<p>一個xbm.cv滿不錯的功能就是early stopping。該功能讓我們可在cross validation error在連續第n棵樹不再下降的情況下，告訴函式該停止。以上面的例子來說，我們可以設定當cv error在連續10個樹模型(迭代)沒有下降時停止迭代(early_stopping_rounds = 10)。該功能有助於加速下一個tuning校正過程。</p><pre class="crayon-plain-tag"># reproducibility
set.seed(123)

xgb.fit2 &lt;- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)</pre><p>將迭代的過程細節繪出：</p><pre class="crayon-plain-tag"># plot error vs number trees
ggplot(xgb.fit2$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-38-1-1.png" alt="gradient boosting machines, GBM" /></p>
<h4>Tuning</h4>
<p>要tune XGBoost模型，我們會傳入一個parameters的list物件給params參數。幾個最常見的參數如下：</p>
<ul>
<li>eta : 控制學習步伐</li>
<li>max_depth: 樹的深度</li>
<li>min_child_weight: 末梢節點的最小觀測值個數</li>
<li>subsample: 每棵樹模型所抽樣訓練資料集的比例</li>
<li>colsample_bytrees: 每棵樹模型所抽樣的欄位數目</li>
</ul>
<p>舉例來說，如果想要指定特定參數值，我們可以將上面的模型設定重新編輯如下：</p><pre class="crayon-plain-tag"># create parameter list
  params &lt;- list(
    eta = .1,
    max_depth = 5,
    min_child_weight = 2,
    subsample = .8,
    colsample_bytree = .9
  )

# reproducibility
set.seed(123)

# train model
system.time(
xgb.fit3 &lt;- xgb.cv(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  39.185   0.124  39.446</pre><p></p><pre class="crayon-plain-tag"># assess results
xgb.fit3$evaluation_log %&gt;%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean)
  )</pre><p></p><pre class="crayon-plain-tag">##   ntrees.train rmse.train ntrees.test rmse.test
## 1          220   5054.546         210  24159.13</pre><p>想要執行更大型的search grid，我們可以使用和gbm相同的程序。先產生一個超參數hyperparameter search grid和儲存結果的欄位(最適樹模型個數與最小RMSE)。</p>
<p>以下我們創立一個4*4*4*3*3 = 576個參數排列組合的hyper grid。</p><pre class="crayon-plain-tag"># create hyperparameter grid
hyper_grid &lt;- expand.grid(
  eta = c(.01, .05, .1, .3),
  max_depth = c(1, 3, 5, 7),
  min_child_weight = c(1, 3, 5, 7),
  subsample = c(.65, .8, 1), 
  colsample_bytree = c(.8, .9, 1),
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 576</pre><p>接著，我們使用迴圈一一去執行XGBoost模型套用不同參數組合的結果，並將結果指標儲存。（*這段程序耗時，約6小時以上）</p><pre class="crayon-plain-tag"># grid search 
for(i in 1:nrow(hyper_grid)) {

  # create parameter list
  params &lt;- list(
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )

  # reproducibility
  set.seed(123)

  # train model
  xgb.tune &lt;- xgb.cv(
    params = params,
    data = features_train,
    label = response_train,
    nrounds = 5000,
    nfold = 5,
    objective = "reg:linear",  # for regression models
    verbose = 0,               # silent,
    early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
  )

  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] &lt;- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid$min_RMSE[i] &lt;- min(xgb.tune$evaluation_log$test_rmse_mean)
}

hyper_grid %&gt;%
  dplyr::arrange(min_RMSE) %&gt;%
  head(10)</pre><p></p><pre class="crayon-plain-tag">##     eta max_depth min_child_weight subsample colsample_bytree optimal_trees min_RMSE
## 1  0.01         5                3      0.65              0.8          1501 23255.95
## 2  0.05         5                5      0.80              0.9           486 23269.22
## 3  0.01         5                3      0.65              0.9          1325 23373.22
## 4  0.01         5                1      0.65              0.8          1257 23396.13
## 5  0.05         5                7      0.65              1.0           508 23437.00
## 6  0.01         3                3      0.65              1.0          2301 23457.89
## 7  0.01         5                3      0.80              0.8          1488 23499.83
## 8  0.01         7                3      0.65              0.8          1178 23519.53
## 9  0.01         3                5      0.65              0.9          2274 23526.33
## 10 0.05         5                7      0.80              0.8           401 23555.05</pre><p>經過評估後，可能還會繼續測是幾個不同的參數組合，去找到最能影響模型成效的參數。這邊有<a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">一篇很棒的文章</a>在討論tuning XGBoost的策略法。但為了簡短，在此我們即假設上述結果為globally最適的模型，並使用xgb.train()來配飾最終模型。(*xgboost)</p><pre class="crayon-plain-tag"># parameter list
params &lt;- list(
  eta = 0.01,
  max_depth = 5,
  min_child_weight = 5,
  subsample = 0.65,
  colsample_bytree = 1
)

# train final model
xgb.fit.final &lt;- xgboost(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1576,
  objective = "reg:linear",
  verbose = 0
)</pre><p></p>
<h4>Visualization</h4>
<h5>Variable importance</h5>
<p>xgboost提供內建的變數重要性繪圖功能。首先，可以使用xgb.importance()函數建立一個重要矩陣(importance matrix)的data.table，然後再將這個重要矩陣(importance matrix)投入xgb.plot.importance()函數進行繪圖。</p>
<p>在重要性矩陣中，Gain, Cover, Frequency欄位分別代表三種不同的變數重要性衡量的方法（此為tree model的衡量指標，如果是linear mode，衡量指標則會是Weight(模型中的線性係數)和Class）：</p>
<ol>
<li>Gain(貢獻度): 相應特徵對模型的相對貢獻度，計算特徵在模型中每棵樹的貢獻。其與gbm套件中的relative.influence是同意的。</li>
<li>Cover(觀測值個數涵蓋率): 與該特徵相關的相對觀察值個數(%)。比如說，你有100個觀察值，4個特徵變數3棵樹，並假設feature 1是用來區分葉節點並在樹tree1, tree2, tree3有各有10,5,2個觀察值；於是feature 1的cover會被計算為10+5+2=17個觀察值。feature2 ~ feature4亦會計算各自的cover，並以該特徵涵蓋變數個數相對於所有特徵總涵蓋變數個數計算百分比。</li>
<li>Frequency(出現在模型所有樹的相對次數):代表某特徵出現在模型中樹的相對次數百分比(%)。就上面的範例來說，假設feature 1分別在tree1, tree2,tree3的分割樹(splits)分別是2, 1, 3，那麼feature 1的權重將是2+1+3 = 6。feature 1的frequency則是計算該特徵的權重相對於其他features的權重加總。</li>
</ol>
<p></p><pre class="crayon-plain-tag"># create importance matrix
importance_matrix &lt;- xgb.importance(model = xgb.fit.final)

importance_matrix</pre><p></p><pre class="crayon-plain-tag">##                                                          Feature         Gain        Cover    Frequency
##   1:                                                 Garage_Cars 2.508517e-01 1.780750e-02 8.503913e-03
##   2:                                                 Gr_Liv_Area 1.629517e-01 7.008097e-02 6.596177e-02
##   3:                                   Bsmt_Qual_lev_x_Excellent 1.249057e-01 1.408468e-02 7.111680e-03
##   4:                                               Total_Bsmt_SF 6.487028e-02 5.422924e-02 5.535069e-02
##   5:                                                  Year_Built 5.279356e-02 2.547546e-02 2.697923e-02
##  ---                                                                                                   
## 238:                                   Garage_Type_lev_x_Basment 9.872921e-07 1.100922e-04 3.762793e-05
## 239:                                   Lot_Shape_lev_x_Irregular 8.863554e-07 9.469465e-05 3.762793e-05
## 240:                                       Functional_lev_x_Maj2 8.741157e-07 1.330922e-04 3.762793e-05
## 241: MS_SubClass_lev_x_Two_Family_conversion_All_Styles_and_Ages 7.552052e-07 3.820506e-05 3.762793e-05
## 242:    MS_SubClass_lev_x_One_and_Half_Story_Unfinished_All_Ages 1.986944e-07 1.443516e-06 3.762793e-05</pre><p>將剛剛得到的data.table放入xgb.plot.importance()，繪製指定的&#8221;Gain&#8221;變數重要性圖表。</p><pre class="crayon-plain-tag"># variable importance plot
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-45-1-1.png" alt="gradient boosting machines, GBM" /></p>
<p>改使用&#8217;Cover&#8217;當作變數重要衡量法的結果與上面差很多。</p><pre class="crayon-plain-tag"># variable importance plot using 'cover'
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Cover")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-46-1-1.png" alt="gradient boosting machines, GBM" /></p>
<h5>Partial dependence plots</h5>
<p>PDPs和ICE的運作與之前gbm套件是相似的。唯一差別在於你必須在partial()函數中加入訓練資料(train = features_train)(因為在此case中，partial無法自動擷取object所使用的training data)。我們以Garage_Cars為範例。</p><pre class="crayon-plain-tag">pdp &lt;- xgb.fit.final %&gt;%
  partial(pred.var = "Garage_Cars", n.trees = 1576, grid.resolution = 100, train = features_train) %&gt;%
  autoplot(rug = TRUE, train = features_train) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice &lt;- xgb.fit.final %&gt;%
  partial(pred.var = "Garage_Cars", n.trees = 1576, grid.resolution = 100, train = features_train, ice = TRUE) %&gt;%
  autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")</pre><p></p><pre class="crayon-plain-tag">gridExtra::grid.arrange(pdp, ice, nrow = 1)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-47-1-1.png" alt="gradient boosting machines, GBM" /></p>
<h5>LIME</h5>
<p>LIME內建提供給xgboost物件的功能(可以使用?model.type)。然而需要注意的是，要分析的局部觀察值需要採用與train, test相同的編碼處理程序(one-hot encoded)。並且當將資料投入lime::lime函式時，必須將其從matrix轉換成dataframe。</p><pre class="crayon-plain-tag"># one-hot encode the local observations to be assessed.
local_obs_onehot &lt;- vtreat::prepare(treatplan, local_obs, varRestriction = new_vars)

# apply LIME
explainer &lt;- lime(data.frame(features_train), xgb.fit.final)
explanation &lt;- explain(local_obs_onehot, explainer, n_features = 5)
plot_features(explanation)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-48-1-1.png" alt="gradient boosting machines, GBM" /></p>
<h4>Predicting</h4>
<p>最後，我們使用predict()函數來對新資料集進行預測。然而，不像gbm，我們並不需要提供樹模型的個數。<br />
由下的結果可知，我們測試資料集的RMSE與先前gbm模型的RMSE(22K)是較低的(只差了$600左右，差距很小)。</p><pre class="crayon-plain-tag"># predict values for test data
pred &lt;- predict(xgb.fit.final, features_test)

# results
caret::RMSE(pred, response_test)</pre><p></p><pre class="crayon-plain-tag">## [1] 21433.41</pre><p></p>
<h3>h2o</h3>
<p>R的h2O套件是一個強大高效能的java-based介面，允許基於local和cluster-based的佈署。該套件有相當完整的<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html" target="_blank" rel="noopener noreferrer">線上資源</a>，包括方法、code文件與教學。</p>
<p>h2o的幾個特色包括：</p>
<ul>
<li>在單一節點或多節點群集上進行分散式或平行式運算。</li>
<li>根據用戶指定的指標和使用者指定的相對容忍度收斂時，自動提前停止。</li>
<li>隨機GBM同時對欄位跟資料列進行抽樣(每次分割與每棵樹)以利得到廣義的解。</li>
<li>除了二項式binomial(Bernoulli)、高斯和多項式分佈函式外，亦支援指數型系列(Poisson, Gamma, Tweedie)和損失函數。</li>
<li>Grid Search超參數優化和模型挑選。</li>
<li>數據分散(data-distributed) &#8211; 代表整個資料集不必侷限在單一節點的記憶體，能夠擴展到任意大小的訓練資料集。</li>
<li>使用直方圖(histogram)來近似連續變數來加速。</li>
<li>使用動態分箱法(dynamic binning)，分箱的標準會依照每一顆樹模型切割時的最大最小值來動態調整。</li>
<li>使用平方誤差(squared error)來決定最適的切割點。</li>
<li>factor levels沒有限制。</li>
</ul>
<h4>基本的h2o實作</h4>
<p></p><pre class="crayon-plain-tag">h2o.no_progress()
h2o.init(max_mem_size = "5g")</pre><p></p><pre class="crayon-plain-tag">##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 days 10 hours 
##     H2O cluster timezone:       Asia/Taipei 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.22.1.1 
##     H2O cluster version age:    3 months and 12 days !!! 
##     H2O cluster name:           H2O_started_from_R_peihsuan_hxs725 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   2.54 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20)</pre><p>gbm.h2o函數可允許我們透過H2O套件來執行GBM。然而，在我們開始執行初始模型時，我們需要將訓練資料轉換成h2o物件。h2o.gbm預設會採用以下參數來建立GBM模型：</p>
<ul>
<li>number of tree (ntrees): 50</li>
<li>learning rate (learning_rate): 0.1</li>
<li>tree depth(max_depth): 5</li>
<li>末梢節點的最小觀測值個數 (min_rows): 10</li>
<li>對資料列和資料欄位沒有抽樣</li>
</ul>
<p></p><pre class="crayon-plain-tag"># create feature names
y &lt;- "Sale_Price"
x &lt;- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o &lt;- as.h2o(ames_train)

# training basic GBM model with defaults
h2o.fit1 &lt;- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 5
)

# assess model results
h2o.fit1</pre><p></p><pre class="crayon-plain-tag">## Model Details:
## ==============
## 
## H2ORegressionModel: gbm
## Model ID:  GBM_model_R_1554713367949_3 
## Model Summary: 
##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves
## 1              50                       50               17160         5         5    5.00000         10         31    22.60000
## 
## 
## H2ORegressionMetrics: gbm
## ** Reported on training data. **
## 
## MSE:  165078993
## RMSE:  12848.31
## MAE:  9243.007
## RMSLE:  0.08504509
## Mean Residual Deviance :  165078993
## 
## 
## 
## H2ORegressionMetrics: gbm
## ** Reported on cross-validation data. **
## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
## 
## MSE:  707783149
## RMSE:  26604.19
## MAE:  15570.2
## RMSLE:  0.1404869
## Mean Residual Deviance :  707783149
## 
## 
## Cross-Validation Metrics Summary: 
##                               mean           sd  cv_1_valid   cv_2_valid   cv_3_valid  cv_4_valid  cv_5_valid
## mae                      15547.765     721.0162   16325.277    16366.696    13728.291    15117.01   16201.549
## mean_residual_deviance 7.0412026E8 1.68594576E8 6.8916365E8 1.12267443E9 3.96582368E8 5.8667603E8 7.2550477E8
## mse                    7.0412026E8 1.68594576E8 6.8916365E8 1.12267443E9 3.96582368E8 5.8667603E8 7.2550477E8
## r2                       0.8918067   0.02474849   0.8868936    0.8282497   0.92912257  0.91696167  0.89780617
## residual_deviance      7.0412026E8 1.68594576E8 6.8916365E8 1.12267443E9 3.96582368E8 5.8667603E8 7.2550477E8
## rmse                     26165.846    3119.9976   26251.926    33506.336    19914.377   24221.396   26935.195
## rmsle                   0.13949537  0.010692091  0.13955544   0.16392557   0.14536715  0.11926634  0.12936236</pre><p>跟XGBoost類似，我們可以使用自動停止功能，這樣就可以提高樹模型的個數，直到模型改善幅度減少或停止在終止訓練程序。亦可設定當執行時間超過一定水準後停止程序(參考max_runtime_secs)。</p>
<p>舉例來說，我們使用5000棵樹訓練一個預設參數的模型，但是設定當連續十棵樹模型在交叉驗證誤差上沒有進步就停止的指令。而在此例可以看到，模型在約使用3743棵樹模型後停止訓練過程，對應的交叉驗證誤差RMSE為$24,684。</p><pre class="crayon-plain-tag"># training basic GBM model with defaults
h2o.fit2 &lt;- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 5,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
)

# model stopped after xx trees
h2o.fit2@parameters$ntrees</pre><p></p><pre class="crayon-plain-tag">## [1] 3712</pre><p></p><pre class="crayon-plain-tag"># cross validated RMSE
h2o.rmse(h2o.fit2, xval = TRUE)</pre><p></p><pre class="crayon-plain-tag">## [1] 24683.95</pre><p></p>
<h4>Tuning</h4>
<p>H2O套件提供需多可調整的參數。這部分值得你花時間閱讀相關的文件<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html#gbm-tuning-guide" target="_blank" rel="noopener noreferrer">H2O.ai</a>。在本筆記中，只會先專注在幾個較長使用的超參數組合。包括：</p>
<ul>
<li>樹的複雜度
<ul>
<li>ntrees: 使用樹模型個數</li>
<li>max_depth: 每棵樹的深度</li>
<li>min_rows: 末梢節點中所允許的最少觀測值個數</li>
</ul>
</li>
<li>學習步伐
<ul>
<li>learn_rate: 損失函數梯度下降的步伐</li>
<li>learn_rate_annealing: 允許使用高的學習步伐當作初始值，但隨著樹個數的增加而降低。</li>
</ul>
</li>
<li>加入隨機的特性
<ul>
<li>sample_rate: 建置每棵數所抽樣的訓練資料列數。</li>
<li>col_sample_rate: 建置每棵樹所抽樣的欄位數(跟xgboost套件中的colsample_bytree是一樣的)。</li>
</ul>
</li>
</ul>
<p>值得注意的是，還有能夠控制類別變數和連續變數如何編碼、分箱、切割的參數。預設的參數可以得到相當不錯的結果，但在特定情境中仍能透過調整這些小地方來微提高模型的效果。</p>
<p>執行h2o模型的grid search tuning有兩種選擇：full 或 random discrete grid search。</p>
<h5>Full grid search</h5>
<p>在full cartesian grid search法中，會完整依序執行grid中指定的所有超參數組合。這就是我們先前在gbm和xgboost手動撰寫for迴圈執行的參數校正過程。然而為了加快H2O訓練的過程，可以使用驗證資料集(validation set)來取代k-fold cross validation。</p>
<p>以下我們製造一個參數的grid，包含468(=3*3*3*2*3*3)種超參數排列組合。我們使用h2o.grid()來執行full grid search並同時設定停止參數來節省訓練的時間。</p><pre class="crayon-plain-tag"># create training &amp; validation sets
split &lt;- h2o.splitFrame(train.h2o, ratios = 0.75)
train &lt;- split[[1]]
valid &lt;- split[[2]]

# create hyperparameter grid
hyper_grid &lt;- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(.99, 1),
  sample_rate = c(.5, .75, 1),
  col_sample_rate = c(.8, .9, 1)
)

# perform grid search 
system.time(
grid &lt;- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid1",
  x = x, 
  y = y, 
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
  )
)</pre><p>full grid search所花的時間約為31分鐘。</p><pre class="crayon-plain-tag"># collect the results and sort by our model performance metric of choice
grid_perf &lt;- h2o.getGrid(
  grid_id = "gbm_grid1", 
  sort_by = "mse", 
  decreasing = FALSE
  )
grid_perf</pre><p></p><pre class="crayon-plain-tag">## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid1 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 486 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 1             1.0       0.05                  1.0         5      1.0        0.75 gbm_grid1_model_213   4.64582634567234E8
## 2             0.8       0.01                  1.0         5      1.0         0.5  gbm_grid1_model_46  4.748883474543088E8
## 3             0.8       0.01                  1.0         5      1.0        0.75 gbm_grid1_model_208 4.7571620498865277E8
## 4             0.9       0.01                  1.0         5      1.0        0.75 gbm_grid1_model_209  4.775898758772956E8
## 5             1.0       0.01                  1.0         5      1.0         0.5  gbm_grid1_model_48 4.7783819706988806E8
## 
## ---
##     col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 481             1.0       0.01                 0.99         1      5.0         1.0 gbm_grid1_model_381 2.9511012504833984E9
## 482             1.0       0.01                 0.99         1      1.0         1.0 gbm_grid1_model_327 2.9511012504833984E9
## 483             1.0       0.01                 0.99         1     10.0         1.0 gbm_grid1_model_435 2.9511012504833984E9
## 484             1.0       0.01                 0.99         1      1.0        0.75 gbm_grid1_model_165  2.951772702040925E9
## 485             1.0       0.01                 0.99         1      5.0        0.75 gbm_grid1_model_219  2.951772702040925E9
## 486             1.0       0.01                 0.99         1     10.0        0.75 gbm_grid1_model_273  2.951772702040925E9</pre><p>由以上資訊可知，模型切割數超過1次的深度、慢的學習步伐和隨機觀測值抽樣是表現的最不錯的組合類型。<br />
我們亦可查看更多有關最佳模型的詳細資訊。最佳模型可達到的驗證誤差RMSE為$21,554。</p><pre class="crayon-plain-tag"># Grab the model_id for the top model, chosen by validation error
best_model_id &lt;- grid_perf@model_ids[[1]]
best_model &lt;- h2o.getModel(best_model_id)

# Now let’s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)</pre><p></p><pre class="crayon-plain-tag">## H2ORegressionMetrics: gbm
## ** Reported on validation data. **
## 
## MSE:  464582635
## RMSE:  21554.18
## MAE:  14389.8
## RMSLE:  0.1268479
## Mean Residual Deviance :  464582635</pre><p></p>
<h5>Random discrete grid search</h5>
<p>當想要測試的超參數排列組合非常多時，每增加一個參數都對grid search所需完成的時間有巨大的影響。因此，h2o亦有提供Random discrete的grid search path法，採取隨機挑選超參數組合來執行，直到指定程度的改善幅度被達成或超過一定的執行時間或只執行過一定的模型數量時（或以上條件的組合）則停止。雖然說Random discrete path不一定會找到最適的模型，但在一定程度上可以找到相當不錯的模型。</p>
<p>以下便採用Random Discrete Path法來執行和剛剛一模一樣的hyperparameter grid。不過，在此我們會加入新的search條件：當連續有10個模型效果都無法超越目前最佳的模型獲得0.5%的MSE改善時，則停止。如果持續有在獲得改善，但超過360秒(60分鐘)時，也停止程序。</p><pre class="crayon-plain-tag"># random grid search criteria
search_criteria &lt;- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 60*60
  )

# perform grid search 
system.time(
grid &lt;- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid2",
  x = x, 
  y = y, 
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid,
  search_criteria = search_criteria, # add search criteria
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
  )
)</pre><p></p><pre class="crayon-plain-tag">##     user   system  elapsed 
##   38.949   11.738 3602.198</pre><p>在此例子中，Random Grid Search花了約60分鐘(=3600/60)，評估了154/486個模型(32%)。</p><pre class="crayon-plain-tag"># collect the results and sort by our model performance metric of choice
grid_perf &lt;- h2o.getGrid(
  grid_id = "gbm_grid2", 
  sort_by = "mse", 
  decreasing = FALSE
  )
grid_perf</pre><p></p><pre class="crayon-plain-tag">## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid2 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 154 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 1             0.9       0.01                  1.0         3      1.0        0.75  gbm_grid2_model_45  4.748951758832882E8
## 2             1.0       0.01                  1.0         3      1.0         0.5  gbm_grid2_model_80  4.803434026601322E8
## 3             1.0        0.1                  1.0         3      1.0        0.75  gbm_grid2_model_17  4.926287246284121E8
## 4             0.9       0.01                  1.0         3      1.0         0.5 gbm_grid2_model_115 5.0211786533732516E8
## 5             0.9       0.05                  1.0         5      1.0         0.5 gbm_grid2_model_107  5.069315439854374E8
## 
## ---
##     col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 149             1.0       0.01                 0.99         1     10.0        0.75 gbm_grid2_model_146 3.4271307547685847E9
## 150             1.0       0.01                 0.99         1      5.0        0.75  gbm_grid2_model_24 3.4271307547685847E9
## 151             0.8       0.01                 0.99         1     10.0        0.75  gbm_grid2_model_41  3.427939673958527E9
## 152             0.8       0.01                 0.99         1     10.0         1.0  gbm_grid2_model_27 3.4288680514316573E9
## 153             1.0       0.01                 0.99         1      5.0         1.0 gbm_grid2_model_131 3.4293746592550063E9
## 154             1.0       0.01                  1.0         3      1.0        0.75 gbm_grid2_model_154  5.038339642534549E9</pre><p>透過Random Grid Search所得到的最佳模型的交叉驗證RMSE為$21,792。雖然沒有full grid search找到的好($21,554)，但通常兩者所找到模型的效果已是差不多的。</p><pre class="crayon-plain-tag"># Grab the model_id for the top model, chosen by validation error
best_model_id &lt;- grid_perf@model_ids[[1]]
best_model &lt;- h2o.getModel(best_model_id)

# Now let’s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)</pre><p></p><pre class="crayon-plain-tag">## H2ORegressionMetrics: gbm
## ** Reported on validation data. **
## 
## MSE:  474895176
## RMSE:  21792.09
## MAE:  13738.25
## RMSLE:  0.1245287
## Mean Residual Deviance :  474895176</pre><p>一旦我們找到了最佳模型後，就可以用所有的訓練資料再重新訓練一個模型。我們使用從full grid search所得到的最佳模型的參數組合並使用5-fold cross validation來估計穩健的誤差。</p><pre class="crayon-plain-tag"># train final model
h2o.final &lt;- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 5,
  ntrees = 10000,
  learn_rate = 0.01,
  learn_rate_annealing = 1,
  max_depth = 3,
  min_rows = 10,
  sample_rate = 0.75,
  col_sample_rate = 1,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
)</pre><p></p><pre class="crayon-plain-tag"># model stopped after xx trees
h2o.final@parameters$ntrees</pre><p></p><pre class="crayon-plain-tag">## [1] 8660</pre><p></p><pre class="crayon-plain-tag"># cross validated RMSE
h2o.rmse(h2o.final, xval = TRUE)</pre><p></p><pre class="crayon-plain-tag">## [1] 23214.71</pre><p></p>
<h4>Visualization</h4>
<h5>Variable importance</h5>
<p>h2o套件有提供內建的變數重要性繪圖功能。該函式只有一個衡量變數重要性的方法-relative importance，一種衡量每一個變數在每一個模型中，平均能對loss function造成多少影響。能對損失函數帶來最大平均影響者的變數被當作最重要的變數，且其他變數的重要性也是相對於最重要變數所計算而得的數值。另外，vip套件亦可繪製h2o物件的變數重要性圖。</p><pre class="crayon-plain-tag">h2o.varimp_plot(h2o.final, num_of_features = 10)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-63-1.png" alt="gradient boosting machines, GBM" /></p>
<h5>Partial dependence plots</h5>
<p>我們亦可像之前一樣使用vip套件繪製PDP和ICE圖型來了解不同解釋變數邊際變動下對目標變數造成的影響。我們只需要透過一段專用函數，將投入的資料(newdata)轉換成h2o物件(as.h2o)，並將預測的結果在轉換為data frame型態，在當成pred.fun的參數投入。</p><pre class="crayon-plain-tag">pfun &lt;- function(object, newdata) {
  as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]]
}

pdp &lt;- h2o.final %&gt;%
  partial(
    pred.var = "Gr_Liv_Area", 
    pred.fun = pfun, # 研究一下
    grid.resolution = 20, 
    train = ames_train
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")</pre><p></p><pre class="crayon-plain-tag">ice &lt;- h2o.final %&gt;%
  partial(
    pred.var = "Gr_Liv_Area", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = ames_train,
    ice = TRUE
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")</pre><p></p><pre class="crayon-plain-tag">gridExtra::grid.arrange(pdp, ice, nrow = 1)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-64-1.png" alt="gradient boosting machines, GBM" /></p>
<p>h2o並沒有提供內建的ICE曲線繪圖功能，但是他可以繪製平均邊際效益(標準的PDP圖)外加衡量不確定性的一個標準誤差的PDP圖。</p><pre class="crayon-plain-tag">h2o.partialPlot(h2o.final, data = train.h2o, cols = "Overall_Qual")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-65-1.png" alt="gradient boosting machines, GBM" /></p><pre class="crayon-plain-tag">## PartialDependence: Partial Dependence Plot of model GBM_model_R_1554713367949_5 on column 'Overall_Qual'
##      Overall_Qual mean_response stddev_response std_error_mean_response
## 1   Above_Average 173439.744454    59206.888258             1307.342580
## 2         Average 169032.396430    58675.968674             1295.619387
## 3   Below_Average 165991.718878    61198.281190             1351.314368
## 4       Excellent 227862.381764    66306.055141             1464.098718
## 5            Fair 161119.245285    62126.019654             1371.799687
## 6            Good 185468.115459    62195.668588             1373.337600
## 7            Poor 156505.758628    63260.388765             1396.847601
## 8  Very_Excellent 228227.228142    66731.640843             1473.496042
## 9       Very_Good 206801.916781    64639.301392             1427.295261
## 10      Very_Poor 151217.679576    64366.405585             1421.269471</pre><p>但不幸的事，h2o的函數會把類別變數的levels以字母排序的方式繪出，而pdp()函式則是以他們所指定的level順序繪出，使推理更加直觀。</p><pre class="crayon-plain-tag">pdp &lt;- h2o.final %&gt;%
  partial(
    pred.var = "Overall_Qual", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = as.data.frame(ames_train)
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice &lt;- h2o.final %&gt;%
  partial(
    pred.var = "Overall_Qual", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = as.data.frame(ames_train),
    ice = TRUE
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-66-1.png" alt="gradient boosting machines, GBM" /></p>
<h5>LIME</h5>
<p>LIME套件亦有提供內建的函數來處理h2o物件。</p><pre class="crayon-plain-tag"># apply LIME
explainer &lt;- lime(ames_train, h2o.final)
explanation &lt;- explain(local_obs, explainer, n_features = 5)
plot_features(explanation)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-67-1.png" alt="gradient boosting machines, GBM" /></p>
<h4>Predicting</h4>
<p>最後，我們可以使用h2o.predict()或predict()兩種方式來進行模型對新資料的預測，並使用h2o.performance()來衡量模型模型套用在測試資料集的成效。驗證結果的RMSE為$20,198，跟gbm和xgboost是類似的($21~22K)。</p><pre class="crayon-plain-tag"># convert test set to h2o object
test.h2o &lt;- as.h2o(ames_test)

# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)</pre><p></p><pre class="crayon-plain-tag">## H2ORegressionMetrics: gbm
## 
## MSE:  407897003
## RMSE:  20196.46
## MAE:  12679.15
## RMSLE:  0.1008391
## Mean Residual Deviance :  407897003</pre><p></p><pre class="crayon-plain-tag"># predict with h2o.predict
h2o.predict(h2o.final, newdata = test.h2o)</pre><p></p><pre class="crayon-plain-tag">##    predict
## 1 129814.6
## 2 161987.0
## 3 262990.7
## 4 484455.6
## 5 218094.0
## 6 209430.8
## 
## [879 rows x 1 column]</pre><p></p><pre class="crayon-plain-tag"># predict values with predict
predict(h2o.final, test.h2o)</pre><p></p><pre class="crayon-plain-tag">##    predict
## 1 129814.6
## 2 161987.0
## 3 262990.7
## 4 484455.6
## 5 218094.0
## 6 209430.8
## 
## [879 rows x 1 column]</pre><p></p>
<h3>小結</h3>
<ul>
<li>Gradient Boosting Machines (GBM)是一個強大的集成學習演算法，通常具有一流的預測能力。雖然相較於其他演算法它比較不直覺且需要大型運算，但絕對是機器學習工具箱的必備款！</li>
</ul>
<hr />
<p>參考連結：</p>
<ol>
<li><a href="http://uc-r.github.io/gbm_regression" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines (GBM)</a></li>
<li><a href="https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC2-4%E8%AC%9B-%E8%B3%87%E6%96%99%E5%89%8D%E8%99%95%E7%90%86-missing-data-one-hot-encoding-feature-scaling-3b70a7839b4a" target="_blank" rel="noopener noreferrer">類別資料的處理(有序、無序):one-hot encoding</a></li>
<li><a href="https://towardsdatascience.com/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b" target="_blank" rel="noopener noreferrer">Choosing the right Encoding method-Label vs OneHot Encoder</a></li>
</ol>
<hr />
<p>更多Decision Tree相關的統計學習筆記：</p>
<p><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></p>
<p><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a></p>
<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></p>
<p><a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a></p>
<p>更多Regression相關統計學習筆記：</p>
<p><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></p>
<p><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 – 資料探勘與處理 | 統計 R語言</a></p>
<p><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 – 模型建置、診斷與比較 | R語言</a></p>
<p><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 – Ridge, Lasso, Elastic Net | R語言</a></p>
<p>更多Clustering集群分析統計學習筆記：</p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></p>
<p><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a></p>
<p>其他統計學習筆記：</p>
<p><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></p>
<p>這篇文章 <a rel="nofollow" href="/gradient-boosting-machines-gbm/">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/gradient-boosting-machines-gbm/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Random Forests 隨機森林 &#124; randomForest, ranger, h2o &#124; R語言</title>
		<link>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/</link>
					<comments>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Tue, 19 Mar 2019 14:33:31 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[h2o]]></category>
		<category><![CDATA[random forests]]></category>
		<category><![CDATA[randomForest]]></category>
		<category><![CDATA[ranger]]></category>
		<category><![CDATA[Regression Tree]]></category>
		<category><![CDATA[隨機森林]]></category>
		<guid isPermaLink="false">/?p=2802</guid>

					<description><![CDATA[<p>Bagging法綜合多個樹模型結果，可以降低單一樹模型的高變異性並提升預測正確率。但Bagging法中樹與樹之間的相關性會降低模型整體的表現。隨機森林 Rand [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Bagging法</a>綜合多個樹模型結果，可以降低單一樹模型的高變異性並提升預測正確率。但Bagging法中樹與樹之間的相關性會降低模型整體的表現。隨機森林 Random forests 是Bagging修改後的版本，它是由「去相關性」的樹模型所組成的集成演算法，有很不錯的預測正確率且是一個受歡迎、開箱即用的演算法。</p>
<h3>載入所需套件</h3>
<p></p><pre class="crayon-plain-tag">library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # an extremely fast java-based platform
library(dplyr)
library(magrittr)</pre><p>準備資料。</p>
<ul>
<li>使用AmesHousing套件中的Ames Housing Data。</li>
<li>使用resample package中的initial_split()將資料切分成7:3(將參數設定為prop = 0.7)。</li>
<li>再分別用training()和testing()函數將切分好的資料萃取出。</li>
<li>並使用set.seed()來確保資料切分結果是可再現的。</li>
</ul>
<p></p><pre class="crayon-plain-tag"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split &lt;- initial_split(data = AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</pre><p></p>
<h3>Random Forests 概念介紹</h3>
<p>隨機森林模型的基礎概念和Decision Trees和Bagging一樣的(可以參考<a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">決策樹,Decision Trees</a>和<a href="/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/" target="_blank" rel="noopener noreferrer">Bagging</a>)。Bagging Trees模型在演算法中納入了隨機的元素，有效的降低了單一樹模型的高變異性與提升模型預測正確率。然而在bagging中的trees並非所有都是彼此相互獨立的，因為在每一棵樹切分節點時都是考慮所有原始的預測變數。也因爲上述關係，來自不同bootstrapped samples的樹彼此的結構都會有些類似（尤其是在樹的上半部，用來切割的前幾大變數都會非常類似）。</p>
<p>樹的結構相遇的這個特性就稱作tree correlation，它阻礙了Bagging最適地降低預測目標值的變異(variance)。為了更近一步降低變異，我們需要最小化樹與樹之間的相關性。這可以透過注入更多的隨機性到長樹的過程。 Random Forests是透過以下兩步驟來達成的：</p>
<ol>
<li>Bootstrap (拔靴法) : 跟Bagging很類似，每一顆樹都是建立自不同的bootstrapped sample，讓他們稍稍不一樣並稍稍去相關性。</li>
<li>Split-variable randomization (變數切割的隨機性) : 每一次在執行變數切割時，搜尋切割變數的範圍被限縮為隨機的子集合，即隨機挑選m個隸屬於總p個變數的子集合作為切割搜尋變數的範圍。對回歸樹來說，預設使用\(m=\frac{p}{3}\)，是個可經調教的參數。當\(m = p\)的時候，則跟只進行步驟1的結果一樣。</li>
</ol>
<p>因為每棵樹都是來自不同的隨機bootstrapped sample且每一次切割都是隨機挑選變數的子集合，因此樹與樹之間的關聯性會下降地較Bagging更低。</p>
<h4>OOB error vs. test set error</h4>
<p>和Bagging一樣，bootstrap resample法的一個天然好處，就是隨機森林模型可以透過out-of-bag(OOB)的樣本誤差來作為有效與合理近似實驗誤差(test error)。不需要額外產生或犧牲訓練資料集，OOB sample可供作為一個內建的驗證子集合。OOB sample 的存在讓尋找使模型錯誤率趨於穩定的最適樹模型數量(ntree)更有效率。但是OOB error和test error終究還是預期會不太相同。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2804" src="/wp-content/uploads/2019/03/Rplot.png" alt="random forests-隨機森林" width="800" height="500" srcset="/wp-content/uploads/2019/03/Rplot.png 800w, /wp-content/uploads/2019/03/Rplot-300x188.png 300w, /wp-content/uploads/2019/03/Rplot-768x480.png 768w, /wp-content/uploads/2019/03/Rplot-230x144.png 230w, /wp-content/uploads/2019/03/Rplot-350x219.png 350w, /wp-content/uploads/2019/03/Rplot-480x300.png 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>[上圖：Random forest out-of-bag error versus validation error.]</p>
<p>&nbsp;</p>
<p>此外，有很多package都沒有可以追蹤在某一棵樹模型中，哪些是OOB sample哪些不是的功能。這樣在比須比較多個模型的成效時，想要使用相同的驗證資料集來幫每個模型打分數是不可行的。而且，技術上雖然可以對OOB sample計算特定指標(metrics)如root mean squared logarithmic error (RMSLE)，但並非所有package都有內建這樣的運算功能。所以如果你想要比較多個模型的成效或是使用較不傳統的損失函數指標，你可能還是會選擇cross validation。</p>
<h4>Advantages &amp; Disadvantages of Random Forests</h4>
<h5>優點</h5>
<ol>
<li>隨機森林模型通常具有非常好的成效。</li>
<li>非常好的「開箱即用」的模型-不太需要調整什麼參數。</li>
<li>有內建的驗證資料集validation set &#8211; 不須為了額外驗證而犧牲資料。</li>
<li>不需前處理(pre-processing)。</li>
<li>對於離群值的處理是強大的。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>當運算大型資料時會變的非常慢。</li>
<li>雖然模型預測正確率高，但通常無法跟更先進的boosting演算法相比。</li>
<li>較不易解釋。</li>
</ol>
<h3>基本實作</h3>
<p>在R中有超過20種的Random Forests Packages。以下會使用歷史最悠久且最受歡迎randomForest套件來說明示範基本的Random Forests模型實作。但必須注意，當你的資料集變得很大的時候，randomForest回無法很好的擴展到大型資料（即使使用foreach進行平行運算）。此外，為了探索和比較不同模型參數的效果，我們也可找到更多有效的套件。因此，在模型tuning階段，我們會說明如何使用ranger和h2o package來進行更有效率的Random Forests modeling。</p>
<p>randomForest::randomForest可以使用formula或x,y matrix表示的方式來指定模型資料。我們以下以formula的方式來指定模型設定並使用randomForest模型預設參數。</p><pre class="crayon-plain-tag"># for reproduciblity
set.seed(123)

# default RF model
m1 &lt;- randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train
)

m1</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 661089658
##                     % Var explained: 89.8</pre><p>從m1結果來看：</p>
<ul>
<li>randomForest預設會使用500棵樹。</li>
<li>每一次切割(each split)會隨機篩選出\(\frac{Features}{3}=26\)個預測變數作為base。(原始data扣除目標變數Sale_Price的number of features = 80)</li>
<li>m1$mse(regression only)代表的是由OOB sample所計算出的「平均誤差平方(mean squared erre)」向量，即殘差平方和(sum of squared residuals)除上樹的個數(n)。</li>
<li>綜合500棵樹的mse(OOB error)為m1$mse[500] = 6.6108966 × 10<sup>8</sup>。</li>
</ul>
<p>我們可以進一步將不同棵樹所組成的隨機森林模型(ntree)對應的平均誤差(MSE)給繪出：</p><pre class="crayon-plain-tag">plot(m1)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-4-1-3.png" alt="random forests-隨機森林" /></p>
<p>從上圖可以發現，模型平均誤差大概在100棵樹時開始趨於穩定，且平均誤差值降低的速度開始於300棵樹左右時開始變緩。</p>
<p>我們可以進一步找出使得MSE最小的樹的個數。</p><pre class="crayon-plain-tag">which.min(m1$mse)</pre><p></p><pre class="crayon-plain-tag">## [1] 447</pre><p>最適隨機森林的RMSE則為(平均Sale_Price的誤差值)：</p><pre class="crayon-plain-tag">sqrt(m1$mse[which.min(m1$mse)])</pre><p></p><pre class="crayon-plain-tag">## [1] 25648.78</pre><p>如果我們不想要OOB error，randomForest函數亦提供驗證資料集(validation set)來幫助我們計算模型預測正確率。</p>
<p>要計算驗證誤差，首先我們進一步將訓練資料集依據8:2的比例切成訓練和驗證資料集，並分別使用analysis()和assessment()函數萃取出訓練和驗證資料。</p><pre class="crayon-plain-tag"># create training and validation data 
set.seed(123)
valid_split &lt;- initial_split(ames_train, .8)

# training data
ames_train_v2 &lt;- analysis(valid_split)

# validation datas
ames_valid &lt;- assessment(valid_split)

# 將驗證資料整理成x_test和y_test
x_test &lt;- ames_valid[setdiff(names(ames_valid),"Sale_Price")]
y_test &lt;- ames_valid$Sale_Price

# 在randomForest函數中使用x-test和y-test當作驗證資料集的參數
rf_oob_comp &lt;- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train_v2,
  xtest = x_test,
  ytest = y_test
)

rf_oob_comp</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train_v2,      xtest = x_test, ytest = y_test) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 667486651
##                     % Var explained: 89.17
##                        Test set MSE: 841004412
##                     % Var explained: 89.16</pre><p>可以看到模型結果多了Test set MSE，和原始的OOB MES不太一樣。我們將不同顆樹組成的模型所對應的OOB error和test error萃取出來並畫出誤差隨著樹的數量的變化圖，並比較兩者的差距。</p><pre class="crayon-plain-tag"># extract OOB &amp; validation errors
oob &lt;- sqrt(rf_oob_comp$mse)
validation &lt;- sqrt(rf_oob_comp$test$mse)

data.frame(
  ntrees = 1:rf_oob_comp$ntree,
  OOB.error = oob,
  Test.error = validation
) %&gt;% 
  gather(key = metric, value = RMSE, 2:3) %&gt;% 
  ggplot(aes(x = ntrees, y = RMSE, color = metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-8-1-3.png" alt="random forests-隨機森林" /></p>
<p>Random Forests是其中一個很好的「開箱即用」的演算法之一。基本上不需要調整什麼參數(tuning)，模型的預測能力就可ㄧ有很好的成效。</p>
<p>舉例來說，從上圖中，我們不需要調整參數就可以得到小於$30K的RMSE，比完整tuning後的<a href="/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/" target="_blank" rel="noopener noreferrer">Bagging模</a>RMSE降低超過$6K；比完整tuning的<a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">elastic net模型</a>RMSE降低超過$2K(沒有將目標變數取log的elastic net model版本請參考以下)。而我們還可以進一步透過參數調整tuning將Random Forests模型的預測正確性優化。</p><pre class="crayon-plain-tag"># elastic net
library(caret)
ames_train_x &lt;- model.matrix(object = Sale_Price ~ ., data =  ames_train)[, -1]
ames_train_y &lt;- ames_train$Sale_Price

# ames_test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1]
# ames_test_y &lt;- ames_test$Sale_Price

train_control &lt;- trainControl(method = "cv", number = 10)

caret_mod &lt;- train(
  x = ames_train_x, 
  y = ames_train_y,
  method = "glmnet", 
  prePro = c("center","scale","zv","nzv"),
  trControl = train_control,
  tuneLength = 10
)

min(caret_mod$results$RMSE)</pre><p></p><pre class="crayon-plain-tag">## [1] 32268.14</pre><p></p>
<h4>tuning</h4>
<p>Random Forests在tuning上非常簡單，因為只有幾個tuning parameters。通常tuning模型一開始最主要的考量點就是每一次分割所用來挑選的潛在變數名單，另外就是幾個需要注意的hyperparameters，包括如下：(這些hyperparameters在不同package的命名可能有所不同)</p>
<ul>
<li>ntree : number of trees。我們希望有足夠的樹來穩定模型的誤差，但過多的樹會是沒效率且沒必要的，特別是遇到大型資料集的時候。</li>
<li>mtry : 每次在決定切割變數時，所隨機抽樣的潛在變數清單數量。當mtry = p(即所有特徵變數數量)，Random Forests的結果就會和bagging一樣。而當mtry = 1，會造就每一次split所使用的變數completely random，每個變數都有機會但會造成非常偏差的結果。</li>
<li>sampsize : 訓練每棵樹模型的樣本數大小。預設是使用63.25%訓練資料集的比例，因為這個是獨立觀察值出現在bootstrapped sample的期望機率值。較低的樣本數大小雖然會降低訓練時間，但可能會產生不必要的偏差。增加樣本數大小可以提升模型正確率，但有可能會產生overfitting(因為會增加模型變異性)。所以一般來說，我們校正此樣本大小參數時會使用60-80%的比例。</li>
<li>nodesize : 末梢(葉)節點最小觀察資料個數。用來控制樹模型的複雜度。小的葉節點大小允許更深更複雜的樹模型，大的葉節點大小則會產生叫淺的樹模型。這又是一種「偏差v.s.變異度(bias -variance)」的權衡，當樹長得越深時，模型變異性愈高(有過度配適的風險)，而當樹長得越潛時則會有較多偏差(有沒辦法完整捕捉資料中的模式跟關係)。</li>
<li>maxnode : 內部節點最大個數值。另一種控制模型複雜度的變數，內部節點樹越多則會長出更深的樹，內部節點越少則產生越淺的樹。</li>
</ul>
<h4>Initial tuning with randomForest</h4>
<p>如果一開始只針對mtry進行校正，可以使用randomForest::tuneRF來進行簡易快速的評估。tunRF會從指定的mtry值開始，並每次增加給定的間距，直到模型OOB error降低的幅度開始小於特定幅度為止。</p>
<p>比如說，以下想知道mtry從5開始，每間隔相加1.5，所得到到的OOB error，直到OOB error改善的不度不超過1%為止。</p>
<p>因為tuneRF需要使用x,y形式指定資料，因此首先我們先使用setdiff()函數將變數名稱依據目標變數與預測變數分開。</p>
<p>模型跑完後會自動將每個mtry值所對應的OOB error值繪出。我們可以發現，當mtry &gt; 22後，OOB開始不再下降，最適mtry水準值約與features/3 = 80/3 = 26差不多。</p><pre class="crayon-plain-tag"># 篩選出預測變數名稱
features &lt;- setdiff(x = names(ames_train), y = "Sale_Price")

# 固定不同mtry參數值的模型所使用的隨機OOB sample一樣的
set.seed(123)

m2 &lt;- tuneRF(x = ames_train[features], y = ames_train$Sale_Price, mtryStart = 5, ntreeTry = 500 ,stepFactor = 1.5, improve = 0.01, trace = FALSE)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-10-1-4.png" alt="random forests-隨機森林" /></p><pre class="crayon-plain-tag">plot(m2)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-10-2-4.png" alt="random forests-隨機森林" /></p>
<h4>Full grid search with ranger</h4>
<p>如果想要套用綜合mtry以及其他參數的hyperparameter組合，我們需要建立一個grid並使用loop迴圈的方式，去測試每一個hyperparameter的組合和模型的成效。但因為randomForest()函數無法有效的將運算擴展至大型數據運算，因此我們會使用以c++執行的ranger()函數來解決。</p>
<p>我們可先使用Sys.time()稍稍比較tuneRF和ranger執行一種隨機森林模型所需的時間。</p><pre class="crayon-plain-tag">system.time(
  ames_randomForest &lt;- randomForest(
    formula = Sale_Price ~ ., 
    data = ames_train, 
    ntree = 500, 
    mtry = floor(length(features)/3)
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
## 111.957   0.735 169.989</pre><p></p><pre class="crayon-plain-tag">system.time(
  ames_ranger &lt;- ranger(
    formula   = Sale_Price ~ ., 
    data      = ames_train, 
    num.trees = 500,
    mtry      = floor(length(features) / 3)
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  10.367   0.174   5.665</pre><p>由以上結果我們可以看到，同樣是執行一次隨機森林，ranger()所需的時間僅約6秒，而randomForests則需要169秒。</p>
<p>為了進行grid search，我們首先先建立一個hyperparameters的grid，由許多不同的mtry, minimum node size,和 sample size所組成。總共會有96種組合。</p><pre class="crayon-plain-tag"># hyperparameter grid search
hyper_grid &lt;- expand.grid(
  mtry = seq(20, 30, by = 2),
  node_size = seq(3, 9, by = 2),
  sample_size = c(0.55, 0.632, 0.7, 0.8),
  OOB_RMSE = 0
)

# total number of combinations
nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 96</pre><p>我們使用loop迴圈，一一帶入不同hyperparameters到ranger()函數中，並固定每一次randomForests的的森林樹木數量為500(因為從前面經驗我們知道500棵樹即足夠使OOB error趨於穩定並收斂)。另外每一次randomForests執行時，我們也固定隨機亂數種子，讓同樣sample_size參數值所對應的抽樣樣本可以相同，凸顯其他參數變化所帶來的效果。</p><pre class="crayon-plain-tag">for (i in 1:nrow(hyper_grid)) {
  # train model
  model &lt;- ranger(
    formula = Sale_Price ~ .,
    data = ames_train, 
    num.trees = 500, 
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i], 
    sample.fraction = hyper_grid$sample_size[i],
    seed = 123
  )

  # 並將每一此訓練模型的OOB RMSE萃取儲存
  hyper_grid$OOB_RMSE[i] &lt;- sqrt(model$prediction.error)
}

# 我們將結果依序OOB_RMSE由小至大排列，取模型成效前十名印出
hyper_grid %&gt;% 
  dplyr::arrange(OOB_RMSE) %&gt;% 
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    mtry node_size sample_size OOB_RMSE
## 1    20         5         0.8 25918.20
## 2    20         3         0.8 25963.96
## 3    28         3         0.8 25997.78
## 4    22         5         0.8 26041.05
## 5    22         3         0.8 26050.63
## 6    20         7         0.8 26061.72
## 7    26         3         0.8 26069.40
## 8    28         5         0.8 26069.83
## 9    26         7         0.8 26075.71
## 10   20         9         0.8 26091.08</pre><p>從前十名模型成效結果我們可以發現：</p>
<ul>
<li>OOB_RMSE大致落在26K左右。</li>
<li>最適mtry的值落在所有20~30範圍區間。表示mtry在此區間對於OOB_RMSE沒有太大影響。</li>
<li>最適最小節點觀察值數量大約落在3~5。</li>
<li>最適抽樣比例約為0.8。</li>
<li>表示抽樣比例高(~80%)和深度較長(葉節點觀測個數大小3~5)的隨機森林成效較好(OOB RMSE)。</li>
</ul>
<h5>調整變數型態</h5>
<p>雖然我們已知random forests對於原始類別型變數處理效果是不錯的，我們還是進一步來試試將類別變數重新編碼為dummy variables是否能提升random forests的預測表現。</p>
<p>以下，我們使用dummyVars()函數將類別變數重新編碼為虛擬變數。</p><pre class="crayon-plain-tag">to_dum &lt;- dummyVars(formula = ~., data = ames_train, fullRank = FALSE)</pre><p>原始類別預測變數(80)被轉換完後變成353個欄位。</p><pre class="crayon-plain-tag">ames_to_dum &lt;- predict(to_dum, newdata = ames_train) %&gt;% as.data.frame()</pre><p>將資料名稱變成ranger相容的名稱。</p><pre class="crayon-plain-tag">names(ames_to_dum) &lt;- make.names(names = names(ames_to_dum), allow_ = FALSE)</pre><p>建立hyperparameter grid。並將mtry的區間調整為更大範圍。並執行grid search。</p><pre class="crayon-plain-tag">hyper_grid_2 &lt;- expand.grid(
  mtry = seq(50, 200, by = 25),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE  = 0
)

for(i in 1:nrow(hyper_grid_2)){
  model &lt;- ranger(
    formula = Sale.Price ~.,
    data = ames_to_dum, 
    num.trees = 500, 
    mtry = hyper_grid_2$mtry[i],
    min.node.size = hyper_grid_2$node_size[i], 
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed = 123
  )

  hyper_grid_2$OOB_RMSE[i] &lt;- sqrt(model$prediction.error)
}</pre><p></p><pre class="crayon-plain-tag">hyper_grid_2 %&gt;% 
  dplyr::arrange(OOB_RMSE) %&gt;% 
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    mtry node_size sampe_size OOB_RMSE
## 1    50         3        0.8 26981.17
## 2    75         3        0.8 27000.85
## 3    75         5        0.8 27040.55
## 4    75         7        0.8 27086.80
## 5    50         5        0.8 27113.23
## 6   125         3        0.8 27128.26
## 7   100         3        0.8 27131.08
## 8   125         5        0.8 27136.93
## 9   125         3        0.7 27155.03
## 10  200         3        0.8 27171.37</pre><p>由結果可分發線</p>
<ul>
<li>OOB RMSE 落在27K左右，並沒有比類別變數重新編碼前的26K來得好。</li>
<li>將類別變數重新編碼成dummy variables是無法提升模型成效的。</li>
</ul>
<p>所以到目前為至，最適的random forests模型參數分別為mtry = 20, node_size = 5, sample_size = 0.8。我們重複執行100次這個參數設定的模型，來計算此模型的error rate的期望值大小。</p><pre class="crayon-plain-tag">OOB_RMSE &lt;- vector(mode = "numeric", length = 100)

for(i in 1:length(OOB_RMSE)){
  optimal_ranger &lt;- ranger(
    formula         = Sale_Price ~ ., 
    data            = ames_train, 
    num.trees       = 500,
    mtry            = 20,
    min.node.size   = 5,
    sample.fraction = .8,
    importance      = 'impurity'
  )

  OOB_RMSE[i] &lt;- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-19-1-3.png" alt="random forests-隨機森林" /></p>
<p>執行100次random forests的結果後，我們可以觀察到OOB RMSE的期望值約落在26000~26200區間。</p>
<p>另外，我們在在模型參數importance = &#8216;impurity&#8217;。這表示我們是依據節點不純度(node impurity)的改善幅度來衡量每個變數的重要性。變數的重要性是計算每一次使用不同變數切割結點後，總能使MSE下降的程度(跨多棵樹模型)，而那些無法透過該變數切割所降低的模型錯誤率，則被稱作node impurity。而能夠降低越多MSE的變數則被重要性越高。</p>
<p>因此，在每一次節點分割時，我們都會計算每個變數所造成的MSE下降程度，而累積下降MSE幅度最高者，則被認為是較為重要的變數。</p>
<p>我們將變數重要性結果繪出：</p><pre class="crayon-plain-tag">options(scipen = -1)
optimal_ranger$variable.importance %&gt;% 
  as.matrix() %&gt;% 
  as.data.frame() %&gt;% 
  add_rownames() %&gt;% 
  `colnames&lt;-`(c("varname","imp")) %&gt;%
  arrange(desc(imp)) %&gt;% 
  top_n(25,wt = imp) %&gt;% 
  ggplot(mapping = aes(x = reorder(varname, imp), y = imp)) +
  geom_col() +
  coord_flip() +
  ggtitle(label = "Top 25 important variables") +
  theme(
    axis.title = element_blank()
  )</pre><p></p><pre class="crayon-plain-tag">## Warning: Deprecated, use tibble::rownames_to_column() instead.</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-20-1-3.png" alt="random forests-隨機森林" /></p>
<p>由上圖我們可以看到前三名重要變數依序為：Overall_Qual, Gr_Liv_Area, Garage_Cars。</p>
<h4>Full grid search with H2O</h4>
<p>我們已經知道剛剛在執行ranger進行hyperparameter grid計算時，還花滿長一段時間的。雖然ranger在計算上是有效率的，<br />
但是當遇到大型的grid時，我們手寫的loop迴圈會變得非常沒有效率。</p>
<p>而這時候，h2o套件則是一個強大有效率的java-based介面，可以提供平行分布式運算方法。此外，h20還可以提供不同的&#8221;search path&#8221;。有別於一一執行每一種hyperparameter grid組合，h2o可允許不同的最適搜尋路徑來執行，直到模型成效改善達一定程度等search path。使得在tuning模型上更具效率。以下便來介紹如何使用h2o套件執行random forests。</p><pre class="crayon-plain-tag"># start up h2o
h2o.no_progress() # turn off progress bars when creating reports/tutorials)
h2o.init(max_mem_size = "4g")</pre><p></p><pre class="crayon-plain-tag">##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 days 3 hours 
##     H2O cluster timezone:       Asia/Taipei 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.22.1.1 
##     H2O cluster version age:    2 months and 19 days  
##     H2O cluster name:           H2O_started_from_R_peihsuan_qcx617 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   0.04 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        FALSE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20)</pre><p>首先我們先來使用完整grid search path (又叫做full cartesian)，即表示會逐一檢視所有我們所指派的參數組合(hyper_grid.h2o)。<br />
根據hyper_grid.h2o的參數組合，共會有4*3*2 = 24種組合。也因為這邊是採用cartisian法，所以也不會比上面的方法快多少。</p><pre class="crayon-plain-tag"># create feature names
y &lt;- "Sale_Price"
x &lt;- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o &lt;- as.h2o(ames_train)</pre><p></p><pre class="crayon-plain-tag"># 指派參數組合 hyperparameter grid
hyper_grid.h2o &lt;- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 25, by = 2),
  sample_rate = c(.70, .80)
)

# build grid search 
# 以「cartesian」法逐一執行每一個參數組合的隨機森林模型

# 測試24種組合所需的時間
system.time(
grid &lt;- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid",
  x = x, 
  y = y, 
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = list(strategy = "Cartesian")
  )
)</pre><p></p><pre class="crayon-plain-tag">##   user  system elapsed 
##  9.628   3.104 874.954</pre><p></p><pre class="crayon-plain-tag"># 蒐集結果並依照每一種參數組合模型的MSE誤差來排名
# collect the results and sort by our model performance metric of choice
grid_perf &lt;- h2o.getGrid(
  grid_id = "rf_grid", 
  sort_by = "mse", 
  decreasing = FALSE
  )</pre><p></p><pre class="crayon-plain-tag">print(grid_perf)</pre><p></p><pre class="crayon-plain-tag">## H2O Grid Details
## ================
## 
## Grid ID: rf_grid 
## Used hyper parameters: 
##   -  mtries 
##   -  ntrees 
##   -  sample_rate 
## Number of models: 91 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   mtries ntrees sample_rate        model_ids                 mse
## 1     20    400         0.8 rf_grid_model_85 6.073124636667156E8
## 2     26    300         0.8 rf_grid_model_82 6.119220802095695E8
## 3     20    300         0.8 rf_grid_model_79 6.148210280053709E8
## 4     26    500         0.7 rf_grid_model_70 6.154919117655025E8
## 5     26    400         0.8 rf_grid_model_88 6.155747553830479E8
## 
## ---
##    mtries ntrees sample_rate        model_ids                 mse
## 86     24    200        0.55  rf_grid_model_3 6.629919566473577E8
## 87     28    200         0.7 rf_grid_model_53 6.643770708728626E8
## 88     28    300        0.55 rf_grid_model_11 6.644256979260525E8
## 89     30    200        0.55  rf_grid_model_6 6.658494098992392E8
## 90     22    500        0.55 rf_grid_model_20 6.755417850891622E8
## 91     28    200        0.55  rf_grid_model_5 6.786043491214715E8</pre><p>然而我們從上述結果可以注意到，最好的模型的OOB RMSE只有2.464371 × 10<sup>4</sup> (&lt;25K)，比我們先前所校正的模型效果都還更好。<br />
這是由於h2o套件在參數包括「最小節點大小(minimum node size)」、「樹的深度(tree depth)」等都更「慷慨」，比如說h2o預設最小節點大小為1，而ranger和randomForest該參數則都預設為5。</p>
<h4>Random Discrete grid search with H2O</h4>
<p>當遇到參數變很多的情況下，額外增加一個參數將會巨幅拉大grid search執行時間。為了因應這樣的不變，h2o提供了一種叫做「RandomDiscrete」的grid search搜尋路徑，有別於「Cartisian」逐一執行所有組合，「RandomDiscrete」會隨機挑選參數組合，直到達到一定程度的改善幅度、或是超過一定的時間、或是已執行一定數目的模型數（或是以上三種情況的交叉組合）。雖然使用「RandomDiscrete」搜尋可能會錯過最佳的參數組合效果，但基本上他已經能夠調教出不錯的模型了。</p>
<p>比如說，以下範例的參數組同樣為24種，我們設計一個「RandomDiscrete」grid search，停止條件為達成以下任一條件：近10組模型的MSE相較於最佳模性的改善幅度未超過0.5%、執行時間超過600秒(30 min)。</p><pre class="crayon-plain-tag"># hyperparameter grid
hyper_grid.h2o &lt;- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 25, by = 2),
  sample_rate = c(.70, .80)
)

# random grid search criteria
search_criteria &lt;- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 30*60
  )


# build grid search 
system.time(
random_grid &lt;- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = x,
  y = y,
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )
)</pre><p></p><pre class="crayon-plain-tag">##  user  system elapsed 
##  6.721   1.877 713.918</pre><p></p><pre class="crayon-plain-tag"># collect the results and sort by our model performance metric of choice
grid_perf2 &lt;- h2o.getGrid(
  grid_id = "rf_grid2",
  sort_by = "mse",
  decreasing = FALSE
  )</pre><p></p><pre class="crayon-plain-tag">print(grid_perf2)</pre><p></p><pre class="crayon-plain-tag">H2O Grid Details
================

Grid ID: rf_grid2 
Used hyper parameters: 
  -  mtries 
  -  ntrees 
  -  sample_rate 
Number of models: 24 
Number of failed models: 0 

Hyper-Parameter Search Summary: ordered by increasing mse
  mtries ntrees sample_rate         model_ids                 mse
1     20    500         0.8 rf_grid2_model_19 5.996100879211967E8
2     22    500         0.8 rf_grid2_model_24  6.09073686599265E8
3     22    500         0.7  rf_grid2_model_4 6.096855932933546E8
4     24    400         0.8  rf_grid2_model_8 6.132880206896532E8
5     22    400         0.8 rf_grid2_model_13 6.151492899918578E8

---
   mtries ntrees sample_rate         model_ids                 mse
19     20    500         0.7 rf_grid2_model_12 6.347692770119294E8
20     22    300         0.7 rf_grid2_model_21 6.361713674445038E8
21     24    400         0.7  rf_grid2_model_6 6.431105576136292E8
22     20    400         0.7  rf_grid2_model_9   6.4353236575248E8
23     22    200         0.7  rf_grid2_model_5 6.477837564818393E8
24     24    200         0.8  rf_grid2_model_1  6.50234231493715E8</pre><p>檢視「RandomDiscrete」grid search結果我們發現，經「隨機」檢視24組參數組合後，最佳的模型MSE只有2.448694 × 10<sup>4</sup>  (v.s. Cartisian grid search找到的2.464371 × 10<sup>4</sup>已非常接近)。且random discrete法只花了約11分鐘(=713/60)的時間(v.s. complete search的15分鐘(=874 sec / 60)。</p>
<p>一旦找到了最佳模型，我們就可以將模型套用在hold-out test set測試資料上來計算最後的「驗證誤差 test error」。結果顯示驗證RMSE誤差為23K，比elastic nets(32K)和bagging(36K)法低了$10K左右。</p><pre class="crayon-plain-tag"># 根據Cartesian法中，選出MSE最低的model，
# Grab the model_id for the top model, chosen by validation error
best_model_id &lt;- grid_perf2@model_ids[[1]]
best_model &lt;- h2o.getModel(best_model_id)</pre><p></p><pre class="crayon-plain-tag"># Now let’s evaluate the model performance on a test set
ames_test.h2o &lt;- as.h2o(ames_test)</pre><p></p><pre class="crayon-plain-tag">best_model_perf &lt;- h2o.performance(model = best_model, newdata = ames_test.h2o)</pre><p></p><pre class="crayon-plain-tag"># RMSE of best model
h2o.mse(best_model_perf) %&gt;% sqrt()</pre><p></p><pre class="crayon-plain-tag">## [1] 23303.05</pre><p></p>
<h3>預測</h3>
<p>一但我們挑出了我們偏好的模型，就像之前一樣，可以使用predict()函數將模型套用在新的資料集上做預測。我們可以來比較所有模型類別的預測效果(randomForest, ranger, h2o)(隨然呈現的結果有稍稍的不一樣)。另外要注意的就是h2o模型使用資料的格式不太依樣。</p>
<p>randomForest</p><pre class="crayon-plain-tag"># randomForest
pred_randomForest &lt;- predict(ames_randomForest, ames_test)
head(pred_randomForest)</pre><p></p><pre class="crayon-plain-tag">##        1        2        3        4        5        6 
## 128454.9 155459.6 264329.4 382519.7 211966.4 214173.0</pre><p>ranger</p><pre class="crayon-plain-tag"># ranger
pred_ranger &lt;- predict(ames_ranger, ames_test)
head(pred_ranger$predictions)</pre><p></p><pre class="crayon-plain-tag">## [1] 128893.3 154095.1 270183.4 389106.2 222629.6 210352.8</pre><p>h2o</p><pre class="crayon-plain-tag"># h2o
pred_h2o &lt;- predict(best_model, ames_test.h2o)</pre><p></p><pre class="crayon-plain-tag">head(pred_h2o)</pre><p></p><pre class="crayon-plain-tag">##    predict
## 1 119430.6
## 2 152900.0
## 3 278208.3
## 4 283966.7
## 5 225166.7
## 6 200583.3</pre><p></p>
<h3>小結</h3>
<ol>
<li>random forest是一個非常強大的「開箱即用」的演算法。不用太多的參數調教就會有相當不錯的預測能力。</li>
<li>此外，random forest也是模型中對資料前處理要求最少的模型，不太需要做資料轉換，是許多解決預測問題時最快能應用的方法。</li>
<li>以bagging為基礎，並透過(1)每棵樹進行bootstrapped sample 與 (2)節點分割時隨機挑選變數清單來「去除樹模型間的相關性」，能有效提升模型的高變異性與提升預測精準度。</li>
</ol>
<hr />
<p>參考文章連結：</p>
<p><a href="http://uc-r.github.io/random_forests" target="_blank" rel="noopener noreferrer">隨機森林 random forest</a></p>
<p>更多Decision Tree相關的統計學習筆記：</p>
<p><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></p>
<p><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a></p>
<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></p>
<p><a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a></p>
<p>更多Regression相關統計學習筆記：</p>
<p><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></p>
<p><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 – 資料探勘與處理 | 統計 R語言</a></p>
<p><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 – 模型建置、診斷與比較 | R語言</a></p>
<p><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 – Ridge, Lasso, Elastic Net | R語言</a></p>
<p>更多Clustering集群分析統計學習筆記：</p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></p>
<p><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a></p>
<p>其他統計學習筆記：</p>
<p><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></p>
<p>這篇文章 <a rel="nofollow" href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>Bounce rate v.s. Exit rate &#124; 跳出率與離開率的差別 &#124; google analytics</title>
		<link>/bounce-rate-exit-rate-%e8%b7%b3%e5%87%ba%e7%8e%87-%e9%9b%a2%e9%96%8b%e7%8e%87-ga/</link>
					<comments>/bounce-rate-exit-rate-%e8%b7%b3%e5%87%ba%e7%8e%87-%e9%9b%a2%e9%96%8b%e7%8e%87-ga/#respond</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Sun, 03 Mar 2019 04:42:46 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計分析]]></category>
		<category><![CDATA[bounce rate]]></category>
		<category><![CDATA[exit rate]]></category>
		<category><![CDATA[GA]]></category>
		<category><![CDATA[google analytics]]></category>
		<category><![CDATA[跳出率]]></category>
		<category><![CDATA[離開率]]></category>
		<guid isPermaLink="false">/?p=2595</guid>

					<description><![CDATA[<p>本篇筆記想要介紹，在使用google analytics分析網站流量狀況時，常見的兩個指標: bounce rate (跳出率)與 exit rate (離開率 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/bounce-rate-exit-rate-%e8%b7%b3%e5%87%ba%e7%8e%87-%e9%9b%a2%e9%96%8b%e7%8e%87-ga/">Bounce rate v.s. Exit rate | 跳出率與離開率的差別 | google analytics</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>本篇筆記想要介紹，在使用google analytics分析網站流量狀況時，常見的兩個指標: bounce rate (跳出率)與 exit rate (離開率) 間的定義與計算差別，分子分母的組成與意義，並分「網站」層次和「個別頁面」層次說明計算邏輯。</p>
<p>&nbsp;</p>
<h3>bounce rate 跳出率</h3>
<p><span style="color: #9f6ad4;">是判斷所有session(工作階段)中，哪些session只看了單一頁面(pageview = 1)即離站的比例。</span></p>
<p>比如說，今天整體網站的session數為100，其中有50個session只看了單一頁面即離站，沒有與網站近一步互動，那整體網站的bounce rate = bounce / session = 50/100 = 50%。而個別頁面的bounce rate = (屬於該頁面的bounce)/(含此頁面的session數)。</p>
<h3> </h3>
<h3><strong>exit rate 離開率</strong></h3>
<p><span style="color: #9f6ad4;">是判斷所有pageview(網頁瀏覽)中，哪些瀏覽是工作階段中的最後一頁的比例。</span></p>
<p>比如說頁面A共有100次瀏覽次數，其中有30次瀏覽是工作階段中的最後一頁(exit)，沒有再繼續瀏覽其他頁面的紀錄，則網頁A的exit rate = exit / pageview = 30/100 = 30%。那整體網站的離開率則是：(所有的頁面加總的exit)/(所有頁面加總的pageview)。</p>
<h3> </h3>
<h3>全站v.s.個別頁面的跳出率與離開率</h3>
<p>我們可以想像原始瀏覽資料的紀錄格式可能如下：</p>
<p>紀錄了基本工作階段流水號(session)、每個工作階段瀏覽頁面的資訊(page ID)、標記每個工作階段是否為跳出(bounce)、標記每個瀏覽頁面是否為該工作階段的最後一頁(exit)。</p>
<table class="wp-block-table has-fixed-layout">
<tbody>
<tr>
<td>session ID</td>
<td>page ID</td>
<td>bounce (0/1)</td>
<td>exit (0/1)</td>
</tr>
<tr>
<td>1</td>
<td>a</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>b</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>c</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>2</td>
<td>a</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><!-- /wp:table -->

<!-- wp:paragraph --></p>
<p><span style="text-decoration: underline;"><strong>計算指標如下：</strong></span></p>
<p>頁面a的跳出率bounce rate = bounce / session = 1/2 = 50%</p>
<p>全站的跳出率bounce rate = sum(bounce)/sum(session) = 1/2 = 50%</p>
<p>頁面a的離開率exit rate = exit / pageview = 1/2 = 50%</p>
<p>全站的離開率exit rate = sum(exit)/sum(pageview) = 2/4 = 50%</p>
<p>&nbsp;</p>
<hr />
<p>參考文章：</p>
<p><a href="https://support.google.com/analytics/answer/2525491?hl=en" target="_blank" rel="noopener noreferrer">Exit Rate vs. Bounce Rate</a></p>
<hr class="wp-block-separator" />
<p>更多有關「流量 traffic」的學習筆記：</p>
<p><a href="/app-mobile-ad-revenue-admob-%e8%a1%8c%e5%8b%95%e5%bb%a3%e5%91%8a%e6%94%b6%e5%85%a5/" target="_blank" rel="noopener noreferrer">開發一個免費App能賺多少錢？ 靠 AdMob 廣告月收3萬實例分享</a></p><p>這篇文章 <a rel="nofollow" href="/bounce-rate-exit-rate-%e8%b7%b3%e5%87%ba%e7%8e%87-%e9%9b%a2%e9%96%8b%e7%8e%87-ga/">Bounce rate v.s. Exit rate | 跳出率與離開率的差別 | google analytics</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/bounce-rate-exit-rate-%e8%b7%b3%e5%87%ba%e7%8e%87-%e9%9b%a2%e9%96%8b%e7%8e%87-ga/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Text Mining &#038; 網路爬蟲 web crawler &#124; Google新聞與文章文字雲 &#124; Python</title>
		<link>/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-text-mining-python/</link>
					<comments>/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-text-mining-python/#respond</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Sun, 03 Mar 2019 03:42:35 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[資料處理]]></category>
		<category><![CDATA[google news]]></category>
		<category><![CDATA[python]]></category>
		<category><![CDATA[text mining]]></category>
		<category><![CDATA[web crawler]]></category>
		<category><![CDATA[word cloud]]></category>
		<category><![CDATA[文字雲]]></category>
		<category><![CDATA[網路爬蟲]]></category>
		<guid isPermaLink="false">/?p=2647</guid>

					<description><![CDATA[<p>本篇學習筆記將要示範如何使用 Python 來執行 網路爬蟲 web crawler 與 basic text mining ，並以爬取Google News  [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-text-mining-python/">Text Mining &#038; 網路爬蟲 web crawler | Google新聞與文章文字雲 | Python</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>本篇學習筆記將要示範如何使用 Python 來執行 網路爬蟲 web crawler 與 basic text mining ，並以爬取<a href="https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0JYcG9MVlJYS0FBUAE?hl=zh-TW&amp;gl=TW&amp;ceid=TW%3Azh-Hant" target="_blank" rel="noopener noreferrer">Google News 的「健康」類別新聞</a>為例。筆記包含以下部分：(1) 爬取新聞標題 (2) 爬取新聞連結 (3) 新聞文章斷詞(jieba)與字詞頻率分析(文字雲) 。</p>
<h2>網路爬蟲 web crawler</h2>
<p>載入套件、指定目標URL、並解析HTML</p><pre class="crayon-plain-tag">import requests 
from bs4 import BeautifulSoup
import pandas as pd


url = 'https://news.google.com/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0JYcG9MVlJYS0FBUAE?hl=zh-TW&amp;gl=TW&amp;ceid=TW%3Azh-Hant'
r = requests.get(url)
web_content = r.text
soup = BeautifulSoup(web_content,'lxml')</pre><p></p>
<h3></h3>
<h3>1. 找出所有新聞的標題</h3>
<p>先檢視新聞標題在HTML的規則。</p>
<p>可以發現：</p>
<ul>
<li>每個文章小卡都是包覆在class = &#8216;xrnccd&#8217;的div物件中
<ul>
<li>整個文章小卡點擊區塊連結: class = &#8220;VDXfz&#8221;</li>
</ul>
</li>
<li>文章小卡中的標題則是包覆在class為&#8217;mEaVNd&#8217;的div物件中
<ul>
<li>標題文字則是在&lt;span&gt;&lt;/span&gt;標籤中</li>
</ul>
</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-2670" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.33.06-1.png" alt="螢幕快照 2019-03-03 上午9.33.06" width="1599" height="548" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.33.06-1.png 1599w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.33.06-1-300x103.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.33.06-1-768x263.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.33.06-1-1024x351.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.33.06-1-830x284.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.33.06-1-230x79.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.33.06-1-350x120.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.33.06-1-480x165.png 480w" sizes="(max-width: 1599px) 100vw, 1599px" /></p>
<p>找出所有文章小卡中class = &#8216;mEaVNd&#8217;的「標題區塊」的元素並儲存為一個list (named title)</p>
<p>list中每一個「標題區塊」元素是以逗號做區隔</p><pre class="crayon-plain-tag">title = soup.find_all('div', class_='mEaVNd')
print(title)</pre><p><img loading="lazy" class="alignnone size-full wp-image-2667" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.26.02.png" alt="螢幕快照 2019-03-03 上午9.26.02" width="889" height="484" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.26.02.png 889w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.26.02-300x163.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.26.02-768x418.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.26.02-830x452.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.26.02-230x125.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.26.02-350x191.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午9.26.02-480x261.png 480w" sizes="(max-width: 889px) 100vw, 889px" /></p>
<p>再從「標題區塊」list中，找出每個元素的&lt;span&gt;&lt;/span&gt;標籤，並萃取出標題文字</p><pre class="crayon-plain-tag">titles = [t.find('span').text for t in title]
titles</pre><p></p><pre class="crayon-plain-tag">['腰頸常疼痛嗎？研究發現這或許與糖尿病有關',
 '不認真刷牙？日本專家：忽略口腔護理，恐引發致命疾病',
 '不舉、憂鬱、沒精神，恐怕是「男性更年期」來報到？醫生：出現這8種症狀，趕快去檢查',
 '一餐吃十幾塊蘿蔔糕，血糖就意外飆高…營養師4大建議，吃東西真的要謹慎啊！',
 '男性更年期同樣讓人崩潰！50歲上下都得注意，醫師3招緩解情緒不穩、早洩問題',
 '原來做巧克力餅乾這麼簡單！微波爐5分鐘搞定，颱風天在家做做看吧']</pre><p></p>
<h3></h3>
<h3>2. 找出新聞標題所對應的資料來源links</h3>
<p>新聞連結的取得是比較tricky的部分，<span style="text-decoration: underline;">因為google news網頁中html &lt;a&gt; &lt;/a&gt;標籤中的連結是經過另外轉換的</span>，<span style="text-decoration: underline;"><strong><span style="color: #9d6ad4; text-decoration: underline;">所以需要request.get()另外取得轉換後的真實文章URL</span></strong></span>。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2650" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.58.10.png" alt="web-crawler-text-mining" width="2536" height="1074" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.58.10.png 2536w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.58.10-300x127.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.58.10-768x325.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.58.10-1024x434.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.58.10-830x352.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.58.10-230x97.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.58.10-350x148.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.58.10-480x203.png 480w" sizes="(max-width: 2536px) 100vw, 2536px" /></p>
<p>取得新聞文章真實URL:</p>
<ol>
<li>找出標題區塊&lt;a&gt;&lt;/a&gt;標籤並<span style="color: #9d6ad4;">萃取出href的值</span></li>
<li><span style="color: #9d6ad4;">將href值中&#8217;.&#8217;取代為當下的domain</span>，得到<span style="text-decoration: underline;">須經轉換的URL</span></li>
<li>將<span style="text-decoration: underline;">須經轉換的URL</span>丟入<span style="color: #9d6ad4;">request.get()</span>函式中，並使用<span style="color: #9f6ad4;">.url</span>取得文章真實的URL</li>
</ol>
<p></p><pre class="crayon-plain-tag">newUrls = [requests.get(t.find('a')['href'].replace('.','https://news.google.com',1)).url for t in title]
newUrls</pre><p></p><pre class="crayon-plain-tag">['https://tw.news.yahoo.com/%E8%85%B0%E9%A0%B8%E5%B8%B8%E7%96%BC%E7%97%9B%E5%97%8E-%E7%A0%94%E7%A9%B6%E7%99%BC%E7%8F%BE%E9%80%99%E6%88%96%E8%A8%B1%E8%88%87%E7%B3%96%E5%B0%BF%E7%97%85%E6%9C%89%E9%97%9C-090011577.html',
 'https://n.yam.com/Article/20190301775078',
 'https://www.storm.mg/lifestyle/469667',
 'https://www.storm.mg/lifestyle/219203',
 'https://www.storm.mg/lifestyle/294616',
 'https://www.storm.mg/lifestyle/110852']</pre><p></p>
<h3></h3>
<h3>3. 將標題與連結合併成data frame</h3>
<p>這樣就能新聞標題和真實出處URL做一個簡易的資訊表格整理</p><pre class="crayon-plain-tag">df = pd.DataFrame(
{
    'title': titles,
    'links': newUrls
})

df</pre><p><img loading="lazy" class="alignnone size-full wp-image-2651" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.02.11.png" alt="web-crawler-text-mining" width="1718" height="442" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.02.11.png 1718w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.02.11-300x77.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.02.11-768x198.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.02.11-1024x263.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.02.11-830x214.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.02.11-230x59.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.02.11-350x90.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.02.11-480x123.png 480w" sizes="(max-width: 1718px) 100vw, 1718px" /></p>
<p>其他爬蟲結果輸出可以參考「<a href="/python-web-crawler-beautifulsoup-%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2/" target="_blank" rel="noopener noreferrer">網路爬蟲 Web Crawler | 資料不求人 基礎篇 | using Python BeautifulSoup</a>」。</p>
<p>&nbsp;</p>
<h2>Text mining 簡易字頻分析 (文字雲 word cloud)</h2>
<h3>4. 使用jieba套件，進行文章中文斷詞分析</h3>
<p>先使用某一篇文章內文為範例，我們使用剛剛新聞清單的<a href="https://tw.news.yahoo.com/%E8%85%B0%E9%A0%B8%E5%B8%B8%E7%96%BC%E7%97%9B%E5%97%8E-%E7%A0%94%E7%A9%B6%E7%99%BC%E7%8F%BE%E9%80%99%E6%88%96%E8%A8%B1%E8%88%87%E7%B3%96%E5%B0%BF%E7%97%85%E6%9C%89%E9%97%9C-090011577.html" target="_blank" rel="noopener noreferrer">第一個連結</a>去得到文章內容。並以該文章內容進行字頻分析(文字雲)。</p><pre class="crayon-plain-tag">url = df['links'][0]
print(url)
r = requests.get(url)
web_content = r.text
soup = BeautifulSoup(web_content,'lxml')</pre><p>看一下文章段落在HTML的規則：</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2652" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.06.00.png" alt="web-crawler-text-mining" width="2522" height="1190" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.06.00.png 2522w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.06.00-300x142.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.06.00-768x362.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.06.00-1024x483.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.06.00-830x392.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.06.00-230x109.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.06.00-350x165.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午4.06.00-480x226.png 480w" sizes="(max-width: 2522px) 100vw, 2522px" /></p>
<p>擷取文章內容如下:</p><pre class="crayon-plain-tag">articleContent = soup.find_all('p')
articleContent</pre><p></p><pre class="crayon-plain-tag">[&lt;p&gt;&lt;span class=""&gt;腰頸疼痛是常見的肌肉骨骼疾病，根據統計，大約80%的成年人會經歷腰痛，47%的人會經歷脖子痛。同樣，糖尿病也是一種越來越普遍的慢性病，​​全球大約有3.82億人患有這種代謝疾病。&lt;/span&gt;&lt;/p&gt;,
 &lt;p&gt;近日，一項刊載於《PLOS ONE》期刊的研究指出，與沒有糖尿病的人相比，糖尿病患者發生腰痛的風險要高35%，脖子痛的風險要高24%。糖尿病患者更容易出現慢性軀體疼痛，包括糖尿病周圍神經病變。而且，同時患有糖尿病和肌肉骨骼疼痛，會導致更高程度的疼痛、殘疾和心理困擾，其所帶來的負擔遠遠大於僅患有其中一種疾病。&lt;/p&gt;,
 &lt;p&gt;而糖尿病和背痛也有共同的危險因素，比如說肥胖、缺乏體育。年輕的時候身體質量指數（BMI）越高的人，在晚年患糖尿病的可能性，要高出9倍以上；同樣的，肥胖是導致嚴重腰痛的重要因子。糖尿病患者也不太可能經常參加運動活動，因此有較高的風險發展成慢性肌肉骨骼疾病，如背部和頸部疼痛。&lt;/p&gt;,
 &lt;p&gt;過去的研究表明，糖尿病患者可能會因為繼發性糖尿病相關的腰椎間盤微血管病變，而發展為腰椎間盤疾病。此外，糖尿病動物模型中，椎間盤退變更佳迅速。&lt;/p&gt;,
 &lt;p&gt;該論文的作者Manuela Ferreira副教授說，目前還沒有足夠的證據證明糖尿病與腰頸疼痛之間存在因果關係，但這種積極聯繫值得進一步調查和研究。論文還建議醫護人員，應該考慮對長期頸痛或腰痛患者進行糖尿病篩查。&lt;/p&gt;,
 &lt;p&gt;「越來越多的人同時受到頸背疼痛和糖尿病的折磨，」論文共同作者Paulo Ferreira說道，「所以，值得投入更多的資源來研究它們之間的相互關係。改變糖尿病的治療干預措施可，或許以減少背痛的發病率，反之亦然。」&lt;/p&gt;,
 &lt;p&gt;&lt;strong&gt;參考資料：&lt;/strong&gt;&lt;a class="link rapid-noclick-resp" data-ylk="slk:Study links diabetes and back pain" href="https://medicalxpress.com/news/2019-02-links-diabetes-pain.html" rel="nofollow noopener" target="_blank"&gt;Study links diabetes and back pain&lt;/a&gt;&lt;/p&gt;,
 &lt;p&gt;&lt;strong&gt;期刊小檔案：&lt;/strong&gt;《PLOS ONE》為一份同行評審的開放獲取科學期刊，由公共科學圖書館自2006年發行，為全世界文章刊載數量最多的期刊，所刊載的文章包含科學及醫學各領域的基礎研究。&lt;/p&gt;,
 &lt;p&gt;&lt;span&gt;更多Heho健康網文章&lt;/span&gt;&lt;br/&gt;&lt;a class="link rapid-noclick-resp" data-ylk="slk:糖友肺炎風險高2.8倍 醫：加打肺炎鏈球菌疫苗保健康" href="https://heho.com.tw/archives/40152" rel="nofollow noopener" target="_blank"&gt;糖友肺炎風險高2.8倍 醫：加打肺炎鏈球菌疫苗保健康&lt;/a&gt;&lt;br/&gt;&lt;a class="link rapid-noclick-resp" data-ylk="slk:讓其他細胞分泌胰島素 動物實驗逆轉糖尿病！" href="https://heho.com.tw/archives/39757" rel="nofollow noopener" target="_blank"&gt;讓其他細胞分泌胰島素 動物實驗逆轉糖尿病！&lt;/a&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;,
 &lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;,
 &lt;p&gt;文／林以璿 圖／許嘉真&lt;/p&gt;]</pre><p>將所有&lt;p&gt;&lt;/p&gt;中的text取出並放進list。</p><pre class="crayon-plain-tag"># 將所有tag p中的text取出並放進list。
article = []
for p in articleContent:
    article.append(p.text)</pre><p>將list中的text元素join合併成一個字串，並以特殊字元&#8217;\n&#8217;來間隔 (主要是印出好看用)</p><pre class="crayon-plain-tag">articleAll = '\n'.join(article)
print(articleAll)</pre><p></p><pre class="crayon-plain-tag">腰頸疼痛是常見的肌肉骨骼疾病，根據統計，大約80%的成年人會經歷腰痛，47%的人會經歷脖子痛。同樣，糖尿病也是一種越來越普遍的慢性病，​​全球大約有3.82億人患有這種代謝疾病。
近日，一項刊載於《PLOS ONE》期刊的研究指出，與沒有糖尿病的人相比，糖尿病患者發生腰痛的風險要高35%，脖子痛的風險要高24%。糖尿病患者更容易出現慢性軀體疼痛，包括糖尿病周圍神經病變。而且，同時患有糖尿病和肌肉骨骼疼痛，會導致更高程度的疼痛、殘疾和心理困擾，其所帶來的負擔遠遠大於僅患有其中一種疾病。
而糖尿病和背痛也有共同的危險因素，比如說肥胖、缺乏體育。年輕的時候身體質量指數（BMI）越高的人，在晚年患糖尿病的可能性，要高出9倍以上；同樣的，肥胖是導致嚴重腰痛的重要因子。糖尿病患者也不太可能經常參加運動活動，因此有較高的風險發展成慢性肌肉骨骼疾病，如背部和頸部疼痛。
過去的研究表明，糖尿病患者可能會因為繼發性糖尿病相關的腰椎間盤微血管病變，而發展為腰椎間盤疾病。此外，糖尿病動物模型中，椎間盤退變更佳迅速。
該論文的作者Manuela Ferreira副教授說，目前還沒有足夠的證據證明糖尿病與腰頸疼痛之間存在因果關係，但這種積極聯繫值得進一步調查和研究。論文還建議醫護人員，應該考慮對長期頸痛或腰痛患者進行糖尿病篩查。
「越來越多的人同時受到頸背疼痛和糖尿病的折磨，」論文共同作者Paulo Ferreira說道，「所以，值得投入更多的資源來研究它們之間的相互關係。改變糖尿病的治療干預措施可，或許以減少背痛的發病率，反之亦然。」
參考資料：Study links diabetes and back pain
期刊小檔案：《PLOS ONE》為一份同行評審的開放獲取科學期刊，由公共科學圖書館自2006年發行，為全世界文章刊載數量最多的期刊，所刊載的文章包含科學及醫學各領域的基礎研究。
更多Heho健康網文章糖友肺炎風險高2.8倍 醫：加打肺炎鏈球菌疫苗保健康讓其他細胞分泌胰島素 動物實驗逆轉糖尿病！</pre><p>載入斷詞分析的套件</p><pre class="crayon-plain-tag">import jieba
import nltk</pre><p>指定字典(使用預設字典)</p><pre class="crayon-plain-tag">jieba.load_userdict('/user_dict.txt') # 輸入自己字典的路徑</pre><p>移除標點符號 punctuation removal</p><pre class="crayon-plain-tag">d = c.replace('[^\w\s]','').replace('／',"").replace('《','').replace('》','').replace('，','').replace('。','').replace('「','').replace('」','').replace('（','').replace('）','').replace('！','').replace('？','').replace('、','').replace('▲','').replace('…','').replace('：','')
print(d)</pre><p></p><pre class="crayon-plain-tag">腰頸疼痛是常見的肌肉骨骼疾病根據統計大約80%的成年人會經歷腰痛47%的人會經歷脖子痛同樣糖尿病也是一種越來越普遍的慢性病​​全球大約有3.82億人患有這種代謝疾病
近日一項刊載於PLOS ONE期刊的研究指出與沒有糖尿病的人相比糖尿病患者發生腰痛的風險要高35%脖子痛的風險要高24%糖尿病患者更容易出現慢性軀體疼痛包括糖尿病周圍神經病變而且同時患有糖尿病和肌肉骨骼疼痛會導致更高程度的疼痛殘疾和心理困擾其所帶來的負擔遠遠大於僅患有其中一種疾病
而糖尿病和背痛也有共同的危險因素比如說肥胖缺乏體育年輕的時候身體質量指數BMI越高的人在晚年患糖尿病的可能性要高出9倍以上；同樣的肥胖是導致嚴重腰痛的重要因子糖尿病患者也不太可能經常參加運動活動因此有較高的風險發展成慢性肌肉骨骼疾病如背部和頸部疼痛
過去的研究表明糖尿病患者可能會因為繼發性糖尿病相關的腰椎間盤微血管病變而發展為腰椎間盤疾病此外糖尿病動物模型中椎間盤退變更佳迅速
該論文的作者Manuela Ferreira副教授說目前還沒有足夠的證據證明糖尿病與腰頸疼痛之間存在因果關係但這種積極聯繫值得進一步調查和研究論文還建議醫護人員應該考慮對長期頸痛或腰痛患者進行糖尿病篩查
越來越多的人同時受到頸背疼痛和糖尿病的折磨論文共同作者Paulo Ferreira說道所以值得投入更多的資源來研究它們之間的相互關係改變糖尿病的治療干預措施可或許以減少背痛的發病率反之亦然
參考資料Study links diabetes and back pain
期刊小檔案PLOS ONE為一份同行評審的開放獲取科學期刊由公共科學圖書館自2006年發行為全世界文章刊載數量最多的期刊所刊載的文章包含科學及醫學各領域的基礎研究
更多Heho健康網文章糖友肺炎風險高2.8倍 醫加打肺炎鏈球菌疫苗保健康讓其他細胞分泌胰島素 動物實驗逆轉糖尿病

文林以璿 圖許嘉真</pre><p>避免過多的文字log訊息出現</p><pre class="crayon-plain-tag">jieba.setLogLevel(20)</pre><p>分別使用不同斷詞模式</p><pre class="crayon-plain-tag">Sentence = jieba.cut(d, cut_all=True)
print('全模式'+": "  + "/ ".join(Sentence) + '\n')   

Sentence = jieba.cut(d, cut_all=False)
print('精確模式'+": " + "/ ".join(Sentence)+ '\n')  

Sentence = jieba.cut(d)  
print('Default為精確模式'+": " + "/ ".join(Sentence)+ '\n')

Sentence = jieba.cut_for_search(d)  
print('搜索引擎模式'+": " + "/ ".join(Sentence)+ '\n')</pre><p></p><pre class="crayon-plain-tag">全模式: 腰/ 頸/ 疼痛/ 是/ 常/ 見/ 的/ 肌肉/ 骨骼/ 疾病/ 病根/ 據/ 統/ 計/ 大/ 約/ 80/ / 的/ 成年/ 成年人/ 會/ 經/ 歷/ 腰痛/ 47/ / 的/ 人/ 會/ 經/ 歷/ 脖子/ 痛/ 同/ 樣/ 糖尿/ 糖尿病/ 也/ 是/ 一/ 種/ 越/ 來/ 越/ 普遍/ 的/ 慢性/ 慢性病/ 性病/ / / / 全球/ 大/ 約/ 有/ 3/ 82/ 億/ 人/ 患有/ 這/ 種/ 代/ 謝/ 疾病/ 
/ 近日/ 一/ 項/ 刊/ 載/ 於/ PLOS/ ONE/ 期刊/ 的/ 研究/ 指出/ 與/ 沒/ 有/ 糖尿/ 糖尿病/ 的/ 人/ 相比/ 糖尿/ 糖尿病/ 病患/ 病患者/ 患者/ 發/ 生/ 腰痛/ 的/ 風/ 險/ 要/ 高/ 35/ / 脖子/ 痛/ 的/ 風/ 險/ 要/ 高/ 24/ / 糖尿/ 糖尿病/ 病患/ 病患者/ 患者/ 更/ 容易/ 出/ 現/ 慢性/ 軀/ 體/ 疼痛/ 包括/ 糖尿/ 糖尿病/ 周/ 圍/ 神/ 經/ 病/ 變/ 而且/ 同/ 時/ 患有/ 糖尿/ 糖尿病/ 和/ 肌肉/ 骨骼/ 疼痛/ 會/ 導/ 致/ 更/ 高程/ 程度/ 的/ 疼痛/ 殘/ 疾/ 和/ 心理/ 困/ 擾/ 其/ 所/ 帶/ 來/ 的/ 負/ 擔/ 遠/ 遠/ 大/ 於/ 僅/ 患有/ 其中/ 一/ 種/ 疾病/ 
/ 而/ 糖尿/ 糖尿病/ 和/ 背痛/ 也/ 有/ 共同/ 的/ 危/ 險/ 因素/ 比如/ 說/ 肥胖/ 缺乏/ 體/ 育/ 年/ 輕/ 的/ 時/ 候/ 身/ 體/ 質/ 量/ 指/ 數/ BMI/ 越/ 高/ 的/ 人/ 在/ 晚年/ 患/ 糖尿/ 糖尿病/ 的/ 可能/ 可能性/ 要/ 高出/ 9/ 倍/ 以上/ / / 同/ 樣/ 的/ 肥胖/ 是/ 導/ 致/ 嚴/ 重/ 腰痛/ 的/ 重要/ 因子/ 糖尿/ 糖尿病/ 病患/ 病患者/ 患者/ 也/ 不太可能/ 可能/ 經/ 常/ 參/ 加/ 運/ 動/ 活/ 動/ 因此/ 有/ 較/ 高/ 的/ 風/ 險/ 發/ 展/ 成/ 慢性/ 肌肉/ 骨骼/ 疾病/ 如/ 背部/ 和/ 頸/ 部/ 疼痛/ 
/ 過/ 去/ 的/ 研究/ 表明/ 糖尿/ 糖尿病/ 病患/ 病患者/ 患者/ 可能/ 會/ 因/ 為/ 繼/ 發/ 性/ 糖尿/ 糖尿病/ 相/ 關/ 的/ 腰椎/ 間/ 盤/ 微血管/ 血管/ 血管病/ 變/ 而/ 發/ 展/ 為/ 腰椎/ 間/ 盤/ 疾病/ 此外/ 糖尿/ 糖尿病/ 動/ 物/ 模型/ 中/ 椎/ 間/ 盤/ 退/ 變/ 更佳/ 迅速/ 
/ 該/ 論/ 文/ 的/ 作者/ Manuela/ Ferreira/ 副教授/ 教授/ 說/ 目前/ 還/ 沒/ 有/ 足/ 夠/ 的/ 證/ 據/ 證/ 明/ 糖尿/ 糖尿病/ 與/ 腰/ 頸/ 疼痛/ 之/ 間/ 存在/ 因果/ 關/ 係/ 但/ 這/ 種/ 積/ 極/ 聯/ 繫/ 值得/ 進/ 一步/ 調/ 查/ 和/ 研究/ 論/ 文/ 還/ 建/ 議/ 醫/ 護/ 人/ 員/ 應/ 該/ 考/ 慮/ 對/ 長/ 期/ 頸/ 痛/ 或/ 腰痛/ 患者/ 進/ 行/ 糖尿/ 糖尿病/ 篩/ 查/ 
/ 越/ 來/ 越多/ 的/ 人/ 同/ 時/ 受到/ 頸/ 背/ 疼痛/ 和/ 糖尿/ 糖尿病/ 的/ 折磨/ 論/ 文/ 共同/ 作者/ Paulo/ Ferreira/ 說/ 道/ 所以/ 值得/ 投入/ 更多/ 的/ 資/ 源/ 來/ 研究/ 它/ 們/ 之/ 間/ 的/ 相互/ 關/ 係/ 改/ 變/ 糖尿/ 糖尿病/ 的/ 治療/ 干/ 預/ 措施/ 可/ 或/ 許/ 以/ 減/ 少背/ 背痛/ 的/ 發/ 病/ 率/ 反之/ 反之亦然/ 亦然/ 
/ 參/ 考/ 資/ 料/ Study/ links/ diabetes/ and/ back/ pain
/ 期刊/ 小/ 檔/ 案/ PLOS/ ONE/ 為/ 一份/ 同行/ 評/ 審/ 的/ 開/ 放/ 獲/ 取/ 科/ 學/ 期刊/ 由/ 公共/ 科/ 學/ 圖/ 書/ 館/ 自/ 2006/ 年/ 發/ 行/ 為/ 全世界/ 世界/ 文章/ 刊/ 載/ 數/ 量/ 最多/ 的/ 期刊/ 所/ 刊/ 載/ 的/ 文章/ 包含/ 科/ 學/ 及/ 醫/ 學/ 各/ 領/ 域/ 的/ 基/ 礎/ 研究/ 
/ 更多/ Heho/ 健康/ 網/ 文章/ 糖/ 友/ 肺炎/ 風/ 險/ 高/ 2/ 8/ 倍/ / / 醫/ 加打/ 肺炎/ 鏈/ 球菌/ 疫苗/ 保健/ 健康/ 讓/ 其他/ 細/ 胞/ 分泌/ 胰/ 島/ 素/ / / 動/ 物/ 實/ 驗/ 逆/ 轉/ 糖尿/ 糖尿病/ 

/ 文林/ 以/ 璿/ / / 圖/ 許/ 嘉/ 真

精確模式: 腰頸/ 疼痛/ 是/ 常見/ 的/ 肌肉/ 骨骼/ 疾病/ 根據/ 統計大約/ 80%/ 的/ 成年人/ 會經歷/ 腰痛/ 47%/ 的/ 人會/ 經歷/ 脖子/ 痛同樣/ 糖尿病/ 也/ 是/ 一種/ 越來/ 越/ 普遍/ 的/ 慢性病/ ​/ ​/ 全球/ 大約/ 有/ 3.82/ 億人/ 患有/ 這種/ 代謝/ 疾病/ 
/ 近日/ 一項/ 刊載/ 於/ PLOS/  / ONE/ 期刊/ 的/ 研究/ 指出/ 與/ 沒/ 有/ 糖尿病/ 的/ 人/ 相比/ 糖尿病/ 患者/ 發生/ 腰痛/ 的/ 風險/ 要/ 高/ 35%/ 脖子/ 痛/ 的/ 風險/ 要/ 高/ 24%/ 糖尿病/ 患者/ 更/ 容易/ 出現/ 慢性/ 軀體/ 疼痛/ 包括/ 糖尿病/ 周圍/ 神經/ 病變/ 而且/ 同時/ 患有/ 糖尿病/ 和/ 肌肉/ 骨骼/ 疼痛/ 會導致/ 更/ 高/ 程度/ 的/ 疼痛/ 殘疾/ 和/ 心理/ 困擾/ 其/ 所/ 帶/ 來/ 的/ 負擔/ 遠遠大/ 於僅/ 患有/ 其中/ 一種/ 疾病/ 
/ 而/ 糖尿病/ 和/ 背痛/ 也/ 有/ 共同/ 的/ 危險/ 因素/ 比如/ 說/ 肥胖/ 缺乏/ 體育/ 年/ 輕/ 的/ 時候/ 身體/ 質量/ 指數/ BMI/ 越高/ 的/ 人/ 在/ 晚年/ 患/ 糖尿病/ 的/ 可能性/ 要/ 高出/ 9/ 倍/ 以上/ ；/ 同樣/ 的/ 肥胖/ 是/ 導致/ 嚴重/ 腰痛/ 的/ 重要/ 因子/ 糖尿病/ 患者/ 也/ 不太可能/ 經常/ 參加/ 運動/ 活動/ 因此/ 有/ 較/ 高/ 的/ 風險/ 發展/ 成/ 慢性/ 肌肉/ 骨骼/ 疾病/ 如/ 背部/ 和/ 頸部/ 疼痛/ 
/ 過去/ 的/ 研究/ 表明/ 糖尿病/ 患者/ 可能/ 會/ 因為/ 繼發性/ 糖尿病/ 相關/ 的/ 腰椎/ 間盤/ 微血管/ 病變/ 而/ 發展/ 為/ 腰椎/ 間盤/ 疾病/ 此外/ 糖尿病/ 動物/ 模型/ 中椎間/ 盤/ 退變/ 更佳/ 迅速/ 
/ 該論文/ 的/ 作者/ Manuela/  / Ferreira/ 副教授/ 說/ 目前/ 還沒有/ 足夠/ 的/ 證據/ 證明/ 糖尿病/ 與/ 腰/ 頸/ 疼痛/ 之間/ 存在/ 因果/ 關/ 係/ 但/ 這種/ 積極/ 聯/ 繫/ 值得/ 進/ 一步/ 調查/ 和/ 研究/ 論文/ 還建議/ 醫護/ 人員應/ 該/ 考慮/ 對長/ 期頸痛/ 或/ 腰痛/ 患者/ 進行/ 糖尿病/ 篩查/ 
/ 越來/ 越/ 多/ 的/ 人/ 同時/ 受到/ 頸/ 背/ 疼痛/ 和/ 糖尿病/ 的/ 折磨/ 論文/ 共同/ 作者/ Paulo/  / Ferreira/ 說道/ 所以/ 值得/ 投入/ 更/ 多/ 的/ 資源/ 來/ 研究/ 它們/ 之間/ 的/ 相互/ 關/ 係/ 改變/ 糖尿病/ 的/ 治療/ 干預/ 措施/ 可/ 或/ 許以/ 減少/ 背痛/ 的/ 發病率/ 反之亦然/ 
/ 參考/ 資料/ Study/  / links/  / diabetes/  / and/  / back/  / pain/ 
/ 期刊/ 小檔案/ PLOS/  / ONE/ 為/ 一份/ 同行/ 評審/ 的/ 開放/ 獲取/ 科學/ 期刊/ 由/ 公共/ 科學/ 圖書館/ 自/ 2006/ 年/ 發行/ 為/ 全世界/ 文章/ 刊載/ 數量/ 最/ 多/ 的/ 期刊/ 所刊/ 載/ 的/ 文章/ 包含/ 科學及/ 醫學/ 各/ 領域/ 的/ 基礎/ 研究/ 
/ 更/ 多/ Heho/ 健康/ 網/ 文章/ 糖友/ 肺炎/ 風險/ 高/ 2.8/ 倍/  / 醫加/ 打/ 肺炎/ 鏈/ 球菌/ 疫苗/ 保/ 健康/ 讓/ 其他/ 細胞/ 分泌/ 胰島素/  / 動物/ 實驗/ 逆轉/ 糖尿病/ 
/ 
/ 文林/ 以/ 璿/  / 圖許/ 嘉真

Default為精確模式: 腰頸/ 疼痛/ 是/ 常見/ 的/ 肌肉/ 骨骼/ 疾病/ 根據/ 統計大約/ 80%/ 的/ 成年人/ 會經歷/ 腰痛/ 47%/ 的/ 人會/ 經歷/ 脖子/ 痛同樣/ 糖尿病/ 也/ 是/ 一種/ 越來/ 越/ 普遍/ 的/ 慢性病/ ​/ ​/ 全球/ 大約/ 有/ 3.82/ 億人/ 患有/ 這種/ 代謝/ 疾病/ 
/ 近日/ 一項/ 刊載/ 於/ PLOS/  / ONE/ 期刊/ 的/ 研究/ 指出/ 與/ 沒/ 有/ 糖尿病/ 的/ 人/ 相比/ 糖尿病/ 患者/ 發生/ 腰痛/ 的/ 風險/ 要/ 高/ 35%/ 脖子/ 痛/ 的/ 風險/ 要/ 高/ 24%/ 糖尿病/ 患者/ 更/ 容易/ 出現/ 慢性/ 軀體/ 疼痛/ 包括/ 糖尿病/ 周圍/ 神經/ 病變/ 而且/ 同時/ 患有/ 糖尿病/ 和/ 肌肉/ 骨骼/ 疼痛/ 會導致/ 更/ 高/ 程度/ 的/ 疼痛/ 殘疾/ 和/ 心理/ 困擾/ 其/ 所/ 帶/ 來/ 的/ 負擔/ 遠遠大/ 於僅/ 患有/ 其中/ 一種/ 疾病/ 
/ 而/ 糖尿病/ 和/ 背痛/ 也/ 有/ 共同/ 的/ 危險/ 因素/ 比如/ 說/ 肥胖/ 缺乏/ 體育/ 年/ 輕/ 的/ 時候/ 身體/ 質量/ 指數/ BMI/ 越高/ 的/ 人/ 在/ 晚年/ 患/ 糖尿病/ 的/ 可能性/ 要/ 高出/ 9/ 倍/ 以上/ ；/ 同樣/ 的/ 肥胖/ 是/ 導致/ 嚴重/ 腰痛/ 的/ 重要/ 因子/ 糖尿病/ 患者/ 也/ 不太可能/ 經常/ 參加/ 運動/ 活動/ 因此/ 有/ 較/ 高/ 的/ 風險/ 發展/ 成/ 慢性/ 肌肉/ 骨骼/ 疾病/ 如/ 背部/ 和/ 頸部/ 疼痛/ 
/ 過去/ 的/ 研究/ 表明/ 糖尿病/ 患者/ 可能/ 會/ 因為/ 繼發性/ 糖尿病/ 相關/ 的/ 腰椎/ 間盤/ 微血管/ 病變/ 而/ 發展/ 為/ 腰椎/ 間盤/ 疾病/ 此外/ 糖尿病/ 動物/ 模型/ 中椎間/ 盤/ 退變/ 更佳/ 迅速/ 
/ 該論文/ 的/ 作者/ Manuela/  / Ferreira/ 副教授/ 說/ 目前/ 還沒有/ 足夠/ 的/ 證據/ 證明/ 糖尿病/ 與/ 腰/ 頸/ 疼痛/ 之間/ 存在/ 因果/ 關/ 係/ 但/ 這種/ 積極/ 聯/ 繫/ 值得/ 進/ 一步/ 調查/ 和/ 研究/ 論文/ 還建議/ 醫護/ 人員應/ 該/ 考慮/ 對長/ 期頸痛/ 或/ 腰痛/ 患者/ 進行/ 糖尿病/ 篩查/ 
/ 越來/ 越/ 多/ 的/ 人/ 同時/ 受到/ 頸/ 背/ 疼痛/ 和/ 糖尿病/ 的/ 折磨/ 論文/ 共同/ 作者/ Paulo/  / Ferreira/ 說道/ 所以/ 值得/ 投入/ 更/ 多/ 的/ 資源/ 來/ 研究/ 它們/ 之間/ 的/ 相互/ 關/ 係/ 改變/ 糖尿病/ 的/ 治療/ 干預/ 措施/ 可/ 或/ 許以/ 減少/ 背痛/ 的/ 發病率/ 反之亦然/ 
/ 參考/ 資料/ Study/  / links/  / diabetes/  / and/  / back/  / pain/ 
/ 期刊/ 小檔案/ PLOS/  / ONE/ 為/ 一份/ 同行/ 評審/ 的/ 開放/ 獲取/ 科學/ 期刊/ 由/ 公共/ 科學/ 圖書館/ 自/ 2006/ 年/ 發行/ 為/ 全世界/ 文章/ 刊載/ 數量/ 最/ 多/ 的/ 期刊/ 所刊/ 載/ 的/ 文章/ 包含/ 科學及/ 醫學/ 各/ 領域/ 的/ 基礎/ 研究/ 
/ 更/ 多/ Heho/ 健康/ 網/ 文章/ 糖友/ 肺炎/ 風險/ 高/ 2.8/ 倍/  / 醫加/ 打/ 肺炎/ 鏈/ 球菌/ 疫苗/ 保/ 健康/ 讓/ 其他/ 細胞/ 分泌/ 胰島素/  / 動物/ 實驗/ 逆轉/ 糖尿病/ 
/ 
/ 文林/ 以/ 璿/  / 圖許/ 嘉真

搜索引擎模式: 腰頸/ 疼痛/ 是/ 常見/ 的/ 肌肉/ 骨骼/ 疾病/ 根據/ 統計大約/ 80%/ 的/ 成年/ 成年人/ 會經歷/ 腰痛/ 47%/ 的/ 人會/ 經歷/ 脖子/ 痛同樣/ 糖尿/ 糖尿病/ 也/ 是/ 一種/ 越來/ 越/ 普遍/ 的/ 慢性/ 性病/ 慢性病/ ​/ ​/ 全球/ 大約/ 有/ 3.82/ 億人/ 患有/ 這種/ 代謝/ 疾病/ 
/ 近日/ 一項/ 刊載/ 於/ PLOS/  / ONE/ 期刊/ 的/ 研究/ 指出/ 與/ 沒/ 有/ 糖尿/ 糖尿病/ 的/ 人/ 相比/ 糖尿/ 糖尿病/ 患者/ 發生/ 腰痛/ 的/ 風險/ 要/ 高/ 35%/ 脖子/ 痛/ 的/ 風險/ 要/ 高/ 24%/ 糖尿/ 糖尿病/ 患者/ 更/ 容易/ 出現/ 慢性/ 軀體/ 疼痛/ 包括/ 糖尿/ 糖尿病/ 周圍/ 神經/ 病變/ 而且/ 同時/ 患有/ 糖尿/ 糖尿病/ 和/ 肌肉/ 骨骼/ 疼痛/ 會導致/ 更/ 高/ 程度/ 的/ 疼痛/ 殘疾/ 和/ 心理/ 困擾/ 其/ 所/ 帶/ 來/ 的/ 負擔/ 遠遠大/ 於僅/ 患有/ 其中/ 一種/ 疾病/ 
/ 而/ 糖尿/ 糖尿病/ 和/ 背痛/ 也/ 有/ 共同/ 的/ 危險/ 因素/ 比如/ 說/ 肥胖/ 缺乏/ 體育/ 年/ 輕/ 的/ 時候/ 身體/ 質量/ 指數/ BMI/ 越高/ 的/ 人/ 在/ 晚年/ 患/ 糖尿/ 糖尿病/ 的/ 可能/ 可能性/ 要/ 高出/ 9/ 倍/ 以上/ ；/ 同樣/ 的/ 肥胖/ 是/ 導致/ 嚴重/ 腰痛/ 的/ 重要/ 因子/ 糖尿/ 糖尿病/ 患者/ 也/ 可能/ 不太可能/ 經常/ 參加/ 運動/ 活動/ 因此/ 有/ 較/ 高/ 的/ 風險/ 發展/ 成/ 慢性/ 肌肉/ 骨骼/ 疾病/ 如/ 背部/ 和/ 頸部/ 疼痛/ 
/ 過去/ 的/ 研究/ 表明/ 糖尿/ 糖尿病/ 患者/ 可能/ 會/ 因為/ 繼發性/ 糖尿/ 糖尿病/ 相關/ 的/ 腰椎/ 間盤/ 血管/ 微血管/ 病變/ 而/ 發展/ 為/ 腰椎/ 間盤/ 疾病/ 此外/ 糖尿/ 糖尿病/ 動物/ 模型/ 中椎間/ 盤/ 退變/ 更佳/ 迅速/ 
/ 該論文/ 的/ 作者/ Manuela/  / Ferreira/ 教授/ 副教授/ 說/ 目前/ 還沒有/ 足夠/ 的/ 證據/ 證明/ 糖尿/ 糖尿病/ 與/ 腰/ 頸/ 疼痛/ 之間/ 存在/ 因果/ 關/ 係/ 但/ 這種/ 積極/ 聯/ 繫/ 值得/ 進/ 一步/ 調查/ 和/ 研究/ 論文/ 還建議/ 醫護/ 人員應/ 該/ 考慮/ 對長/ 期頸痛/ 或/ 腰痛/ 患者/ 進行/ 糖尿/ 糖尿病/ 篩查/ 
/ 越來/ 越/ 多/ 的/ 人/ 同時/ 受到/ 頸/ 背/ 疼痛/ 和/ 糖尿/ 糖尿病/ 的/ 折磨/ 論文/ 共同/ 作者/ Paulo/  / Ferreira/ 說道/ 所以/ 值得/ 投入/ 更/ 多/ 的/ 資源/ 來/ 研究/ 它們/ 之間/ 的/ 相互/ 關/ 係/ 改變/ 糖尿/ 糖尿病/ 的/ 治療/ 干預/ 措施/ 可/ 或/ 許以/ 減少/ 背痛/ 的/ 發病率/ 反之/ 亦然/ 反之亦然/ 
/ 參考/ 資料/ Study/  / links/  / diabetes/  / and/  / back/  / pain/ 
/ 期刊/ 小檔案/ PLOS/  / ONE/ 為/ 一份/ 同行/ 評審/ 的/ 開放/ 獲取/ 科學/ 期刊/ 由/ 公共/ 科學/ 圖書館/ 自/ 2006/ 年/ 發行/ 為/ 世界/ 全世界/ 文章/ 刊載/ 數量/ 最/ 多/ 的/ 期刊/ 所刊/ 載/ 的/ 文章/ 包含/ 科學及/ 醫學/ 各/ 領域/ 的/ 基礎/ 研究/ 
/ 更/ 多/ Heho/ 健康/ 網/ 文章/ 糖友/ 肺炎/ 風險/ 高/ 2.8/ 倍/  / 醫加/ 打/ 肺炎/ 鏈/ 球菌/ 疫苗/ 保/ 健康/ 讓/ 其他/ 細胞/ 分泌/ 胰島素/  / 動物/ 實驗/ 逆轉/ 糖尿/ 糖尿病/ 
/ 
/ 文林/ 以/ 璿/  / 圖許/ 嘉真</pre><p></p>
<h3>5.將字詞頻率以word cloud(文字雲)呈現</h3>
<p>將字詞頻率以文字雲繪出<br />
載入文字雲套件</p><pre class="crayon-plain-tag">import matplotlib.pyplot as plt
from wordcloud import WordCloud</pre><p>設定停用字stopwords(排除常用詞、無法代表特殊意義的字詞)，投入WordClour()函數中，其中參數stopwords形式可以是：</p>
<ul>
<li>dictionary {key : value}的格式</li>
<li>set {&#8216;a&#8217;, &#8216;b&#8217;, &#8216;c&#8217;, &#8230;}的格式</li>
</ul>
<p>*stopwords中， \n 是前面因為避免印出一長串沒有換行符號的article而加入的，故這邊再另外挑除</p><pre class="crayon-plain-tag"># 設定停用字詞 
stopwords = {}.fromkeys(["也","但","來","個","再","的","和","是","有","更","會","可能","有何","從","對","就", '\n','越','為','這種','多','越來',' '])
#stopwords = {"也","但","來","個","再","的","和","是","有","更","會","可能","有何","從","對","就",'\n','越','為','這種','多','越來',' '}</pre><p><strong>產生文字雲(方法1): 使用<span style="color: #9f6ad4;">generate_from_frequencies()</span></strong></p>
<ul>
<li><span style="color: #9f6ad4;">generate_from_frequencies()方法會忽略stopwords參數的部分</span>，<span style="text-decoration: underline;">故在產生hash dictionary字詞頻率的時候，須先自行處理挑除stopwords的部分</span>。</li>
</ul>
<p></p><pre class="crayon-plain-tag"># 使用cut_for_search(搜尋引擎)斷詞模式並產生字詞頻率的dictionary (並剔除stopwords的計算)
Sentence = jieba.cut_for_search(d) 

# create a python dictionary
hash = {}
for item in Sentence:

    if item in stopwords:
        continue
    
    if item in hash:
        hash[item] += 1
    else:
        hash[item] = 1

# 文字雲樣式設定
wc = WordCloud(font_path="/Users/hsuanlee/Library/Fonts/NotoSansCJKtc-Regular.otf", #設置字體
               background_color="white", #背景顏色
               max_words = 2000 ,        #文字雲顯示最大詞數
               stopwords=stopwords)      #停用字詞

# 使用dictionary的內容產生文字雲
wc.generate_from_frequencies(hash)

# 視覺化呈現
plt.imshow(wc)
plt.axis("off")
plt.figure(figsize=(20,10), dpi =200)
plt.show()</pre><p><img loading="lazy" class="alignnone size-full wp-image-2699" src="/wp-content/uploads/2019/03/文章文字雲3.png" alt="文章文字雲3" width="385" height="202" srcset="/wp-content/uploads/2019/03/文章文字雲3.png 385w, /wp-content/uploads/2019/03/文章文字雲3-300x157.png 300w, /wp-content/uploads/2019/03/文章文字雲3-230x121.png 230w, /wp-content/uploads/2019/03/文章文字雲3-350x184.png 350w" sizes="(max-width: 385px) 100vw, 385px" /></p>
<p><strong>產生文字雲(方法2): 使用generate_from_text()</strong></p>
<ul>
<li>由於generate_from_text()方法會採納stopwords參數，故事前不需要特別對text做stopwords的挑除</li>
</ul>
<p></p><pre class="crayon-plain-tag">Sentence = jieba.cut_for_search(d) 
text = ' '.join(Sentence)

# 文字雲樣式設定
wc = WordCloud(font_path="/Users/hsuanlee/Library/Fonts/NotoSansCJKtc-Regular.otf", #設置字體
               background_color="white", #背景顏色
               max_words = 2000 ,        #文字雲顯示最大詞數
               stopwords=stopwords)      #停用字詞

# 直接使用結巴斷詞後的文字來產生文字雲
wc.generate_from_text(text)

# 視覺化呈現
plt.imshow(wc)
plt.axis("off")
plt.figure(figsize=(20,10), dpi =200)
plt.show()</pre><p><img loading="lazy" class="alignnone size-full wp-image-2700" src="/wp-content/uploads/2019/03/文章文字雲4.png" alt="文章文字雲4" width="385" height="202" srcset="/wp-content/uploads/2019/03/文章文字雲4.png 385w, /wp-content/uploads/2019/03/文章文字雲4-300x157.png 300w, /wp-content/uploads/2019/03/文章文字雲4-230x121.png 230w, /wp-content/uploads/2019/03/文章文字雲4-350x184.png 350w" sizes="(max-width: 385px) 100vw, 385px" /></p>
<h3>6. 將dictionary轉換成data frame(並以key當作row: orient = &#8216;index&#8217;)</h3>
<p>將字詞頻率統計表依照「詞頻」由多至寡排列。</p><pre class="crayon-plain-tag">artDf = pd.DataFrame.from_dict(hash, orient='index', columns = ['詞頻'])
artDf.sort_values(by= ['詞頻'], ascending= False)</pre><p><img loading="lazy" class="alignnone size-full wp-image-2701" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-03-下午12.06.24.png" alt="螢幕快照 2019-03-03 下午12.06.24" width="101" height="295" /></p>
<hr />
<p>文章參考連結：</p>
<h3><a title="Python網路爬蟲：大數據擷取、清洗、儲存與分析：王者歸來" href="https://tinyurl.com/yc6la3vd" target="_blank" rel="mid_name noopener noreferrer"><span style="font-size: 12pt;">Python<em>網路</em><em>爬蟲</em>：大數據擷取、清洗、儲存與分析：王者歸來</span></a></h3>
<p><a href="https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud.generate_from_frequencies" target="_blank" rel="noopener noreferrer">wordcloud.WordCloud</a></p>
<hr />
<p>更多Python網路爬蟲學習筆記：</p>
<p><a href="/python-web-crawler-beautifulsoup-%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2/" target="_blank" rel="noopener noreferrer">網路爬蟲 Web Crawler | 資料不求人 基礎篇 | using Python BeautifulSoup</a></p>
<p><a href="/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-python-yahoo-movies/" target="_blank" rel="noopener noreferrer">網路爬蟲 web crawler | 奇摩電影 yahoo movies | using Python</a></p>
<p><a href="/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-text-mining-python/" target="_blank" rel="noopener noreferrer">Text Mining &amp; 網路爬蟲 web crawler | Google新聞與文章文字雲 | Python</a></p>
<p>這篇文章 <a rel="nofollow" href="/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-text-mining-python/">Text Mining &#038; 網路爬蟲 web crawler | Google新聞與文章文字雲 | Python</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-text-mining-python/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>網路爬蟲 web crawler &#124; 奇摩電影 yahoo movies &#124; using Python</title>
		<link>/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-python-yahoo-movies/</link>
					<comments>/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-python-yahoo-movies/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Sun, 03 Mar 2019 03:12:25 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[資料處理]]></category>
		<guid isPermaLink="false">/?p=2603</guid>

					<description><![CDATA[<p>本篇學習筆記將要示範如何使用 Python 來執行 網路爬蟲 web crawler，並以爬取雅虎奇摩電影的「每週新片」頁面資訊為例。筆記包含以下部分：(1)解 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-python-yahoo-movies/">網路爬蟲 web crawler | 奇摩電影 yahoo movies | using Python</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>本篇學習筆記將要示範如何使用 Python 來執行 網路爬蟲 web crawler，並以爬取雅虎奇摩電影的「<a href="https://movies.yahoo.com.tw/movie_thisweek.html" target="_blank" rel="noreferrer noopener" aria-label="每週新片 (在新分頁中開啟)">每週新片</a>」頁面資訊為例。筆記包含以下部分：(1)解讀網頁資訊 (2) 萃取資訊 (3) 將資訊整理成data frame (4) 將電影爬蟲寫成函式 (5) 自動化判斷所有分頁url並套用電影爬蟲函數。</p>
<h3>(1) 解讀網頁資訊</h3>
<h4>載入所需套件</h4>
<p></p><pre class="crayon-plain-tag"># import libraries
import pandas as pd
import requests
from bs4 import BeautifulSoup</pre><p></p>
<h4>指定URL資訊</h4>
<p></p><pre class="crayon-plain-tag"># 指定所要爬網的URL
url = 'https://movies.yahoo.com.tw/movie_thisweek.html'
# GET request from url and parse via BeautifulSoup
r = requests.get(url)
# 擷取request回傳的文字部分
web_content = r.text</pre><p></p>
<h4>解讀(parse)網頁HTML</h4>
<p></p><pre class="crayon-plain-tag"># 使用BeautifulSoup來parse HTMl
soup = BeautifulSoup(web_content,'lxml')</pre><p></p>
<h3></h3>
<h3>(2) 將「每週新片」的中英文片名、電影介紹、預告片連結爬下來</h3>
<h4>2-1. 中英文片名</h4>
<p>先找到中英文片名在html架構中的規則(可<strong><span style="color: #9f6ad4;">右鍵點擊</span></strong>感興趣物件，並選擇「<span style="color: #9f6ad4;"><strong>檢查</strong></span>」)(如下圖)。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2628" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午12.54.30.png" alt="網路爬蟲-web-crawler" width="2344" height="1106" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午12.54.30.png 2344w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午12.54.30-300x142.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午12.54.30-768x362.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午12.54.30-1024x483.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午12.54.30-830x392.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午12.54.30-230x109.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午12.54.30-350x165.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午12.54.30-480x226.png 480w" sizes="(max-width: 2344px) 100vw, 2344px" /></p>
<p>並使用.find_all()找出soup中物件為&#8217;div&#8217;且class_類別為&#8221;release_movie_name&#8221;的所有元素，集結成一個list (named newMovie2)，每個元素彼此使用逗號間隔。</p><pre class="crayon-plain-tag">newMovie2 = soup.find_all('div', class_ = "release_movie_name")
newMovie2</pre><p></p><pre class="crayon-plain-tag">[&lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','為副不仁']" href="https://movies.yahoo.com.tw/movieinfo_main/%E7%82%BA%E5%89%AF%E4%B8%8D%E4%BB%81-vice-8963"&gt;
                   為副不仁&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','為副不仁']" href="https://movies.yahoo.com.tw/movieinfo_main/%E7%82%BA%E5%89%AF%E4%B8%8D%E4%BB%81-vice-8963"&gt;
                     Vice&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;62%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="3.4" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;, &lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','吉娃娃羅曼死']" href="https://movies.yahoo.com.tw/movieinfo_main/%E5%90%89%E5%A8%83%E5%A8%83%E7%BE%85%E6%9B%BC%E6%AD%BB-chiwawa-9256"&gt;
                   吉娃娃羅曼死&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','吉娃娃羅曼死']" href="https://movies.yahoo.com.tw/movieinfo_main/%E5%90%89%E5%A8%83%E5%A8%83%E7%BE%85%E6%9B%BC%E6%AD%BB-chiwawa-9256"&gt;
                     Chiwawa&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;43%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="3.8" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;, &lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','酷寒殺手']" href="https://movies.yahoo.com.tw/movieinfo_main/%E9%85%B7%E5%AF%92%E6%AE%BA%E6%89%8B-cold-pursuit-9377"&gt;
                   酷寒殺手&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','酷寒殺手']" href="https://movies.yahoo.com.tw/movieinfo_main/%E9%85%B7%E5%AF%92%E6%AE%BA%E6%89%8B-cold-pursuit-9377"&gt;
                     Cold Pursuit&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;87%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="3.4" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;, &lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','法律女王']" href="https://movies.yahoo.com.tw/movieinfo_main/%E6%B3%95%E5%BE%8B%E5%A5%B3%E7%8E%8B-on-the-basis-of-sex-9389"&gt;
                   法律女王&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','法律女王']" href="https://movies.yahoo.com.tw/movieinfo_main/%E6%B3%95%E5%BE%8B%E5%A5%B3%E7%8E%8B-on-the-basis-of-sex-9389"&gt;
                     On the Basis of Sex&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;78%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="3.4" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;, &lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','恭喜八婆']" href="https://movies.yahoo.com.tw/movieinfo_main/%E6%81%AD%E5%96%9C%E5%85%AB%E5%A9%86-miss-behavior-9432"&gt;
                   恭喜八婆&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','恭喜八婆']" href="https://movies.yahoo.com.tw/movieinfo_main/%E6%81%AD%E5%96%9C%E5%85%AB%E5%A9%86-miss-behavior-9432"&gt;
                     Miss Behavior&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;88%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="3.8" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;, &lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','非．虛構情事']" href="https://movies.yahoo.com.tw/movieinfo_main/%E9%9D%9E-%E8%99%9B%E6%A7%8B%E6%83%85%E4%BA%8B-non-fiction-9475"&gt;
                   非．虛構情事&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','非．虛構情事']" href="https://movies.yahoo.com.tw/movieinfo_main/%E9%9D%9E-%E8%99%9B%E6%A7%8B%E6%83%85%E4%BA%8B-non-fiction-9475"&gt;
                     Non-fiction&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;23%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="2.6" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;, &lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','地獄自拍']" href="https://movies.yahoo.com.tw/movieinfo_main/%E5%9C%B0%E7%8D%84%E8%87%AA%E6%8B%8D-selfie-from-hell-9494"&gt;
                   地獄自拍&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','地獄自拍']" href="https://movies.yahoo.com.tw/movieinfo_main/%E5%9C%B0%E7%8D%84%E8%87%AA%E6%8B%8D-selfie-from-hell-9494"&gt;
                     Selfie From Hell&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;57%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="1.4" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;, &lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','證人']" href="https://movies.yahoo.com.tw/movieinfo_main/%E8%AD%89%E4%BA%BA-innocent-witness-9552"&gt;
                   證人&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','證人']" href="https://movies.yahoo.com.tw/movieinfo_main/%E8%AD%89%E4%BA%BA-innocent-witness-9552"&gt;
                     Innocent Witness&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;96%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="4.8" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;, &lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','溫泉屋小女將']" href="https://movies.yahoo.com.tw/movieinfo_main/%E6%BA%AB%E6%B3%89%E5%B1%8B%E5%B0%8F%E5%A5%B3%E5%B0%87-okkos-inn-9562"&gt;
                   溫泉屋小女將&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','溫泉屋小女將']" href="https://movies.yahoo.com.tw/movieinfo_main/%E6%BA%AB%E6%B3%89%E5%B1%8B%E5%B0%8F%E5%A5%B3%E5%B0%87-okkos-inn-9562"&gt;
                     Okko’s Inn&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;80%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="3.7" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;, &lt;div class="release_movie_name"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','密弑遊戲']" href="https://movies.yahoo.com.tw/movieinfo_main/%E5%AF%86%E5%BC%91%E9%81%8A%E6%88%B2-escape-room-9572"&gt;
                   密弑遊戲&lt;/a&gt;
 &lt;div class="en"&gt;
 &lt;a class="gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','密弑遊戲']" href="https://movies.yahoo.com.tw/movieinfo_main/%E5%AF%86%E5%BC%91%E9%81%8A%E6%88%B2-escape-room-9572"&gt;
                     Escape Room&lt;/a&gt;
 &lt;/div&gt;
 &lt;dl class="levelbox"&gt;
 &lt;dt&gt;
 &lt;div class="level_name"&gt;期待度&lt;/div&gt;
 &lt;div class="leveltext"&gt;
 &lt;span&gt;84%&lt;/span&gt;
                     網友想看
                   &lt;/div&gt;
 &lt;/dt&gt;
 &lt;dd&gt;
 &lt;div class="level_name"&gt;滿意度&lt;/div&gt;
 &lt;div class="leveltext starwithnum"&gt;
 &lt;span class="count" data-num="4.4" data-run="0"&gt;0&lt;/span&gt;&lt;span&gt;分&lt;/span&gt;
 &lt;div class="starbox starnumbox"&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;div class="star"&gt;
 &lt;div class="starovr"&gt;&lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;/dd&gt;
 &lt;/dl&gt;
 &lt;/div&gt;]</pre><p>但每個class = &#8221;release_movie_name&#8221;的元素中，並不是每個標籤都是我們感興趣的，為了萃取感興趣資訊，<span style="text-decoration: underline; color: #9f6ad4;">我們再從newMovies2 list中，一個一個元素去找出class = &#8216;gabtn&#8217; 的&lt;a&gt;&lt;/a&gt;標籤中的文字(如下段落所示)，並將換行與空白字符取消，存取成一個新的list (named NameCHs)</span>。</p>
<p><span style="text-decoration: underline;"><span style="font-size: 12pt;"><em>&lt;a class=&#8221;</em>gabtn<em>&#8221; data-ga=&#8221;[&#8216;本週新片&#8217;,&#8217;本週新片_本週新片第1頁&#8217;,&#8217;密弑遊戲&#8242;]&#8221;<br />
href=&#8221;https://movies.yahoo.com.tw/movieinfo_main/<br />
</em></span></span><span style="text-decoration: underline;"><span style="font-size: 12pt;"><em>%E5%AF%86%E5%BC%91%E9%81%8A%E6%88%B2-escape-room-9572&#8243;&gt;</em><em><span style="color: #0000ff; text-decoration: underline;">密弑遊戲</span>&lt;/a&gt;</em></span></span></p><pre class="crayon-plain-tag">NameCHs = [t.find('a', class_='gabtn').text.replace('\n','').replace(' ','') for t in newMovie2]
NameCHs</pre><p></p><pre class="crayon-plain-tag">['為副不仁',
 '吉娃娃羅曼死',
 '酷寒殺手',
 '法律女王',
 '恭喜八婆',
 '非．虛構情事',
 '地獄自拍',
 '證人',
 '溫泉屋小女將',
 '密弑遊戲']</pre><p>同理，找出英文片名的文字。</p><pre class="crayon-plain-tag">NameENs = [t.find('div', class_='en').find('a').text.replace('\n','').replace(' ','') for t in newMovie2]
NameENs</pre><p></p><pre class="crayon-plain-tag">['Vice',
 'Chiwawa',
 'ColdPursuit',
 'OntheBasisofSex',
 'MissBehavior',
 'Non-fiction',
 'SelfieFromHell',
 'InnocentWitness',
 'Okko’sInn',
 'EscapeRoom']</pre><p></p>
<h4>2-2. 預告片連結</h4>
<p>一樣，檢視感興趣目標在HTML中的結構規則。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2638" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.05.14.png" alt="網路爬蟲-web-crawler" width="2352" height="1126" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.05.14.png 2352w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.05.14-300x144.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.05.14-768x368.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.05.14-1024x490.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.05.14-830x397.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.05.14-230x110.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.05.14-350x168.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.05.14-480x230.png 480w" sizes="(max-width: 2352px) 100vw, 2352px" /></p>
<p>找出所有class_為release_btn color_btnbox的div。</p><pre class="crayon-plain-tag">newMovie3 = soup.find_all('div',class_="release_btn color_btnbox")
newMovie3</pre><p></p><pre class="crayon-plain-tag">[&lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','為副不仁']" href="https://movies.yahoo.com.tw/movieinfo_main/%E7%82%BA%E5%89%AF%E4%B8%8D%E4%BB%81-vice-8963"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','為副不仁']" href="https://movies.yahoo.com.tw/video/%E7%82%BA%E5%89%AF%E4%B8%8D%E4%BB%81-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-021801838.html?movie_id=8963"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=8963"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','為副不仁']" href="https://movies.yahoo.com.tw/movietime_result.html/id=8963"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;, &lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','吉娃娃羅曼死']" href="https://movies.yahoo.com.tw/movieinfo_main/%E5%90%89%E5%A8%83%E5%A8%83%E7%BE%85%E6%9B%BC%E6%AD%BB-chiwawa-9256"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','吉娃娃羅曼死']" href="https://movies.yahoo.com.tw/video/%E5%90%89%E5%A8%83%E5%A8%83%E7%BE%85%E6%9B%BC%E6%AD%BB-%E7%B2%BE%E5%BD%A9%E7%89%87%E9%A0%AD%E6%90%B6%E5%85%88%E7%9C%8B-090102582.html?movie_id=9256"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=9256"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','吉娃娃羅曼死']" href="https://movies.yahoo.com.tw/movietime_result.html/id=9256"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;, &lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','酷寒殺手']" href="https://movies.yahoo.com.tw/movieinfo_main/%E9%85%B7%E5%AF%92%E6%AE%BA%E6%89%8B-cold-pursuit-9377"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','酷寒殺手']" href="https://movies.yahoo.com.tw/video/%E9%85%B7%E5%AF%92%E6%AE%BA%E6%89%8B-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-105035779.html?movie_id=9377"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=9377"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','酷寒殺手']" href="https://movies.yahoo.com.tw/movietime_result.html/id=9377"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;, &lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','法律女王']" href="https://movies.yahoo.com.tw/movieinfo_main/%E6%B3%95%E5%BE%8B%E5%A5%B3%E7%8E%8B-on-the-basis-of-sex-9389"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','法律女王']" href="https://movies.yahoo.com.tw/video/%E6%B3%95%E5%BE%8B%E5%A5%B3%E7%8E%8B-%E6%9C%80%E6%96%B0%E9%A0%90%E5%91%8A-093658482.html?movie_id=9389"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=9389"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','法律女王']" href="https://movies.yahoo.com.tw/movietime_result.html/id=9389"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;, &lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','恭喜八婆']" href="https://movies.yahoo.com.tw/movieinfo_main/%E6%81%AD%E5%96%9C%E5%85%AB%E5%A9%86-miss-behavior-9432"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','恭喜八婆']" href="https://movies.yahoo.com.tw/video/%E6%81%AD%E5%96%9C%E5%85%AB%E5%A9%86-%E6%9C%80%E6%96%B0%E9%A0%90%E5%91%8A-025738371.html?movie_id=9432"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=9432"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','恭喜八婆']" href="https://movies.yahoo.com.tw/movietime_result.html/id=9432"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;, &lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','非．虛構情事']" href="https://movies.yahoo.com.tw/movieinfo_main/%E9%9D%9E-%E8%99%9B%E6%A7%8B%E6%83%85%E4%BA%8B-non-fiction-9475"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','非．虛構情事']" href="https://movies.yahoo.com.tw/video/%E9%9D%9E-%E8%99%9B%E6%A7%8B%E6%83%85%E4%BA%8B-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-023735492.html?movie_id=9475"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=9475"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','非．虛構情事']" href="https://movies.yahoo.com.tw/movietime_result.html/id=9475"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;, &lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','地獄自拍']" href="https://movies.yahoo.com.tw/movieinfo_main/%E5%9C%B0%E7%8D%84%E8%87%AA%E6%8B%8D-selfie-from-hell-9494"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','地獄自拍']" href="https://movies.yahoo.com.tw/video/%E5%9C%B0%E7%8D%84%E8%87%AA%E6%8B%8D-%E6%9C%80%E6%96%B0%E9%A0%90%E5%91%8A-144352730.html?movie_id=9494"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=9494"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','地獄自拍']" href="https://movies.yahoo.com.tw/movietime_result.html/id=9494"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;, &lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','證人']" href="https://movies.yahoo.com.tw/movieinfo_main/%E8%AD%89%E4%BA%BA-innocent-witness-9552"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','證人']" href="https://movies.yahoo.com.tw/video/%E8%AD%89%E4%BA%BA-%E5%82%AC%E6%B7%9A%E9%9F%B3%E6%A8%82%E7%89%88%E9%A0%90%E5%91%8A-231957458.html?movie_id=9552"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=9552"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','證人']" href="https://movies.yahoo.com.tw/movietime_result.html/id=9552"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;, &lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','溫泉屋小女將']" href="https://movies.yahoo.com.tw/movieinfo_main/%E6%BA%AB%E6%B3%89%E5%B1%8B%E5%B0%8F%E5%A5%B3%E5%B0%87-okkos-inn-9562"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','溫泉屋小女將']" href="https://movies.yahoo.com.tw/video/%E6%BA%AB%E6%B3%89%E5%B1%8B%E5%B0%8F%E5%A5%B3%E5%B0%87-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-105610983.html?movie_id=9562"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=9562"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','溫泉屋小女將']" href="https://movies.yahoo.com.tw/movietime_result.html/id=9562"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;, &lt;div class="release_btn color_btnbox"&gt;
 &lt;a class="btn_s_introduction" data-ga="['本週新片','本週新片_本週新片第1頁','密弑遊戲']" href="https://movies.yahoo.com.tw/movieinfo_main/%E5%AF%86%E5%BC%91%E9%81%8A%E6%88%B2-escape-room-9572"&gt;電影介紹&lt;/a&gt;
 &lt;a class="btn_s_vedio gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','密弑遊戲']" href="https://movies.yahoo.com.tw/video/%E5%AF%86%E5%BC%91%E9%81%8A%E6%88%B2-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-100235196.html?movie_id=9572"&gt;
                   預告片
                 &lt;/a&gt;
 &lt;a class="btn_s_foto" href="https://movies.yahoo.com.tw/movieinfo_photos.html/id=9572"&gt;劇照&lt;/a&gt;
 &lt;a class="btn_s_time gabtn" data-ga="['本週新片','本週新片_本週新片第1頁','密弑遊戲']" href="https://movies.yahoo.com.tw/movietime_result.html/id=9572"&gt;
                   時刻表
                 &lt;/a&gt;
 &lt;/div&gt;]</pre><p>並從newMovie3 list中，進一步取出元素中class_為btn_s_vedio gabtn&lt;a&gt;&lt;/a&gt;連結標籤中的連結href。</p><pre class="crayon-plain-tag">links = [t.find('a',class_="btn_s_vedio gabtn")['href'] for t in newMovie3]
links</pre><p></p><pre class="crayon-plain-tag">['https://movies.yahoo.com.tw/video/%E7%82%BA%E5%89%AF%E4%B8%8D%E4%BB%81-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-021801838.html?movie_id=8963',
 'https://movies.yahoo.com.tw/video/%E5%90%89%E5%A8%83%E5%A8%83%E7%BE%85%E6%9B%BC%E6%AD%BB-%E7%B2%BE%E5%BD%A9%E7%89%87%E9%A0%AD%E6%90%B6%E5%85%88%E7%9C%8B-090102582.html?movie_id=9256',
 'https://movies.yahoo.com.tw/video/%E9%85%B7%E5%AF%92%E6%AE%BA%E6%89%8B-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-105035779.html?movie_id=9377',
 'https://movies.yahoo.com.tw/video/%E6%B3%95%E5%BE%8B%E5%A5%B3%E7%8E%8B-%E6%9C%80%E6%96%B0%E9%A0%90%E5%91%8A-093658482.html?movie_id=9389',
 'https://movies.yahoo.com.tw/video/%E6%81%AD%E5%96%9C%E5%85%AB%E5%A9%86-%E6%9C%80%E6%96%B0%E9%A0%90%E5%91%8A-025738371.html?movie_id=9432',
 'https://movies.yahoo.com.tw/video/%E9%9D%9E-%E8%99%9B%E6%A7%8B%E6%83%85%E4%BA%8B-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-023735492.html?movie_id=9475',
 'https://movies.yahoo.com.tw/video/%E5%9C%B0%E7%8D%84%E8%87%AA%E6%8B%8D-%E6%9C%80%E6%96%B0%E9%A0%90%E5%91%8A-144352730.html?movie_id=9494',
 'https://movies.yahoo.com.tw/video/%E8%AD%89%E4%BA%BA-%E5%82%AC%E6%B7%9A%E9%9F%B3%E6%A8%82%E7%89%88%E9%A0%90%E5%91%8A-231957458.html?movie_id=9552',
 'https://movies.yahoo.com.tw/video/%E6%BA%AB%E6%B3%89%E5%B1%8B%E5%B0%8F%E5%A5%B3%E5%B0%87-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-105610983.html?movie_id=9562',
 'https://movies.yahoo.com.tw/video/%E5%AF%86%E5%BC%91%E9%81%8A%E6%88%B2-%E4%B8%AD%E6%96%87%E9%A0%90%E5%91%8A-100235196.html?movie_id=9572']</pre><p></p>
<h4>2-3. 電影介紹文</h4>
<p>檢查介紹文的HTML規則，發現介紹文會夾帶在class = &#8216;release_text&#8217;的div中。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2639" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.16.21.png" alt="網路爬蟲-web-crawler" width="2352" height="1110" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.16.21.png 2352w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.16.21-300x142.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.16.21-768x362.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.16.21-1024x483.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.16.21-830x392.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.16.21-230x109.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.16.21-350x165.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.16.21-480x227.png 480w" sizes="(max-width: 2352px) 100vw, 2352px" /></p><pre class="crayon-plain-tag">newMovie4 = soup.find_all('div',class_="release_text")
newMovie4</pre><p></p><pre class="crayon-plain-tag">[&lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E7%82%BA%E5%89%AF%E4%B8%8D%E4%BB%81-vice-8963"&gt;
                   ★奧斯卡金獎得主亞當麥凱繼《大賣空》後，攜手克里斯汀貝爾、史提夫卡爾三巨頭再度合作
 ★「蝙蝠俠」克里斯汀貝爾再展變色龍演技，傳神詮釋史上最有權勢副總統迪克錢尼
 ★奧斯卡最佳男配角山姆洛克威爾與克里斯汀貝爾聯袂扮老，精湛化身小布希
 ★布萊德彼特與《月光下的藍色男孩》監製共同打造，強勢問鼎2019奧斯卡最佳影片、影帝等獎項！
  
 《為副不仁》講述美國史上最有權勢的副總統迪克錢尼（克里斯汀貝爾 飾），如何從白宮幕僚逐漸爬上權力的頂峰，成為全世界最有權勢的人－－美國總統小布希（山姆洛克威爾 飾）的副手，以及他的政策如何重塑美國並影響至今我們身處的世界。
                 &lt;/span&gt;
 &lt;/div&gt;, &lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E5%90%89%E5%A8%83%E5%A8%83%E7%BE%85%E6%9B%BC%E6%AD%BB-chiwawa-9256"&gt;
                   ★《惡女羅曼死》原著作者又一爭議話題傑作改編電影化！
 ★ 殺人分屍案死者身份確認，完全沒想到，竟然就是我的好友！
 ★ 現代東京版《羅生門》，懸疑曲折超越《渴望》、《聽說桐島退社了》！ 
 ★ 新生代實力演員群，共同演繹謳歌青春電影傑作！
 ★ 絢爛畫面搭配動感配樂，滿足視覺與聽覺的雙重饗宴！
 ★ 藤岡靛、吉岡里帆、櫻井友紀、白石和彌等日本影視圈名人爭相力讚！
  
 話題人物慘遭殺害，連鎖效應在好友圈無限蔓延
 一群揮霍青春的青年男女，其中一位長相可愛貌美，猶如眾人「吉祥物」般存在、暱稱為「吉娃娃」的20歲少女，某天驚傳遭人分屍的命案，部分屍體並在東京灣尋獲。她周遭的好友們，開始聚在一起回憶青春，緬懷他們心目中的吉娃娃。大家這時才發現，根本沒人知道她的真名、境遇還有真正的個性。原來在大家對她毫無了解的情況下，她就跟眾人混在一起、談戀愛、甚至發生性關係。究竟真正的「吉娃娃」，到底是位什麼樣的女孩呢？
  
 【關於電影】
  
 《惡女羅曼死》作者話題傑作，懸疑曲折超越《聽說桐島退社了》
 《吉娃娃羅曼死》改編自日本知名漫畫家岡崎京子的短篇漫畫作品。曾打造《惡女羅曼死》等轟動不已作品的岡崎京子，特別擅長描繪情慾與悖德，並透過毫無忌諱的性愛場面表現人物內在。這部短篇漫畫，終於在2019年翻拍成劇情長片登上大銀幕。全片描述個性活潑好動、集萬千寵愛於一身的20歲美少女「吉娃娃」，某天竟被殘忍分屍，而她的遭遇也在朋友圈中引發連鎖效應。眾人開始回想自己與「吉娃娃」的每個相處細節，竟發現自己連她的本名、境遇與背景都不知道。也因為好友們各說各話，在不同說法中拼湊出吉娃娃的生活與背景，更讓本片頗有現代女性版《羅生門》的懸疑性，更猶如《渴望》、《聽說桐島退社了》的加乘綜合版。
  
 本片找來曾執導《裸睡美人》的二宮健擔任導演。二宮導演透露，親手拍攝《吉娃娃羅曼史》的真人電影版，一直是他給自己設立的目標，他並回憶：「初次拜讀原作，是我22歲剛從大阪到東京來的時候。那時我雖然想在東京拍攝更大規模的電影，卻諸事不順；即便想讓自己做些什麼，到頭來總是繞回原點，恰好就是那樣的時期，讓我遇到了這部作品。」他並補充：「只花34頁篇幅描繪的青春群像劇，卻帶來非常大的震撼。這群想試著跨越些什麼而掙扎的人物，逐漸跟我的自身形象交疊，以致於在看完原作的瞬間，我就決定要把這部作品給拍成電影。」值得一提的是，二宮導演的作品總有著最前衛的影像，以及動感的配樂，讓他的導演風格非常鮮明並深受好評。尤其本片的派對群戲眾多，導演這次也採用讓演員自由發揮的即興表演，再由鏡頭一一收錄，讓全片的派對、玩樂、性愛等場面顯得特別真實。
  
 本片演員也是一時之選，找來2019年新科藍絲帶獎影后門脇麥擔綱演出「美樹」這個角色，而讀完劇本的她也覺得「非常有趣，很想試試看」而馬上答應邀約。至於靈魂人物「吉娃娃」，則找來新銳女星吉田志織擔綱。她獨特又魅力十足，時而無邪、時而深沉，立體的人物塑造和強烈的存在感，讓人簡直無法相信這是她的首部主演作品。此外，本片也集結成田凌、寬一郎、村上虹郎、玉城蒂娜、栗山千明、淺野忠信等多位新銳、資深實力派演員共同演繹，聯手呈現年輕人面對愛情、金錢、嫉妒、慾望等如夢境般閃亮不已的絢爛青春世界。而全片所勾勒「青春的爆發」與「告終的覺悟」，導演也精準使用過去日本電影從未有過的影像與音樂勾勒出來，勢必滿足觀眾視覺與聽覺的雙重享受，一同進入青春大無畏的美麗與哀愁！
  
                 &lt;/span&gt;
 &lt;/div&gt;, &lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E9%85%B7%E5%AF%92%E6%AE%BA%E6%89%8B-cold-pursuit-9377"&gt;
                   ★2019開春最受期待復仇爽片! 冷暴力美學、血花散遍!
 ★地表最強老爸-連恩尼遜，重拾槍火即刻開戰!  復仇是人性最原始的反撲!
  
 殺我兒，我滅你全團!
  
 剷雪車司機-尼爾剛獲頒榮譽市民表揚，卻收到兒子吸毒致死的訊息，讓他生活頓時起了波瀾。他堅信兒子清白，便開始追查，赫然發現幕後有黑道組織操刀，他從朋友手中獲得黑道兇嫌名單，並依姓名順序送凶手上斷頭台。冰天雪地中，黑道們一個個死於非命，尼爾的復仇行為讓毒梟跟槍火幫派，以為是雙方在找麻煩，進而引發兩個幫派大火拼。為了幫兒子報仇，尼爾也陷入這場世紀廝殺中，雙方頓時火力全開，復仇計畫大快人心…
  
 【幕後花絮】
  
 《酷寒殺手》翻拍自爛番茄新鮮度90％超強好評《該死的順序》，導演漢斯彼得穆蘭開創新暴力美學風格，電影公司買下版權後並無改換導演，仍由漢斯繼續拍攝美語版本，融入更多美式暴力作風。導演當時會想拍這部片，是來自本身被霸凌的怨念：「我從小就喜歡報仇，只要有人欺負我，我就會反擊。」導演認為，復仇是人類非常原始的直覺，儘管沒有達到報仇目的，也能享受到樂趣。
  
 《該死的順序》當年在柏林影展首映之後，全球影評佳評如潮！帝國雜誌給予極高的讚賞，「這才是我們期待已久的復仇爽片！」知名影評網站爛番茄新鮮度近90％超強好評！國際評論對於《該死的順序》師法柯恩兄弟，也超越昆汀塔倫提諾和盧貝松的暴力美學大加讚揚！
  
 片商看中電影超強口碑便著手進行翻拍，將新暴力美學旋風帶入好萊塢；找來具有票房號召力男星-連恩尼遜擔任片中的復仇老爸。雖然連恩尼遜演出這類型的電影並不陌生，但在新片中，導演讓連恩尼遜有更多的內心戲與謀略策畫，他也不再英勇無敵，沒有特務、警察等背景，單純是一位平凡的復仇父親。
  
 在製作方面，麥可雪恩伯格曾擔任過《決殺令》製片，負責本片的暴力美學演繹，而《疾速救援》製片-麥可德雷爾，則擔綱起片中動作炫技保證，讓電影《酷寒殺手》重新定義暴力極致。
  
                 &lt;/span&gt;
 &lt;/div&gt;, &lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E6%B3%95%E5%BE%8B%E5%A5%B3%E7%8E%8B-on-the-basis-of-sex-9389"&gt;
                   ★真人實事改編 一生致力平權 美國傳奇女法官成功奇蹟登上大銀幕 
 ★《愛的萬物論》金獎提名費莉絲蒂瓊斯X《以你的名字呼喚我》艾米漢默 聯手主演
 ★《彗星撞地球》票房女導演咪咪蕾德 睽違大銀幕多年最新力作
 ★為信念奮鬥不懈 為歧視挺身而出 繼《永不妥協》後又一激勵人心銀幕佳作 
 ★故事主角露絲拜德金斯伯格 親自客串演出 
  
 「如果連法律都不能一視同仁，兩性如何平權？」取材自現今美國最高法院唯二女大法官露絲拜德金斯伯格（菲莉絲蒂瓊斯 飾）的真實成功故事，本片聚焦在年紀輕輕、剛踏出法學院校門的她，如何在男性至上、女性不被重視的工作環境中，力爭上游成為律師，為兩性爭取平權，成為美國史上最有影響力的法官之一，現在仍任職於最高法院大法官。
                 &lt;/span&gt;
 &lt;/div&gt;, &lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E6%81%AD%E5%96%9C%E5%85%AB%E5%A9%86-miss-behavior-9432"&gt;
                   ★2019開年最爆笑噴飯喜劇，香港鬼才彭浩翔瘋狂作品，《春嬌與志明》班底火力全開，一定要你笑翻天
 ★ 香港最不可思議的八婆女星大集合，為了一瓶奶！？展開瘋狂大作戰
  
 由8個閨密組成的「八婆」whatsapp群組，因彼此的恩怨，讓群組不再對話。
 而June也在認識了現在的老公後，漸漸疏遠其他7位閨密。她的女魔頭上司最近生了小孩，需要定時抽母奶放進冰箱。沒想到，June竟然誤把母奶加進咖啡裡，還被大老闆喝個精光，要是被發現肯定工作不保。
  
 情急之下，她只好找回很久沒聯絡的群組成員，希望在下班前，幫忙找到母奶頂替。此時，即將襲港的颱風逼近，交通大亂，八婆們能否在時間之內，完成尋奶任務呢？
  
                 &lt;/span&gt;
 &lt;/div&gt;, &lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E9%9D%9E-%E8%99%9B%E6%A7%8B%E6%83%85%E4%BA%8B-non-fiction-9475"&gt;
                   亞倫是法國出版界的風雲人物，然而時代在變，亞倫也忙於適應電子世代帶給出版業的衝擊，他不僅要學習如何發行電子書，就連讀者的消費取向也令他匪夷所思。人氣博主、暢銷部落客只要將在社群媒體發表的短文集結成冊，也能成為暢銷書？亞倫拒絕出版與他年紀相仿且合作多年的作家李奧納多的新書，他的作品寫得多半是自己的真實經驗，再將筆下人物稍加改名換姓，他的前妻、前任女友以及被影射的周遭人物深受其擾。這次的新書是李奧納多跟某個名人芝麻綠豆般的緋聞軼事，亞倫質疑他根本沒有創意以及想像力，然而出人意表的是亞倫身為名人的妻子卻認為這是李奧納多最好的作品…
  
 曾經榮獲坎城影展最佳導演、法國中生代最知名的導演奧利維耶阿薩亞斯今年入圍威尼斯影展競賽之作，這也是他與茱麗葉畢諾許第三度合作。導演精心設計的日常對話，涵蓋文化、民主、政治以及時事等多面向，幽默諷刺又饒富哲學寓意，尤其對現實世界女明星茱麗葉畢諾許的一番嘲弄，令人會心一笑。茱麗葉畢諾許與吉翁卡列精彩的演出令人驚艷。
  
 【關於電影】
  
 強勢挑戰大時代議題 阿薩亞斯回歸16釐米力抗數位狂潮
 《非．虛構情事》講述近年討論度最高的議題之一——傳統紙本出版業與電子書的價值觀拉扯。在銳不可擋的現代數位化潮流當中，阿薩亞斯從未停止關注紙本書出版面臨到的夕陽產業議題，延續了過去作品《夏日時光》、《星光雲寂》中對於全球化、經濟、科技以及文化在時代下變遷的討論，從個人及群體角度出發，採用了輕鬆幽默卻又充滿哲學性的論調，讓觀眾能夠輕巧的進入議題核心。本片透過絕妙對話──無論是兩人世界或是團體聚會，還是身為編輯與明星、作家與政治狂熱組合的夫妻檔、偷情對象、工作夥伴──建構出不同背景的人們處於變遷的世界中，被傳統以及新潮拉扯的糾結感。另外，在片中描述面對現代化的掙扎時，阿薩亞斯的敘事角度選擇欣然理解當前數位化的崇高地位，但在技術方面卻選擇回歸以16釐米膠捲拍攝，盼望以創作初衷，在與資本主義的抗衡中找到更多討論空間。
 被問到劇中作家因大量採用自身故事作為創作題材而備受爭議，阿薩亞斯笑說：「我應該比他有道德一點吧！」法文片名原意為「雙面人生」，跟英文片名「非．虛構情事」交錯出真實與虛構從來就無法完全切割的深層寓意；電影中出版社提議可以找大明星茱麗葉畢諾許來為有聲書配音，以刺激銷量，飾演女主角瑟琳娜的正是茱麗葉本人，她在片中神回覆「我認識她的經紀人，或許可以關說一下」。劇中被嘲諷的茱麗葉畢諾許同時活靈活現地存在在電影裡，「故事無非就是現實生活的一面鏡子啊」。
  
 阿薩亞斯三度合作大滿貫影后 全新卡司歡樂滿堂
 茱麗葉畢諾許的影后身影無庸置疑是片中一大亮點，《非．虛構情事》是她第三次與阿薩亞斯合作，相知多年的兩人早有絕佳默契，阿薩亞斯受訪時表示：「她的不羈、詼諧都讓我的靈感源源不絕。」並認為只有她能夠讓瑟琳娜的幽默立體化；而曾被封為法國最帥男演員的吉翁卡列則是在阿薩亞斯寫作過程中逐漸成形的想法，認為身為這一代最有代表性的男演員之一，只有他有資格撐得起這個角色的權威感，成功吸引觀眾的目光；而出身舞台劇演員及導演的文森馬格恩也自帶氣場，讓劇中金牌編輯與風流作家的對手戲，與吉翁卡列第一次合作就激盪出充滿電力的精彩火花。
 除了茱麗葉畢諾許與阿薩亞斯以外，其他卡司皆是首度合作，但執導過程中，阿薩亞斯忍不住盛讚演員們，「比我更清楚如何演繹出腳本中的幽默，甚至共鳴出新的層次！」
  
                 &lt;/span&gt;
 &lt;/div&gt;, &lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E5%9C%B0%E7%8D%84%E8%87%AA%E6%8B%8D-selfie-from-hell-9494"&gt;
                   ★ 改編自2015年在YouTube創下逾2000萬次點閱同名恐怖短片
 ★ 《愛殺瑪莉》製片群傾力打造，驚嚇程度直逼《鬼關燈》《陰兒房》！
 ★ 21世紀的科技驚悚預言，直達地獄的恐怖自拍！
 ★ 新世紀文明病，這次真的會送命！
  
 德國部落客茱莉亞來到美國拜訪她的姪女漢娜，卻在抵達後突然怪病纏身，臥床不起。漢娜家也開始接二連三地出現各種不尋常的超自然現象，不堪其擾的漢娜不禁懷疑一切乃因茱莉亞而起。當漢娜找到了茱莉亞的部落格後，發現茱莉亞沉迷暗網多時，並將蟄伏在暗網深處的恐怖力量帶到了現實之中，沾有嗜血意志的自拍照正威脅每一個人……
  
                 &lt;/span&gt;
 &lt;/div&gt;, &lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E8%AD%89%E4%BA%BA-innocent-witness-9552"&gt;
                   ★《與神同行》超人氣女星金香起突破演技暖心動人力作！
 ★「男神」鄭雨盛、「國民萌妹」金香起睽違17年再度攜手！
 ★《向左愛向右愛》《記憶中的美好歌聲》票房大導李翰再執導演筒！
 ★「小迷糊」李奎炯、鄭雨盛韓國兩大型男互飆演技！
  
 信任 是全世界最有力的證詞
 律師楊淳鎬（鄭雨盛飾）加入了一家知名的法律事務所，為一名被指控殺害雇主一家的女傭辯護，這樁謀殺案廣受媒體與社會關注，淳鎬必須贏得官司才能成為事務所合夥人。然案發現場的唯一目擊證人林智友（金香起飾）是一名自閉症少女，淳鎬必須想盡辦法讓智友敞開心房出庭作證才能贏得官司。這兩個不同世界的人漸漸成為了朋友，但在案情膠著的壓力下，他必須在有限的時間內，讓智友說出真相…
  
 【關於電影】
  
 金香起繼《與神同行》後演技再突破　搭檔男神鄭雨盛續17年之緣！
  
 以賣座強片《與神同行》系列電影爆紅的「國民妹妹」金香起奪下青龍電影獎「最佳女配角獎」後，睽違17年再度與「國民暖男」鄭雨盛攜手演出催淚動人新作《證人》，而金香起在２歲時拍攝第一支廣告就是和鄭雨盛合作，讓鄭雨盛驚訝表示這是非常神奇的緣分。
  
 金香起在6歲時從廣告界踏入影壇，超齡的演技天賦讓她獲得「天才童星」的美譽，即使與黃晸玟、宋允兒、宋仲基、朴寶英、河正宇、車太鉉、朱智勛、李政宰等影帝后級別的知名影星合作也絲毫不遜色，而她在新作《證人》中首次挑戰演出自閉症少女，金香起表示：「其實接演的時候我猶豫了很久，因為這個角色難度很高。」她擔心自己無法完美詮釋，但高難度的演技挑戰又讓她躍躍欲試，最終她決定接受挑戰。鄭雨盛則表示：「金香起對表演的專注力非常驚人，她的演技魅力自然而然就流露出來了。」導演李翰也坦言金香起在《證人》中的表演非常令人動容，有好幾場戲都讓劇組人員忍不住淚灑現場，他說：「金香起的演出非常多變，你會被她真摯的演技觸動，忍不住為她哭、為她笑。」因此在所有同輩女星中，金香起是導演選角的第一人選。
                 &lt;/span&gt;
 &lt;/div&gt;, &lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E6%BA%AB%E6%B3%89%E5%B1%8B%E5%B0%8F%E5%A5%B3%E5%B0%87-okkos-inn-9562"&gt;
                   ★ 熱銷300萬冊，日本超人氣國民讀物改編！
 ★《神隱少女》《崖上的波妞》作畫監督x《電影版聲之形》知名編劇攜手打造溫暖新作！
 ★ 榮獲2019年每日映畫大賞最佳動畫片！
 ★ 入選2019年日本奧斯卡優秀動畫電影獎！
 ★ 入選2018法國安錫動畫影展競賽片！
  
 就讀小學六年級的小織，父母不幸在車禍中喪生，由經營「春之屋」溫泉旅館的奶奶收養了她，因這場意外獲得通靈能力的她，也認識了旅館中三個心地善良卻調皮的幽靈。在旅館面臨後繼無人將被收購的困境時，年僅12歲的小織在奶奶的幫助下，展開一場成為旅館老闆娘必經的「小女將修行」之路，雖然每天都遭遇嘲笑與挫折，但天性樂觀的她勇於接受挑戰，絕不認輸！《溫泉屋小女將》將於2019年2月27日上映，更多電影資訊歡迎上官方粉絲團查詢：https://www.facebook.com/VVPfans。
                 &lt;/span&gt;
 &lt;/div&gt;, &lt;div class="release_text"&gt;
 &lt;span class="jq_text_overflow_180 jq_text_overflow_href_list" data-url="https://movies.yahoo.com.tw/movieinfo_main/%E5%AF%86%E5%BC%91%E9%81%8A%E6%88%B2-escape-room-9572"&gt;
                   索尼影業最新電影《密弑遊戲》以時下最流行的密室逃脫遊戲為主題，由《陰兒房第4章：鎖命亡靈》亞當羅勃提爾導演執導、《玩命關頭》系列億萬製片打造，並由《太空迷航》影集的泰勒羅素與《親愛的初戀》羅根米勒等明星主演，台灣將於2月27日上映。
  
 《密弑遊戲》電影講述6名從不同意外中生還的陌生人，在努力回歸正常生活時，突然收到不明來歷的密室逃脫遊戲邀請。遊戲中總共有5大關卡：「毒火煉獄」、「冰天雪地」、「顛倒世界」、「毒氣密室」與「迷幻空間」，在不同闖關過程中，他們亦被迫面對自我最黑暗的過往瘡疤，他們也才意識到這不只是場遊戲，為求生存與贏取高達百萬美元的獎金，所有人不僅需互相幫助，還必須賭上性命互相傷害，因為最後只能有一位贏家倖存…
                 &lt;/span&gt;
 &lt;/div&gt;]</pre><p>取出介紹文，並清理換行等符號。</p><pre class="crayon-plain-tag">Intros = [t.find('span').text.replace('\n','').replace('\r','').replace('\xa0','').replace(' ','') for t in newMovie4]
Intros</pre><p></p><pre class="crayon-plain-tag">['★奧斯卡金獎得主亞當麥凱繼《大賣空》後，攜手克里斯汀貝爾、史提夫卡爾三巨頭再度合作★「蝙蝠俠」克里斯汀貝爾再展變色龍演技，傳神詮釋史上最有權勢副總統迪克錢尼★奧斯卡最佳男配角山姆洛克威爾與克里斯汀貝爾聯袂扮老，精湛化身小布希★布萊德彼特與《月光下的藍色男孩》監製共同打造，強勢問鼎2019奧斯卡最佳影片、影帝等獎項！《為副不仁》講述美國史上最有權勢的副總統迪克錢尼（克里斯汀貝爾飾），如何從白宮幕僚逐漸爬上權力的頂峰，成為全世界最有權勢的人－－美國總統小布希（山姆洛克威爾飾）的副手，以及他的政策如何重塑美國並影響至今我們身處的世界。',
 '★《惡女羅曼死》原著作者又一爭議話題傑作改編電影化！★殺人分屍案死者身份確認，完全沒想到，竟然就是我的好友！★現代東京版《羅生門》，懸疑曲折超越《渴望》、《聽說桐島退社了》！★新生代實力演員群，共同演繹謳歌青春電影傑作！★絢爛畫面搭配動感配樂，滿足視覺與聽覺的雙重饗宴！★藤岡靛、吉岡里帆、櫻井友紀、白石和彌等日本影視圈名人爭相力讚！話題人物慘遭殺害，連鎖效應在好友圈無限蔓延一群揮霍青春的青年男女，其中一位長相可愛貌美，猶如眾人「吉祥物」般存在、暱稱為「吉娃娃」的20歲少女，某天驚傳遭人分屍的命案，部分屍體並在東京灣尋獲。她周遭的好友們，開始聚在一起回憶青春，緬懷他們心目中的吉娃娃。大家這時才發現，根本沒人知道她的真名、境遇還有真正的個性。原來在大家對她毫無了解的情況下，她就跟眾人混在一起、談戀愛、甚至發生性關係。究竟真正的「吉娃娃」，到底是位什麼樣的女孩呢？【關於電影】《惡女羅曼死》作者話題傑作，懸疑曲折超越《聽說桐島退社了》《吉娃娃羅曼死》改編自日本知名漫畫家岡崎京子的短篇漫畫作品。曾打造《惡女羅曼死》等轟動不已作品的岡崎京子，特別擅長描繪情慾與悖德，並透過毫無忌諱的性愛場面表現人物內在。這部短篇漫畫，終於在2019年翻拍成劇情長片登上大銀幕。全片描述個性活潑好動、集萬千寵愛於一身的20歲美少女「吉娃娃」，某天竟被殘忍分屍，而她的遭遇也在朋友圈中引發連鎖效應。眾人開始回想自己與「吉娃娃」的每個相處細節，竟發現自己連她的本名、境遇與背景都不知道。也因為好友們各說各話，在不同說法中拼湊出吉娃娃的生活與背景，更讓本片頗有現代女性版《羅生門》的懸疑性，更猶如《渴望》、《聽說桐島退社了》的加乘綜合版。本片找來曾執導《裸睡美人》的二宮健擔任導演。二宮導演透露，親手拍攝《吉娃娃羅曼史》的真人電影版，一直是他給自己設立的目標，他並回憶：「初次拜讀原作，是我22歲剛從大阪到東京來的時候。那時我雖然想在東京拍攝更大規模的電影，卻諸事不順；即便想讓自己做些什麼，到頭來總是繞回原點，恰好就是那樣的時期，讓我遇到了這部作品。」他並補充：「只花34頁篇幅描繪的青春群像劇，卻帶來非常大的震撼。這群想試著跨越些什麼而掙扎的人物，逐漸跟我的自身形象交疊，以致於在看完原作的瞬間，我就決定要把這部作品給拍成電影。」值得一提的是，二宮導演的作品總有著最前衛的影像，以及動感的配樂，讓他的導演風格非常鮮明並深受好評。尤其本片的派對群戲眾多，導演這次也採用讓演員自由發揮的即興表演，再由鏡頭一一收錄，讓全片的派對、玩樂、性愛等場面顯得特別真實。本片演員也是一時之選，找來2019年新科藍絲帶獎影后門脇麥擔綱演出「美樹」這個角色，而讀完劇本的她也覺得「非常有趣，很想試試看」而馬上答應邀約。至於靈魂人物「吉娃娃」，則找來新銳女星吉田志織擔綱。她獨特又魅力十足，時而無邪、時而深沉，立體的人物塑造和強烈的存在感，讓人簡直無法相信這是她的首部主演作品。此外，本片也集結成田凌、寬一郎、村上虹郎、玉城蒂娜、栗山千明、淺野忠信等多位新銳、資深實力派演員共同演繹，聯手呈現年輕人面對愛情、金錢、嫉妒、慾望等如夢境般閃亮不已的絢爛青春世界。而全片所勾勒「青春的爆發」與「告終的覺悟」，導演也精準使用過去日本電影從未有過的影像與音樂勾勒出來，勢必滿足觀眾視覺與聽覺的雙重享受，一同進入青春大無畏的美麗與哀愁！',
 '★2019開春最受期待復仇爽片!冷暴力美學、血花散遍!★地表最強老爸-連恩尼遜，重拾槍火即刻開戰!復仇是人性最原始的反撲!殺我兒，我滅你全團!剷雪車司機-尼爾剛獲頒榮譽市民表揚，卻收到兒子吸毒致死的訊息，讓他生活頓時起了波瀾。他堅信兒子清白，便開始追查，赫然發現幕後有黑道組織操刀，他從朋友手中獲得黑道兇嫌名單，並依姓名順序送凶手上斷頭台。冰天雪地中，黑道們一個個死於非命，尼爾的復仇行為讓毒梟跟槍火幫派，以為是雙方在找麻煩，進而引發兩個幫派大火拼。為了幫兒子報仇，尼爾也陷入這場世紀廝殺中，雙方頓時火力全開，復仇計畫大快人心…【幕後花絮】《酷寒殺手》翻拍自爛番茄新鮮度90％超強好評《該死的順序》，導演漢斯彼得穆蘭開創新暴力美學風格，電影公司買下版權後並無改換導演，仍由漢斯繼續拍攝美語版本，融入更多美式暴力作風。導演當時會想拍這部片，是來自本身被霸凌的怨念：「我從小就喜歡報仇，只要有人欺負我，我就會反擊。」導演認為，復仇是人類非常原始的直覺，儘管沒有達到報仇目的，也能享受到樂趣。《該死的順序》當年在柏林影展首映之後，全球影評佳評如潮！帝國雜誌給予極高的讚賞，「這才是我們期待已久的復仇爽片！」知名影評網站爛番茄新鮮度近90％超強好評！國際評論對於《該死的順序》師法柯恩兄弟，也超越昆汀塔倫提諾和盧貝松的暴力美學大加讚揚！片商看中電影超強口碑便著手進行翻拍，將新暴力美學旋風帶入好萊塢；找來具有票房號召力男星-連恩尼遜擔任片中的復仇老爸。雖然連恩尼遜演出這類型的電影並不陌生，但在新片中，導演讓連恩尼遜有更多的內心戲與謀略策畫，他也不再英勇無敵，沒有特務、警察等背景，單純是一位平凡的復仇父親。在製作方面，麥可雪恩伯格曾擔任過《決殺令》製片，負責本片的暴力美學演繹，而《疾速救援》製片-麥可德雷爾，則擔綱起片中動作炫技保證，讓電影《酷寒殺手》重新定義暴力極致。',
 '★真人實事改編一生致力平權美國傳奇女法官成功奇蹟登上大銀幕★《愛的萬物論》金獎提名費莉絲蒂瓊斯X《以你的名字呼喚我》艾米漢默聯手主演★《彗星撞地球》票房女導演咪咪蕾德睽違大銀幕多年最新力作★為信念奮鬥不懈為歧視挺身而出繼《永不妥協》後又一激勵人心銀幕佳作★故事主角露絲拜德金斯伯格親自客串演出「如果連法律都不能一視同仁，兩性如何平權？」取材自現今美國最高法院唯二女大法官露絲拜德金斯伯格（菲莉絲蒂瓊斯飾）的真實成功故事，本片聚焦在年紀輕輕、剛踏出法學院校門的她，如何在男性至上、女性不被重視的工作環境中，力爭上游成為律師，為兩性爭取平權，成為美國史上最有影響力的法官之一，現在仍任職於最高法院大法官。',
 '★2019開年最爆笑噴飯喜劇，香港鬼才彭浩翔瘋狂作品，《春嬌與志明》班底火力全開，一定要你笑翻天★香港最不可思議的八婆女星大集合，為了一瓶奶！？展開瘋狂大作戰由8個閨密組成的「八婆」whatsapp群組，因彼此的恩怨，讓群組不再對話。而June也在認識了現在的老公後，漸漸疏遠其他7位閨密。她的女魔頭上司最近生了小孩，需要定時抽母奶放進冰箱。沒想到，June竟然誤把母奶加進咖啡裡，還被大老闆喝個精光，要是被發現肯定工作不保。情急之下，她只好找回很久沒聯絡的群組成員，希望在下班前，幫忙找到母奶頂替。此時，即將襲港的颱風逼近，交通大亂，八婆們能否在時間之內，完成尋奶任務呢？',
 '亞倫是法國出版界的風雲人物，然而時代在變，亞倫也忙於適應電子世代帶給出版業的衝擊，他不僅要學習如何發行電子書，就連讀者的消費取向也令他匪夷所思。人氣博主、暢銷部落客只要將在社群媒體發表的短文集結成冊，也能成為暢銷書？亞倫拒絕出版與他年紀相仿且合作多年的作家李奧納多的新書，他的作品寫得多半是自己的真實經驗，再將筆下人物稍加改名換姓，他的前妻、前任女友以及被影射的周遭人物深受其擾。這次的新書是李奧納多跟某個名人芝麻綠豆般的緋聞軼事，亞倫質疑他根本沒有創意以及想像力，然而出人意表的是亞倫身為名人的妻子卻認為這是李奧納多最好的作品…曾經榮獲坎城影展最佳導演、法國中生代最知名的導演奧利維耶阿薩亞斯今年入圍威尼斯影展競賽之作，這也是他與茱麗葉畢諾許第三度合作。導演精心設計的日常對話，涵蓋文化、民主、政治以及時事等多面向，幽默諷刺又饒富哲學寓意，尤其對現實世界女明星茱麗葉畢諾許的一番嘲弄，令人會心一笑。茱麗葉畢諾許與吉翁卡列精彩的演出令人驚艷。【關於電影】強勢挑戰大時代議題阿薩亞斯回歸16釐米力抗數位狂潮《非．虛構情事》講述近年討論度最高的議題之一——傳統紙本出版業與電子書的價值觀拉扯。在銳不可擋的現代數位化潮流當中，阿薩亞斯從未停止關注紙本書出版面臨到的夕陽產業議題，延續了過去作品《夏日時光》、《星光雲寂》中對於全球化、經濟、科技以及文化在時代下變遷的討論，從個人及群體角度出發，採用了輕鬆幽默卻又充滿哲學性的論調，讓觀眾能夠輕巧的進入議題核心。本片透過絕妙對話──無論是兩人世界或是團體聚會，還是身為編輯與明星、作家與政治狂熱組合的夫妻檔、偷情對象、工作夥伴──建構出不同背景的人們處於變遷的世界中，被傳統以及新潮拉扯的糾結感。另外，在片中描述面對現代化的掙扎時，阿薩亞斯的敘事角度選擇欣然理解當前數位化的崇高地位，但在技術方面卻選擇回歸以16釐米膠捲拍攝，盼望以創作初衷，在與資本主義的抗衡中找到更多討論空間。被問到劇中作家因大量採用自身故事作為創作題材而備受爭議，阿薩亞斯笑說：「我應該比他有道德一點吧！」法文片名原意為「雙面人生」，跟英文片名「非．虛構情事」交錯出真實與虛構從來就無法完全切割的深層寓意；電影中出版社提議可以找大明星茱麗葉畢諾許來為有聲書配音，以刺激銷量，飾演女主角瑟琳娜的正是茱麗葉本人，她在片中神回覆「我認識她的經紀人，或許可以關說一下」。劇中被嘲諷的茱麗葉畢諾許同時活靈活現地存在在電影裡，「故事無非就是現實生活的一面鏡子啊」。阿薩亞斯三度合作大滿貫影后全新卡司歡樂滿堂茱麗葉畢諾許的影后身影無庸置疑是片中一大亮點，《非．虛構情事》是她第三次與阿薩亞斯合作，相知多年的兩人早有絕佳默契，阿薩亞斯受訪時表示：「她的不羈、詼諧都讓我的靈感源源不絕。」並認為只有她能夠讓瑟琳娜的幽默立體化；而曾被封為法國最帥男演員的吉翁卡列則是在阿薩亞斯寫作過程中逐漸成形的想法，認為身為這一代最有代表性的男演員之一，只有他有資格撐得起這個角色的權威感，成功吸引觀眾的目光；而出身舞台劇演員及導演的文森馬格恩也自帶氣場，讓劇中金牌編輯與風流作家的對手戲，與吉翁卡列第一次合作就激盪出充滿電力的精彩火花。除了茱麗葉畢諾許與阿薩亞斯以外，其他卡司皆是首度合作，但執導過程中，阿薩亞斯忍不住盛讚演員們，「比我更清楚如何演繹出腳本中的幽默，甚至共鳴出新的層次！」',
 '★改編自2015年在YouTube創下逾2000萬次點閱同名恐怖短片★《愛殺瑪莉》製片群傾力打造，驚嚇程度直逼《鬼關燈》《陰兒房》！★21世紀的科技驚悚預言，直達地獄的恐怖自拍！★新世紀文明病，這次真的會送命！德國部落客茱莉亞來到美國拜訪她的姪女漢娜，卻在抵達後突然怪病纏身，臥床不起。漢娜家也開始接二連三地出現各種不尋常的超自然現象，不堪其擾的漢娜不禁懷疑一切乃因茱莉亞而起。當漢娜找到了茱莉亞的部落格後，發現茱莉亞沉迷暗網多時，並將蟄伏在暗網深處的恐怖力量帶到了現實之中，沾有嗜血意志的自拍照正威脅每一個人……',
 '★《與神同行》超人氣女星金香起突破演技暖心動人力作！★「男神」鄭雨盛、「國民萌妹」金香起睽違17年再度攜手！★《向左愛向右愛》《記憶中的美好歌聲》票房大導李翰再執導演筒！★「小迷糊」李奎炯、鄭雨盛韓國兩大型男互飆演技！信任是全世界最有力的證詞律師楊淳鎬（鄭雨盛飾）加入了一家知名的法律事務所，為一名被指控殺害雇主一家的女傭辯護，這樁謀殺案廣受媒體與社會關注，淳鎬必須贏得官司才能成為事務所合夥人。然案發現場的唯一目擊證人林智友（金香起飾）是一名自閉症少女，淳鎬必須想盡辦法讓智友敞開心房出庭作證才能贏得官司。這兩個不同世界的人漸漸成為了朋友，但在案情膠著的壓力下，他必須在有限的時間內，讓智友說出真相…【關於電影】金香起繼《與神同行》後演技再突破\u3000搭檔男神鄭雨盛續17年之緣！以賣座強片《與神同行》系列電影爆紅的「國民妹妹」金香起奪下青龍電影獎「最佳女配角獎」後，睽違17年再度與「國民暖男」鄭雨盛攜手演出催淚動人新作《證人》，而金香起在２歲時拍攝第一支廣告就是和鄭雨盛合作，讓鄭雨盛驚訝表示這是非常神奇的緣分。金香起在6歲時從廣告界踏入影壇，超齡的演技天賦讓她獲得「天才童星」的美譽，即使與黃晸玟、宋允兒、宋仲基、朴寶英、河正宇、車太鉉、朱智勛、李政宰等影帝后級別的知名影星合作也絲毫不遜色，而她在新作《證人》中首次挑戰演出自閉症少女，金香起表示：「其實接演的時候我猶豫了很久，因為這個角色難度很高。」她擔心自己無法完美詮釋，但高難度的演技挑戰又讓她躍躍欲試，最終她決定接受挑戰。鄭雨盛則表示：「金香起對表演的專注力非常驚人，她的演技魅力自然而然就流露出來了。」導演李翰也坦言金香起在《證人》中的表演非常令人動容，有好幾場戲都讓劇組人員忍不住淚灑現場，他說：「金香起的演出非常多變，你會被她真摯的演技觸動，忍不住為她哭、為她笑。」因此在所有同輩女星中，金香起是導演選角的第一人選。',
 '★熱銷300萬冊，日本超人氣國民讀物改編！★《神隱少女》《崖上的波妞》作畫監督x《電影版聲之形》知名編劇攜手打造溫暖新作！★榮獲2019年每日映畫大賞最佳動畫片！★入選2019年日本奧斯卡優秀動畫電影獎！★入選2018法國安錫動畫影展競賽片！就讀小學六年級的小織，父母不幸在車禍中喪生，由經營「春之屋」溫泉旅館的奶奶收養了她，因這場意外獲得通靈能力的她，也認識了旅館中三個心地善良卻調皮的幽靈。在旅館面臨後繼無人將被收購的困境時，年僅12歲的小織在奶奶的幫助下，展開一場成為旅館老闆娘必經的「小女將修行」之路，雖然每天都遭遇嘲笑與挫折，但天性樂觀的她勇於接受挑戰，絕不認輸！《溫泉屋小女將》將於2019年2月27日上映，更多電影資訊歡迎上官方粉絲團查詢：https://www.facebook.com/VVPfans。',
 '索尼影業最新電影《密弑遊戲》以時下最流行的密室逃脫遊戲為主題，由《陰兒房第4章：鎖命亡靈》亞當羅勃提爾導演執導、《玩命關頭》系列億萬製片打造，並由《太空迷航》影集的泰勒羅素與《親愛的初戀》羅根米勒等明星主演，台灣將於2月27日上映。《密弑遊戲》電影講述6名從不同意外中生還的陌生人，在努力回歸正常生活時，突然收到不明來歷的密室逃脫遊戲邀請。遊戲中總共有5大關卡：「毒火煉獄」、「冰天雪地」、「顛倒世界」、「毒氣密室」與「迷幻空間」，在不同闖關過程中，他們亦被迫面對自我最黑暗的過往瘡疤，他們也才意識到這不只是場遊戲，為求生存與贏取高達百萬美元的獎金，所有人不僅需互相幫助，還必須賭上性命互相傷害，因為最後只能有一位贏家倖存…']</pre><p>檢查NameCHs, NameENs, Intros, links陣列的長度是否相同(結果皆為10)。</p><pre class="crayon-plain-tag">len(NameCHs)
len(NameENs)
len(Intros)
len(links)</pre><p></p>
<h3></h3>
<h3>(3) 將以上欄位合併成data frame</h3>
<p></p><pre class="crayon-plain-tag">df = pd.DataFrame(
{
    'Name':NameCHs,
    'EnName':NameENs,
    'Intro': Intros,
    'Trailer': links
})
df</pre><p><img loading="lazy" class="alignnone size-full wp-image-2641" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33.png" alt="網路爬蟲-web-crawler" width="1780" height="968" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33.png 1780w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-300x163.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-768x418.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-1024x557.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-830x451.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-230x125.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-350x190.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-480x261.png 480w" sizes="(max-width: 1780px) 100vw, 1780px" /></p>
<p>由於以上只是「本週新片」中的其中一個分頁的電影爬蟲結果，如果想要快速爬取其他分頁的資訊，可將電影爬蟲寫成一個function。</p>
<h3></h3>
<h3>(4) 將電影爬網規則寫成function</h3>
<p>define 一個名為「yahooMovieParser」的函數，並以url為投入參數。</p><pre class="crayon-plain-tag"># create function
def yahooMovieParser(url):
    r = requests.get(url)
    web_content = r.text
    soup = BeautifulSoup(web_content,'lxml')
    
    # 中英文片名
    newMovie2 = soup.find_all('div', class_ = "release_movie_name")
    NameCHs = [t.find('a', class_='gabtn').text.replace('\n','').replace(' ','') for t in newMovie2]
    NameENs = [t.find('div', class_='en').find('a').text.replace('\n','').replace(' ','') for t in newMovie2]
    # 預告片
    newMovie3 = soup.find_all('div',class_="release_btn color_btnbox")
    links = [t.find('a',class_="btn_s_vedio gabtn")['href'] for t in newMovie3]
    # 電影介紹
    newMovie4 = soup.find_all('div',class_="release_text")
    Intros = [t.find('span').text.replace('\n','').replace('\r','').replace('\xa0','').replace(' ','') for t in newMovie4]
    #合併成data frame
    df = pd.DataFrame(
    {
        'Name':NameCHs,
        'EnName':NameENs,
        'Intro': Intros,
        'Trailer': links
    })
    return df</pre><p>執行單一URL: 手動指定url並投入函式。</p><pre class="crayon-plain-tag">url = "https://movies.yahoo.com.tw/movie_thisweek.html"
df1 = yahooMovieParser(url)
df1</pre><p><img loading="lazy" class="alignnone size-full wp-image-2641" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33.png" alt="網路爬蟲-web-crawler" width="1780" height="968" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33.png 1780w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-300x163.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-768x418.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-1024x557.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-830x451.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-230x125.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-350x190.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午2.29.33-480x261.png 480w" sizes="(max-width: 1780px) 100vw, 1780px" /></p>
<p>執行多個URLs</p>
<p>將多個 url包成一個陣列 list 使用迴圈執行並回傳一個data frame</p><pre class="crayon-plain-tag">urlList = ['https://movies.yahoo.com.tw/movie_thisweek.html','https://movies.yahoo.com.tw/movie_thisweek.html?page=2']
MovieInfo = None

for u in urlList:
    d1 = yahooMovieParser(u)
    if MovieInfo is None:
        MovieInfo = d1
    else:
        MovieInfo = MovieInfo.append(d1,ignore_index=True)
    
MovieInfo</pre><p><img loading="lazy" class="alignnone size-full wp-image-2674" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13.png" alt="螢幕快照 2019-03-03 上午10.32.13" width="888" height="674" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13.png 888w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-300x228.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-768x583.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-830x630.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-230x175.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-350x266.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-480x364.png 480w" sizes="(max-width: 888px) 100vw, 888px" /></p>
<p>但由於分頁的URLs都還是人工判斷，如果可以用程式規則判斷並蒐集所有分頁，將省去更多工。</p>
<h3></h3>
<h3>(5) 自動判斷分頁資訊，蒐集urlList</h3>
<p>首先，一樣先找到分頁資訊的區塊，並檢視其HTML結構。發現分頁資訊會被包含在class = &#8216;page_numbox&#8217;的div物件中。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2644" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.10.54.png" alt="網路爬蟲-web-crawler" width="2352" height="1226" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.10.54.png 2352w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.10.54-300x156.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.10.54-768x400.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.10.54-1024x534.png 1024w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.10.54-830x433.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.10.54-230x120.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.10.54-350x182.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-02-下午3.10.54-480x250.png 480w" sizes="(max-width: 2352px) 100vw, 2352px" /></p><pre class="crayon-plain-tag">url = "https://movies.yahoo.com.tw/movie_thisweek.html"
r = requests.get(url)
web_content = r.text
soup = BeautifulSoup(web_content,'lxml')
pageInfo = soup.find('div', class_='page_numbox')
pageInfo</pre><p></p><pre class="crayon-plain-tag">&lt;div class="page_numbox"&gt;
&lt;ul&gt;
&lt;li class="num_ar"&gt;‹&lt;/li&gt;
&lt;li class="prevtxt disabled"&gt;&lt;span&gt;上一頁&lt;/span&gt;&lt;/li&gt;
&lt;li class="active"&gt;&lt;span&gt;1&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://movies.yahoo.com.tw/movie_thisweek.html?page=2"&gt;2&lt;/a&gt;&lt;/li&gt;
&lt;li class="nexttxt"&gt;&lt;a href="http://movies.yahoo.com.tw/movie_thisweek.html?page=2" rel="next"&gt;下一頁&lt;/a&gt;&lt;/li&gt;
&lt;li class="num_ar"&gt;›&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</pre><p>撰寫一個萃取分頁URL資訊的函式。</p>
<ul>
<li>將分頁資訊的每個&lt;li&gt;&lt;/li&gt;元素中的連結取出，假設沒有連結資訊，則回傳None。</li>
</ul>
<p></p><pre class="crayon-plain-tag">def getNext(url):
    r = requests.get(url)
    web_content = r.text
    soup = BeautifulSoup(web_content,'lxml')
    pageInfo = soup.find('div', class_='page_numbox')
    tagA = pageInfo.find('li', class_="nexttxt").find('a')
    if tagA:
        return tagA['href']
    else:
        return None</pre><p>測試函式getNext()執行一次的結果。</p><pre class="crayon-plain-tag">url = "https://movies.yahoo.com.tw/movie_thisweek.html"
url2 = getNext(url)
url2</pre><p></p><pre class="crayon-plain-tag">'http://movies.yahoo.com.tw/movie_thisweek.html?page=2'</pre><p>使用while迴圈反覆執行，直到getNext()回傳None。</p><pre class="crayon-plain-tag">url = 'http://movies.yahoo.com.tw/movie_thisweek.html'
urlList = []

while url:
    urlList.append(url)
    url = getNext(url)</pre><p>檢視所有的分頁連結 (當分頁數很多時特別有效)</p><pre class="crayon-plain-tag">urlList</pre><p></p><pre class="crayon-plain-tag">['http://movies.yahoo.com.tw/movie_thisweek.html',
 'http://movies.yahoo.com.tw/movie_thisweek.html?page=2']</pre><p>綜合(1) ~ (5)電影爬蟲程式碼，自動化執行url List蒐集並爬取每個分頁下的最新電影資訊的程式碼如下：</p><pre class="crayon-plain-tag">url = 'http://movies.yahoo.com.tw/movie_thisweek.html'
urlList = []

while url:
    urlList.append(url)
    url = getNext(url)
    
MovieInfo = None

for u in urlList:
    d1 = yahooMovieParser(u)
    if MovieInfo is None:
        MovieInfo = d1
    else:
        MovieInfo = MovieInfo.append(d1,ignore_index=True)

MovieInfo</pre><p><img loading="lazy" class="alignnone size-full wp-image-2674" src="/wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13.png" alt="螢幕快照 2019-03-03 上午10.32.13" width="888" height="674" srcset="/wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13.png 888w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-300x228.png 300w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-768x583.png 768w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-830x630.png 830w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-230x175.png 230w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-350x266.png 350w, /wp-content/uploads/2019/03/螢幕快照-2019-03-03-上午10.32.13-480x364.png 480w" sizes="(max-width: 888px) 100vw, 888px" /></p>
<hr />
<p>更多Python網路爬蟲學習筆記：</p>
<p><a href="/python-web-crawler-beautifulsoup-%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2/" target="_blank" rel="noopener noreferrer">網路爬蟲 Web Crawler | 資料不求人 基礎篇 | using Python BeautifulSoup</a></p>
<p><a href="/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-python-yahoo-movies/" target="_blank" rel="noopener noreferrer">網路爬蟲 web crawler | 奇摩電影 yahoo movies | using Python</a></p>
<p><a href="/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-text-mining-python/" target="_blank" rel="noopener noreferrer">Text Mining &amp; 網路爬蟲 web crawler | Google新聞與文章文字雲 | Python</a></p>
<p>這篇文章 <a rel="nofollow" href="/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-python-yahoo-movies/">網路爬蟲 web crawler | 奇摩電影 yahoo movies | using Python</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/%e7%b6%b2%e8%b7%af%e7%88%ac%e8%9f%b2-web-crawler-python-yahoo-movies/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>Regularized Regression &#124; 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net &#124; R語言</title>
		<link>/regularized-regression-ridge-lasso-elastic/</link>
					<comments>/regularized-regression-ridge-lasso-elastic/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Fri, 04 Jan 2019 05:38:32 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[elastic net]]></category>
		<category><![CDATA[general linear regression]]></category>
		<category><![CDATA[lasso]]></category>
		<category><![CDATA[multicollinearity]]></category>
		<category><![CDATA[overfitting]]></category>
		<category><![CDATA[regression]]></category>
		<category><![CDATA[regularization]]></category>
		<category><![CDATA[regularized regression]]></category>
		<category><![CDATA[ridge]]></category>
		<guid isPermaLink="false">/?p=2359</guid>

					<description><![CDATA[<p>在線性回歸模型中，為了最佳化目標函式(最小化誤差平方和)，資料需符合許多假設，才能得到不偏回歸係數，使得模型變異量最低。可現實中數據非常可能有多個特徵變數，使得 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/regularized-regression-ridge-lasso-elastic/">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>在線性回歸模型中，為了最佳化目標函式(最小化誤差平方和)，資料需符合許多假設，才能得到不偏回歸係數，使得模型變異量最低。可現實中數據非常可能有多個特徵變數，使得模型假設不成立而產生過度配適問題，這時則需透過正規化法(regularized regression)來控制回歸係數，藉此降低模型變異以及樣本外誤差。</p>
<h3>Regularized Regression</h3>
<h3>載入實作所需的套件</h3>
<p></p><pre class="crayon-plain-tag">library(rsample)  # data splitting 
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting</pre><p>資料準備：Data 使用 AmesHousing package中的 Ames Housing data</p><pre class="crayon-plain-tag"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split &lt;- initial_split(data = AmesHousing::make_ames(),prop = 0.7, strata = "Sale_Price")
ames_train &lt;- training(ames_split)
ames_test &lt;- testing(ames_split)</pre><p></p>
<h3>為何需要資料正規化(Regularization)?</h3>
<p>我們知道，OLS(Ordinary Least Squares)最小平方和線性回歸的最佳化目標函式就是尋找一個平面，使得預測與實際值的誤差平方和(Sum of Squared Error, SSE)最小化（如下圖，紫色點代表實際觀測值，粉紅色為預測平面，黑色實線則表示實際與預測值得殘差）。</p>
<p><img src="/wp-content/uploads/2019/01/unnamed-chunk-170-1.png" alt="Regularized Regression" /></p>
<p>線性回歸的目標函式：</p>
<p>\[minimize\bigg \{ SSE = \sum_{i=1}^n (y_{i} &#8211; \hat{y}_{i})^2 \bigg \}\]</p>
<p>而要最佳化目標函式，資料必須符合以下幾個基本假設：</p>
<ul>
<li>線性關係</li>
<li>變數成常態分配</li>
<li>變數間無自相關</li>
<li>殘差變異同質性</li>
<li>觀測值個數(n)需大於特徵個數(p)(n&gt;p)</li>
<li>模型不能有共線性問題(否則估計回歸係數會有問題)</li>
</ul>
<p>但現實中，數據往往存在許多特徵變數(p很大)，隨著特徵變數量增加，許多基本假設不再成立，以至於我們必須用替代方法來解決線性預測問題。具體來說，當特徵變數量增加(p增加)，我們常會遇到的三個主要問題包括：</p>
<h4>1. Multicollinearity 多元共線性</h4>
<p>當特徵變數個數增加(p增加)，我們就有越高的機會捕捉到存在共線性的變數。而當模型存在共線性時，回歸係數項就會變得非常不穩定(high variance, 高變異)。<br />
舉例來說，我們先從多達81個特徵變數中:</p>
<ol>
<li>找出相關係數絕對值高於0.6的變數組合。</li>
<li>找出哪些變數與自變數(Sale_Price)具有高度相關性(相關係數絕對值大於 0.6)。</li>
</ol>
<p>首先我們先計算出每對變數間的相關係數和P-value的data frame</p><pre class="crayon-plain-tag">library(Hmisc) # rcorr()函數
data &lt;- as.data.frame(AmesHousing::make_ames())
# 使用rcorr()產生Matrix of correlations and P-values
res2 &lt;- rcorr(as.matrix(data[,sapply(data, is.numeric)]))

# 將相關係數與p-value矩陣轉換成data frame的函數
flattenCorrMatrix &lt;- function(cormat, pmat) {
  ut &lt;- upper.tri(cormat) # Lower and Upper Triangular Part of a Matrix
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

cor_table &lt;- flattenCorrMatrix(res2$r, res2$P)
head(cor_table)</pre><p></p><pre class="crayon-plain-tag">##            row         column        cor                     p
## 1 Lot_Frontage       Lot_Area 0.13686214 0.0000000000001005862
## 2 Lot_Frontage     Year_Built 0.02613050 0.1573419061953282849
## 3     Lot_Area     Year_Built 0.02325850 0.2081737048985332628
## 4 Lot_Frontage Year_Remod_Add 0.06950923 0.0001662509781792387
## 5     Lot_Area Year_Remod_Add 0.02168222 0.2406818444157394765
## 6   Year_Built Year_Remod_Add 0.61209525 0.0000000000000000000</pre><p>列出相關係數絕對值高於0.6的變數組合</p><pre class="crayon-plain-tag">cor_table %&gt;% filter(abs(cor) &gt; 0.6) %&gt;% arrange(desc(cor))</pre><p></p><pre class="crayon-plain-tag">##              row         column       cor p
## 1    Garage_Cars    Garage_Area 0.8898660 0
## 2    Gr_Liv_Area  TotRms_AbvGrd 0.8077721 0
## 3  Total_Bsmt_SF   First_Flr_SF 0.8004287 0
## 4    Gr_Liv_Area     Sale_Price 0.7067799 0
## 5  Bedroom_AbvGr  TotRms_AbvGrd 0.6726472 0
## 6  Second_Flr_SF    Gr_Liv_Area 0.6552512 0
## 7    Garage_Cars     Sale_Price 0.6475616 0
## 8    Garage_Area     Sale_Price 0.6401383 0
## 9  Total_Bsmt_SF     Sale_Price 0.6325288 0
## 10   Gr_Liv_Area      Full_Bath 0.6303208 0
## 11  First_Flr_SF     Sale_Price 0.6216761 0
## 12    Year_Built Year_Remod_Add 0.6120953 0
## 13 Second_Flr_SF      Half_Bath 0.6116337 0</pre><p>列出與目標變數Sale_Price有高度相關(相關係數絕對值高於0.5)的變數組合</p><pre class="crayon-plain-tag">cor_table %&gt;% filter(row == "Sale_Price" | column == "Sale_Price") %&gt;% filter(abs(round(cor,digits = 2)) &gt;= 0.5) %&gt;% arrange(desc(cor))</pre><p></p><pre class="crayon-plain-tag">##               row     column       cor p
## 1     Gr_Liv_Area Sale_Price 0.7067799 0
## 2     Garage_Cars Sale_Price 0.6475616 0
## 3     Garage_Area Sale_Price 0.6401383 0
## 4   Total_Bsmt_SF Sale_Price 0.6325288 0
## 5    First_Flr_SF Sale_Price 0.6216761 0
## 6      Year_Built Sale_Price 0.5584261 0
## 7       Full_Bath Sale_Price 0.5456039 0
## 8  Year_Remod_Add Sale_Price 0.5329738 0
## 9    Mas_Vnr_Area Sale_Price 0.5021960 0
## 10  TotRms_AbvGrd Sale_Price 0.4954744 0</pre><p>我們取具有高相關性的兩變數Gr_Liv_Area、TotRms_AbvGrd來說明。兩變數相關係數高達0.81。其中，Gr_Liv_Area和TotRms_AbvGrd皆分別與目標變數具有高度相關(相關係數分別為：cor = 0.71 and cor = 0.50)。</p>
<p>我們將兩變數投入線性模型進行配適，並比較各自投入線性回歸的係數。</p>
<p>模型1: 投入兩高相關性變數</p><pre class="crayon-plain-tag">lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)    Gr_Liv_Area  TotRms_AbvGrd  
##       49953.6          137.3       -11788.2</pre><p>模型2: 單獨使用Gr_Liv_Area進行回歸的結果。</p><pre class="crayon-plain-tag">lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)
## 
## Coefficients:
## (Intercept)  Gr_Liv_Area  
##       17797          108</pre><p>模型3: 單獨使用TotRms_AbvGrd進行回歸的結果。</p><pre class="crayon-plain-tag">lm(Sale_Price ~ TotRms_AbvGrd, data = ames_train)</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
## lm(formula = Sale_Price ~ TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)  TotRms_AbvGrd  
##         26820          23731</pre><p>可以發現：</p>
<ul>
<li>兩高相關變數同時進行回歸配適時，會得到Gr_Liv_Area與目標變數為正相關而TotRms_AbvGrd與目標變數為負相關的係數結果。</li>
<li>單獨投入回歸配適時，Gr_Liv_Area和TotRms_AbvGrd都變成正向係數。</li>
</ul>
<p>以上係數正負號有問題的結果，就是模型存在共線性問題時常遇到的結果。具高度相關的變數係數會過度膨脹(over-inflated)且變得很不穩定(fluctuate significantly)。係數項大幅波動的結果，就是過度配適(overfitting)，也就是說，在權衡“bias-variance”(即誤差v.s模型變異度)階段中具模型有很高的變異度。雖然我們可以在建模後(事後)，使用VIF(variance inflation factors)變異數膨脹因子來檢查哪些變數有共線性並移除，但仍不太清楚要移除哪個變數好，或者是擔心移除變數會讓模型失去有價值的訊號。</p>
<h4>2. Insufficient Solution 解決方案不充分</h4>
<p>當特徵個數(p)超過觀測個數(n)(p&gt;n)時，OLS(最小平方法)回歸解矩陣是不可逆的(solution matrix is not invertible)。這代表(1)最小平方估計參數解不是唯一。會存在無限的可用的解，但這些解大多都過度配適資料。(2)在大多數的情況下，這些結果在計算上是不可行的(computationally infeasible)。</p>
<p>因此，只能透過移除特徵變數直到(n&gt;p)再將資料投入最小平方回歸模型進行配適。雖然可以透過人工的方式事前處理特徵變數過多的問題，但可能很麻煩且容易出錯。</p>
<h4>3. Interpretability 可解釋性</h4>
<p>當我們的特徵變數個數量很大時，我們會希望識別出具有最強解釋效果的較小子集合(Subsetting)。通常我們會偏好透過變數選取(feature selection)的方法來解決。其中一個變數選取法叫做「hard thresholding feature selection(硬閾值特徵選取)」，可以透過線性模型選取(linear model selection)來進行（best subsets &amp; stepwise regression)，但這個方法通常計算上效率較低也不好擴展，而且是直接透過增加或減少特徵變數的方式來進行模型比較。另一個方法叫做「soft thresholding feature selection(軟閾值特徵選取)」，此法將慢慢的將特徵效果推向0。</p>
<h3>Regularized Regression 正規化回歸</h3>
<p>當遇到以上問題，一個替代OLS回歸的方法就是透過「正規化回歸 regularized regression」(又稱作penalized models或shrinkage method)來對回歸係數做管控。正規化回歸模型會對回歸係數大小做出約束，並逐漸的將回歸係數壓縮到零。而對回歸係數的限制將有助於降低係數的幅度和波動，並降低模型的變異。</p>
<p>正規化回歸的目標函式與OLS回歸類似，但多了一個懲罰參數(penalty parameter, P)：</p>
<p>\[minimize\bigg \{ SSE + P \bigg \}\]</p>
<p>而常見的懲罰係數有兩種(分別對應到ridge回歸模型 &amp; lasso回歸模型)，效果是類似的。懲罰係數將會限制回歸係數的大小，除非該變數可以使誤差平方和(SSE)降低對應水準，該特徵係數才會上升。以下就來進一步介紹兩種最常見的正規化回歸法。</p>
<h3>Ridge Regression</h3>
<p>Ridge Regression透過將懲罰參數\(\lambda \sum_{j=1}^p \beta_{j}^2\)加入目標函式中。也因為該參數為對係數做出二階懲罰，故又稱為L2 Penalty懲罰參數。</p>
<p>\[minimize\bigg \{ SSE +\lambda \sum_{j=1}^p \beta_{j}^2 \bigg \}\]</p>
<p>而L2懲罰參數的值可以透過「tuninig parameter, \(\lambda\)」來控制。當\(\lambda \rightarrow 0\)，L2懲罰參數就跟OLS回歸一樣，目標函式只有最小化SSE；而當\(\lambda \rightarrow \infty\)時，懲罰效果最大，迫使所有係數都趨近於0。(如下圖範例所示，係數隨著\(\lambda\)由0變化到821(log(821) = 6.7)，係數逐漸被ridge法正規化的過程。)</p>
<p><img src="/wp-content/uploads/2019/01/unnamed-chunk-177-1.png" alt="Regularized Regression, Ridge" /></p>
<p>由上圖我們可以觀察到：</p>
<ul>
<li>部分特徵係數會波動，直到\(\log(\lambda) \simeq 0\)才逐漸穩定開始收斂至0。這表示存在多重共線性，唯有透過\(\log(\lambda) &gt; 0\)校正參數來限制係數，以降低模型變異和誤差。</li>
</ul>
<p>但如何決定最適的shrinkage的程度(校正參數\(\lambda\))來最小化模型誤差呢？我們會透過以下的R程式語言實作來說明。</p>
<h4>實作ridge regression using R</h4>
<p>這邊主要會使用glmnet套件來執行。因為glmnet函數沒使用formula參數，故我們需要將資料分成x和y來帶入glmnet函數參數值，且x參數須為matrix型態。而我們將會使用model.matrix()函數來將質性變數轉換成dummy variables虛擬變量（如果資料維度很大的時候，則可參考更有效率的處理函數 Matrix::sparse.model.matrix）(並將第一行的截距項忽略)。另外，我們也將目標變數進行log轉換(因為目標變數分佈有偏)。</p>
<p>step 1: 將資料分成預測變數矩陣與目標變數，並替目標變數進行log轉換</p><pre class="crayon-plain-tag">ames_train_x &lt;- model.matrix(object = Sale_Price ~ ., data =  ames_train)[, -1]
ames_train_y &lt;- log(ames_train$Sale_Price)

ames_test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1]
ames_test_y &lt;- log(ames_test$Sale_Price)</pre><p>查看質化變數經轉換成虛擬變量後，我們特徵變數維度從原始81個維度，增加為307個維度（不含截距項）。</p><pre class="crayon-plain-tag"># What is the dimension of of your feature matrix?
dim(ames_train_x)</pre><p></p><pre class="crayon-plain-tag">## [1] 2054  307</pre><p>step 2: 執行ridge model</p>
<p>我們使用glmnet::glmnet()來建立Ridge模型，並使用參數alpha來指定說要使用哪種懲罰參數，alpha = 0為Ridge，alpha = 1為lasso，或是 0 \(\leq\) alpha \(\leq\) 1為elastic net。</p>
<p>glmnet會在執行時，主要會做兩件事：</p>
<ol>
<li>預測變數在投入正規化回歸模型(regularized regression)前是需要被標準化(standardized)。而glmnet函數則會幫你處理標準化這件事。如果你已在使用glmnet前標準化預測變數，則可將參數設定為standaradize = FALSE。</li>
<li>glmnet會跨非常大的\(\lambda\)區間來執行ridge model。如下圖所示：</li>
</ol>
<p></p><pre class="crayon-plain-tag"># Apply Ridge regression to ames data
ames_ridge &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

plot(ames_ridge, xvar = "lambda")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-181-1.png" alt="Regularized Regression, Ridge" /></p>
<p>我們可以用以下方式看\(\lambda\)的區間。glmnet預設為使用100組\(\lambda\)，當然你也可以自行指定\(\lambda\)區間，但絕大多數時候是不太需要去調整的。</p><pre class="crayon-plain-tag">ames_ridge$lambda</pre><p></p><pre class="crayon-plain-tag">##   [1] 279.10348741 254.30870285 231.71661862 211.13155287 192.37520764
##   [6] 175.28512441 159.71327708 145.52478975 132.59676852 120.81723707
##  [11] 110.08416672 100.30459276  91.39380920  83.27463509  75.87674603
##  [16]  69.13606504  62.99420758  57.39797580  52.29889783  47.65280789
##  [21]  43.41946378  39.56219829  36.04760164  32.84523206  29.92735217
##  [26]  27.26868869  24.84621355  22.63894441  20.62776299  18.79524938
##  [31]  17.12553123  15.60414624  14.21791689  12.95483634  11.80396439
##  [36]  10.75533273   9.79985861   8.92926618   8.13601479   7.41323366
##  [41]   6.75466241   6.15459682   5.60783940   5.10965440   4.65572679
##  [46]   4.24212485   3.86526617   3.52188658   3.20901188   2.92393211
##  [51]   2.66417804   2.42749981   2.21184742   2.01535299   1.83631458
##  [56]   1.67318146   1.52454063   1.38910464   1.26570041   1.15325908
##  [61]   1.05080672   0.95745595   0.87239820   0.79489675   0.72428031
##  [66]   0.65993724   0.60131024   0.54789149   0.49921832   0.45486914
##  [71]   0.41445982   0.37764035   0.34409183   0.31352366   0.28567108
##  [76]   0.26029285   0.23716915   0.21609970   0.19690199   0.17940976
##  [81]   0.16347149   0.14894914   0.13571691   0.12366019   0.11267456
##  [86]   0.10266486   0.09354440   0.08523417   0.07766220   0.07076291
##  [91]   0.06447653   0.05874861   0.05352954   0.04877413   0.04444117
##  [96]   0.04049314   0.03689584   0.03361811   0.03063157   0.02791035</pre><p>我們亦可透過coef()函數去查看每個\(\lambda\)懲罰係數模型產生的係數。glmnet會儲存每個模型的所有係數，並按照\(\lambda\)由大到小排列。<br />
但由於預測變數實在太多，我們只挑Gr_Liv_Area和TotRms_AbvGrd兩個變數分別在\(lamda\)為最大(279.1034874)和最小值(0.0279103)的係數值。</p>
<p>對應到最小\(\lambda\)值的係數。</p><pre class="crayon-plain-tag"># coefficients for the smallest lambda parameters
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 100]</pre><p></p><pre class="crayon-plain-tag">##   Gr_Liv_Area TotRms_AbvGrd 
##  0.0001004011  0.0096383231</pre><p>對應到最大\(\lambda\)值的係數。可以看到當\(\lambda\)懲罰係數很大時，係數幾乎都被壓縮逼近到0。</p><pre class="crayon-plain-tag"># coefficients for the largest lambda parameters
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 1]</pre><p></p><pre class="crayon-plain-tag">##                                      Gr_Liv_Area 
## 0.0000000000000000000000000000000000000005551202 
##                                    TotRms_AbvGrd 
## 0.0000000000000000000000000000000000001236183840</pre><p>雖然看到了懲罰係數\(\lambda\)對係數限制的效果，但到目前為止我們還不了解懲罰限制式對模型的改善程度。</p>
<h4>Tuning</h4>
<p>\(\lambda\)是一個用來校正參數，用來避免模型對訓練資料集產生過度配適的情況。然而，為了找出最適的\(\lambda\)，我們會需要利用cross-validation來協助。我們可以透過cv.glmnet()函數來執行k-fold cross validation，預設k=10。</p><pre class="crayon-plain-tag"># Apply CV Ridge regression to ames data
ames_ridge &lt;- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

# plot result
plot(ames_ridge)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-185-1.png" alt="Regularized Regression, Ridge" /></p>
<p>上圖為執行10-fold cross validation，不同\(\lambda\)值所對應的MSE(mean squared error)的結果。我們可以就MSE變化的區間發現，其實改善幅度有限；但是我們發現，當我們對係數使用\(\log(\lambda) \geq 0\)的懲罰係數，則MSE就會大幅上升。圖上方的數字(299)表示模型中的變數數目。因為Ridge Model不會直接強制讓變數係數變成0，所以模型中的變數數目維持不變。(但在lasso和elastic模型中就不大一樣)。</p>
<p>圖中的第一和第二條垂直虛線則分別代表：</p>
<ul>
<li>對應最小MSE的\(\lambda\)</li>
<li>對應最小MSE一個標準差內的最大\(\lambda\)</li>
</ul>
<p>我們可以透過以下指令查看兩者的值：</p>
<p>最小MSE值</p><pre class="crayon-plain-tag">min(ames_ridge$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02207459</pre><p>對應最小MSE的\(\lambda\)</p><pre class="crayon-plain-tag">ames_ridge$lambda.min</pre><p></p><pre class="crayon-plain-tag">## [1] 0.1489491</pre><p>對應最小MSE的\(\log(\lambda)\)</p><pre class="crayon-plain-tag">log(ames_ridge$lambda.min)</pre><p></p><pre class="crayon-plain-tag">## [1] -1.90415</pre><p>距離最小MSE一個標準差內的MSE值</p><pre class="crayon-plain-tag">ames_ridge$cvm[ames_ridge$lambda == ames_ridge$lambda.1se]</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02474565</pre><p>對應距離最小MSE一個標準差內的\(\lambda\)</p><pre class="crayon-plain-tag">ames_ridge$lambda.1se</pre><p></p><pre class="crayon-plain-tag">## [1] 0.6013102</pre><p>對應距離最小MSE一個標準差內的\(\log(\lambda)\)</p><pre class="crayon-plain-tag">log(ames_ridge$lambda.1se)</pre><p></p><pre class="crayon-plain-tag">## [1] -0.5086443</pre><p>在lasso和elastic模型中，使用對應距離最小MSE一個標準差內的\(\lambda\)是很明顯的結果。但在ridge模型中，我們可先用視覺化圖表來衡量。</p>
<p>我們將所有對應不同\(\lambda\)的係數值繪出，並以紅色垂直虛線表示對應最小MSE一個標準差內\(\lambda\)。</p><pre class="crayon-plain-tag">ames_ridge_min &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

{
  plot(ames_ridge_min, xvar = "lambda")
  abline(v = log(ames_ridge$lambda.1se), col = "red", lty = "dashed")
  }</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-192-1.png" alt="Regularized Regression, Ridge" /></p>
<p>由上圖我們可以看到，在極大化預測精準度的同時，我們可以對係數做出多少限制。</p>
<h4>Advantages &amp; Disadvantages</h4>
<h5>優點</h5>
<ol>
<li>實質上，Ridge模型會將具有相關性的變數推向彼此，並避免使得其中一個有極大正係數另一個有極大負係數的情況。</li>
<li>此外，許多不相干的變數係數會被逼近為0(不會等於0)。表示我們可以降低我們資料集中的雜訊，幫助我們更清楚的識別出模型中真正的訊號(signals)。</li>
</ol>
<p>比如說以下我們來看對應\(\lambda\)值為最小MSE一個標準差的模型中，Top 25個具有影響力的變數分別有哪些。</p><pre class="crayon-plain-tag">coef(ames_ridge, s = "lambda.1se") %&gt;% # 308 x 1 sparse Matrix of class "dgCMatrix"
  as.matrix() %&gt;% 
  as.data.frame() %&gt;% 
  add_rownames(var = "var") %&gt;% 
  `colnames&lt;-`(c("var","coef")) %&gt;%
  filter(var != "(Intercept)") %&gt;%  #剔除截距項
  top_n(25, wt = coef) %&gt;% 
  ggplot(aes(coef, reorder(var, coef))) +
  geom_point() +
  ggtitle("Top 25 influential variables") +
  xlab("Coefficient") +
  ylab(NULL)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-193-1.png" alt="Regularized Regression, Ridge" /></p>
<h5>缺點</h5>
<ol>
<li>然而，Ridge模型會保留所有變數。假如你覺得需要保留所有變數並將較無影響力的變數雜訊給減弱並最小化共線性，則模型是好的。</li>
<li>Ridge 模型是不具有變數挑選(feature selection)功能的。假如妳需要更近一步減少資料中的訊號(signals)並尋找subset來解釋，則lasso模型會更適合。</li>
</ol>
<h3>Lasso Regression</h3>
<p>Lasso的全名為Least absolute shrinkage and selection operator(<a href="https://www.jstor.org/stable/2346178?seq=1#page_scan_tab_contents" target="_blank" rel="noopener noreferrer"> Tibshirani, 1996</a>)。是Ridge模型外的另一個選擇，並在目標函式的限制式有所調整。有別於二階懲罰(L2 Penalty)，Lasso模型在目標函式中所使用的是一階懲罰式(L1 Penalty)\(\lambda \sum_{j=1}^p |\beta_{j}|\)。</p>
<p>\[ minimize \bigg \{ SSE + \lambda \sum_{j=1}^p |\beta_{j}| \bigg\} \]</p>
<p>不像Ridge模型只會將係數逼近到接近零（但不會真的是0），Lasso模型則真的會將係數推進成0(如下圖)。因此，Lasso模型不僅能使用正規化(regulariztion)來優化模型，亦可以自動執行變數篩選(Feature selection)。</p>
<p><img src="/wp-content/uploads/2019/01/unnamed-chunk-194-1.png" alt="Regularized Regression, Lasso" /></p>
<p>從上圖我們可以看到，在\(\log(\lambda) = -6\)時，所有8個變數（圖表上方數字）都包還在模型內，而當在\(\log(\lambda) = -3\)時只剩下6個變數，最後當在\(\log(\lambda) = -1\)時，只剩2個變數被保留在模型內。因此，當遇到資料變數非常多時，Lasso模型是可以幫你識別並挑選出有最強（也最一致）訊號的變數。</p>
<h4>使用R實作Lasso模型</h4>
<p>建置Lasso模型的方法跟Ridge模型作法一樣，只需將glmnet()函數中alpha參數改設定為1。</p><pre class="crayon-plain-tag">ames_lasso &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso, xvar = "lambda")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-195-1.png" alt="Regularized Regression, Lasso" /></p>
<p>從上圖中可以看到大約是在\(\log(\lambda) \rightarrow -6\)的時候，被保留在模型中的變數數目大量下降。而且，當\(\log(\lambda) = -10 \rightarrow \lambda = 0\)，在沒有懲罰限制式(跟OLS模型一樣)時，數個變數的係數值都非常大，表示可能變數與變數間存在高度相關而導致他們的係數變得極大。當我們對模型做出限制，這些資料中為雜訊的變數係數就會被推進成0。</p>
<p>同樣的，如何選擇最適的懲罰係數\(\lambda\)亦和Ridge模型一樣透過執行cross validation來看不同\(\lambda\)對應的MSE值，並找出使MSE發生最小值(或在一個標準差之內)的\(\lambda\)來決定。</p>
<h4>Tuning</h4>
<p>為了找出最適的\(\lambda\)，我們跟Ridge模型一樣，使用cv.glmnet()函數來看不同\(\lambda\)對應的MSE值，並將alpha參數設定為1。</p><pre class="crayon-plain-tag">ames_lasso &lt;- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-196-1.png" alt="Regularized Regression, Lasso" /></p>
<p>從上圖可以發現，我們可以透過採用大約\( -6 \leq \log(\lambda) \leq -4\)的懲罰係數區間來最小化模型的MSE。而且在該懲罰係數區間，不僅最小化模型的MSE，同時也壓縮模型內被保留下來的變數數目至大約\(131 \geq p \geq 63\)的區間。</p>
<p>同樣的，我們可以透過以下方法來取得最小MSE和一個標準差內的MSE以及對應的\(\lambda\)。</p>
<p>minimum MSE</p><pre class="crayon-plain-tag">min(ames_lasso$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02309004</pre><p>對應最小MSE的\(\log(\lambda)\)</p><pre class="crayon-plain-tag">log(ames_lasso$lambda.min)</pre><p></p><pre class="crayon-plain-tag">## [1] -5.555725</pre><p>within 1 S.E. of minimum MSE</p><pre class="crayon-plain-tag">ames_lasso$cvm[ames_lasso$lambda == ames_lasso$lambda.1se]</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02626365</pre><p>對應到最小MSE一個標準差內的最大\(\log(\lambda)\)</p><pre class="crayon-plain-tag">ames_lasso$lambda.1se</pre><p></p><pre class="crayon-plain-tag">## [1] 0.01295484</pre><p>此時在Lasso模型的例子中，使用對應到最小MSE一個標準差內的最大\(\lambda\)當作懲罰係數的優勢是更明顯的。假設我們使用對應最小MSE的\(\lambda\)當作懲罰係數，那麼我們可以將原始變數集個數從307降到小於150左右。但考量到MSE也會有些變化性(variability)，因此我們可以合理假設，可以使用稍微更高的懲罰係數來達成類似的MSE程度，其\(\lambda\)所對應的變數數目約為小於70的水準左右。如果你分析的目的志在說明與詮釋預測變數，這樣懲罰係數的選擇應會有很大的幫助。</p><pre class="crayon-plain-tag">ames_lasso_min &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

{
  plot(ames_lasso_min, xvar = "lambda")
  abline(v = log(ames_lasso$lambda.min), col = "red", lty = "dashed")
  abline(v = log(ames_lasso$lambda.1se), col = "red", lty = "dashed")
}</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-201-1.png" alt="Regularized Regression, Lasso" /></p>
<h4>Advantages and Disadvantages</h4>
<h5>優點</h5>
<ol>
<li>與Ridge模型一樣，Lasso模型亦會將具有相關性的變數推向彼此，並避免使得其中一個有極大正係數另一個有極大負係數的情況。</li>
<li>與Ridge模型最大的差別，就是Lasso會將不具影響力的變數係數變成0，自動進行變數篩選(Feature selection)。這樣的處理方式簡化並自動化識別出那些對模型預測正確性有高度影響力的變數。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>然而，時常在我們移除變數的同時也會犧牲掉模型的正確性。所以為了得到Lasso產生的更清楚與簡潔的模型結果，我們也會降低模型的正確性。</li>
</ol>
<p>一般來說，Ridge和Lasso模型所產生的最小MSE不會有太大差別(如下結果所示)。所以除非你單純只看最小化MSE的結果，實質上他們兩的差異並不顯著。</p><pre class="crayon-plain-tag"># minimum Ridge MSE
min(ames_ridge$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02207459</pre><p></p><pre class="crayon-plain-tag"># minimum Ridge MSE
min(ames_lasso$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02309004</pre><p></p>
<h3>Elastic Net</h3>
<p>涵蓋Ridge和Lasso兩個模型的就是Elastic Net模型(<a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2005.00503.x" target="_blank" rel="noopener noreferrer">Zou and Hasie,005</a>)，該模型綜合了兩個懲罰限制式。</p>
<p>\[ minimize \bigg \{ SSE + \lambda_{1} \sum_{j=1}^p \beta_{j}^2 + \lambda_{2} \sum_{j=1}^p |\beta_{j}| \bigg \} \]</p>
<p>雖然Lasso模型會執行變數挑選，但一個源自於懲罰參數的結果就是，通常當兩個高度相關的變數的係數在被逼近成為0的過程中，可能一個會完全變成0但另為一個仍保留在模型中。此外，這種一個在內、一個在外的處理方法不是很有系統。相對的，Ridge模型的懲罰參數就稍具效率一點，可以有系統的<br />
將高相關性變數的係數一起降低。於是，Elastic Net模型的優勢就在於，它綜合了Ridge Penalty達到有效正規化優勢以及Lasso Penalty能夠進行變數挑選優勢。</p>
<h4>使用R實作 Elastic Net</h4>
<p>跟Ridge和Lasso一樣是使用glmnet()，並調整介於0~1之間的alpha參數。當alpha = 0.5時，Ridge和Lasso的組合是平均的，而當alpha\(\rightarrow\)0時，會有較多的Ridge Penalty權重，而當alpha\(\rightarrow\)1時，則會有較多的Lasso Penalty權重。</p><pre class="crayon-plain-tag">lasso    &lt;- glmnet(ames_train_x, ames_train_y, alpha = 1.0) 
elastic1 &lt;- glmnet(ames_train_x, ames_train_y, alpha = 0.25) 
elastic2 &lt;- glmnet(ames_train_x, ames_train_y, alpha = 0.75) 
ridge    &lt;- glmnet(ames_train_x, ames_train_y, alpha = 0.0)

par(mfrow = c(2,2), mar = c(4,2,4,2), + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1) \n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = 0.75) \n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = 0.25) \n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0) \n\n")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-204-1.png" alt="Regularized Regression, Elastic Net" /></p>
<h4>tuning</h4>
<p>在Ridge和Lasso模型中，\(\lambda\)是我們主要調整的參數，然而在Elastic Net模型中，我們會需要調教\(\lambda\)和alpha兩個參數。</p>
<p>首先，我們先建立一個fold_id，用來固定每一次建模每筆資料所在的fold id(1 ~ nfold)，即每次建模使用的CV folds都是相同的(如果單純只使用nfolds = 10，雖然每次建模都會進行10-fold的CV，但每一次資料所在的fold id卻是不一樣的)。(*在前的Ridge和Lasso的例子中，因為只會run一次CV.glmnet，來看不同\(\lambda\)值交叉驗證的MSE值，因此只需指定nfold數即可；但在Elastic Net模型中，會根據不同alpha(i.e., Ridge和Lasso權重佔比)來跑不同的cv.glmnet，為了確保模型結果不受亂數fold if結果影響，故固定每一次資料所在的fold id）</p>
<p>接著我們再建立一個tuning grid用來搜羅從0-1的區間的alpha值以及對應的空欄位值（包括最小MSE和一個標準差內的MSE以及兩者分別所對應的\(\lambda\)值）後續將用來儲存不同alpha參數跑出的CV模型結果。</p><pre class="crayon-plain-tag"># maintain the same folds across all models
fold_id &lt;- sample(x = 1:10, size = length(ames_train_y), replace = TRUE)

# search across a range of alphas
tuning_grid &lt;- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)</pre><p>接著，我們便可以開始迭代不同的alpha值(不同的Ridge &amp; Lasso權重組成)，套用CV Elastic Net，萃取出每一個alpha值對應的模型結果，包括最小MSE以及一個標準差內的MSE、以及兩者分別所對應的\(\lambda\)值。</p><pre class="crayon-plain-tag">for(i in seq_along(tuning_grid$alpha)){
  # fit CV model for each alpha value
  fit &lt;- cv.glmnet(x = ames_train_x, y = ames_train_y, alpha = tuning_grid$alpha[i],foldid = fold_id)

  # extract MSE and lambda values
  tuning_grid$mse_min[i]    &lt;- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    &lt;- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] &lt;- fit$lambda.min
  tuning_grid$lambda_1se[i] &lt;- fit$lambda.1se
}

tuning_grid</pre><p></p><pre class="crayon-plain-tag">## # A tibble: 11 x 5
##    alpha mse_min mse_1se lambda_min lambda_1se
##    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1   0    0.0217  0.0247    0.124       0.601 
##  2   0.1  0.0217  0.0251    0.0292      0.108 
##  3   0.2  0.0219  0.0257    0.0146      0.0590
##  4   0.3  0.0221  0.0257    0.0107      0.0393
##  5   0.4  0.0222  0.0258    0.00802     0.0295
##  6   0.5  0.0223  0.0258    0.00642     0.0236
##  7   0.6  0.0223  0.0264    0.00535     0.0216
##  8   0.7  0.0223  0.0265    0.00458     0.0185
##  9   0.8  0.0224  0.0265    0.00401     0.0162
## 10   0.9  0.0224  0.0265    0.00357     0.0144
## 11   1    0.0226  0.0262    0.00292     0.0118</pre><p>接著我們每一個alpha值跑出的模型中，最適\(\lambda\)值所對應的「最小MSE\(\pm\)1個標準差內的MSE區間」繪出，由下圖我們可以發現，<br />
正負一個標準差內的MSE都落在相近的正確率區間(即不同alpha水準值間，模型正確率沒有太大差別)。因此，我們可以選取alpha = 1的Lasso模型來重重倚賴他的變數挑選功能，並合理假設模型正確率不會有損失(因為alpha從1變化到0正確率都沒有什麼變化)。</p><pre class="crayon-plain-tag">tuning_grid %&gt;%
  mutate(se = mse_1se - mse_min) %&gt;% # 計算1個SE的距離
  ggplot(aes(alpha, mse_min)) + # 繪製不同alpha參數下，cv所得的最小MSE值
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
  ggtitle("MSE ± one standard error")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-207-1.png" alt="Regularized Regression, Elastic Net" /></p>
<h4>優缺點</h4>
<h5>優點</h5>
<ol>
<li>Elastic Model的優勢就是它綜合了Ridge Penalty的有效的正規化過程以及Lasso Penalty的變數篩選功能。讓我們能夠控制變數共線性的問題，能夠在p&gt;n時執行回歸，並降低資料中過多的雜訊，以幫助我們將具有影響力的變數獨立出來且維持住模型正確率。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>然而，Elastic Net，以及一般的regularization models，依舊有假設預測變數和目標變數需具有線性關係。雖然我們可以結合non-additive models(一種無母數回歸模型，non-parametric regression)交互作用，但當資料變數很多的時候，會是非常繁瑣與困難的。因此，當非線性關係存在時，可能考慮使用非線性迴歸的方法。</li>
</ol>
<h3>Predicting</h3>
<p>一旦決定好最適的模型後，我們可以使用predict()函數，來將模型套用在新的資料集上做預測。唯一要注意的是，你需要提供predict()s參數，來指定你要的\(\lambda\)值。</p>
<p>比如說以下我們想要建立一個Lasso模型，並使用對應最小MSE的\(\lambda\)值來做預測。</p><pre class="crayon-plain-tag"># create a lasso model
cv_lasso &lt;- cv.glmnet(x = ames_train_x, y = ames_train_y, alpha = 1)
min(cv_lasso$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02243528</pre><p></p><pre class="crayon-plain-tag">pred &lt;- predict(cv_lasso,newx = ames_test_x, s = cv_lasso$lambda.min)
mean((ames_test_y - pred)^2) #計算MSE</pre><p></p><pre class="crayon-plain-tag">## [1] 0.01483042</pre><p>我們預測結果的平均誤差平方為0.01483，比cv MSE的(0.02244)還來的更低些。</p>
<h3>使用其他Package來實作: caret &amp; h20</h3>
<p>glmnet不是唯一能夠處理Regularized Regression的套件。常用的幾種套件包括caret和h20。以下僅簡單介紹caret套件的執行方法。</p>
<p>caret</p><pre class="crayon-plain-tag"># load caret package
library(caret)

train_control &lt;- trainControl(method = "cv", number = 10)

caret_mod &lt;- train(
  x = ames_train_x, 
  y = ames_train_y,
  method = "glmnet", 
  prePro = c("center","scale","zv","nzv"),
  trControl = train_control,
  tuneLength = 10
)

head(caret_mod$results)</pre><p></p><pre class="crayon-plain-tag">##   alpha       lambda      RMSE  Rsquared       MAE     RMSESD RsquaredSD
## 1   0.1 0.0001289530 0.1621572 0.8442073 0.1056494 0.02614671 0.04570000
## 2   0.1 0.0002978982 0.1621441 0.8442298 0.1056370 0.02616059 0.04573050
## 3   0.1 0.0006881835 0.1619713 0.8445118 0.1053968 0.02646506 0.04638065
## 4   0.1 0.0015897932 0.1617801 0.8448180 0.1050524 0.02695754 0.04747639
## 5   0.1 0.0036726286 0.1615155 0.8453152 0.1045659 0.02754710 0.04877829
## 6   0.1 0.0084842486 0.1610666 0.8462117 0.1039097 0.02810395 0.04988037
##         MAESD
## 1 0.005227902
## 2 0.005231369
## 3 0.005283405
## 4 0.005294270
## 5 0.005145444
## 6 0.005222816</pre><p></p>
<h3>小結</h3>
<ol>
<li>鑑於傳統一般線性回歸模型沒有挑選變數之功能，且又遇到變數特徵數量非常大的時候，會容易使得模型假設不成立並發生模型過度配適的問題（變異度高），為降低此變異和樣本外誤差，可以使用Regularized Regression。</li>
<li>常見的Regularized Regression方法包括，Ridge(alpha = 0)、Lasso(alpha=1)和Elastic Net(\(0\leq alpha \leq 1\))。他們分別透過二階的Ridge Penalty和一階的Lasso Penalty或綜合兩種Penalty(並以alpha設定Ridge &amp; Lasso Penalty的權重)來對傳統OLS的目標函示的加上係數懲罰限制式，唯有該係數能夠降低的SSE幅度夠大才能增加其的係數大小。</li>
<li>Ridge會有系統的將干擾變數係數逼近0(但不會等於0)，而Lasso則會將較無影響力的變數係數變成0。前者正規化過程較有效(即同為高度相關的變數，最終不會產生一個係數為零一個係數不為零的情況)，但從頭到尾都會保留模型中所有變數；後者則具有變數挑選的功能，但正規化過程較不具系統性。而Elastic Net則綜合上述兩者的優勢，同時兼顧有效的正規化過程以及變數挑選功能。</li>
<li>但不管是Ridge, Lasso還是Elastic Net，這些一般性線性回歸模型都收限於「預測變數需與目標變數成線性關係」之假設，若假設不成立，仍得考慮非線性的回歸模型。</li>
<li>此學習筆記僅介紹Regularized Regrssion的基本概念。Regularized Regression方法亦延伸出其他parametric generalized linear models（包括logistic regression, multinomial, poisson, support vector machines）。此外亦存在許多其他可替代方法如Least Angle Regression和The Bayesian Lasso。</li>
</ol>
<hr />
<p><strong>更多統計學習筆記：</strong></p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p><strong>參考文章連結：</strong></p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
<li><a href="http://uc-r.github.io/regularized_regression" target="_blank" rel="noopener noreferrer">正規化回歸 Regularized Regression</a></li>
<li><a href="http://rpubs.com/skydome20/R-Note18-Subsets_Shrinkage_Methods" target="_blank" rel="noopener noreferrer">R筆記 – (18) Subsets &amp; Shrinkage Regression (Stepwise &amp; Lasso)</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/regularized-regression-ridge-lasso-elastic/">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/regularized-regression-ridge-lasso-elastic/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
			</item>
		<item>
		<title>Regression Tree &#124; 迴歸樹, Bagging, Bootstrap Aggregation &#124; R語言</title>
		<link>/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/</link>
					<comments>/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Wed, 02 Jan 2019 14:06:06 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[Bagging]]></category>
		<category><![CDATA[Bootstrap Aggregation]]></category>
		<category><![CDATA[ensemble learning]]></category>
		<category><![CDATA[Regression Tree]]></category>
		<category><![CDATA[集成學習]]></category>
		<guid isPermaLink="false">/?p=2322</guid>

					<description><![CDATA[<p>有別於「分類」樹(classification tree)是用來找尋「最能區分標籤資料類別」的一系列變數，「迴歸」樹(regression tree)則是用來找 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>有別於「分類」樹(classification tree)是用來找尋「最能區分標籤資料類別」的一系列變數，「迴歸」樹(regression tree)則是用來找尋「最能區分目標連續變數相近度」的一系列變數。迴歸樹投入變數可以式任何資料型態(與分類樹一樣)，唯一差別是迴歸樹的目標變數是連續型變數。</p>
<p>不管是哪種型態的決策樹，單一決策樹模型的結果不穩定度高（high variance），預測能力也較弱，也因此我們多半會搭配使用bootstrap aggregating(or Bagging)(一種集成學習法ensemble learning)，綜合多顆決策樹的預測結果，降低單一決策樹的變異度或不穩定性，避免overfitting過度配適的問題。而諸多更複雜的決策數模型也是由此衍伸，如隨機森林(Random Forest)和(Gradient Boosting Machine)。</p>
<p>而此篇學習筆記將會實作Regression Tree以及bagging。</p>
<h3>Regression Tree</h3>
<h3>載入實作所需的套件</h3>
<p>其中決策樹模型會使用到的套件就是rpart(分類決策樹也是使用該套件)。</p><pre class="crayon-plain-tag">library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging</pre><p>而範例資料則是使用AmesHousing package中的Ames Housing數據。</p>
<p>將資料分成70%訓練集，30%測試集：</p><pre class="crayon-plain-tag">set.seed(123)
ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</pre><p></p>
<h3>決定切點 (Deciding on splits)</h3>
<ol>
<li>決策樹的分枝演算法會由上而下進行，貪心的切割出最完整的樹。而每一個分裂點的選取，都會去檢視每一個投入變數的值切分的結果，找出使得切分後兩群(R1, R2)的組內誤差平方和(SSE, sums of squares error)最小的變數和切點。目標函示如下：<br />
\[minimize\space \bigg\{SSE = \sum_{i \in R1}(y_{i} &#8211; c_{1})^2 + \sum_{i \in R2}(y_{i} &#8211; c_{2})^2 \bigg\}\]</li>
<li>分枝演算法會反覆在各個切分後的子群聚中執行，直到達到停止切割的條件。</li>
<li>而這一棵經過貪心分枝結果的決策樹，會長的非常大，複雜度高。即便可能可以很好的預測訓練資料集，但很可能已過度配適(overfit)，當將預測規則套用在新資料集，預測能力不穩定度高。</li>
</ol>
<h3>Cost complexity criterion</h3>
<p>為了優化決策樹在新資料上的預測能力穩定度，我們會需要在訓練結果的複雜度、深度與預測穩定度取得平衡。</p>
<p>要找到這樣的平衡，我們的方法就是先長出一棵完整和最複雜的樹，並加入樹的複雜度作為目標函式的懲罰因子(cost complexity parameter,\(\alpha\))，並以此為目標來修剪出最適的決策樹模型。加入懲罰因子限制是的目標函數如下：</p>
<p>\[minimize \bigg \{ SSE + \alpha |T| \bigg \}\]</p>
<p>其中，T代表的是決策樹的終端節點(Terminal nodes)數量，\(\alpha\)則為懲罰參數，cost complexity parameter。</p>
<p>在不同給定的懲罰參數\(\alpha\)值下，我們可以找出修剪後最小的一棵樹，使得使得上述加入懲罰限制式的懲罰誤差(penalized error)(\(SSE + \alpha |T|\))最小(the lowest penalized error)。(以上概念與regularized regression的懲罰限制式很像。)</p>
<p>懲罰參數越小，會傾向得到較大較複雜的修剪後決策樹，而懲罰參數越大，修剪後決策樹則會較小較精簡。</p>
<p>因此，上述加入懲罰的誤差目標函示可以確保說，樹繼續變複雜的前提就是SSE的下降幅度要高於複雜度的懲罰成本(cost complexity penalty)。</p>
<p>而要怎麼決定最適的懲罰參數(\(\alpha\))，我們會評估多組對應不同水準\(\alpha\)值的模型組合，並使用交叉驗證法來計算每一個\(\alpha\)值的交叉驗證誤差(cross-validated error, X-val error)，來尋找使得交叉驗證誤差最小的的最適\(\alpha\)值(optimal \(\alpha\))和最適的決策樹子集合(optimal subtree)。</p>
<p>「交叉驗證誤差(X-Val error)」：</p>
<ul>
<li>此處的「誤差」就是加入懲罰因子的「懲罰誤差」\(SSE + \alpha |T|\)，只是是透過k-fold交叉驗證所計算出來的，所以亦稱做「交叉驗證誤差(X-val error)」。</li>
<li>也因為此誤差是計算在<strong>測試資料集</strong>與真實值的誤差，故亦可稱作<strong>Predicted Residual sum of squares(PRESS)</strong>，跟<strong>SSE</strong>不太一樣，SSE是使用<strong>訓練資料</strong>本身來計算與真實資料的誤差，因此SSE通常會比PRESS來的小。</li>
</ul>
<h3>優缺點</h3>
<p>迴歸樹的優點包括：</p>
<ol>
<li>結果很好解釋。</li>
<li>可以快速產生預測規則（每一次分割只需找出最適的變數與切點，沒有太複雜的計算）。</li>
<li>可以從樹的長相與順序得知變數的重要性，越先被拿來切割的變數，該變數所能降低的SSE幅度越大。</li>
<li>如果模型預測途中遇到遺失值，雖然不能從上而下將該筆資料列分類，但我們可以在遇遺失值的節點處，由下而上取每個節點的平均值回溯估計該遺失值。</li>
<li>決策樹模型提供一個非線性的&#8221;jagged&#8221;(參差不齊的) response平面，所以他可以配適真實不是很平滑(smooth)的回歸平面。而如果真實回歸平面是平滑的，則片段的常數(piece-wise constant)可以任意逼近它。</li>
<li>存在許多快速、可靠的演算法來學習這些樹模型。</li>
</ol>
<p>而迴歸樹的幾個缺點包括：</p>
<ol>
<li>單一迴歸樹的變異數大，預測能力不穩定（使用訓練資料的子集合即能大大改變樹的末梢節點長相。）</li>
<li>也因為單一迴歸樹的變異數大，預測精準度也不太佳。</li>
</ol>
<h3>使用R來執行基本的迴歸樹演算法</h3>
<p>我們可以使用rpart()來執行迴歸樹演算法，並使用rpart.plot()來繪製模型結果。方式大致與分類樹相同，唯一差別在於method參數要設定為&#8221;anova&#8221;。</p><pre class="crayon-plain-tag">m1 &lt;- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = "anova"
  )</pre><p>我們可以執行m1來看一下模型訓練結果，會詳細說明每一次切割所使用的變數和切點，以及切割前後的資料筆數變化。</p><pre class="crayon-plain-tag">m1</pre><p></p><pre class="crayon-plain-tag">## n= 2051 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 2051 13299200000000 181620.20  
##    2) Overall_Qual=Very_Poor,Poor,Fair,Below_Average,Average,Above_Average,Good 1699  4001092000000 156147.10  
##      4) Neighborhood=North_Ames,Old_Town,Edwards,Sawyer,Mitchell,Brookside,Iowa_DOT_and_Rail_Road,South_and_West_of_Iowa_State_University,Meadow_Village,Briardale,Northpark_Villa,Blueste 1000  1298629000000 131787.90  
##        8) Overall_Qual=Very_Poor,Poor,Fair,Below_Average 195   173369900000  98238.33 *
##        9) Overall_Qual=Average,Above_Average,Good 805   852605100000 139914.80  
##         18) First_Flr_SF&amp;lt; 1150.5 553   302338400000 129936.80 *
##         19) First_Flr_SF&amp;gt;=1150.5 252   374390700000 161810.90 *
##      5) Neighborhood=College_Creek,Somerset,Northridge_Heights,Gilbert,Northwest_Ames,Sawyer_West,Crawford,Timberland,Northridge,Stone_Brook,Clear_Creek,Bloomington_Heights,Veenker,Green_Hills 699  1260199000000 190995.90  
##       10) Gr_Liv_Area&amp;lt; 1477.5 300   247261100000 164045.20 *
##       11) Gr_Liv_Area&amp;gt;=1477.5 399   631199000000 211259.60  
##         22) Total_Bsmt_SF&amp;lt; 1004.5 232   164042700000 192946.30 *
##         23) Total_Bsmt_SF&amp;gt;=1004.5 167   281257000000 236700.80 *
##    3) Overall_Qual=Very_Good,Excellent,Very_Excellent 352  2874510000000 304571.10  
##      6) Overall_Qual=Very_Good 254   885511300000 273369.50  
##       12) Gr_Liv_Area&amp;lt; 1959.5 155   325667700000 247662.30 *
##       13) Gr_Liv_Area&amp;gt;=1959.5 99   297033800000 313618.30 *
##      7) Overall_Qual=Excellent,Very_Excellent 98  1100817000000 385440.30  
##       14) Gr_Liv_Area&amp;lt; 1990 42    78801640000 325358.30 *
##       15) Gr_Liv_Area&amp;gt;=1990 56   756691700000 430501.80  
##         30) Neighborhood=College_Creek,Edwards,Timberland,Veenker 8   115305100000 281887.50 *
##         31) Neighborhood=Old_Town,Somerset,Northridge_Heights,Northridge,Stone_Brook 48   435248600000 455270.80  
##           62) Total_Bsmt_SF&amp;lt; 1433 12    31430660000 360094.20 *
##           63) Total_Bsmt_SF&amp;gt;=1433 36   258880600000 486996.40 *</pre><p>我們可以將上述模型結果用rpart.plot()函數來進行視覺化呈現。該函數參數可以進階調整圖形視覺畫呈現方式，可參考<a href="http://www.milbo.org/rpart-plot/prp.pdf" target="_blank" rel="noopener noreferrer">這篇文件</a>。</p><pre class="crayon-plain-tag">rpart.plot(m1)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-76-1.png" alt="plot of chunk unnamed-chunk-76" /></p>
<p>從上圖可以觀察到：</p>
<ul>
<li>每一個節點的觀測值個數和目標變數平均值，顏色越深表示目標變數平均值越大。</li>
<li>該完整的決策樹共12個葉節點(leaf nodes or terminal nodes)以及11個內部節點(internal nodes)。</li>
</ul>
<p>此外，我們也可以使用plotcp()函數，看一下在rpart()修樹過程中，套用不同懲罰參數(\(\alpha\))值，使用預設10-fold cross validation (k=10)，計算hold-out資料與真實資料的誤差(懲罰誤差or交叉驗證誤差)變化圖。</p><pre class="crayon-plain-tag">plotcp(m1)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-77-1.png" alt="plot of chunk unnamed-chunk-77" /></p>
<p>由上圖可以發現：</p>
<ul>
<li>下x軸為cp or cost complexity parameter (\(\alpha\))，上x軸為末梢節點數(number of terminal nodes, |T|)，y軸為交叉驗證誤差(X-val relative error)。</li>
<li>隨著懲罰係數減少，X-val error降低幅度呈遞減趨勢，直到cp達到預設的0.01則停止。</li>
<li>而根據專家建議，通常可以接受使用<strong>與最小X-val error相距一個標準差以內</strong>所對應的Tree Size(|T|)來作為修樹的最佳大小<strong>（1-SE rule）</strong>。比如說上圖中，水平虛線通過minimum X-val error &#8211; 1 SE的誤差值約對應在cp = 0.015和T = 9，因此我們亦可以選擇T = 9。</li>
</ul>
<p>為了進一步說明為何要選擇T=12（或T=9 如果你採用 1 -SE rule)。我們可以將參數最小cp值設定為0，即沒有任何懲罰條件存在的狀況下，讓決策樹長到最大最完整。</p><pre class="crayon-plain-tag">m2 &lt;- rpart(
    formula = Sale_Price ~ .,
    data    = ames_train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

{plotcp(m2)
  abline(v = 12,lty = "dashed", col = "red")}</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-78-1.png" alt="plot of chunk unnamed-chunk-78" /></p>
<p>由上圖我們可以發現，當T&gt;12後，即使樹繼續長大，但交叉驗證誤差下降幅度遞減，因此我們可以果斷將樹修剪到最精簡T=12且誤差也是最小的程度。</p>
<p>我們也可以看一下詳細的cp值和對應的X-Val error。rpart()預設的懲罰參數為cp=0.1，並執行一連串由大到小的\(\alpha\)值來計算誤差，直到cp=0.1所對應最小樹的大小為12(or 11 splits)，最小交叉驗證誤差為0.262(xerror)。</p><pre class="crayon-plain-tag">m1$cptable</pre><p></p><pre class="crayon-plain-tag">##            CP nsplit rel error    xerror       xstd
## 1  0.48300624      0 1.0000000 1.0017486 0.05769371
## 2  0.10844747      1 0.5169938 0.5189120 0.02898242
## 3  0.06678458      2 0.4085463 0.4126655 0.02832854
## 4  0.02870391      3 0.3417617 0.3608270 0.02123062
## 5  0.02050153      4 0.3130578 0.3325157 0.02091087
## 6  0.01995037      5 0.2925563 0.3228913 0.02127370
## 7  0.01976132      6 0.2726059 0.3175645 0.02115401
## 8  0.01550003      7 0.2528446 0.3096765 0.02117779
## 9  0.01397824      8 0.2373446 0.2857729 0.01902451
## 10 0.01322455      9 0.2233663 0.2833382 0.01936841
## 11 0.01089820     10 0.2101418 0.2687777 0.01917474
## 12 0.01000000     11 0.1992436 0.2621273 0.01957837</pre><p></p>
<h3>Tuning 修樹</h3>
<p>除了使用懲罰參數cost complexity parameter (\(\alpha\))來限制樹的大小，我們也常透過以下參數來修樹：</p>
<ul>
<li>minsplit : 分裂前至少/最小所需的資料筆數。預設為20筆。如果將此數值調小，可讓末梢節點(terminal nodes)即使僅有少數一些資料，也可以產生對應的預測值。</li>
<li>maxdepth : 從根節點(root nodes)到葉節點(terminal nodes)間的最大內部節點數量上限。預設為30，可以長出還滿大一棵樹。</li>
</ul>
<p>rpart()函數中，是使用control這個參數來設定一系列的參數水準值(hyperparameter setting)。<br />
比如說，我們想要建立一顆minsplit = 10和maxdepth = 12的決策樹模型。可以透過以下方式：</p><pre class="crayon-plain-tag">m3 &lt;-
  rpart(formula = Sale_Price ~ .,
        data = ames_train,
        method = "anova",
        control = list(minsplit = 10, maxdepth = 12, xval = 10))

m3$cptable</pre><p></p><pre class="crayon-plain-tag">##            CP nsplit rel error    xerror       xstd
## 1  0.48300624      0 1.0000000 1.0007911 0.05768347
## 2  0.10844747      1 0.5169938 0.5192042 0.02900726
## 3  0.06678458      2 0.4085463 0.4140423 0.02835387
## 4  0.02870391      3 0.3417617 0.3556013 0.02106960
## 5  0.02050153      4 0.3130578 0.3251197 0.02071312
## 6  0.01995037      5 0.2925563 0.3151983 0.02095032
## 7  0.01976132      6 0.2726059 0.3106164 0.02101621
## 8  0.01550003      7 0.2528446 0.2913458 0.01983930
## 9  0.01397824      8 0.2373446 0.2750055 0.01725564
## 10 0.01322455      9 0.2233663 0.2677136 0.01714828
## 11 0.01089820     10 0.2101418 0.2506827 0.01561141
## 12 0.01000000     11 0.1992436 0.2480154 0.01583340</pre><p>雖然這個參數設定的方法挺管用，但如果想要評估不同參數水準組合的模型效果，手動調整就不具效率，此時，可以使用grid search的方法，來自動執行與比較不同參數水準值組合的效果並依據此來選擇最適合的模型參數設定。</p>
<p>執行grid search之前，我們先建立一個hyperparameter grid。舉例來說，想測試minsplit落在5-20區間和maxdepth落在8~15之間的參數組合(因為原始模型顯示最是模型節點數目為12)。這樣一來總共就會有16*8=128種參數組合的模型。</p><pre class="crayon-plain-tag"># 建立grid
hyper_grid &lt;- expand.grid(
  minsplit = seq(5,20,1),
  maxdepth = seq(8,15,1)
)
head(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">##   minsplit maxdepth
## 1        5        8
## 2        6        8
## 3        7        8
## 4        8        8
## 5        9        8
## 6       10        8</pre><p>來看grid中的總共組合數。</p><pre class="crayon-plain-tag">nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 128</pre><p>接著使用loop迴圈來一一建立這128種不同參數組合的模型。</p><pre class="crayon-plain-tag">models &lt;- list()

for(i in 1:nrow(hyper_grid)){
  minsplit &lt;- hyper_grid$minsplit[i]
  maxdepth &lt;- hyper_grid$maxdepth[i]

  models[[i]] &lt;- rpart(
    formula = Sale_Price ~., 
    data = ames_train, 
    method = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
  )
}</pre><p>接著，我們在撰寫一個函式來將每一組模型的最小交叉驗證誤差（x-val error)和對應的\(\alpha\)值取出，加到grid的資料集中。</p><pre class="crayon-plain-tag"># create the function to extract minimum error and associated alpha
# get CP
get_cp &lt;- function(x){
  min &lt;- which.min(x$cptable[,"xerror"]) # 回傳發生最小xerror的列index
  cp &lt;- x$cptable[min, "CP"]
}

get_min_error &lt;- function(x){
  min &lt;- which.min(x$cptable[,"xerror"])
  xerror &lt;- x$cptable[min, "xerror"]
}

# 新增每組cp產生模型的cp和minXerror兩個欄位，並依照交叉驗證誤差由小到大排列，取前五個組合
hyper_grid %&gt;% 
  mutate(
    cp = purrr::map_dbl(models,get_cp),
    error = purrr::map_dbl(models,get_min_error)
  ) %&gt;% 
  arrange(error) %&gt;% 
  top_n(-5, wt = error)</pre><p></p><pre class="crayon-plain-tag">##   minsplit maxdepth        cp     error
## 1        5       13 0.0108982 0.2421256
## 2        6        8 0.0100000 0.2453631
## 3       12       10 0.0100000 0.2454067
## 4        8       13 0.0100000 0.2459588
## 5       19        9 0.0100000 0.2460173</pre><p>可發現hyperparameter參數組選出的最適模型的交叉驗證誤差較原始模型改善了一些(m1 的最小交叉驗證誤差 0.262 v.s. 最適模型的 0.242)。</p>
<p>我們將這組得到最小交叉誤差的模型套用在test data進行預測。</p><pre class="crayon-plain-tag">optimal_tree &lt;- rpart(
  formula = Sale_Price ~ .,
  data = ames_train,
  method = "anova",
  control = list(minsplit = 5, maxdepth = 13, cp = 0.01)
)

pred &lt;- predict(optimal_tree, newdata = ames_test)
RMSE(pred = pred, obs = ames_test$Sale_Price)</pre><p></p><pre class="crayon-plain-tag">## [1] 39145.39</pre><p>我們預測的Root-Mean-Squared-Error 平均誤差平方和開根號(均方根誤差)為39145。</p>
<h3>Bagging</h3>
<h4>Bagging概念介紹</h4>
<p>就像之前提到的，單一棵決策樹有變異度高(high variance)的問題。雖然修剪樹規則可以降低變異度，但其實有更有效的方法來大副降低變異度並提升決策樹的預測效果，其中一個方法就是Boostrap Aggregation (或稱Bagging)(概念來自於 <a href="https://link.springer.com/article/10.1023%2FA%3A1018054314350" target="_blank" rel="noopener noreferrer">Breiman, 1996</a>)。</p>
<p>Bagging整合和平均多組模型的預測結果。透過平均每組模型的預測結果，<br />
可以有效降低來自單一模型的變異度，並且避免過度配適overfitting。基本上Bagging可以分為三個基本步驟：</p>
<ol>
<li>從訓練資料集(training data)中產生m組 bootstrap samples。Bootstrapped sample可以讓我們創造出有些差異的資料集，且這些資料集都是與原始訓練資料擁有相同分佈的。</li>
<li>對每一個bootstrapped sample訓練一組單一且未修剪的決策樹。</li>
<li>將每一組模型的結果平均，產生一個整體平均的預測值。</li>
</ol>
<p><img loading="lazy" class="alignnone size-full wp-image-2333" src="/wp-content/uploads/2019/01/Untitled-presentation.jpg" alt="Regression Tree, Bagging" width="960" height="720" srcset="/wp-content/uploads/2019/01/Untitled-presentation.jpg 960w, /wp-content/uploads/2019/01/Untitled-presentation-300x225.jpg 300w, /wp-content/uploads/2019/01/Untitled-presentation-768x576.jpg 768w, /wp-content/uploads/2019/01/Untitled-presentation-830x623.jpg 830w, /wp-content/uploads/2019/01/Untitled-presentation-230x173.jpg 230w, /wp-content/uploads/2019/01/Untitled-presentation-350x263.jpg 350w, /wp-content/uploads/2019/01/Untitled-presentation-480x360.jpg 480w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>Bagging的概念可以套用在任何回歸和分類模型上；然而Bagging主要還是對於具有高變異度的模型較有效果。比如說，較為穩定的有母數模型(parametric model)如線性回歸和multi-adaptive regression splines模型，使用Bagging所能改善的預測能力幅度都較小。</p>
<p>Bagging的一個好處為，平均來說，一個Bootstrap sample會包含63%(2/3)的訓練資料集，剩餘約33%(1/3)的訓練資料不再bootstrapped sample內。我們稱這一包不在bootstrapped sample內的資料為out-of-bag (OOB) sample。我們可以透過這個OOB sample來衡量模型的準確度，產生一個自然而然的交叉驗證過程。</p>
<h4>Bagging with ipred</h4>
<p>配飾bagged tree model是簡單的。我們改使用ipred::bagging來建立模型。並將參數coob設定為TRUE來表示我們將使用OOB sample來估計模型錯誤率。</p><pre class="crayon-plain-tag"># make bootstrapping reproducible
set.seed(123)

bagged_m1 &lt;- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  coob = TRUE
)

bagged_m1</pre><p></p><pre class="crayon-plain-tag">## 
## Bagging regression trees with 25 bootstrap replications 
## 
## Call: bagging.data.frame(formula = Sale_Price ~ ., data = ames_train, 
##     coob = TRUE)
## 
## Out-of-bag estimate of root mean squared error:  36543.37</pre><p>我們可以發現，經由bagged後的決策數模型錯誤率將近下降了快2602(from 39145 to 36543)。</p>
<p>需要注意的一點就是，一般來說，越多樹模型效果越好。當我們加入更多樹模型，就可以平均更多高變異度的單一樹模型。隨著模型的數量增加，我們可以得到大幅降低的變異度（以及錯誤率），並且直到某一數量水準值為止，變異度下降的幅度開始趨緩收斂，即可得到建立穩定模型所需的最適的樹數量。從經驗上來說，通常不太會用到超過50棵樹來穩定模型的誤差。</p>
<p>bagging預設會產生25組bootstrap sample和樹模型，但有時候我們可能需要更多顆樹模型。我們可以觀察樹的多寡和誤差的變化。我們觀察10~50棵樹在穩定模型誤差的效果變化。</p><pre class="crayon-plain-tag"># assess 10-50 bagged trees
ntree &lt;- 10:50

# create empty vector to store OOB RMSE values
rmse &lt;- vector(mode = "numeric",length = length(ntree))

for(i in seq_along(ntree)){
  # reproducibility: 固定bootstrap亂數結果
  set.seed(123)

  # perform bagged model
  model &lt;- bagging(
    formula = Sale_Price ~.,
    data = ames_train,
    coob = TRUE,
    nbagg = ntree[i]
  )

  # get OOB error
  rmse[i] &lt;- model$err
}

# 將不同樹數量與誤差的圖繪出
{plot(x = ntree, y = rmse,type = "l",lwd = 2)
abline(v = 25, col = "red", lty = "dashed")}</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-87-1.png" alt="plot of chunk unnamed-chunk-87" /></p>
<p>我們可以發現差不多在樹數量為25時誤差水準趨於穩定。所以如果單純僅加入更多樹不太能在優化模型的錯誤率。</p>
<h4>Bagging with caret</h4>
<p>使用ipred::bagging來進行bagging是簡單的，但使用caret來進行bagging會有更多好處如下：</p>
<ol>
<li>他可以更簡單的進行cross validation。雖然我們可以使用OOB error，但使用cross validation可以提供強大的真實test error的誤差期望值。</li>
<li>我們可以跨多個bagged trees來衡量變數的重要性。</li>
</ol>
<p>我們來建立一個10-fold cross validation的模型。</p><pre class="crayon-plain-tag"># Specify 10-fold cross validation
ctrl &lt;- trainControl(method = "cv", number = 10)

# CV bagged model
bagged_cv &lt;- train(
  Sale_Price ~.,
  data = ames_train,
  method = "treebag", 
  trControl = ctrl,
  importance = TRUE
)


# assess result
bagged_cv</pre><p></p><pre class="crayon-plain-tag">## Bagged CART 
## 
## 2051 samples
##   80 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 1846, 1845, 1847, 1845, 1846, 1847, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   36477.25  0.8001783  24059.85</pre><p>我們看到，交叉驗證的RMSE為36477。</p>
<p>我們亦可以看前20名重要變數分別為哪些。</p><pre class="crayon-plain-tag"># plot most important variables
plot(varImp(bagged_cv),20)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-89-1.png" alt="plot of chunk unnamed-chunk-89" /></p>
<p>在回歸模型中，衡量變數的重要性為根據該變數切割(split)所能讓總SSE減少的量，並綜合平均m顆樹的效果。有最大影響SSE平均下降幅度的變數會被認為是最重要的變數。而上圖中重要性的值(importance value)則是每個變數平均使SSE下降程度相較於最重要變數的相對百分比(值的區間為0-100)。</p>
<p>同時，我們亦來比較將此模型套用在測試資料集的錯誤率(v.s. 交叉驗證錯誤率)。</p><pre class="crayon-plain-tag">pred &lt;- predict(object = bagged_cv,newdata = ames_test)
RMSE(pred = pred,obs = ames_test$Sale_Price)</pre><p></p><pre class="crayon-plain-tag">## [1] 35262.59</pre><p>可以發現cross validation(36477)和套用在test set(out of sample)的估計錯誤率(estimated error)(35263)是非常相近的。</p>
<p>而在之後的學習筆記中，會看從bagging概念所延伸的模型(如隨機森林Random Forest和GBMs)可以如何更好的改善模型錯誤率。</p>
<h3>小結</h3>
<ol>
<li>決策樹是一個非常直覺的模型演算法，有許多好處，但卻有高變異度(high variance)的問題。雖然可以透過更改修剪樹的規則(如cp,minsplit,maxdepth)來改善誤差和過度配飾(overfitting)，但程度有限。</li>
<li>能有效解決高變異度的問題可以透過Bagging(Bootstrap Aggregation)法，綜合和平均多棵樹模型的預測值，降低單一棵樹模型所產生的高變異度，並提升模型預側能力（降低錯誤率）。隨著樹的數量的增加，Bagging模型錯誤率會在某一最適值下趨於收斂與穩定。</li>
<li>評估Bagging模型錯誤率常用的方法可以是簡單的OOB error或是更強大的cross-validation error，兩者分別可以透過inpred::bagging和caret::train來得到。而caret::train 另外的好處就是可以計算出跨bagged tree所計算出來的變數重要性。</li>
<li>Bagging的概念可以套用在任何回歸或分類演算法上，但對於本身就非常穩定的有母數模型如linear regression等效果就沒有這麼大。</li>
<li>Bagging概念也是更複雜模強大模型如隨機森林(Random Forest)和Gradient Boosting Machines (GBMs)的延伸。</li>
</ol>
<hr />
<p>更多統計模型學習筆記：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考連結：</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
<li><a href="http://uc-r.github.io/regression_trees" target="_blank" rel="noopener noreferrer">Regression Trees</a></li>
<li><a href="https://rpubs.com/skydome20/R-Note16-Ensemble_Learning" target="_blank" rel="noopener noreferrer">R筆記 – (16) Ensemble Learning(集成學習)</a></li>
<li><a href="http://www.milbo.org/rpart-plot/prp.pdf" target="_blank" rel="noopener noreferrer">Plotting rpart trees with the rpart.plot package</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Reshaping Data using tidyr &#124; 資料合併與分離、變長或變寬的小技巧 &#124; R 統計</title>
		<link>/tidyr-reshaping-data/</link>
					<comments>/tidyr-reshaping-data/#respond</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Thu, 06 Dec 2018 04:17:02 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[資料處理]]></category>
		<guid isPermaLink="false">/?p=2230</guid>

					<description><![CDATA[<p>介紹 tidyr 套件中四款基本資料整理函數，包括gather(), spread(), separate(), unite()。其中gather()和spre [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/tidyr-reshaping-data/">Reshaping Data using tidyr | 資料合併與分離、變長或變寬的小技巧 | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>介紹 tidyr 套件中四款基本資料整理函數，包括gather(), spread(), separate(), unite()。其中gather()和spread()功能互補，一個將「寬」資料堆疊成「長」資料，另一個則將「長」資料擴展為「寬」資料。而separate()和unite()則是用做分裂或合併變數。</p>
<h3>tidyr package</h3>
<p>本學習筆記會介紹 tidyr 套件所提供數據整理的基本四種功能，包括：</p>
<ul>
<li>gather(): 將「寬」資料變「長」</li>
<li>spread(): 將「長」資料變「寬」</li>
<li>separate(): 將單一欄位切分成多個欄位</li>
<li>unite(): 將多個欄位合併成單一欄位</li>
</ul>
<p>以下載入所需套件 tydyr和dplyr</p><pre class="crayon-plain-tag">library(tidyr)
library(dplyr)</pre><p></p>
<h3>%&gt;% Operator</h3>
<p></p><pre class="crayon-plain-tag">library(magrittr)</pre><p>雖然非必要，但套件tidyr和dplyr都會使用magrittr套件中的pipe operator %&gt;%，可以方便快速的串接多行指令。該運算元會將前一個指令或函數運算所產生的結果投入下一個指令或函數，比如說，在篩選資料時的指令：</p>
<p>filter(data, variable == <em>numeric_value</em>)</p>
<p>or</p>
<p>data %&gt;% filter(variable == <em>numeric_value</em>)</p>
<p>以上兩者的結果會是一樣的。</p>
<h3>gather() function</h3>
<p><strong>目的</strong>: 將「寬」資料重塑成「長」資料</p>
<p><strong>說明</strong>: 有時會遇到資料沒有妥當堆疊，諸多感興趣的共同屬性分在多個欄位，此時就需要將這些感興趣的共同屬性重新聚合成單一變數，透過gather()函數，可將多個欄位折疊為成對的key-value。比如說：</p><pre class="crayon-plain-tag"># original data frame
df</pre><p></p><pre class="crayon-plain-tag">## # A tibble: 12 x 6
##    Group  Year Qtr.1 Qtr.2 Qtr.3 Qtr.4
##  1     1  2006    15    16    19    17
##  2     1  2007    12    13    27    23
##  3     1  2008    22    22    24    20
##  4     1  2009    10    14    20    16
##  5     2  2006    12    13    25    18
##  6     2  2007    16    14    21    19
##  7     2  2008    13    11    29    15
##  8     2  2009    23    20    26    20
##  9     3  2006    11    12    22    16
## 10     3  2007    13    11    27    21
## 11     3  2008    17    12    23    19
## 12     3  2009    14     9    31    24</pre><p></p><pre class="crayon-plain-tag">long_df &lt;- df %&gt;% gather(key = Quarter, value = Revenue, Qtr.1:Qtr.4)
head(long_df,24)</pre><p></p><pre class="crayon-plain-tag">## # A tibble: 24 x 4
##    Group  Year Quarter Revenue
##  1     1  2006 Qtr.1        15
##  2     1  2007 Qtr.1        12
##  3     1  2008 Qtr.1        22
##  4     1  2009 Qtr.1        10
##  5     2  2006 Qtr.1        12
##  6     2  2007 Qtr.1        16
##  7     2  2008 Qtr.1        13
##  8     2  2009 Qtr.1        23
##  9     3  2006 Qtr.1        11
## 10     3  2007 Qtr.1        13
## # ... with 14 more rows</pre><p>而以下指令亦可產生相同結果</p><pre class="crayon-plain-tag">df %&gt;% gather(Quarter, Revenue, Qtr.1:Qtr.4)
df %&gt;% gather(Quarter, Revenue, -Group, -Year)
df %&gt;% gather(Quarter, Revenue, 3:6)
df %&gt;% gather(Quarter, Revenue, Qtr.1, Qtr.2, Qtr.3, Qtr.4)</pre><p></p>
<h3>separate() function</h3>
<p><strong>目的</strong>: 將單一變數一切為二</p>
<p><strong>說明</strong>: 很多時候單一變數欄位會包含多個變數，或是部分變數你不感興趣，如以下範例：</p><pre class="crayon-plain-tag">## # A tibble: 9 x 5
##   Grp_Ind Yr_Mo    City_State       First_Last        Extra_variable      
## 1 1.a     2006_Jan Dayton (OH)      George Washington XX01person_1  
## 2 1.b     2006_Feb Grand Forks (ND) John Adams        XX02person_2  
## 3 1.c     2006_Mar Fargo (ND)       Thomas Jefferson  XX03person_3  
## 4 2.a     2007_Jan Rochester (MN)   James Madison     XX04person_4  
## 5 2.b     2007_Feb Dubuque (IA)     James Monroe      XX05person_5  
## 6 2.c     2007_Mar Ft. Collins (CO) John Adams        XX06person_6  
## 7 3.a     2008_Jan Lake City (MN)   Andrew Jackson    XX07person_7  
## 8 3.b     2008_Feb Rushford (MN)    Martin Van Buren  XX08person_8  
## 9 3.c     2008_Mar Unknown          William Harrison  XX09person_9</pre><p>在這個範例中，我們目標是希望將每個變數字串分離。可以使用separate()函數來達成此目標，將單一字串變數拆分成多個欄位。</p><pre class="crayon-plain-tag">separate_df &lt;- 
  df2 %&gt;% 
  separate(col = Yr_Mo, into = c("Year","Month")) %&gt;% 
  separate(col = City_State, into = c("City","State"), sep = " \\(") %&gt;% 
  mutate(State = gsub(pattern = "\\)",replacement = "",x = State)) %&gt;% 
  separate(col = First_Last, into = c("First", "Last"))
head(separate_df)</pre><p></p><pre class="crayon-plain-tag">## # A tibble: 6 x 8
##   Grp_Ind Year  Month City        State First  Last       Extra_variable
## 1 1.a     2006  Jan   Dayton      OH    George Washington XX01person_1  
## 2 1.b     2006  Feb   Grand Forks ND    John   Adams      XX02person_2  
## 3 1.c     2006  Mar   Fargo       ND    Thomas Jefferson  XX03person_3  
## 4 2.a     2007  Jan   Rochester   MN    James  Madison    XX04person_4  
## 5 2.b     2007  Feb   Dubuque     IA    James  Monroe     XX05person_5  
## 6 2.c     2007  Mar   Ft. Collins CO    John   Adams      XX06person_6</pre><p>而以下指令亦可產生相同效果</p><pre class="crayon-plain-tag">df2 %&gt;% separate(col = Yr_Mo, into = c("Year","Month"))
df2 %&gt;% separate(col = Yr_Mo, into = c("Year","Month"), sep = "_")

df2 %&gt;% separate(col = First_Last, into = c("First", "Last"))
df2 %&gt;% separate(col = First_Last, into = c("First", "Last"), sep = " ")</pre><p></p>
<h3>unite() function:</h3>
<p><strong>目的</strong>: 將兩個變數合併為一</p>
<p><strong>說明</strong>: 很多時候會想將兩個變數合併成一個新的變數，此時就可透過unite()函數來達成(與separate()為互補函數)，如以下範例：</p>
<p>separate_df</p><pre class="crayon-plain-tag">unite_df &lt;- 
  separate_df %&gt;% 
  unite(col = City_State, City, State, sep = " (") %&gt;% 
  mutate(City_State = paste(City_State,")", sep = "")) %&gt;% 
  unite(Year_Month, Year, Month, sep = "_") %&gt;% 
  unite(First_Last, First, Last, sep = " ")
head(unite_df)</pre><p></p><pre class="crayon-plain-tag">## # A tibble: 6 x 5
##   Grp_Ind Year_Month City_State       First_Last        Extra_variable
## 1 1.a     2006_Jan   Dayton (OH)      George Washington XX01person_1  
## 2 1.b     2006_Feb   Grand Forks (ND) John Adams        XX02person_2  
## 3 1.c     2006_Mar   Fargo (ND)       Thomas Jefferson  XX03person_3  
## 4 2.a     2007_Jan   Rochester (MN)   James Madison     XX04person_4  
## 5 2.b     2007_Feb   Dubuque (IA)     James Monroe      XX05person_5  
## 6 2.c     2007_Mar   Ft. Collins (CO) John Adams        XX06person_6</pre><p>而以下指令亦可產生相同效果</p><pre class="crayon-plain-tag"># 如果沒有指定sep，則會預設使用"_"
separate_df %&gt;% unite(Year_Month, Year, Month)
separate_df %&gt;% unite(Year_Month, Year, Month, sep = "_")</pre><p></p>
<h3>spread() function:</h3>
<p><strong>目的</strong>: 將「長」資料重塑成「寬」資料</p>
<p><strong>說明</strong>: 有時會我們會需要將長的資料轉換成寬的資料，spread()函數則能將指定的成對key-value擴展成多個欄位（是gather()函數的互補函數）。</p><pre class="crayon-plain-tag">wide_df &lt;- long_df %&gt;% spread(key = Quarter, value = Revenue)
head(wide_df)</pre><p></p><pre class="crayon-plain-tag">## # A tibble: 6 x 6
##   Group  Year Qtr.1 Qtr.2 Qtr.3 Qtr.4
## 1     1  2006    15    16    19    17
## 2     1  2007    12    13    27    23
## 3     1  2008    22    22    24    20
## 4     1  2009    10    14    20    16
## 5     2  2006    12    13    25    18
## 6     2  2007    16    14    21    19</pre><p></p>
<hr />
<p>更多資料處理學習筆記：</p>
<ol>
<li><a href="/r-data-processing-top-10-faq-%E8%B3%87%E6%96%99%E8%99%95%E7%90%86/" target="_blank" rel="noopener noreferrer">資料處理-實用的10個小技巧 | Data Processing Basics FAQ | R 語言</a></li>
</ol>
<hr />
<p>參考資料連結：</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
<li><a href="https://uc-r.github.io/tidyr#spread" target="_blank" rel="noopener noreferrer">Reshaping Your Data with tidyr</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/tidyr-reshaping-data/">Reshaping Data using tidyr | 資料合併與分離、變長或變寬的小技巧 | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/tidyr-reshaping-data/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>RFM Model using R &#124; CRM 客戶分群模型 Customer Segmentation &#124; R 統計</title>
		<link>/rfm-model-crm-customer-segmentation-r/</link>
					<comments>/rfm-model-crm-customer-segmentation-r/#respond</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Wed, 05 Dec 2018 03:23:22 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計分析]]></category>
		<category><![CDATA[CRM]]></category>
		<category><![CDATA[RFM Model]]></category>
		<guid isPermaLink="false">/?p=2221</guid>

					<description><![CDATA[<p>RFM Model 是個簡易客戶分群的模型，依據消費者的Recency, Frequency, Monetary維度資訊來快速檢視客群組成，並能幫助行銷者快速評 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/rfm-model-crm-customer-segmentation-r/">RFM Model using R | CRM 客戶分群模型 Customer Segmentation | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>RFM Model 是個簡易客戶分群的模型，依據消費者的Recency, Frequency, Monetary維度資訊來快速檢視客群組成，並能幫助行銷者快速評估最適化的CRM方案，極大化投資報酬率。比如說，若能知道各客群歷史回應率，就能預測不同客群的行銷效果(預期回應人數與報酬率)，並客製化CRM行銷策略。</p>
<h3>RFM Model</h3>
<p>在精準行銷的時代，行銷者需要評估資源（或預算）如何有效的分配給對的目標受眾輪廓，最大化投資報酬率。主要需評估的問題包括兩個面向：</p>
<ol>
<li><strong>Customer Segmentation</strong>
<ul>
<li>如何有效區隔消費者，並找出各消費者分群的特色如：高貢獻度、高往來程度、以及高回應率等。</li>
</ul>
</li>
<li><strong>CRM strategies</strong>
<ul>
<li>根據消費者所屬分群特性，挑選合適的目標受眾進行行銷，以獲取最大報酬率。</li>
</ul>
</li>
</ol>
<p>RFM模型則是一個簡單卻有效的顧客分析模型，能幫助我們解決以上兩個問題。其中RFM分別代表：</p>
<ul>
<li>R &#8211; Recency : 消費者最後一次購買距今天數。</li>
<li>F &#8211; Frequency: 消費者購買頻率。</li>
<li>M &#8211; Monetary: 消費者平均花費金額（每一次購物/訂單結帳）。</li>
</ul>
<p>雖然RFM模型在部分統計分析軟體中有現成快速套用的RFM功能節點(ex: IBM SPSS RFM)，在R裡面並沒有簡易現成的套件能快速套用。故本學習筆記將說明如何使用R語言進行RFM分析。</p>
<h3>Data Explore</h3>
<p>我們使用CDNOW dataset (1/10th sample)，<a href="http://www.brucehardie.com/datasets/" target="_blank" rel="noopener noreferrer">下載連結</a>。</p><pre class="crayon-plain-tag"># read CDNOW_SAMPLE.txt
rawdf &lt;- read.table(file = "CDNOW_sample.txt", header = FALSE)
head(rawdf)</pre><p></p><pre class="crayon-plain-tag">##   V1 V2       V3 V4    V5
## 1  4  1 19970101  2 29.33
## 2  4  1 19970118  2 29.73
## 3  4  1 19970802  1 14.96
## 4  4  1 19971212  2 26.48
## 5 21  2 19970101  3 63.34
## 6 21  2 19970113  1 11.77</pre><p></p><pre class="crayon-plain-tag"># select columns of interest :
df &lt;- rawdf[,c(1,3,5)]
# rename columns' names
names(df) &lt;- c("ID","Date","Amount")
# transfer column Date type from int type to date type
df$Date &lt;- as.Date(as.character(df$Date), format="%Y%m%d")
head(df)</pre><p></p><pre class="crayon-plain-tag">##   ID       Date Amount
## 1  4 1997-01-01  29.33
## 2  4 1997-01-18  29.73
## 3  4 1997-08-02  14.96
## 4  4 1997-12-12  26.48
## 5 21 1997-01-01  63.34
## 6 21 1997-01-13  11.77</pre><p></p><pre class="crayon-plain-tag"># see total row&amp;column number 
dim(df)</pre><p></p><pre class="crayon-plain-tag">## [1] 6919    3</pre><p></p><pre class="crayon-plain-tag"># count the number of distinct ID 
uid &lt;- df[!duplicated(df$ID),]
dim(uid)</pre><p></p><pre class="crayon-plain-tag">## [1] 2357    3</pre><p>我們可以知道獨立用戶ID數為2357，交易比數共為6919。</p>
<h3>Segment the Customers into RFM Cells</h3>
<p>根據剛剛載入的資料，我們可以依據R,F,M三個維度將客戶分群到各個RFM<span style="color: #9f6ad4;"> Cell</span>。<br />
而通常，我們會幫客戶在每個維度給予1~5級的評分（維度bin數量可以根據情境調整），分數越高表示在該維度程度越高。</p>
<p>且根據過去經驗來說，Recency是高回應率的主要關鍵影響因素(x100)，Frequency是次要影響因素(x10)，最後才是Monetary(x1)。被分類到同一個RFM Cell的客戶分數會是相同的。分數“531&#8243;代表著該客戶在Recency得到5分，Frequency得到3分，Monetary則為1分。</p>
<p>在每一維度進行客戶切群(分箱，Binning)的方法有兩種：一種是Nested Binning巢狀法，另一種是Independent Binning獨立法。</p>
<ol>
<li><strong>Nested Binning 巢狀法</strong>：先將客群在Recency維度切成若干等分(aliquots)，然後從每組Recency等分中再依據Frequency切割成若干等份，Monetary亦然。此法的好處是，每一群RFM Cell的人數會差不多，但缺點就是不同Recency分群下的Frequency和Monetary的意義就無法比較，分數相同，但意義不同。</li>
<li><strong>Independent Binning 獨立法</strong>：則是獨立將客戶在各維度進行等分切群，這樣的好處就是，同樣RFM分數的意義是相同的，但缺點就是每群RFM Cell中的人數大小會有差距。</li>
</ol>
<p>更多有關維度切bin(分箱)方法的優缺點比較(pros &amp; cons)可參考此<a href="https://www.quora.com/SPSS/SPSS-Pros-and-Cons-of-Nested-and-Independent-Binning-for-RFM-Analysis">連結</a>。</p>
<p><span style="color: #9f6ad4;">為了讓每一群RFM Cell可以相互比較，本學習筆記將先以Independent法，依據各維度分佈來決定bin的切點(breaks)</span>。</p>
<h3>Calculate the Recency, Frequency, and Monetary</h3>
<p>為了進行RFM分析，我們需要進一步處理加工CDNOW Sample資料集，分為以下幾個步驟：</p>
<ol>
<li>group by unique customer ID</li>
<li>計算每個ID的Recency: 計算最新的一筆交易距離今日或其他指定日期的天數。</li>
<li>計算每個ID的Frequency: 計算每個人的總交易次數。</li>
<li>計算每個ID的Monetary: 計算每個人的交易金額加總除上總交易次數，即平均每筆交易的金額。</li>
</ol>
<p>首先，先限制分析資料的時間區間如下：</p><pre class="crayon-plain-tag">startDate &lt;- as.Date("19970101","%Y%m%d")
endDate &lt;- as.Date("19980701","%Y%m%d")</pre><p>group by 每個獨立ID計算以上時間區間的R,F,M值。其中Recency是最近一次交易距離endDate(1998-07-01)的天數。</p><pre class="crayon-plain-tag">library(dplyr)
library(magrittr)

df.1 &lt;- 
  df %&gt;% 
  filter(Date &gt;= startDate &amp; Date &lt;= endDate) %&gt;% 
  group_by(ID) %&gt;% 
  summarise(
    MaxTransDate = max(Date),
    Amount = sum(Amount),
    Recency = as.numeric(endDate - MaxTransDate),
    Frequency = n(),
    Monetary = Amount/Frequency
         ) %&gt;% 
  ungroup() %&gt;% 
  as.data.frame()

head(df.1)</pre><p></p><pre class="crayon-plain-tag">##   ID MaxTransDate Amount Recency Frequency Monetary
## 1  4   1997-12-12 100.50     201         4   25.125
## 2 18   1997-01-04  14.96     543         1   14.960
## 3 21   1997-01-13  75.11     534         2   37.555
## 4 50   1997-01-01   6.79     546         1    6.790
## 5 60   1997-02-01  21.75     515         1   21.750
## 6 71   1997-01-01  13.97     546         1   13.970</pre><p></p>
<h3>Independent RFM Scoring</h3>
<h4>檢視Recency維度的分佈資訊。</h4>
<p></p><pre class="crayon-plain-tag">quantile(df.1$Recency, probs = seq(0,1,by = 0.2))</pre><p></p><pre class="crayon-plain-tag">##   0%  20%  40%  60%  80% 100% 
##    1  153  443  487  513  546</pre><p></p><pre class="crayon-plain-tag">hist(df.1$Recency)</pre><p><img src="/wp-content/uploads/2018/12/unnamed-chunk-5-1.png" alt="RFM Model CRM " /></p><pre class="crayon-plain-tag">plot(density(df.1$Recency))</pre><p><img src="/wp-content/uploads/2018/12/unnamed-chunk-5-2.png" alt="RFM Model CRM " /></p>
<h4>Recency binning</h4>
<p>依據Recency維度的百分位數(quantile)進行切群(binning)，每間隔20%百分位數切一等份。</p><pre class="crayon-plain-tag">labels = c(5,4,3,2,1)
df.1$R_Score &lt;- labels[as.numeric(cut(df.1$Recency, breaks=c(quantile(df.1$Recency, probs = seq(0,1,by = 0.2))), labels=labels, include.lowest=TRUE))]

head(df.1)</pre><p></p><pre class="crayon-plain-tag">##   ID MaxTransDate Amount Recency Frequency Monetary R_Score
## 1  4   1997-12-12 100.50     201         4   25.125       4
## 2 18   1997-01-04  14.96     543         1   14.960       1
## 3 21   1997-01-13  75.11     534         2   37.555       1
## 4 50   1997-01-01   6.79     546         1    6.790       1
## 5 60   1997-02-01  21.75     515         1   21.750       1
## 6 71   1997-01-01  13.97     546         1   13.970       1</pre><p></p>
<h4>檢視Frequency維度的分佈資訊。</h4>
<p>可以發現前80%百分位數都集中在1,2,3,4等數值。</p><pre class="crayon-plain-tag">quantile(df.1$Frequency, probs = seq(0,1,by = 0.2))</pre><p></p><pre class="crayon-plain-tag">##   0%  20%  40%  60%  80% 100% 
##    1    1    1    2    4   56</pre><p></p><pre class="crayon-plain-tag">summary(df.1$Frequency)</pre><p></p><pre class="crayon-plain-tag">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.000   1.000   1.000   2.936   3.000  56.000</pre><p></p><pre class="crayon-plain-tag">hist(df.1$Frequency, breaks = 100)</pre><p><img src="/wp-content/uploads/2018/12/unnamed-chunk-7-1.png" alt="RFM Model CRM " /></p><pre class="crayon-plain-tag">plot(density(df.1$Frequency))</pre><p><img src="/wp-content/uploads/2018/12/unnamed-chunk-7-2.png" alt="RFM Model CRM " /></p>
<h4>Frequency binning</h4>
<p>Frequency的部分<span style="color: #9f6ad4;">則使用指定的1,2,3,4,5+的bin區間來進行分箱</span>。</p><pre class="crayon-plain-tag">labels.freq = c(1,2,3,4,5)
df.1$F_Score &lt;- labels.freq[as.numeric(cut(df.1$Frequency, breaks=c(0,1,2,3,4,60), labels=labels.freq, include.lowest=TRUE))]

head(df.1)</pre><p></p><pre class="crayon-plain-tag">##   ID MaxTransDate Amount Recency Frequency Monetary R_Score F_Score
## 1  4   1997-12-12 100.50     201         4   25.125       4       4
## 2 18   1997-01-04  14.96     543         1   14.960       1       1
## 3 21   1997-01-13  75.11     534         2   37.555       1       2
## 4 50   1997-01-01   6.79     546         1    6.790       1       1
## 5 60   1997-02-01  21.75     515         1   21.750       1       1
## 6 71   1997-01-01  13.97     546         1   13.970       1       1</pre><p></p><pre class="crayon-plain-tag">table(df.1$F_Score)</pre><p></p><pre class="crayon-plain-tag">## 
##    1    2    3    4    5 
## 1205  406  208  150  388</pre><p></p>
<h4>檢視Monetary維度的分佈資訊。</h4>
<p></p><pre class="crayon-plain-tag">quantile(df.1$Monetary, probs = seq(0,1,by = 0.2))</pre><p></p><pre class="crayon-plain-tag">##        0%       20%       40%       60%       80%      100% 
##   0.00000  14.37000  20.40800  29.33098  43.08773 506.97000</pre><p></p><pre class="crayon-plain-tag">summary(df.1$Monetary)</pre><p></p><pre class="crayon-plain-tag">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00   15.24   24.99   32.44   38.51  506.97</pre><p></p><pre class="crayon-plain-tag">hist(df.1$Monetary,200)</pre><p><img src="/wp-content/uploads/2018/12/unnamed-chunk-10-1.png" alt="RFM Model CRM " /></p><pre class="crayon-plain-tag">plot(density(df.1$Monetary))</pre><p><img src="/wp-content/uploads/2018/12/unnamed-chunk-10-2.png" alt="RFM Model CRM " /></p>
<h4>Monetary binning</h4>
<p>Monetary維度則使用百分位數法來進行分箱。</p><pre class="crayon-plain-tag">labels.mon = c(1,2,3,4,5)
df.1$M_Score &lt;- labels.mon[as.numeric(cut(df.1$Monetary, breaks=c(quantile(df.1$Monetary, probs = seq(0,1,0.2))), labels=labels.mon, include.lowest=TRUE))]

table(df.1$M_Score)</pre><p></p><pre class="crayon-plain-tag">## 
##   1   2   3   4   5 
## 487 456 471 471 472</pre><p></p><pre class="crayon-plain-tag">head(df.1)</pre><p></p><pre class="crayon-plain-tag">##   ID MaxTransDate Amount Recency Frequency Monetary R_Score F_Score
## 1  4   1997-12-12 100.50     201         4   25.125       4       4
## 2 18   1997-01-04  14.96     543         1   14.960       1       1
## 3 21   1997-01-13  75.11     534         2   37.555       1       2
## 4 50   1997-01-01   6.79     546         1    6.790       1       1
## 5 60   1997-02-01  21.75     515         1   21.750       1       1
## 6 71   1997-01-01  13.97     546         1   13.970       1       1
##   M_Score
## 1       3
## 2       2
## 3       4
## 4       1
## 5       3
## 6       1</pre><p></p>
<h4>計算total score</h4>
<p>將R_Score, F_Score, M_Score三個數值合併為新的三位數值。</p><pre class="crayon-plain-tag">df.1$Total_Score &lt;- as.numeric(paste(df.1$R_Score,df.1$F_Score,df.1$M_Score, sep = ""))
head(df.1)</pre><p></p><pre class="crayon-plain-tag">##   ID MaxTransDate Amount Recency Frequency Monetary R_Score F_Score
## 1  4   1997-12-12 100.50     201         4   25.125       4       4
## 2 18   1997-01-04  14.96     543         1   14.960       1       1
## 3 21   1997-01-13  75.11     534         2   37.555       1       2
## 4 50   1997-01-01   6.79     546         1    6.790       1       1
## 5 60   1997-02-01  21.75     515         1   21.750       1       1
## 6 71   1997-01-01  13.97     546         1   13.970       1       1
##   M_Score Total_Score
## 1       3         443
## 2       2         112
## 3       4         124
## 4       1         111
## 5       3         113
## 6       1         111</pre><p></p>
<h3>查看每一個RFM Cell的人數分佈</h3>
<p>R_Score和M_Score兩維度交叉表。</p><pre class="crayon-plain-tag">library(ggplot2)
# plot data
df.1 %&gt;% 
  mutate(R_Score = as.factor(R_Score),
         F_Score = as.factor(F_Score)) %&gt;%
  group_by(R_Score, F_Score) %&gt;% 
  summarise(value = n()) %&gt;% 
  ungroup() %&gt;% 
  ggplot(aes(R_Score, F_Score)) +
  geom_tile(aes(fill = value), col = "black") + 
  geom_text(aes(label = round(value, 1))) +
  scale_fill_gradient2() +
  theme(panel.background = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        text = element_text(family = "黑體-繁 中黑", size = 14),
        legend.background = element_rect(fill = "gray96",colour = "gray88"),
        plot.background = element_rect(fill = "gray99"),
        plot.title = element_text(hjust = 0.5,size = 16), #標題置中
        strip.background = element_rect(fill = "lightgray", colour = NA)
  )</pre><p><img src="/wp-content/uploads/2018/12/unnamed-chunk-14-1.png" alt="RFM Model CRM " /></p>
<p>根據行銷目的斟酌參考R_ScorexM_Score和R_ScorexF_Score交叉資訊表，有了基本分群資訊後，<span style="color: #9f6ad4;">就可套用各客群歷史行銷回應率資訊，來輔助分配行銷預算，尋找目標客群，並估算預期行銷效果（預期回應人數與投報率）</span>。</p>
<hr />
<p><strong>更多<span style="color: #9f6ad4;">統計分析(Analysis)</span>與<span style="color: #9f6ad4;">資料分群(Clustering)</span>學習筆記:</strong></p>
<ol>
<li><a href="/app-mobile-ad-revenue-admob-%e8%a1%8c%e5%8b%95%e5%bb%a3%e5%91%8a%e6%94%b6%e5%85%a5/" target="_blank" rel="noopener noreferrer">開發一個免費App能賺多少錢？靠AdMob廣告月收3萬實例分享</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering 切割式分群 | K-means, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest </a></li>
</ol>
<hr />
<p><strong>參考資料連結：</strong></p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
<li><a href="https://www.r-bloggers.com/rfm-customer-analysis-with-r-language/" target="_blank" rel="noopener noreferrer">RFM Customer Analysis with R Language</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/rfm-model-crm-customer-segmentation-r/">RFM Model using R | CRM 客戶分群模型 Customer Segmentation | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/rfm-model-crm-customer-segmentation-r/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>開發一個免費App能賺多少錢？ 靠 AdMob 廣告月收3萬實例分享</title>
		<link>/app-mobile-ad-revenue-admob-%e8%a1%8c%e5%8b%95%e5%bb%a3%e5%91%8a%e6%94%b6%e5%85%a5/</link>
					<comments>/app-mobile-ad-revenue-admob-%e8%a1%8c%e5%8b%95%e5%bb%a3%e5%91%8a%e6%94%b6%e5%85%a5/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Sun, 07 Oct 2018 04:01:49 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計分析]]></category>
		<category><![CDATA[admob]]></category>
		<category><![CDATA[app monetization]]></category>
		<category><![CDATA[Linear Regression]]></category>
		<category><![CDATA[廣告]]></category>
		<category><![CDATA[行動廣告]]></category>
		<guid isPermaLink="false">/?p=1479</guid>

					<description><![CDATA[<p>相信很多人都跟我一樣會好奇一個只有放廣告的免費app能賺多少錢呢？ 剛好最近有位朋友分享給我他的一個app的 Google Admob 收入報表，原來他的app [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/app-mobile-ad-revenue-admob-%e8%a1%8c%e5%8b%95%e5%bb%a3%e5%91%8a%e6%94%b6%e5%85%a5/">開發一個免費App能賺多少錢？ 靠 AdMob 廣告月收3萬實例分享</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>相信很多人都跟我一樣會好奇一個只有放廣告的免費app能賺多少錢呢？ 剛好最近有位朋友分享給我他的一個app的 Google Admob 收入報表，原來他的app靠展示廣告就可以輕鬆月入3萬多塊錢呢，真是一筆好令人羨慕的被動收入啊<img src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f924.png" alt="🤤" class="wp-smiley" style="height: 1em; max-height: 1em;" />。至於一天要有多少點擊，曝光次數要有多少才能達到這樣的收入水平呢，別著急，下面會畫個圖表給大家看，另外我會再用線性迴歸做個簡單的模型來玩玩。</p>
<p>&nbsp;</p>
<h3>行動廣告月收3萬台幣app 實例分享</h3>
<p>他的這個app很簡單，沒有複雜的介面，一打開就是一個清單(ListView)，畫面下方固定顯示一個橫幅廣告(Banner AdView)，然後 <b>.</b><b>.</b><b>.</b>就沒了。其中清單中的每個item其實都是爬蟲定期自動抓取來的(例如中央氣象等政府開放資料)，所以不需要人力去更新內容。</p>
<p>App介面看起來就像下圖這樣：<br />
(因這位朋友愛搞神秘，交代我不能透露他的app內容，所以我只好畫個示意圖囉！)</p>
<p><img loading="lazy" class="aligncenter wp-image-1967 size-medium" src="/wp-content/uploads/2018/10/S__6840342_jam-218x300.jpg" alt="AdMob" width="218" height="300" srcset="/wp-content/uploads/2018/10/S__6840342_jam-218x300.jpg 218w, /wp-content/uploads/2018/10/S__6840342_jam-768x1058.jpg 768w, /wp-content/uploads/2018/10/S__6840342_jam-743x1024.jpg 743w, /wp-content/uploads/2018/10/S__6840342_jam-230x317.jpg 230w, /wp-content/uploads/2018/10/S__6840342_jam-350x482.jpg 350w, /wp-content/uploads/2018/10/S__6840342_jam-480x661.jpg 480w, /wp-content/uploads/2018/10/S__6840342_jam.jpg 815w" sizes="(max-width: 218px) 100vw, 218px" />所以收益來源完全是靠這個螢幕畫面下方的橫幅廣告的曝光及點擊。</p>
<p>這位朋友告訴我，寫一個app上架不難，甚至沒學過程式的學生去買一本仿間的教學書照著做都能輕鬆上架app，原來技術門檻如此之低。他還說，這個app只花一個月的時間開發而已，之後就是放在Google Play上給它自生自滅，好久都不去管它，真是比當房東還輕鬆！</p>
<p>&nbsp;</p>
<h3>窺探 App 行動廣告 Google AdMob 報表</h3>
<p>他給我的報表是2018上半年的Google Admob報表數據，裡頭的資料欄如下:</p><pre class="crayon-plain-tag">AdMob廣告聯播網請求
媒合率
比對成功的請求
顯示率
曝光
Active View  符合資格的曝光
可評估的曝光 %
可評估的曝光
可見的曝光 %
可見的曝光
曝光點擊率
點擊數
預估收益
AdMob廣告聯播網請求千次曝光收益
曝光千次曝光收益</pre><p><a href="https://support.google.com/admob/answer/2772142" target="_blank" rel="noopener noreferrer">預估收益是對近期帳戶活動的預估，實際收益要到月底才會結算（扣除掉無效點擊及曝光等）</a>，但朋友說他的每月結算收益跟預估差不到10美金，所以這邊可以把預估收益當成就是他的實際收益囉。<br />
好，那我就直接把他每天的收益畫出來看吧，嘿嘿 <b>.</b><b>.</b><b>.</b></p>
<p><img loading="lazy" class="alignnone size-full wp-image-1924" src="/wp-content/uploads/2018/09/Rplot05.jpg" alt="AdMob" width="2600" height="1200" srcset="/wp-content/uploads/2018/09/Rplot05.jpg 2600w, /wp-content/uploads/2018/09/Rplot05-300x138.jpg 300w, /wp-content/uploads/2018/09/Rplot05-768x354.jpg 768w, /wp-content/uploads/2018/09/Rplot05-1024x473.jpg 1024w, /wp-content/uploads/2018/09/Rplot05-830x383.jpg 830w, /wp-content/uploads/2018/09/Rplot05-230x106.jpg 230w, /wp-content/uploads/2018/09/Rplot05-350x162.jpg 350w, /wp-content/uploads/2018/09/Rplot05-480x222.jpg 480w" sizes="(max-width: 2600px) 100vw, 2600px" /></p>
<p>哇～4月初的時候曾一天收入衝到＄70美元（約2100台幣），而且從3月之後每天收益幾乎都有一千台幣，光是2018年上半年就有約20萬台幣的被動收入入帳，能不讓人羨慕嗎<img src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f602.png" alt="😂" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f602.png" alt="😂" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f602.png" alt="😂" class="wp-smiley" style="height: 1em; max-height: 1em;" />。</p>
<p>至於其他欄位剛開始看可能不是明白是什麼意思，但看下面的行動廣告效益衡量指標應該就比較好理解了。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>行動廣告效益衡量指標</h3>
<h5>根據<span style="color: #9f6ad4;">Google Admob報表</span>，幾個常見的<span style="text-decoration: underline;"><span style="color: #9f6ad4; text-decoration: underline;">行動</span><span style="color: #9f6ad4; text-decoration: underline;">廣告效益衡量指標</span></span>包括：</h5>
<ul>
<li><strong>AdMod Network Requests (AdMod廣告聯播網請求數)</strong> ：<br />
每當你的App要求顯示廣告，就算一次廣告請求。該數字代表向廣告聯播網送出請求的廣告數。</li>
<li><strong>Match Request (媒合請求數）</strong>：<br />
經請求後，成功媒合的廣告單元數。</li>
<li><strong>Match Rate (媒合率)</strong>：<br />
成功媒合的廣告次數佔總體廣告請求次數的比例。<br />
$$Match\space Rate=\frac{Match Requests}{AdMod Network Requests}$$</li>
<li><strong>Impression (曝光數）</strong>：<br />
廣告成功媒合後並有效曝光的次數。而其中關於<strong><span style="color: #9f6ad4;">有效曝光</span></strong>又有不同層次的定義。</p>
<ul>
<li><strong><span style="color: #9f6ad4;">Active View</span></strong> Impression: 如果<strong><span style="color: #9f6ad4;">廣告至少有50%的區域出現在畫面中並且顯示至少一秒</span></strong>(根據美國互動廣告局 (IAB) 標準所規定的最低條件)，即為有效曝光（符合資格的曝光）。</li>
<li>Active View <span style="color: #9f6ad4;">Measurable</span> Impression: 有效且可評估的曝光。</li>
<li>Active View <span style="color: #9f6ad4;">Viewable</span> Impression: 有效、可評估、且可見的曝光。</li>
</ul>
</li>
<li><strong>Show Rate (顯示率)</strong>：<br />
有效曝光次數佔成功媒合次數的比例。<br />
$$Show\space Rate=\frac{Impressions}{Matched}$$</li>
<li><strong>Click (點擊）</strong>：<br />
使用者點擊廣告次數。</li>
<li><strong>Impression CTR (曝光點擊率):</strong><br />
點擊次數佔總體曝光次數的比例。<br />
$$Impression\space CTR=\frac{Clicks}{Impressions}$$</li>
<li><strong>Estimate Revenue (預估收益):<br />
</strong>約與實際收益相同。<br />
另外由預估收益衍伸計算的幾個重要指標包括：</p>
<ul>
<li><strong>RPM</strong>: 為Revenue per 1,000 impressions，千次曝光收益。</li>
<li><span style="color: #9f6ad4;"><strong>AdMod Network request RPM</strong></span>: AdMod廣告聯播網請求千次曝光收益。<br />
$$AdMod\space Network\space request\space RPM=\frac{預估收益}{廣告請求次數}\times 1,000$$</li>
<li><span style="color: #9f6ad4;"><strong>Impression RPM</strong></span>: 曝光千次曝光收益。<br />
$$Impression\space RPM=\frac{預估收益}{曝光次數}\times 1,000$$</li>
</ul>
</li>
<li><strong>Fill Rate(填充率):<br />
</strong>計算方式為廣告曝光數除上廣告請求數。<br />
$$Fill\space Rate=\frac{Impressions}{Requests}$$</li>
</ul>
<h5></h5>
<p>&nbsp;</p>
<p>搭配數字來看或許會清楚一點，假設某天的請求數是10,000，並假設當天的各個比率如下表，<span style="color: #9f6ad4;">AdMob報表的各個欄位的意義，其實就是從最一開始的請求到最後的實際點擊這個過程的各項指標囉</span>！<br />
（黑色文字的部分是實際發生，而紫色顏色文字的各個比率是AdMob幫我們算出的。）</p>
<p>&nbsp;</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1988" src="/wp-content/uploads/2018/10/example_admob.png" alt="example_admob" width="757" height="680" srcset="/wp-content/uploads/2018/10/example_admob.png 757w, /wp-content/uploads/2018/10/example_admob-300x269.png 300w, /wp-content/uploads/2018/10/example_admob-230x207.png 230w, /wp-content/uploads/2018/10/example_admob-350x314.png 350w, /wp-content/uploads/2018/10/example_admob-480x431.png 480w" sizes="(max-width: 757px) 100vw, 757px" /></p>
<p>一次廣告的展示其實背後牽涉到競價過程，而AdMob會自動從所有可用的廣告來源中挑選成效最高的來放送。也如同上段所說<a href="https://support.google.com/admob/answer/2772142" target="_blank" rel="noopener noreferrer">預估收益是對近期帳戶活動的預估</a>，不是拿當天全部點擊數或曝光數乘一個固定價格就能算出的。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>&nbsp;</p>
<h3>曝光與收益走勢圖</h3>
<p>我們可以把每天的收益和曝光數畫在一起做個對照：</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1925" src="/wp-content/uploads/2018/09/Rplot03-1.jpg" alt="AdMob" width="2600" height="1200" srcset="/wp-content/uploads/2018/09/Rplot03-1.jpg 2600w, /wp-content/uploads/2018/09/Rplot03-1-300x138.jpg 300w, /wp-content/uploads/2018/09/Rplot03-1-768x354.jpg 768w, /wp-content/uploads/2018/09/Rplot03-1-1024x473.jpg 1024w, /wp-content/uploads/2018/09/Rplot03-1-830x383.jpg 830w, /wp-content/uploads/2018/09/Rplot03-1-230x106.jpg 230w, /wp-content/uploads/2018/09/Rplot03-1-350x162.jpg 350w, /wp-content/uploads/2018/09/Rplot03-1-480x222.jpg 480w" sizes="(max-width: 2600px) 100vw, 2600px" /></p>
<ul>
<li>可以看到收益大致和曝光數有相近的趨勢，越多曝光收益越多，看起來滿合理的。</li>
<li>也可以觀察到曝光數在每個月都有4個高峰 （放大檢視後發現剛好都是在周末），收益也有隨之升高。</li>
<li>但曝光數與收益似乎沒有絕對的關係，因為可以觀察到1月有個超高峰，收益並沒有明顯飆高。</li>
<li>從曝光數看起來App的頁面流量在2018年上半年趨於穩定，而收益似乎也停止成長了。</li>
</ul>
<p>&nbsp;</p>
<p>&nbsp;</p>
<h3>點擊、曝光與收益散佈圖</h3>
<p>我們再將這半年每天的“點擊數vs收益”還有“曝光數vs收益”畫個散佈圖出來看看：</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1935" src="/wp-content/uploads/2018/09/Rplot08.jpg" alt="AdMob" width="2000" height="1500" srcset="/wp-content/uploads/2018/09/Rplot08.jpg 2000w, /wp-content/uploads/2018/09/Rplot08-300x225.jpg 300w, /wp-content/uploads/2018/09/Rplot08-768x576.jpg 768w, /wp-content/uploads/2018/09/Rplot08-1024x768.jpg 1024w, /wp-content/uploads/2018/09/Rplot08-830x623.jpg 830w, /wp-content/uploads/2018/09/Rplot08-230x173.jpg 230w, /wp-content/uploads/2018/09/Rplot08-350x263.jpg 350w, /wp-content/uploads/2018/09/Rplot08-480x360.jpg 480w" sizes="(max-width: 2000px) 100vw, 2000px" /></p>
<p><img loading="lazy" class="alignnone size-full wp-image-1937" src="/wp-content/uploads/2018/09/Rplot06.jpg" alt="AdMob" width="2000" height="1500" srcset="/wp-content/uploads/2018/09/Rplot06.jpg 2000w, /wp-content/uploads/2018/09/Rplot06-300x225.jpg 300w, /wp-content/uploads/2018/09/Rplot06-768x576.jpg 768w, /wp-content/uploads/2018/09/Rplot06-1024x768.jpg 1024w, /wp-content/uploads/2018/09/Rplot06-830x623.jpg 830w, /wp-content/uploads/2018/09/Rplot06-230x173.jpg 230w, /wp-content/uploads/2018/09/Rplot06-350x263.jpg 350w, /wp-content/uploads/2018/09/Rplot06-480x360.jpg 480w" sizes="(max-width: 2000px) 100vw, 2000px" /></p>
<p>正相關係數分別是0.6與0.7。</p>
<p>既使是相同的點擊數，當天的收益範圍也是非常廣的，例如同樣是275個點擊次數，當天收益可能落在$20~$40美金，差了快兩倍。</p>
<p>&nbsp;</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>&nbsp;</p>
<h3>情境題</h3>
<p>假設珍珍也想寫個app來賺錢（假設訪客組成分佈類似），在相同的地方放一塊最基本的橫幅廣告（假設廣告類型分佈類似），然後她想要一個月可以賺台幣8000塊錢來繳房租，這樣她的app平均一天要有多少點擊或曝光數才可以達到呢？</p>
<p>我們預期越多的點擊，應該會有越多的收益，所以我們可以假設點擊與收益存在線性的關係，我們可以利用這份實際的數據，建立一個簡單線性迴歸模型，得到收益的公式。</p>
<p>&nbsp;</p>
<h3>用線性回歸模型找出點擊數與收益的關係式</h3>
<p>因此我用R 的 linear model 套件，丟入每天的點擊數與收益去得到一個簡單線性回歸方程式，並將他畫出如下：<br />
（注意下圖的收益單位是美金)</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1936" src="/wp-content/uploads/2018/09/Rplot07.jpg" alt="AdMob" width="2000" height="1500" srcset="/wp-content/uploads/2018/09/Rplot07.jpg 2000w, /wp-content/uploads/2018/09/Rplot07-300x225.jpg 300w, /wp-content/uploads/2018/09/Rplot07-768x576.jpg 768w, /wp-content/uploads/2018/09/Rplot07-1024x768.jpg 1024w, /wp-content/uploads/2018/09/Rplot07-830x623.jpg 830w, /wp-content/uploads/2018/09/Rplot07-230x173.jpg 230w, /wp-content/uploads/2018/09/Rplot07-350x263.jpg 350w, /wp-content/uploads/2018/09/Rplot07-480x360.jpg 480w" sizes="(max-width: 2000px) 100vw, 2000px" /></p>
<h3>一天要多少點擊才能月入8千？</h3>
<p>一個月想要有收益8000台幣(約等於260美金)，一個月用30天算，所以平均一天要8.67美金。<br />
那麼帶入我們得到的公式：</p>
<p>$$8.67 = -15.09  + 0.18 \times X $$</p>
<p>得到 X = 132<br />
<span style="color: #9f6ad4;">所以一個月若想要賺8000台幣則平均每天要有約132的點擊數以上</span>。</p>
<h3></h3>
<h3>那平均每天要有多少曝光呢？</h3>
<p>一樣可以丟每天的曝光數與收益下去做個線性回歸模型</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1939" src="/wp-content/uploads/2018/09/Rplot09.jpg" alt="Rplot09" width="2000" height="1500" srcset="/wp-content/uploads/2018/09/Rplot09.jpg 2000w, /wp-content/uploads/2018/09/Rplot09-300x225.jpg 300w, /wp-content/uploads/2018/09/Rplot09-768x576.jpg 768w, /wp-content/uploads/2018/09/Rplot09-1024x768.jpg 1024w, /wp-content/uploads/2018/09/Rplot09-830x623.jpg 830w, /wp-content/uploads/2018/09/Rplot09-230x173.jpg 230w, /wp-content/uploads/2018/09/Rplot09-350x263.jpg 350w, /wp-content/uploads/2018/09/Rplot09-480x360.jpg 480w" sizes="(max-width: 2000px) 100vw, 2000px" /></p>
<p>預測每天若想有8.67美金大約要8300個曝光數。</p>
<p>但我們可以看到Adjust R Square Score只有 0.37，比點擊數模型的0.56來得低許多的。其實也很合理，影響開發者收益高低最主要的因素，比起流量(or曝光數)，還是在於用戶(有效)點擊數的多寡。所以真要預測曝光數的話，我倒傾向於拿上一個模型預測到的點擊數，直接除以這半年的平均曝光點擊率。<br />
但朋友叮嚀我，礙於Admob使用條款，這邊實在無法透露平台詳細的點擊率&#8230;等等廣告平台表現的相關數據囉！</p>
<h3></h3>
<h3>小結</h3>
<p>一天要有將近8300的曝光（流量），好像也不是挺容易的，一天能有個800曝光我就要偷笑了吧！</p>
<p>Admob報表其實還好多有趣的東西可以看，例如可以去細分不同廣告單元，廣告類型或是國家地區&#8230;等，對收入的貢獻。不過朋友給我的報表沒有這些資訊，改天再跟他拿多一點資料來給大家看囉，這篇先到這樣。</p>
<p>＊本篇是從開發者的角度出發，探討可控因素包括曝光與點擊對預估收益之影響。</p>
<p>＊本篇預測與推論是基於此APP的樣本性質，不同的樣本則有不同效果。</p>
<hr />
<p>更多統計預測模型筆記連結：</p>
<p><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e5%9b%9e%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">線性回歸模型</a></p>
<p><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">正規化回歸模型Regularized Regression</a></p>
<p><a href="/data-scaling-multiple-linear-r-%e8%b3%87%e6%96%99%e6%a8%99%e6%ba%96%e5%8c%96/" target="_blank" rel="noopener noreferrer">資料標準化(Data Scaling)對複回歸分析(Multiple Regression)的影響 | R統計</a></p>
<p><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e5%9b%9e%e6%ad%b8/" target="_blank" rel="noopener noreferrer">羅吉斯回歸模型part1</a></p>
<p><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e5%9b%9e%e6%ad%b8/" target="_blank" rel="noopener noreferrer">羅吉斯回歸模型part2</a></p>
<p><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">決策樹/隨機森林</a></p>
<p><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forest</a></p>
<p><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines (GBM)</a></p>
<p><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">階層式分群法</a></p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">切割式分群法</a></p>
<p>更多資料視覺化筆記連結：</p>
<p><a href="/r-sqlite-data-visualization-ggplot2/" target="_blank" rel="noopener noreferrer">ggplot2 | 簡易資料視覺化 Basic Data Visualization &#8211; part1 | using R SQLite</a></p>
<p><a href="/ggplot2-theme-r-visualization-%e8%b3%87%e6%96%99%e8%a6%96%e8%a6%ba%e5%8c%96/" target="_blank" rel="noopener noreferrer">ggplot2 theme: 3 elements &#8211; text, line, rectangle | R 資料視覺化 &#8211; part2</a></p>
<p><a href="/dual-y-axis-plotting-%e9%9b%99y%e8%bb%b8-%e7%b9%aa%e5%9c%96-ggplot2/" target="_blank" rel="noopener noreferrer">dual y-axis plotting 雙Y軸 繪圖 | R 資料視覺化 Data visualization 小技巧</a></p>
<p>更多資料處理筆記連結：</p>
<p><a href="/r-data-processing-top-10-faq-%e8%b3%87%e6%96%99%e8%99%95%e7%90%86/" target="_blank" rel="noopener noreferrer">資料處理-實用的10個小技巧 | Data Processing Basics FAQ | R 語言</a></p>
<p>這篇文章 <a rel="nofollow" href="/app-mobile-ad-revenue-admob-%e8%a1%8c%e5%8b%95%e5%bb%a3%e5%91%8a%e6%94%b6%e5%85%a5/">開發一個免費App能賺多少錢？ 靠 AdMob 廣告月收3萬實例分享</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/app-mobile-ad-revenue-admob-%e8%a1%8c%e5%8b%95%e5%bb%a3%e5%91%8a%e6%94%b6%e5%85%a5/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
			</item>
	</channel>
</rss>
