<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>統計模型 &#8211; 果醬珍珍•JamJam</title>
	<atom:link href="/category/programming-statistics/statistics-model/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>健忘女孩Jam的學習筆記和生活雜記</description>
	<lastBuildDate>Fri, 03 Jul 2020 02:31:05 +0000</lastBuildDate>
	<language>zh-TW</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Gradient Boosting Machines GBM &#124; gbm, xgboost, h2o &#124; R語言</title>
		<link>/gradient-boosting-machines-gbm/</link>
					<comments>/gradient-boosting-machines-gbm/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Sun, 14 Apr 2019 14:54:16 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<guid isPermaLink="false">/?p=2875</guid>

					<description><![CDATA[<p>Gradient Boosting Machines 是一個超級受歡迎的機器學習法，在許多領域上都有非常成功的表現，也是Kaggle競賽時常勝出的主要演算法之一 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/gradient-boosting-machines-gbm/">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Gradient Boosting Machines 是一個超級受歡迎的機器學習法，在許多領域上都有非常成功的表現，也是Kaggle競賽時常勝出的主要演算法之一。有別於<a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">隨機森林</a>集成眾多深且獨立的樹模型，GBM則是集成諸多淺且弱連續的樹模型，每個樹模型會以之前的樹模型為基礎去學習和精進，結果通常是難以擊敗的。</p>
<h3>套件與資料準備</h3>
<p></p><pre class="crayon-plain-tag">library(rsample)      # data splitting 
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
library(vtreat)</pre><p>使用AmesHousing套件中的Ames Housing數據，並將數據切分為7:3的訓練測試比例。</p><pre class="crayon-plain-tag"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</pre><p>tree-based演算法往往可以將未處理資料配適的很好(即不需要特別將資料進行normalize, center,scale)，所以在以下筆記中將聚焦在如何使用多種不同套件執行GBMs。而雖然在這邊沒有去進行資料前處理，但我們仍可花時間透過處理變數特徵使得模型成效更佳。</p>
<h3>Gradient Boosting Machines&#8217; Advantage &amp; Disadvantages</h3>
<h4>優勢</h4>
<ol>
<li>產生的預測精準度通常無人能打敗</li>
<li>擁有許多彈性 &#8211; 可以針對不同的Loss Function來進行優化(*Loss Function即所有需要被「最小化」的目標函式，一個最常用來尋找使目標函式最小值的資料點的方法即為gradient descent)，且有hyperparameters的參數選項可以tuning，好讓目標函式配適的很好。</li>
<li>不需要資料前處理 &#8211; 通常可以很好的處理類別和數值型變數。</li>
<li>可以處理遺失值 &#8211; 不需要空值填補。</li>
</ol>
<h4>劣勢</h4>
<ol>
<li>GBMs會持續優化模型來最小化誤差。這樣可能會過度擬和極端值的部分而造成過度配適。因此必須綜合使用cross validation來銷除這個情況。</li>
<li>計算上成本非常高 &#8211; GBMs通常會包含許多樹(&gt;1000)，會佔用許多時間和記憶體。</li>
<li>模型彈性度高，導致許多參數會影響grid search的流程(比如說迭代次數，樹的深度，參數正規化等等)，在tuning模型的時候會需要大型的grid search過程。</li>
<li>可解釋程度稍稍少了一點，但有許多輔助工具如變數重要性、partial dependence plots, LIME等。</li>
</ol>
<h3>Gradient Boosting Machines (GBMs) 概念</h3>
<p>許多監督式機器學習模型都是建立在單一預測模的基礎上（比如說linear regression, penalized models or regularized regression, naive Bayes, support vector machines)。而則有像是Bagging和Random Forests這種集成學習演算法(ensemble learning)，集成眾多預測模型並單純平均每個模型的預測結果得出預測值。另外一種則是Boosting系列，是基於不一樣的建設性策略來進行集成學習。</p>
<p>Boosting的主要概念就是將新模型「有順序、循序漸進」的加入集成學習。在每一次迭代中，會新增一個新的(new)、弱的(weak)、base-learner模型，針對到目前為止集成學習的誤差進行訓練。</p>
<p>以下進一步解釋boosting models的特徵關鍵字：</p>
<ol>
<li>Base-learning models: Boosting是一個迭代改善任何弱學習模型的框架。許多Gradienet Boosting應用可允使用者任意加入眾多類型的weak learners模型。然而在實務上，boosted algorithm幾乎總是使用Decision Trees當作base-learner。也因此本學習筆記會主要討論boosting regression trees的應用。</li>
<li>Training weak models: 所謂的「弱模型(weak model)」指的是模型的錯誤率只有稍稍比隨機猜測好一點點的模型。Boosting背後的概念就是每一個順序模型都會建立一個僅稍稍改善殘餘誤差的弱模型(weak model)。以決策樹來說，較淺的樹就是弱模型的代表。一般來說，淺的決策樹模型切割數約在1~6之間。綜合多個弱模型會有以下好處(將較於綜合多個強模型)：
<ul>
<li>速率: 建構弱模型在計算成本上是便宜的。</li>
<li>精準度的改善: weak model允許演算法「慢慢學習」；即在模型表現不好的新領域進行些微調整。一般來說，慢慢學習的統計方法通常表現不錯。</li>
<li>避免overfitting: 由於在集成學習過程中，每一次訓練模型僅稍稍貢獻一點點額外的成效改善，使得我們有辦法即時在偵測到overfitting時即停止學習(使用的cross validation)。</li>
</ul>
</li>
<li>針對集成學習的殘餘誤差有順序性的訓練: Boosted trees是有順序性的；每一個樹模型都是依據前面的樹模型所得的資訊而訓練而成。基本的boosted regression trees演算法可被一般化為以下步驟(其中x代表features而y代表目標回應變數)：
<ul>
<li>依據原始資料配適一棵樹模型: \(F_{1}(x) = y\)</li>
<li>並依據先前的殘餘誤差訓練下一棵樹模型: \(h_{1}(x) = y &#8211; F_{1}(x)\)</li>
<li>將新的樹新增到演算法：\(F_{2}(x) = F_{1}(x) + h_{1}(x)\)</li>
<li>再一次依據殘餘誤差訓練下一棵樹模型 = \(h_{2}(x) = y &#8211; F_{2}(x)\)</li>
<li>將新的樹新增到演算法：\(F_{3}(x) = F_{2}(x) + h_{2}(x)\)</li>
<li>持續這個過程直到一些機制(如cross validation)告訴演算法可以停止。</li>
</ul>
</li>
</ol>
<p>boosted regression trees的基本演算法概念可以一般化為以下，即最終模型是b個獨立的回歸樹模型階段性相加的結果：<br />
\[ f(x) = \sum_{b=1}^B f^b(x) \]</p>
<h3>Gradient descent</h3>
<p>許多演算法，包括Decision trees，都聚焦在最小化殘差，也因此皆強調「MSE Loss Function」的目標函式。上面所討論到的演算法摘要了boosting法如何循序漸進地利用有順序新的弱回歸樹模型來配飾真實資料趨勢並最小化誤差。這個就是gradient boosting用來最小化Meas Squared Error (MSE) Loss Funciton的方法。雖然有時候我們會想要聚焦在其他Loss function上，如Mean Absolute Error(MAE)，或是遇到分類問題時所使用的deviance。而<strong><em>gradient</em></strong> boosting machines的命名則是取自於這個方法可以擴張至除了MSE以外的Loss function。</p>
<p>Gradient Boosting被認為是一個<strong><em>gradient descent</em></strong>(梯度下降)的演算法。Gradient Descent是一個非常通用的最適化演算法，能夠找到解決各種問題的最佳解。Gradient Descent的概念就是迭代的微調整參數來來最小化損失函數Loss Funciton。Gradient Descent會在給定的一組參數\(\theta\)區間，衡量局部Loss(cost) function的gradient，並沿著gradient下降的方向。直到gradient為0時，則找到局部最小值。</p>
<p><strong><em>Gradient descent</em></strong>可以被應用在任意可微分的loss function上，所以讓GBMs可以針對感興趣的loss function來尋找最適解。在gradient descent中一個重要的參數就是由_learning rate_s所決定的每次的變動率(size of steps)。較小的變動率會使得模型訓練過程中會迭代多次來尋找最小值，而過高的變動率則可能會讓模型錯過最小值和遠遠偏離初始值。</p>
<p>此外，並不是所有的loss function都是凸形的(convex)(如碗型)。可能會有局部的最小值、平坦高原或不規則地形等，使得尋找global minimum變得困難。<strong><em>Stochastic gradient descent</em></strong>(隨機梯度下降)則可處理這樣的問題，透過抽樣一部分比例的觀察值（通常不重複），並使用此子集合建立下一個模型。這使得演算法變得更快一些，但是隨機抽樣的隨機性亦造成下降的loss function gradient的隨機性。雖然這個隨機性使得演算法無法找到absolute global minimum(全域最小值)，但隨機性確實能讓演算法跳脫local minimum(局部最小值)、平坦高原等局部解，並接近全域的最小值。</p>
<p>我們在下一個階段即能看到如何透過多種hyperparameters參數選項來調整如何處理loss function的gradient descent。</p>
<h3>Tuning</h3>
<p>GBMs的好處與壞處就是該演算法提供多個調整參數。好處是GBMs在執行上非常具彈性，但壞處就是在調整與尋找最適參數組合上會很耗時。以下為幾個GBMs最常見的調整hyperparameters參數：</p>
<ul>
<li>決策樹模型的數量：總共要配適的決策樹模型數量。GBMs通常會需要很多很多樹，但不像random Forests，GBMs是可以過度擬和(overfit)的，所以他演算的目標是尋找最適決策樹數量使得感興趣的loss function最小化。</li>
<li>決策樹的深度：d,每棵樹模型的切割(split)數。用來控制boosted集成模型的複雜度。通常\(d=1\)的效果不錯，即此弱模型是由一次分割所得的樹模型所組成。而更常見的split數可能介在\(1&lt;d&lt;10\)之間。</li>
<li>Learning rate: 決定演算法計算gradient descent的速率。較慢的learning rate速率，可以避免overfitting的機會，但同時也會增加尋找最適解的時間。learning rate也被稱作shrinkage。</li>
<li>subsampling: 控制是否要使用原始訓練資料部分比例的抽樣子集合。使用少於100%的訓練資料表示你將使用stochastic gradient descent，這將有助於避免overfitting以及陷在loss function gradient的局部最小最大值。</li>
</ul>
<p>而在本學習筆記中，亦會介紹到專屬於特定package的調整hyperparameters參數，用來改善模型成效以及模型訓練與調整的效率。</p>
<h3>使用R實作Gradient Boosting Machines</h3>
<p>有很多執行GBMs和GBM變種的套件。而本學習筆記會cover到的幾個最受歡迎的套件，包括：</p>
<ul>
<li>gbm: 最原始的執行GBMs的套件。</li>
<li>xgboost: 一個更快速且有效的gradient boosting架構（後端為c++）。</li>
<li>h2o: 強大的java-based的介面，提供平行分散演算法和高銷率的生產。</li>
</ul>
<h3>gbm</h3>
<p>gbm套件是R裡面最原始執行GBM的套件。是來自於Freund &amp; Schapire’s <a href="http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf" target="_blank" rel="noopener noreferrer">AdaBoost algorithm</a>的Friedman’s <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank" rel="noopener noreferrer">gradient boosting machine</a>延伸。由Mark Landry撰寫的GBM套件簡報可參考<a href="https://www.slideshare.net/mark_landry/gbm-package-in-r" target="_blank" rel="noopener noreferrer">此連結</a>。</p>
<p>gbm套件幾個功能與特色包括：</p>
<ul>
<li>Stochastic GBM(隨機 GBM)</li>
<li>可支援到1024個factor levels</li>
<li>支援「分類」和「回歸」樹</li>
<li>包括許多loss functions</li>
<li>提供Out-of-bag 估計法來尋找最適的迭代次數</li>
<li>容易overfitting &#8211; 因為套件中沒有自動使用提早煞車功能來偵測overfitting</li>
<li>如果內部使用cross-validation，這可以被平行分散到所有機器核心</li>
<li>目前gbm套件正在進行重新建構與重寫(並已持續一段時間)。</li>
</ul>
<h4>基本的gbm實作</h4>
<p>gbm套件中有兩個主要的訓練用函數：gbm::gbm跟gbm::fit。</p>
<ul>
<li>gbm::gbm &#8211; 使用「formula介面」來設定模型。</li>
<li>gbm::fit &#8211; 使用「x &amp; y矩陣」來設定模型。</li>
</ul>
<p><span style="color: #9f6ad4;">當變數量很大的時候，使用「x &amp; y 矩陣」會比「formula」介面來的更有效率。</span></p>
<p>gbm()函數預設的幾個參數值如下：</p>
<ul>
<li>learning rate(shrinkage):0.1。學習步伐，通常越小的學習步伐會需要越多模型數(n.tree)來找到最小的MSE。而預設n.tree為100，是相當足夠的。</li>
<li>number of trees(n.tree): 100。總迭代次數(新增模型數)。</li>
<li>depth of tree(interaction.depth): 1。最淺的樹（最弱的模型）。</li>
<li>bag.fraction: 0.5。訓練資料集有多少比例會被抽樣做為下一個樹模型的基礎。用來替模型注入隨機性。</li>
<li>train.fraction: 1。模型首次使用訓練資料的比例，剩餘的觀測資料則最為OOB sample用來估計loss function用。</li>
<li>cv.folds: 0。如果使用&gt;1的cross validation folds，除了會回傳該參數組合下的模型配適結果，也會估計cv.error。</li>
<li>verbose: 預設為FALSE。決定是否要印出程序和成效指標。</li>
<li>n.cores: 使用的CPU核心數。由於在使用cross validation的時候，loop會將不同CV folds分配到不同核心。沒特別設定的話會使用偵測機器核心數函數來處理parallele::detectCores。</li>
</ul>
<p>以下我們來建立一個學習步伐為0.001且模型數量為10000的GBMs。並使用5 folds的交叉驗證計算cross-validated error。</p><pre class="crayon-plain-tag"># 使抽樣結果可以重複
set.seed(123)

# train GBM model
system.time(
  gbm.fit &lt;- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 10000, # 總迭代次數
  interaction.depth = 1, # 弱模型的切割數
  shrinkage = 0.001, # 學習步伐
  cv.folds = 5, # cross validation folds
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  23.348   0.332  80.086</pre><p>GBMs模型約花80秒(約1分多鐘)。</p>
<p>將模型結果印出。結果包括文字資訊以及每一次迭代次數所對應的loss function(squared error loss)變化。</p><pre class="crayon-plain-tag">print(gbm.fit)</pre><p></p><pre class="crayon-plain-tag">## gbm(formula = Sale_Price ~ ., distribution = &quot;gaussian&quot;, data = ames_train, 
##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001, 
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 10000 iterations were performed.
## The best cross-validation iteration was 10000.
## There were 80 predictors of which 45 had non-zero influence.</pre><p>模型結果資訊是由list所儲存。可以使用索引的方式取出。</p>
<p>比如說我們來看最小的CV RMSE值。</p><pre class="crayon-plain-tag">sqrt(min(gbm.fit$cv.error))</pre><p></p><pre class="crayon-plain-tag">## [1] 29133.33</pre><p>表示平均來說模型估計值離真實Sale_Price差了約30K。</p>
<p>我們也可以透過以下方式將GBMs找尋最佳迭代數的過程繪出：(其中黑線的為訓練誤差(train.error)，綠線為cv.error, 若method使用“test&#8221;，則會有紅線表示valid.error)</p><pre class="crayon-plain-tag">gbm.perf(object = gbm.fit, plot.it = TRUE,method = "cv")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-6-1-1.png" alt="gradient boosting machines, GBM" /></p><pre class="crayon-plain-tag">## [1] 10000</pre><p>可以發現以此小學習步伐(0.001)，會需要很多模型來接近最小的loss function(使cv.error最小化)，最佳迭代數為10000。</p>
<h4>Tuning</h4>
<p>「手動tuning」</p>
<p>假設我們將學習步伐加大為0.1，迭代模型數降低為5000，且模型複雜度增加到3 splits。</p><pre class="crayon-plain-tag"># for reproducibility
set.seed(123)

# train GBM model
system.time(
gbm.fit2 &lt;- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 5000,
  interaction.depth = 3,
  shrinkage = 0.1,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  ) 
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  35.555   1.314 151.695</pre><p>調大步伐後(0.1)，花的時間變多為151秒(2.5分鐘)，GBMs模型最小cv.error變得更低(23K)。<br />
(v.s.小步伐(0.001)的cv.error: 29K)</p><pre class="crayon-plain-tag">sqrt(min(gbm.fit2$cv.error))</pre><p></p><pre class="crayon-plain-tag">## [1] 23112.1</pre><p>將模型結果印出。最佳迭代數(所需模型數)為1260。</p><pre class="crayon-plain-tag">gbm.perf(object = gbm.fit2, method = "cv")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-9-1-1.png" alt="gradient boosting machines, GBM" /></p><pre class="crayon-plain-tag">## [1] 1260</pre><p>「grid search自動化tuning」</p>
<p>因為手動調整參數是沒效率的，我們來建立hyperparameters grid並自動套用grid search。</p><pre class="crayon-plain-tag"># create hyperparameter grid
hyper_grid &lt;- expand.grid(
  shrinkage = c(.01, .1, .3), # 學習步伐
  interaction.depth = c(1, 3, 5), # 模型切割數
  n.minobsinnode = c(5, 10, 15), # 節點最小觀測值個數
  bag.fraction = c(.65, .8, 1), # 使用隨機梯度下降(&lt;1)
  optimal_trees = 0,               # 儲存最適模型樹的欄位
  min_RMSE = 0                     # 儲存最小均方差的欄位
)

# total number of combinations
nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 81</pre><p>我們一一測試以上81種超參數排列組合的效果，並指定使用5000個樹模型。<br />
另外，為了降低執行的時間，有別於使用cross validation，我們改使用75%的訓練資料，使用剩下的25%的資料來進行OOB評估效果。需要特別注意的是，當使用train.fraction參數時，模型會直接使用前XX %的資料來使用，因此需要確保說資料是隨機排列的。</p>
<p>先將資料進行隨機排列處理。</p><pre class="crayon-plain-tag"># randomize data
random_index &lt;- sample(1:nrow(ames_train), nrow(ames_train))
random_ames_train &lt;- ames_train[random_index, ]</pre><p>開始執行grid search</p><pre class="crayon-plain-tag"># grid search 
for(i in 1:nrow(hyper_grid)) {

  # reproducibility
  set.seed(123)

  # train model
  gbm.tune &lt;- gbm(
    formula = Sale_Price ~ .,
    distribution = "gaussian",
    data = random_ames_train,
    n.trees = 5000, # 使用5000個樹模型
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75, # 使用75%的訓練資料，並用剩餘資料做OOB成效評估/驗證
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )

  # 將每個GBM模型最小的模型代號和對應的驗證均方誤差(valid RMSE)回傳到
  hyper_grid$optimal_trees[i] &lt;- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] &lt;- sqrt(min(gbm.tune$valid.error))
}</pre><p>將每種參數組合的結果，依照RMSE由小到大排列，並取出排名前10的模型，查看參數組合細節。</p><pre class="crayon-plain-tag">hyper_grid %&gt;% 
  dplyr::arrange(min_RMSE) %&gt;%
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees
## 1       0.01                 5              5         0.65          3867
## 2       0.01                 5              5         0.80          4209
## 3       0.01                 5              5         1.00          4281
## 4       0.10                 3             10         0.80           489
## 5       0.01                 3              5         0.80          4777
## 6       0.01                 3             10         0.80          4919
## 7       0.01                 3              5         0.65          4997
## 8       0.01                 5             10         0.80          4123
## 9       0.01                 5             10         0.65          4850
## 10      0.01                 3             10         1.00          4794
##    min_RMSE
## 1  16647.87
## 2  16960.78
## 3  17084.29
## 4  17093.77
## 5  17121.26
## 6  17139.59
## 7  17139.88
## 8  17162.60
## 9  17247.72
## 10 17353.36</pre><p>我們可以看到以下幾點：</p>
<ol>
<li>最佳模型的最小RMSE(17K)，較先前的RMSE(23K)降低了有6K左右。</li>
<li>前十的模型的學習步伐都小於0.3，表示較小的學習步伐在尋找最小誤差的模型效果是不錯的。</li>
<li>前十的模型都選用&gt;1切割數的樹模型，。</li>
<li>十個模型有8個模型都使用bag.fraction &lt; 1的隨機梯度下降(即使用&lt;100%的訓練資料集進行每個模型的訓練)，這將有助於避免overfitting以及陷在loss function gradient的局部最小最大值。</li>
<li>前十的模型中，也沒有使用採用節點觀測數大於等於15者。因為較小觀測數的節點較能捕捉到更多特徵。</li>
<li>部分參數組合所使用的最佳迭代數（總樹模型數）都很接近5000個。下次執行grid search時或許可以考慮增大樹模型數。</li>
</ol>
<p>根據以上測試，我們已更接近最適的參數組合區間，我們此時在此聚焦範圍內再一次執行81種超參數組合的最佳模型搜尋。</p><pre class="crayon-plain-tag"># 根據上一部的結果，調整參數區間與數值
hyper_grid_2 &lt;- expand.grid(
  shrinkage = c(.01, .05, .1), # 聚焦更小的學習步伐
  interaction.depth = c(3, 5, 7), #聚焦&gt;1的切割數
  n.minobsinnode = c(5, 7, 10), # 聚焦更小的節點觀測值數量
  bag.fraction = c(.65, .8, 1), # 不變
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid_2)</pre><p></p><pre class="crayon-plain-tag">## [1] 81</pre><p>我們再一次的用for loop迴圈執行以上81種超參數組合的模型，找出每一次最適的模型與對應的最小誤差。</p><pre class="crayon-plain-tag"># grid search 
for(i in 1:nrow(hyper_grid_2)) {

  # reproducibility
  set.seed(123)

  # train model
  gbm.tune &lt;- gbm(
    formula = Sale_Price ~ .,
    distribution = "gaussian",
    data = random_ames_train,
    n.trees = 6000,
    interaction.depth = hyper_grid_2$interaction.depth[i],
    shrinkage = hyper_grid_2$shrinkage[i],
    n.minobsinnode = hyper_grid_2$n.minobsinnode[i],
    bag.fraction = hyper_grid_2$bag.fraction[i],
    train.fraction = .75, # 使用剩餘的25%資料估計OOB誤差
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )

  # add min training error and trees to grid
  hyper_grid_2$optimal_trees[i] &lt;- which.min(gbm.tune$valid.error)
  hyper_grid_2$min_RMSE[i] &lt;- sqrt(min(gbm.tune$valid.error))
}</pre><p>檢視結果</p><pre class="crayon-plain-tag">hyper_grid_2 %&gt;% 
  dplyr::arrange(min_RMSE) %&gt;%
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    shrinkage interaction.depth n.minobsinnode bag.fraction
## 1       0.01                 5              5         0.65
## 2       0.01                 7              5         0.65
## 3       0.05                 5              5         0.65
## 4       0.01                 7              5         0.80
## 5       0.01                 7              7         0.65
## 6       0.05                 5             10         0.80
## 7       0.01                 5              5         0.80
## 8       0.01                 3              7         0.80
## 9       0.01                 5              7         0.65
## 10      0.01                 5              7         0.80
##    optimal_trees min_RMSE
## 1           3867 16647.87
## 2           2905 16782.26
## 3            640 16783.13
## 4           4193 16833.74
## 5           3275 16906.53
## 6            958 16933.12
## 7           4209 16960.78
## 8           5813 16998.79
## 9           4798 17003.94
## 10          4753 17004.20</pre><p>本次結果與上一次結果十分類似。最佳模型和上一次選出的最佳模型是一樣的(相同的參數排列組合)，RMSE約是17K。</p>
<p>一旦找到最佳模型，我們則可使用該參數組合來train一個模型。也因為最佳模型約收斂在僅1634個樹模型，我們可以 訓練一個由1634個樹模型所組成的cross validation模型(使用CV來提供更穩健的誤差估計值)。</p><pre class="crayon-plain-tag"># for reproducibility
set.seed(123)

system.time(
# train GBM model
gbm.fit.final &lt;- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 1634,
  interaction.depth = 5,
  shrinkage = 0.05,
  n.minobsinnode = 7,
  bag.fraction = .8, 
  train.fraction = 1, # 如果使用&lt;1(xx%)的訓練比例，就會用剩餘的(1-XX%)資料估計OOB誤差
  cv.folds = 4, # 有別於使用OOB估計誤差，我們估計更穩健的CV誤差
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  18.394   0.617  49.558</pre><p>最佳模型的cv誤差如下(22K)。</p><pre class="crayon-plain-tag">sqrt(min(gbm.fit.final$cv.error)) # 必須是模型參數cv.folds &gt; 1 才會回傳cv.error</pre><p></p><pre class="crayon-plain-tag">## [1] 22165.11</pre><p></p>
<h4>視覺化</h4>
<h5>variable importance</h5>
<p>執行完最後的最佳模型後(gbm.fit.final)，我們會想要看對目標變數sale price來說最有影響力的解釋變數有哪些，以捕捉模型的「可解釋性」。我們可以使用summary()函數來回傳gmb模型中最具影響力的解釋變數清單(data frame &amp; plot)。並可以使用gbm模型summary函式中的cBars參數來指定要顯示的解釋變數清單數(根據影響力排名)。預設使用相對影響力來計算變數重要性。以下說明計算變數重要性的兩個方法：</p>
<ol>
<li>method = relative.influence: 每棵樹在進行節點分割時，gbm會計算每個變數作為切點，切割後對模型誤差所帶來的改善(回歸模型的話就是MSE)。gbm於是會平均每個變數在不同樹模型的誤差改善值，當作變數影響力。具有越高平均誤差降低值得變數即被視作最具影響力的變數。</li>
<li>method = permutation.test.gbm: 模型會隨機置換（一次一個）不同預測變數，來計算個別變數的對預測性能的改善（使用所有training data），並平均每個變數在不同樹模型對正確率造成的改變量。具有越高正確率改變量的預測變數越具重要性。</li>
</ol>
<p></p><pre class="crayon-plain-tag">par(mar = c(5, 8, 1, 1))
# S3 method for class 'gbm'
summary(
  gbm.fit.final, # gbm object
  cBars = 10, # the number of bars to draw. length(object$var.names)
  plotit = TRUE, # an indicator as to whether the plot is generated.defult TRUE.
  method = relative.influence, # The function used to compute the relative influence. 亦可使用permutation.test.gbm
  las = 2
  )</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-19-1-1.png" alt="gradient boosting machines, GBM" /></p><pre class="crayon-plain-tag">##                                   var      rel.inf
## Overall_Qual             Overall_Qual 4.199240e+01
## Gr_Liv_Area               Gr_Liv_Area 1.343609e+01
## Neighborhood             Neighborhood 1.111899e+01
## Garage_Cars               Garage_Cars 5.374303e+00
## Total_Bsmt_SF           Total_Bsmt_SF 5.031406e+00
## First_Flr_SF             First_Flr_SF 3.641743e+00
## Bsmt_Qual                   Bsmt_Qual 2.365967e+00
## Exter_Qual                 Exter_Qual 1.559647e+00
## Kitchen_Qual             Kitchen_Qual 1.210399e+00
## Year_Remod_Add         Year_Remod_Add 9.800489e-01
## MS_SubClass               MS_SubClass 9.168515e-01
## Fireplace_Qu             Fireplace_Qu 7.900484e-01
## Mas_Vnr_Area             Mas_Vnr_Area 7.480704e-01
## Lot_Area                     Lot_Area 7.181146e-01
## Bsmt_Unf_SF               Bsmt_Unf_SF 7.036017e-01
## Second_Flr_SF           Second_Flr_SF 6.564021e-01
## Garage_Area               Garage_Area 5.812072e-01
## Screen_Porch             Screen_Porch 5.697864e-01
## Bsmt_Exposure           Bsmt_Exposure 5.447030e-01
## Overall_Cond             Overall_Cond 5.149690e-01
## BsmtFin_Type_1         BsmtFin_Type_1 5.136834e-01
## Full_Bath                   Full_Bath 4.517156e-01
## Lot_Frontage             Lot_Frontage 4.308841e-01
## Latitude                     Latitude 4.021943e-01
## Sale_Condition         Sale_Condition 3.736402e-01
## Garage_Type               Garage_Type 3.400862e-01
## Bsmt_Full_Bath         Bsmt_Full_Bath 2.830678e-01
## Garage_Finish           Garage_Finish 2.794122e-01
## Open_Porch_SF           Open_Porch_SF 2.731140e-01
## Exterior_1st             Exterior_1st 2.449858e-01
## Fireplaces                 Fireplaces 2.446628e-01
## Sale_Type                   Sale_Type 2.083020e-01
## Year_Built                 Year_Built 2.060348e-01
## Central_Air               Central_Air 1.895762e-01
## Exterior_2nd             Exterior_2nd 1.791393e-01
## TotRms_AbvGrd           TotRms_AbvGrd 1.624508e-01
## Wood_Deck_SF             Wood_Deck_SF 1.619419e-01
## Mo_Sold                       Mo_Sold 1.556240e-01
## Condition_1               Condition_1 1.555862e-01
## Garage_Cond               Garage_Cond 1.433484e-01
## Functional                 Functional 1.195756e-01
## Land_Contour             Land_Contour 1.176260e-01
## Longitude                   Longitude 1.099157e-01
## Heating_QC                 Heating_QC 7.996284e-02
## Year_Sold                   Year_Sold 7.732664e-02
## Roof_Matl                   Roof_Matl 6.581906e-02
## House_Style               House_Style 5.653741e-02
## Mas_Vnr_Type             Mas_Vnr_Type 4.764729e-02
## Bedroom_AbvGr           Bedroom_AbvGr 4.542366e-02
## Lot_Config                 Lot_Config 3.928406e-02
## Paved_Drive               Paved_Drive 3.815510e-02
## BsmtFin_Type_2         BsmtFin_Type_2 3.573880e-02
## Roof_Style                 Roof_Style 3.492513e-02
## Bsmt_Cond                   Bsmt_Cond 3.320548e-02
## Land_Slope                 Land_Slope 3.191664e-02
## Exter_Cond                 Exter_Cond 3.036215e-02
## MS_Zoning                   MS_Zoning 3.016913e-02
## Enclosed_Porch         Enclosed_Porch 2.361207e-02
## Alley                           Alley 1.860542e-02
## Condition_2               Condition_2 1.829088e-02
## Half_Bath                   Half_Bath 1.622231e-02
## Three_season_porch Three_season_porch 1.091252e-02
## Foundation                 Foundation 1.039571e-02
## Low_Qual_Fin_SF       Low_Qual_Fin_SF 8.975948e-03
## BsmtFin_SF_2             BsmtFin_SF_2 8.747153e-03
## Fence                           Fence 7.868905e-03
## Lot_Shape                   Lot_Shape 5.645516e-03
## Electrical                 Electrical 4.850525e-03
## Heating                       Heating 4.405392e-03
## Garage_Qual               Garage_Qual 3.845332e-03
## BsmtFin_SF_1             BsmtFin_SF_1 2.902816e-03
## Misc_Val                     Misc_Val 2.321802e-03
## Bsmt_Half_Bath         Bsmt_Half_Bath 2.263392e-03
## Street                         Street 1.157860e-03
## Bldg_Type                   Bldg_Type 4.970819e-04
## Misc_Feature             Misc_Feature 4.236315e-04
## Pool_QC                       Pool_QC 1.575743e-04
## Pool_Area                   Pool_Area 9.720817e-05
## Utilities                   Utilities 0.000000e+00
## Kitchen_AbvGr           Kitchen_AbvGr 0.000000e+00</pre><p>另外一個方式，就是使用vip套件(variable importance plot)的vip函式，會回傳ggplot形式的重要變數圖表。 為兩個解釋集成樹的兩大重要解釋指標)，是許多機器學習模型常用的變數重要性繪圖框架。</p><pre class="crayon-plain-tag"># install.packages('vip')
vip::vip(gbm.fit.final)</pre><p></p>
<h5><img loading="lazy" class="alignnone size-full wp-image-2879" src="/wp-content/uploads/2019/04/Rplot01.png" alt="gradient boosting machines, GBM" width="631" height="441" srcset="/wp-content/uploads/2019/04/Rplot01.png 631w, /wp-content/uploads/2019/04/Rplot01-300x210.png 300w, /wp-content/uploads/2019/04/Rplot01-230x161.png 230w, /wp-content/uploads/2019/04/Rplot01-350x245.png 350w, /wp-content/uploads/2019/04/Rplot01-480x335.png 480w" sizes="(max-width: 631px) 100vw, 631px" /></h5>
<h5>Partial dependence plots</h5>
<p>一旦識別出最重要的幾個變數後，下一步就是去了解當解釋變數變動時，目標變數是如何變動的(即marginal effects，邊際效果，每變動一單位解釋變數時，對目標變數的影響)。我們可以使用partial dependence plots(PDPs)和individual conditional expectation(ICE)曲線。</p>
<ul>
<li>PDPs: 繪製特定變數邊際變動造成的平均目標預測值的變動。比如說，下圖繪製了預測變數Gr_Liv_Area邊際變動(控制其他變數不變的情況下)對平均目標預測銷售金額(avg. sale price_的影響。下圖PDPs描述隨著房屋基底的面積邊際增加，平均銷售價格增加的變化。</li>
</ul>
<p></p><pre class="crayon-plain-tag">gbm.fit.final %&gt;%
  partial(object = .,# A fitted model object of appropriate class (e.g., "gbm", "lm", "randomForest", "train", etc.).
          pred.var = "Gr_Liv_Area", 
          n.trees = gbm.fit.final$n.trees, # 如果是gbm的話，需指定模型所使用樹個數
          grid.resolution = 100) %&gt;%
  # The autplot function can be used to produce graphics based on ggplot2
  autoplot(rug = TRUE, train = ames_train) + # plotPartial()不支援gbm
  scale_y_continuous(labels = scales::dollar) # 因為是使用ggplot基礎繪圖，故可以使用ggplot相關圖層來調整</pre><p><img loading="lazy" class="alignnone size-full wp-image-2880" src="/wp-content/uploads/2019/04/Rplot02.png" alt="gradient boosting machines, GBM" width="606" height="439" srcset="/wp-content/uploads/2019/04/Rplot02.png 606w, /wp-content/uploads/2019/04/Rplot02-300x217.png 300w, /wp-content/uploads/2019/04/Rplot02-230x167.png 230w, /wp-content/uploads/2019/04/Rplot02-350x254.png 350w, /wp-content/uploads/2019/04/Rplot02-480x348.png 480w" sizes="(max-width: 606px) 100vw, 606px" /></p>
<p>拆解步驟1: 先檢視沒有繪圖(plot = FALSE)的所回傳的data.frame。在一些例子中，使用partial根據object來擷取training data是困難的，此時便會出現錯誤訊息要求使用者透過train參數提供訓練資料集。但絕大部分的時候，partial會預設擷取當下環境下訓練object所使用的訓練資料集，所以很重要的事，在執行partial之前不行改變到訓練資料集的變數內容，而此問題可透過明確指定train參數所對應的訓練資料集而解決。</p><pre class="crayon-plain-tag">partialDf &lt;-partial(
        object = gbm.fit.final,pred.var = "Gr_Liv_Area", 
        n.trees = gbm.fit.final$n.trees, 
        grid.resolution = 100,
        train = ames_train)
partialDf</pre><p></p><pre class="crayon-plain-tag">##     Gr_Liv_Area     yhat
## 1           334 156956.9
## 2           387 156956.9
## 3           441 156956.9
## 4           494 156956.9
## 5           548 156956.9
## 6           602 156956.9
## 7           655 156956.9
## 8           709 156660.6
## 9           762 156755.5
## 10          816 156871.1
## 11          870 156871.1
## 12          923 158251.7
## 13          977 161056.8
## 14         1031 163112.1
## 15         1084 163138.2
## 16         1138 163214.8
## 17         1191 166534.4
## 18         1245 166987.2
## 19         1299 169077.6
## 20         1352 170758.6
## 21         1406 172776.0
## 22         1459 174681.8
## 23         1513 181046.8
## 24         1567 182290.3
## 25         1620 185044.8
## 26         1674 185035.8
## 27         1728 186329.2
## 28         1781 189560.9
## 29         1835 192374.4
## 30         1888 193609.3
## 31         1942 194775.0
## 32         1996 200759.0
## 33         2049 202826.8
## 34         2103 206586.7
## 35         2156 207502.2
## 36         2210 207502.2
## 37         2264 209486.0
## 38         2317 212637.8
## 39         2371 214445.4
## 40         2425 214863.0
## 41         2478 215367.1
## 42         2532 215404.3
## 43         2585 215404.3
## 44         2639 215571.6
## 45         2693 226433.3
## 46         2746 225632.2
## 47         2800 225849.6
## 48         2853 223628.9
## 49         2907 219924.6
## 50         2961 218770.4
## 51         3014 218770.4
## 52         3068 218770.4
## 53         3122 227327.1
## 54         3175 229537.3
## 55         3229 235917.9
## 56         3282 241038.0
## 57         3336 240301.2
## 58         3390 243415.0
## 59         3443 243127.7
## 60         3497 246930.9
## 61         3550 246930.9
## 62         3604 246930.9
## 63         3658 246930.9
## 64         3711 246930.9
## 65         3765 246930.9
## 66         3819 246930.9
## 67         3872 246930.9
## 68         3926 246930.9
## 69         3979 246930.9
## 70         4033 246930.9
## 71         4087 246930.9
## 72         4140 246930.9
## 73         4194 246930.9
## 74         4247 246930.9
## 75         4301 246930.9
## 76         4355 246930.9
## 77         4408 246930.9
## 78         4462 246930.9
## 79         4516 246930.9
## 80         4569 246930.9
## 81         4623 246930.9
## 82         4676 246930.9
## 83         4730 246930.9
## 84         4784 246930.9
## 85         4837 246930.9
## 86         4891 246930.9
## 87         4944 246930.9
## 88         4998 246930.9
## 89         5052 246930.9
## 90         5105 246930.9
## 91         5159 246930.9
## 92         5213 246930.9
## 93         5266 246930.9
## 94         5320 246930.9
## 95         5373 246930.9
## 96         5427 246930.9
## 97         5481 246930.9
## 98         5534 246930.9
## 99         5588 246930.9
## 100        5642 246930.9</pre><p>亦可使用plot = TRUE將以上結果繪出。(不是ggplot2)<br />
但其實比較推薦保留plot = FALSE，將partial回傳結果先儲存。這樣的好處在於繪圖上會更有彈性，當預設繪圖結果不足夠時，不需要重新執行partial()。</p><pre class="crayon-plain-tag">partial(gbm.fit.final, train = ames_train, pred.var = 'Gr_Liv_Area',n.trees = gbm.fit.final$n.trees, plot = TRUE)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-23-1-1.png" alt="gradient boosting machines, GBM" /></p>
<ul>
<li>ICE curves: 是PDPs圖的延伸。與PDPs圖不同的地方在於，PDPs是繪製每個解釋變數邊際變動所造成的「平均」目標數值的變化(平均所有觀測值)，而ICE curves則是繪製「每個」解釋變數邊際變動對所有觀測值的目標數值的變動。下面分別呈現了regular ICE曲線圖(左)和centered ICE曲線圖(右)。當曲線有很寬廣的截距且彼此疊在一起，很難辨別感興趣的預測變數邊際變動所造成的回應變數變動量的異質性。而centered ICE曲線，則能強調結果中的異質性。以下regular ICE圖顯示，當Gr_Liv_Area增加時，大部分的觀測值的目標變數變化具有共通趨勢，而centered ICE圖則凸顯部分與共通趨勢不一致的觀察值變化圖。</li>
</ul>
<p></p><pre class="crayon-plain-tag">ice1 &lt;- gbm.fit.final %&gt;%
  partial(
    pred.var = "Gr_Liv_Area", 
    n.trees = gbm.fit.final$n.trees, 
    grid.resolution = 100,
    ice = TRUE # 當ice = TRUE或給定pred.fun參數，會回傳使用newdata替每個觀測值預測的結果
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) + # alpha參數只在ICE curves繪製上有效
  ggtitle("Non-centered") +
  scale_y_continuous(labels = scales::dollar)</pre><p></p><pre class="crayon-plain-tag">ice2 &lt;- gbm.fit.final %&gt;%
  partial(
    pred.var = "Gr_Liv_Area", 
    n.trees = gbm.fit.final$n.trees, 
    grid.resolution = 100,
    ice = TRUE
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  ggtitle("Centered") +
  scale_y_continuous(labels = scales::dollar)</pre><p></p><pre class="crayon-plain-tag">gridExtra::grid.arrange(ice1, ice2, nrow = 1)</pre><p><img loading="lazy" class="alignnone size-full wp-image-2881" src="/wp-content/uploads/2019/04/Rplot03.png" alt="gradient boosting machines, GBM" width="1042" height="530" srcset="/wp-content/uploads/2019/04/Rplot03.png 1042w, /wp-content/uploads/2019/04/Rplot03-300x153.png 300w, /wp-content/uploads/2019/04/Rplot03-768x391.png 768w, /wp-content/uploads/2019/04/Rplot03-1024x521.png 1024w, /wp-content/uploads/2019/04/Rplot03-830x422.png 830w, /wp-content/uploads/2019/04/Rplot03-230x117.png 230w, /wp-content/uploads/2019/04/Rplot03-350x178.png 350w, /wp-content/uploads/2019/04/Rplot03-480x244.png 480w" sizes="(max-width: 1042px) 100vw, 1042px" /></p>
<h5>LIME</h5>
<p>LIME是一種新程序幫助我們了解，單一觀察值的預測目標值是如何產生的。對gbm物件使用lime套件，我們需要定義模型類型model type和預測方法prediction methods。</p><pre class="crayon-plain-tag">model_type.gbm &lt;- function(x, ...) {
  return("regression")
}

predict_model.gbm &lt;- function(x, newdata, ...) {
  pred &lt;- predict(x, newdata, n.trees = x$n.trees)
  return(as.data.frame(pred))
}</pre><p>以下我們便挑選兩個觀測值來檢視其預測目標值是如何產生的。<br />
結果包括預測目標值（分別為case1: 127K case2: 159K）、局部模型配適（兩者的局部配飾都太好）、以及對不同觀測值來說，對目標變數最具影響力的特徵變數。</p><pre class="crayon-plain-tag"># get a few observations to perform local interpretation on
local_obs &lt;- ames_test[1:2, ]

# apply LIME
explainer &lt;- lime(
  x=ames_train, # The training data used for training the model that should be explained.
  model = gbm.fit.final # The model whose output should be explained
  )
# 一旦使用lime()創建好了explainer，則可將explainer用作解釋模型作用在新觀察值的結果
explanation &lt;- explain(x = local_obs, # New observations to explain
                       explainer = explainer,
                       n_features = 5 # The number of features to use for each explanation.
                       )</pre><p></p><pre class="crayon-plain-tag">plot_features(explanation = explanation)</pre><p><img loading="lazy" class="alignnone size-full wp-image-2882" src="/wp-content/uploads/2019/04/Rplot04.png" alt="gradient boosting machines, GBM" width="1033" height="525" srcset="/wp-content/uploads/2019/04/Rplot04.png 1033w, /wp-content/uploads/2019/04/Rplot04-300x152.png 300w, /wp-content/uploads/2019/04/Rplot04-768x390.png 768w, /wp-content/uploads/2019/04/Rplot04-1024x520.png 1024w, /wp-content/uploads/2019/04/Rplot04-830x422.png 830w, /wp-content/uploads/2019/04/Rplot04-230x117.png 230w, /wp-content/uploads/2019/04/Rplot04-350x178.png 350w, /wp-content/uploads/2019/04/Rplot04-480x244.png 480w" sizes="(max-width: 1033px) 100vw, 1033px" /></p>
<h4>Predicting</h4>
<p>一旦決定好最佳的模型後，便使用模型來預測新的資料集(ames_test)。跟大部分模型一樣，我們可以使用predict()函數，只不過我們需要指定所需要的樹模型個數(可參考?predict.gbm之說明)。我們可以觀察到，測試資料集所得到的RMSE跟我們得到的最佳gbm模型的RMSE(22K)是差不多的。</p><pre class="crayon-plain-tag"># predict values for test data
pred &lt;- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, ames_test)

# results
caret::RMSE(pred, ames_test$Sale_Price)</pre><p></p><pre class="crayon-plain-tag">## [1] 19686.42</pre><p></p>
<h3>xgboost</h3>
<p>xgboost套件提供一個Extreme Gradient Boosting的R API，可以具效率地去執行gradient boosting framework(約比gbm套件快上10倍)。<a href="https://github.com/dmlc/xgboost/tree/master/demo" target="_blank" rel="noopener noreferrer">xgboost</a>文件的知識庫有豐富的資訊。亦可參考非常完整的參數tuning<a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" target="_blank" rel="noopener noreferrer">教學文章</a>。xgboost套件一直以來在kaggle data mining競賽上是滿受歡迎且成功的演算法套件。</p>
<p>xgboost幾個特色包括：</p>
<ul>
<li>提供內建的k-fold cross validation</li>
<li>隨機GBM，兼具column和row的抽樣(per split and per tree)，以達到更好的一般性(避免過度擬合)。</li>
<li>包括高效的線性模型求解器和樹學習演算法。</li>
<li>單一機器上的平行運算。</li>
<li>支援多個目標函數，包括回歸regression，分類classification，和排序ranking。</li>
<li>該套件被設計為有延展性的(extensible)，因此使用者可以自己定義自己的目標函式(objectives)。</li>
<li>Apache 2.0 License</li>
</ul>
<h4>基本的xgboost實作</h4>
<p>xgboost 只能在都是「數值變數的矩陣」下運作(由於在空間中表示點的位置，所有特徵值須為數值)。因此，我們必須先將數據進行編碼轉換。</p>
<p>一般來說編碼類別變數有兩種方式，分別為Label Encoding和one-hot encoding，通常前者會衍伸出編碼後的數值變成有順序意義在的問題，以及在空間維度中代表不同距離的意義之問題，故通常最常使用one-hot encoding法是最適合的，來使類別變數中三個屬性在空間中距離原點的距離是相同的(*但必須注意的是，one-hot encoding只適用在類別種類少的情況，如果種類過多，過多展開的維度會衍伸出其他問題)。</p>
<p>在R中有幾個執行one-hot encoding的方式，包括Matrix::sparse.model.matrix， caret::dummyVars，但我們這邊會使用vtreat套件。vtreat是一個強大的資料前處理套件，且有助於處理因遺失值、或新資料中才新冒出的類別資料級別（原本不為訓練資料類別變數中的選項）等因素而造成的問題。然而vtreat在使用上不是很直覺。本篇學習筆記在此不會說明太多vtreat的功能，如需要了解可先參考<a href="https://arxiv.org/abs/1611.09477" target="_blank" rel="noopener noreferrer">連結1</a>，<a href="https://www.r-bloggers.com/a-demonstration-of-vtreat-data-preparation/" target="_blank" rel="noopener noreferrer">連結2</a>和<a href="https://github.com/WinVector/vtreat" target="_blank" rel="noopener noreferrer">連結3</a>。</p>
<p>以下使用vtreat將training &amp; testing資料即重新one-hot encoding編碼。</p><pre class="crayon-plain-tag"># variable names
features &lt;- setdiff(names(ames_train), "Sale_Price")

# Create the treatment plan from the training data
treatplan &lt;- vtreat::designTreatmentsZ(ames_train, features, verbose = FALSE)

# Get the "clean" variable names from the scoreFrame
new_vars &lt;- treatplan %&gt;%
  magrittr::use_series(scoreFrame) %&gt;%        
  dplyr::filter(code %in% c("clean", "lev")) %&gt;% 
  magrittr::use_series(varName)     

# Prepare the training data
features_train &lt;- vtreat::prepare(treatplan, ames_train, varRestriction = new_vars) %&gt;% as.matrix()
response_train &lt;- ames_train$Sale_Price

# Prepare the test data
features_test &lt;- vtreat::prepare(treatplan, ames_test, varRestriction = new_vars) %&gt;% as.matrix()
response_test &lt;- ames_test$Sale_Price</pre><p>編碼後的維度個數</p><pre class="crayon-plain-tag">dim(features_train)</pre><p></p><pre class="crayon-plain-tag">## [1] 2051  348</pre><p></p><pre class="crayon-plain-tag">dim(features_test)</pre><p></p><pre class="crayon-plain-tag">## [1] 879 348</pre><p>xgboost提供不同的訓練函數(像是xgb.train,xgb.cv)。而我們這邊會使用能夠進行cross-validation的xgb.cv。以下我們訓練一個使用5-fold cv且使用1000棵樹的xgb模型。xgb.cv中有許多可調整的參數(基本的參數都跟xgb.train是一樣的)，幾個比較常出現的參數（與其預設值）分別為以下：</p>
<ul>
<li>data: 只允許數值型矩陣，如xgb.DMatrix, matrix, dgCMartirx類型。</li>
<li>nrounds: 模型所使用的樹個數(迭代數)。</li>
<li>nfold: 將投入的data參數值（在此處為訓練資料集），隨機切割(partition)為n等分的子樣本。</li>
<li>params: list()，參數list。常用的參數包括：
<ul>
<li>objective : 目標函數(亦可使用params = list()進行參數指定)。常用的目標函數包括：</li>
<li>reg:linear : 線性迴歸</li>
<li>binary:logistic : 分類用的羅吉斯迴歸</li>
<li>eta: learning rate,學習步伐(default為0.3)</li>
<li>max_depth: tree depth, 樹模型的深度(default為6)</li>
<li>min_child_weight: minimum node size,最小節點個數值(default為1)</li>
<li>subsample: percentage of training data to sample for each tree (就如同gbm套件中的bag.fraction參數),每棵樹模型將抽樣使用多少比例的訓練資料集(default: 100% -&gt; 沒有OOB sample)，用來避免overfitting，亦可加快運算(分析較少的資料)。</li>
</ul>
</li>
</ul>
<p>其中cross-validation會執行nrounds次，每次迭代中，nfold個子樣本都會輪流作為驗證資料集，來驗證nfold-1子集合所訓練出的模型。</p><pre class="crayon-plain-tag"># reproducibility
set.seed(123)

system.time(
  xgb.fit1 &lt;- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0               # silent,不要顯示詳細資訊
)
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
## 234.376   2.540 244.823</pre><p>檢視每次迭代的cross-validation的結果。分別會有訓練資料集的平均RMSE，和測試資料集的平均RMSE，希望兩者越接近越好。</p><pre class="crayon-plain-tag">print(xgb.fit1,verbose = TRUE)</pre><p></p><pre class="crayon-plain-tag">## ##### xgb.cv 5-folds
## call:
##   xgb.cv(data = features_train, nrounds = 1000, nfold = 5, label = response_train, 
##     verbose = 0, objective = &quot;reg:linear&quot;)
## params (as set within xgb.cv):
##   objective = &quot;reg:linear&quot;, silent = &quot;1&quot;
## callbacks:
##   cb.evaluation.log()
## niter: 1000
## evaluation_log:
##     iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std
##        1    1.420236e+05   850.15595494      142612.98      4504.553
##        2    1.023304e+05   645.18845214      104198.60      4628.932
##        3    7.443451e+04   535.78249514       77533.40      4635.993
##        4    5.484559e+04   468.37246662       59185.94      4420.339
##        5    4.118994e+04   386.94999310       47545.03      4452.632
## ---                                                                 
##      996    4.831320e-02     0.01131165       27373.21      3243.812
##      997    4.831320e-02     0.01131165       27373.21      3243.812
##      998    4.831320e-02     0.01131165       27373.21      3243.812
##      999    4.831320e-02     0.01131165       27373.21      3243.811
##     1000    4.831320e-02     0.01131165       27373.21      3243.812</pre><p>檢視回傳xbg.fit1物件的屬性(attributes)。</p><pre class="crayon-plain-tag">attributes(xgb.fit1)</pre><p></p><pre class="crayon-plain-tag">## $names
## [1] &quot;call&quot;           &quot;params&quot;         &quot;callbacks&quot;     
## [4] &quot;evaluation_log&quot; &quot;niter&quot;          &quot;nfeatures&quot;     
## [7] &quot;folds&quot;         
## 
## $class
## [1] &quot;xgb.cv.synchronous&quot;</pre><p>回傳的xgb.fit1物件包含很多重要資訊。特別像是我們可以擷取xgb.fit1$evaluation_log來觀察發生在訓練資料集和測試資料集的最小的RMSE和最適的樹數量(跟print(xgb.fit1)效果一樣)，以及cross-validation error。</p><pre class="crayon-plain-tag">xgb.fit1$evaluation_log</pre><p></p><pre class="crayon-plain-tag">##       iter train_rmse_mean train_rmse_std test_rmse_mean
##    1:    1    1.420236e+05   850.15595494      142612.98
##    2:    2    1.023304e+05   645.18845214      104198.60
##    3:    3    7.443451e+04   535.78249514       77533.40
##    4:    4    5.484559e+04   468.37246662       59185.94
##    5:    5    4.118994e+04   386.94999310       47545.03
##   ---                                                   
##  996:  996    4.831320e-02     0.01131165       27373.21
##  997:  997    4.831320e-02     0.01131165       27373.21
##  998:  998    4.831320e-02     0.01131165       27373.21
##  999:  999    4.831320e-02     0.01131165       27373.21
## 1000: 1000    4.831320e-02     0.01131165       27373.21
##       test_rmse_std
##    1:      4504.553
##    2:      4628.932
##    3:      4635.993
##    4:      4420.339
##    5:      4452.632
##   ---              
##  996:      3243.812
##  997:      3243.812
##  998:      3243.812
##  999:      3243.811
## 1000:      3243.812</pre><p>我們找出使得訓練和測試誤差最小的迭代數(模型所使用的樹個數)，以及所對應的RMSE。由下表所示，訓練誤差持續下降，並約在924棵樹時逼近為零(0.048)。然而，交叉驗證誤差約在60棵樹左右達到最小RMSE(約27K)。</p><pre class="crayon-plain-tag"># get number of trees that minimize error
xgb.fit1$evaluation_log %&gt;%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean)
  )</pre><p></p><pre class="crayon-plain-tag">##   ntrees.train rmse.train ntrees.test rmse.test
## 1          924  0.0483002          60  27337.79</pre><p></p><pre class="crayon-plain-tag">##   ntrees.train rmse.train ntrees.test rmse.test
## 1          965  0.5022836          60  27572.31</pre><p>將詳細xgboost模型迭代資訊繪出如下，紅色線代表訓練誤差，藍色線表示交叉驗證誤差。</p><pre class="crayon-plain-tag"># plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-36-1-1.png" alt="gradient boosting machines, GBM" /></p>
<p>一個xbm.cv滿不錯的功能就是early stopping。該功能讓我們可在cross validation error在連續第n棵樹不再下降的情況下，告訴函式該停止。以上面的例子來說，我們可以設定當cv error在連續10個樹模型(迭代)沒有下降時停止迭代(early_stopping_rounds = 10)。該功能有助於加速下一個tuning校正過程。</p><pre class="crayon-plain-tag"># reproducibility
set.seed(123)

xgb.fit2 &lt;- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)</pre><p>將迭代的過程細節繪出：</p><pre class="crayon-plain-tag"># plot error vs number trees
ggplot(xgb.fit2$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-38-1-1.png" alt="gradient boosting machines, GBM" /></p>
<h4>Tuning</h4>
<p>要tune XGBoost模型，我們會傳入一個parameters的list物件給params參數。幾個最常見的參數如下：</p>
<ul>
<li>eta : 控制學習步伐</li>
<li>max_depth: 樹的深度</li>
<li>min_child_weight: 末梢節點的最小觀測值個數</li>
<li>subsample: 每棵樹模型所抽樣訓練資料集的比例</li>
<li>colsample_bytrees: 每棵樹模型所抽樣的欄位數目</li>
</ul>
<p>舉例來說，如果想要指定特定參數值，我們可以將上面的模型設定重新編輯如下：</p><pre class="crayon-plain-tag"># create parameter list
  params &lt;- list(
    eta = .1,
    max_depth = 5,
    min_child_weight = 2,
    subsample = .8,
    colsample_bytree = .9
  )

# reproducibility
set.seed(123)

# train model
system.time(
xgb.fit3 &lt;- xgb.cv(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  39.185   0.124  39.446</pre><p></p><pre class="crayon-plain-tag"># assess results
xgb.fit3$evaluation_log %&gt;%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean)
  )</pre><p></p><pre class="crayon-plain-tag">##   ntrees.train rmse.train ntrees.test rmse.test
## 1          220   5054.546         210  24159.13</pre><p>想要執行更大型的search grid，我們可以使用和gbm相同的程序。先產生一個超參數hyperparameter search grid和儲存結果的欄位(最適樹模型個數與最小RMSE)。</p>
<p>以下我們創立一個4*4*4*3*3 = 576個參數排列組合的hyper grid。</p><pre class="crayon-plain-tag"># create hyperparameter grid
hyper_grid &lt;- expand.grid(
  eta = c(.01, .05, .1, .3),
  max_depth = c(1, 3, 5, 7),
  min_child_weight = c(1, 3, 5, 7),
  subsample = c(.65, .8, 1), 
  colsample_bytree = c(.8, .9, 1),
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 576</pre><p>接著，我們使用迴圈一一去執行XGBoost模型套用不同參數組合的結果，並將結果指標儲存。（*這段程序耗時，約6小時以上）</p><pre class="crayon-plain-tag"># grid search 
for(i in 1:nrow(hyper_grid)) {

  # create parameter list
  params &lt;- list(
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )

  # reproducibility
  set.seed(123)

  # train model
  xgb.tune &lt;- xgb.cv(
    params = params,
    data = features_train,
    label = response_train,
    nrounds = 5000,
    nfold = 5,
    objective = "reg:linear",  # for regression models
    verbose = 0,               # silent,
    early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
  )

  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] &lt;- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid$min_RMSE[i] &lt;- min(xgb.tune$evaluation_log$test_rmse_mean)
}

hyper_grid %&gt;%
  dplyr::arrange(min_RMSE) %&gt;%
  head(10)</pre><p></p><pre class="crayon-plain-tag">##     eta max_depth min_child_weight subsample colsample_bytree optimal_trees min_RMSE
## 1  0.01         5                3      0.65              0.8          1501 23255.95
## 2  0.05         5                5      0.80              0.9           486 23269.22
## 3  0.01         5                3      0.65              0.9          1325 23373.22
## 4  0.01         5                1      0.65              0.8          1257 23396.13
## 5  0.05         5                7      0.65              1.0           508 23437.00
## 6  0.01         3                3      0.65              1.0          2301 23457.89
## 7  0.01         5                3      0.80              0.8          1488 23499.83
## 8  0.01         7                3      0.65              0.8          1178 23519.53
## 9  0.01         3                5      0.65              0.9          2274 23526.33
## 10 0.05         5                7      0.80              0.8           401 23555.05</pre><p>經過評估後，可能還會繼續測是幾個不同的參數組合，去找到最能影響模型成效的參數。這邊有<a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">一篇很棒的文章</a>在討論tuning XGBoost的策略法。但為了簡短，在此我們即假設上述結果為globally最適的模型，並使用xgb.train()來配飾最終模型。(*xgboost)</p><pre class="crayon-plain-tag"># parameter list
params &lt;- list(
  eta = 0.01,
  max_depth = 5,
  min_child_weight = 5,
  subsample = 0.65,
  colsample_bytree = 1
)

# train final model
xgb.fit.final &lt;- xgboost(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1576,
  objective = "reg:linear",
  verbose = 0
)</pre><p></p>
<h4>Visualization</h4>
<h5>Variable importance</h5>
<p>xgboost提供內建的變數重要性繪圖功能。首先，可以使用xgb.importance()函數建立一個重要矩陣(importance matrix)的data.table，然後再將這個重要矩陣(importance matrix)投入xgb.plot.importance()函數進行繪圖。</p>
<p>在重要性矩陣中，Gain, Cover, Frequency欄位分別代表三種不同的變數重要性衡量的方法（此為tree model的衡量指標，如果是linear mode，衡量指標則會是Weight(模型中的線性係數)和Class）：</p>
<ol>
<li>Gain(貢獻度): 相應特徵對模型的相對貢獻度，計算特徵在模型中每棵樹的貢獻。其與gbm套件中的relative.influence是同意的。</li>
<li>Cover(觀測值個數涵蓋率): 與該特徵相關的相對觀察值個數(%)。比如說，你有100個觀察值，4個特徵變數3棵樹，並假設feature 1是用來區分葉節點並在樹tree1, tree2, tree3有各有10,5,2個觀察值；於是feature 1的cover會被計算為10+5+2=17個觀察值。feature2 ~ feature4亦會計算各自的cover，並以該特徵涵蓋變數個數相對於所有特徵總涵蓋變數個數計算百分比。</li>
<li>Frequency(出現在模型所有樹的相對次數):代表某特徵出現在模型中樹的相對次數百分比(%)。就上面的範例來說，假設feature 1分別在tree1, tree2,tree3的分割樹(splits)分別是2, 1, 3，那麼feature 1的權重將是2+1+3 = 6。feature 1的frequency則是計算該特徵的權重相對於其他features的權重加總。</li>
</ol>
<p></p><pre class="crayon-plain-tag"># create importance matrix
importance_matrix &lt;- xgb.importance(model = xgb.fit.final)

importance_matrix</pre><p></p><pre class="crayon-plain-tag">##                                                          Feature         Gain        Cover    Frequency
##   1:                                                 Garage_Cars 2.508517e-01 1.780750e-02 8.503913e-03
##   2:                                                 Gr_Liv_Area 1.629517e-01 7.008097e-02 6.596177e-02
##   3:                                   Bsmt_Qual_lev_x_Excellent 1.249057e-01 1.408468e-02 7.111680e-03
##   4:                                               Total_Bsmt_SF 6.487028e-02 5.422924e-02 5.535069e-02
##   5:                                                  Year_Built 5.279356e-02 2.547546e-02 2.697923e-02
##  ---                                                                                                   
## 238:                                   Garage_Type_lev_x_Basment 9.872921e-07 1.100922e-04 3.762793e-05
## 239:                                   Lot_Shape_lev_x_Irregular 8.863554e-07 9.469465e-05 3.762793e-05
## 240:                                       Functional_lev_x_Maj2 8.741157e-07 1.330922e-04 3.762793e-05
## 241: MS_SubClass_lev_x_Two_Family_conversion_All_Styles_and_Ages 7.552052e-07 3.820506e-05 3.762793e-05
## 242:    MS_SubClass_lev_x_One_and_Half_Story_Unfinished_All_Ages 1.986944e-07 1.443516e-06 3.762793e-05</pre><p>將剛剛得到的data.table放入xgb.plot.importance()，繪製指定的&#8221;Gain&#8221;變數重要性圖表。</p><pre class="crayon-plain-tag"># variable importance plot
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-45-1-1.png" alt="gradient boosting machines, GBM" /></p>
<p>改使用&#8217;Cover&#8217;當作變數重要衡量法的結果與上面差很多。</p><pre class="crayon-plain-tag"># variable importance plot using 'cover'
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Cover")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-46-1-1.png" alt="gradient boosting machines, GBM" /></p>
<h5>Partial dependence plots</h5>
<p>PDPs和ICE的運作與之前gbm套件是相似的。唯一差別在於你必須在partial()函數中加入訓練資料(train = features_train)(因為在此case中，partial無法自動擷取object所使用的training data)。我們以Garage_Cars為範例。</p><pre class="crayon-plain-tag">pdp &lt;- xgb.fit.final %&gt;%
  partial(pred.var = "Garage_Cars", n.trees = 1576, grid.resolution = 100, train = features_train) %&gt;%
  autoplot(rug = TRUE, train = features_train) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice &lt;- xgb.fit.final %&gt;%
  partial(pred.var = "Garage_Cars", n.trees = 1576, grid.resolution = 100, train = features_train, ice = TRUE) %&gt;%
  autoplot(rug = TRUE, train = features_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")</pre><p></p><pre class="crayon-plain-tag">gridExtra::grid.arrange(pdp, ice, nrow = 1)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-47-1-1.png" alt="gradient boosting machines, GBM" /></p>
<h5>LIME</h5>
<p>LIME內建提供給xgboost物件的功能(可以使用?model.type)。然而需要注意的是，要分析的局部觀察值需要採用與train, test相同的編碼處理程序(one-hot encoded)。並且當將資料投入lime::lime函式時，必須將其從matrix轉換成dataframe。</p><pre class="crayon-plain-tag"># one-hot encode the local observations to be assessed.
local_obs_onehot &lt;- vtreat::prepare(treatplan, local_obs, varRestriction = new_vars)

# apply LIME
explainer &lt;- lime(data.frame(features_train), xgb.fit.final)
explanation &lt;- explain(local_obs_onehot, explainer, n_features = 5)
plot_features(explanation)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-48-1-1.png" alt="gradient boosting machines, GBM" /></p>
<h4>Predicting</h4>
<p>最後，我們使用predict()函數來對新資料集進行預測。然而，不像gbm，我們並不需要提供樹模型的個數。<br />
由下的結果可知，我們測試資料集的RMSE與先前gbm模型的RMSE(22K)是較低的(只差了$600左右，差距很小)。</p><pre class="crayon-plain-tag"># predict values for test data
pred &lt;- predict(xgb.fit.final, features_test)

# results
caret::RMSE(pred, response_test)</pre><p></p><pre class="crayon-plain-tag">## [1] 21433.41</pre><p></p>
<h3>h2o</h3>
<p>R的h2O套件是一個強大高效能的java-based介面，允許基於local和cluster-based的佈署。該套件有相當完整的<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html" target="_blank" rel="noopener noreferrer">線上資源</a>，包括方法、code文件與教學。</p>
<p>h2o的幾個特色包括：</p>
<ul>
<li>在單一節點或多節點群集上進行分散式或平行式運算。</li>
<li>根據用戶指定的指標和使用者指定的相對容忍度收斂時，自動提前停止。</li>
<li>隨機GBM同時對欄位跟資料列進行抽樣(每次分割與每棵樹)以利得到廣義的解。</li>
<li>除了二項式binomial(Bernoulli)、高斯和多項式分佈函式外，亦支援指數型系列(Poisson, Gamma, Tweedie)和損失函數。</li>
<li>Grid Search超參數優化和模型挑選。</li>
<li>數據分散(data-distributed) &#8211; 代表整個資料集不必侷限在單一節點的記憶體，能夠擴展到任意大小的訓練資料集。</li>
<li>使用直方圖(histogram)來近似連續變數來加速。</li>
<li>使用動態分箱法(dynamic binning)，分箱的標準會依照每一顆樹模型切割時的最大最小值來動態調整。</li>
<li>使用平方誤差(squared error)來決定最適的切割點。</li>
<li>factor levels沒有限制。</li>
</ul>
<h4>基本的h2o實作</h4>
<p></p><pre class="crayon-plain-tag">h2o.no_progress()
h2o.init(max_mem_size = "5g")</pre><p></p><pre class="crayon-plain-tag">##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 days 10 hours 
##     H2O cluster timezone:       Asia/Taipei 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.22.1.1 
##     H2O cluster version age:    3 months and 12 days !!! 
##     H2O cluster name:           H2O_started_from_R_peihsuan_hxs725 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   2.54 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20)</pre><p>gbm.h2o函數可允許我們透過H2O套件來執行GBM。然而，在我們開始執行初始模型時，我們需要將訓練資料轉換成h2o物件。h2o.gbm預設會採用以下參數來建立GBM模型：</p>
<ul>
<li>number of tree (ntrees): 50</li>
<li>learning rate (learning_rate): 0.1</li>
<li>tree depth(max_depth): 5</li>
<li>末梢節點的最小觀測值個數 (min_rows): 10</li>
<li>對資料列和資料欄位沒有抽樣</li>
</ul>
<p></p><pre class="crayon-plain-tag"># create feature names
y &lt;- "Sale_Price"
x &lt;- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o &lt;- as.h2o(ames_train)

# training basic GBM model with defaults
h2o.fit1 &lt;- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 5
)

# assess model results
h2o.fit1</pre><p></p><pre class="crayon-plain-tag">## Model Details:
## ==============
## 
## H2ORegressionModel: gbm
## Model ID:  GBM_model_R_1554713367949_3 
## Model Summary: 
##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves
## 1              50                       50               17160         5         5    5.00000         10         31    22.60000
## 
## 
## H2ORegressionMetrics: gbm
## ** Reported on training data. **
## 
## MSE:  165078993
## RMSE:  12848.31
## MAE:  9243.007
## RMSLE:  0.08504509
## Mean Residual Deviance :  165078993
## 
## 
## 
## H2ORegressionMetrics: gbm
## ** Reported on cross-validation data. **
## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
## 
## MSE:  707783149
## RMSE:  26604.19
## MAE:  15570.2
## RMSLE:  0.1404869
## Mean Residual Deviance :  707783149
## 
## 
## Cross-Validation Metrics Summary: 
##                               mean           sd  cv_1_valid   cv_2_valid   cv_3_valid  cv_4_valid  cv_5_valid
## mae                      15547.765     721.0162   16325.277    16366.696    13728.291    15117.01   16201.549
## mean_residual_deviance 7.0412026E8 1.68594576E8 6.8916365E8 1.12267443E9 3.96582368E8 5.8667603E8 7.2550477E8
## mse                    7.0412026E8 1.68594576E8 6.8916365E8 1.12267443E9 3.96582368E8 5.8667603E8 7.2550477E8
## r2                       0.8918067   0.02474849   0.8868936    0.8282497   0.92912257  0.91696167  0.89780617
## residual_deviance      7.0412026E8 1.68594576E8 6.8916365E8 1.12267443E9 3.96582368E8 5.8667603E8 7.2550477E8
## rmse                     26165.846    3119.9976   26251.926    33506.336    19914.377   24221.396   26935.195
## rmsle                   0.13949537  0.010692091  0.13955544   0.16392557   0.14536715  0.11926634  0.12936236</pre><p>跟XGBoost類似，我們可以使用自動停止功能，這樣就可以提高樹模型的個數，直到模型改善幅度減少或停止在終止訓練程序。亦可設定當執行時間超過一定水準後停止程序(參考max_runtime_secs)。</p>
<p>舉例來說，我們使用5000棵樹訓練一個預設參數的模型，但是設定當連續十棵樹模型在交叉驗證誤差上沒有進步就停止的指令。而在此例可以看到，模型在約使用3743棵樹模型後停止訓練過程，對應的交叉驗證誤差RMSE為$24,684。</p><pre class="crayon-plain-tag"># training basic GBM model with defaults
h2o.fit2 &lt;- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 5,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
)

# model stopped after xx trees
h2o.fit2@parameters$ntrees</pre><p></p><pre class="crayon-plain-tag">## [1] 3712</pre><p></p><pre class="crayon-plain-tag"># cross validated RMSE
h2o.rmse(h2o.fit2, xval = TRUE)</pre><p></p><pre class="crayon-plain-tag">## [1] 24683.95</pre><p></p>
<h4>Tuning</h4>
<p>H2O套件提供需多可調整的參數。這部分值得你花時間閱讀相關的文件<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html#gbm-tuning-guide" target="_blank" rel="noopener noreferrer">H2O.ai</a>。在本筆記中，只會先專注在幾個較長使用的超參數組合。包括：</p>
<ul>
<li>樹的複雜度
<ul>
<li>ntrees: 使用樹模型個數</li>
<li>max_depth: 每棵樹的深度</li>
<li>min_rows: 末梢節點中所允許的最少觀測值個數</li>
</ul>
</li>
<li>學習步伐
<ul>
<li>learn_rate: 損失函數梯度下降的步伐</li>
<li>learn_rate_annealing: 允許使用高的學習步伐當作初始值，但隨著樹個數的增加而降低。</li>
</ul>
</li>
<li>加入隨機的特性
<ul>
<li>sample_rate: 建置每棵數所抽樣的訓練資料列數。</li>
<li>col_sample_rate: 建置每棵樹所抽樣的欄位數(跟xgboost套件中的colsample_bytree是一樣的)。</li>
</ul>
</li>
</ul>
<p>值得注意的是，還有能夠控制類別變數和連續變數如何編碼、分箱、切割的參數。預設的參數可以得到相當不錯的結果，但在特定情境中仍能透過調整這些小地方來微提高模型的效果。</p>
<p>執行h2o模型的grid search tuning有兩種選擇：full 或 random discrete grid search。</p>
<h5>Full grid search</h5>
<p>在full cartesian grid search法中，會完整依序執行grid中指定的所有超參數組合。這就是我們先前在gbm和xgboost手動撰寫for迴圈執行的參數校正過程。然而為了加快H2O訓練的過程，可以使用驗證資料集(validation set)來取代k-fold cross validation。</p>
<p>以下我們製造一個參數的grid，包含468(=3*3*3*2*3*3)種超參數排列組合。我們使用h2o.grid()來執行full grid search並同時設定停止參數來節省訓練的時間。</p><pre class="crayon-plain-tag"># create training &amp; validation sets
split &lt;- h2o.splitFrame(train.h2o, ratios = 0.75)
train &lt;- split[[1]]
valid &lt;- split[[2]]

# create hyperparameter grid
hyper_grid &lt;- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(.99, 1),
  sample_rate = c(.5, .75, 1),
  col_sample_rate = c(.8, .9, 1)
)

# perform grid search 
system.time(
grid &lt;- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid1",
  x = x, 
  y = y, 
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
  )
)</pre><p>full grid search所花的時間約為31分鐘。</p><pre class="crayon-plain-tag"># collect the results and sort by our model performance metric of choice
grid_perf &lt;- h2o.getGrid(
  grid_id = "gbm_grid1", 
  sort_by = "mse", 
  decreasing = FALSE
  )
grid_perf</pre><p></p><pre class="crayon-plain-tag">## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid1 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 486 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 1             1.0       0.05                  1.0         5      1.0        0.75 gbm_grid1_model_213   4.64582634567234E8
## 2             0.8       0.01                  1.0         5      1.0         0.5  gbm_grid1_model_46  4.748883474543088E8
## 3             0.8       0.01                  1.0         5      1.0        0.75 gbm_grid1_model_208 4.7571620498865277E8
## 4             0.9       0.01                  1.0         5      1.0        0.75 gbm_grid1_model_209  4.775898758772956E8
## 5             1.0       0.01                  1.0         5      1.0         0.5  gbm_grid1_model_48 4.7783819706988806E8
## 
## ---
##     col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 481             1.0       0.01                 0.99         1      5.0         1.0 gbm_grid1_model_381 2.9511012504833984E9
## 482             1.0       0.01                 0.99         1      1.0         1.0 gbm_grid1_model_327 2.9511012504833984E9
## 483             1.0       0.01                 0.99         1     10.0         1.0 gbm_grid1_model_435 2.9511012504833984E9
## 484             1.0       0.01                 0.99         1      1.0        0.75 gbm_grid1_model_165  2.951772702040925E9
## 485             1.0       0.01                 0.99         1      5.0        0.75 gbm_grid1_model_219  2.951772702040925E9
## 486             1.0       0.01                 0.99         1     10.0        0.75 gbm_grid1_model_273  2.951772702040925E9</pre><p>由以上資訊可知，模型切割數超過1次的深度、慢的學習步伐和隨機觀測值抽樣是表現的最不錯的組合類型。<br />
我們亦可查看更多有關最佳模型的詳細資訊。最佳模型可達到的驗證誤差RMSE為$21,554。</p><pre class="crayon-plain-tag"># Grab the model_id for the top model, chosen by validation error
best_model_id &lt;- grid_perf@model_ids[[1]]
best_model &lt;- h2o.getModel(best_model_id)

# Now let’s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)</pre><p></p><pre class="crayon-plain-tag">## H2ORegressionMetrics: gbm
## ** Reported on validation data. **
## 
## MSE:  464582635
## RMSE:  21554.18
## MAE:  14389.8
## RMSLE:  0.1268479
## Mean Residual Deviance :  464582635</pre><p></p>
<h5>Random discrete grid search</h5>
<p>當想要測試的超參數排列組合非常多時，每增加一個參數都對grid search所需完成的時間有巨大的影響。因此，h2o亦有提供Random discrete的grid search path法，採取隨機挑選超參數組合來執行，直到指定程度的改善幅度被達成或超過一定的執行時間或只執行過一定的模型數量時（或以上條件的組合）則停止。雖然說Random discrete path不一定會找到最適的模型，但在一定程度上可以找到相當不錯的模型。</p>
<p>以下便採用Random Discrete Path法來執行和剛剛一模一樣的hyperparameter grid。不過，在此我們會加入新的search條件：當連續有10個模型效果都無法超越目前最佳的模型獲得0.5%的MSE改善時，則停止。如果持續有在獲得改善，但超過360秒(60分鐘)時，也停止程序。</p><pre class="crayon-plain-tag"># random grid search criteria
search_criteria &lt;- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 60*60
  )

# perform grid search 
system.time(
grid &lt;- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid2",
  x = x, 
  y = y, 
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid,
  search_criteria = search_criteria, # add search criteria
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
  )
)</pre><p></p><pre class="crayon-plain-tag">##     user   system  elapsed 
##   38.949   11.738 3602.198</pre><p>在此例子中，Random Grid Search花了約60分鐘(=3600/60)，評估了154/486個模型(32%)。</p><pre class="crayon-plain-tag"># collect the results and sort by our model performance metric of choice
grid_perf &lt;- h2o.getGrid(
  grid_id = "gbm_grid2", 
  sort_by = "mse", 
  decreasing = FALSE
  )
grid_perf</pre><p></p><pre class="crayon-plain-tag">## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid2 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 154 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 1             0.9       0.01                  1.0         3      1.0        0.75  gbm_grid2_model_45  4.748951758832882E8
## 2             1.0       0.01                  1.0         3      1.0         0.5  gbm_grid2_model_80  4.803434026601322E8
## 3             1.0        0.1                  1.0         3      1.0        0.75  gbm_grid2_model_17  4.926287246284121E8
## 4             0.9       0.01                  1.0         3      1.0         0.5 gbm_grid2_model_115 5.0211786533732516E8
## 5             0.9       0.05                  1.0         5      1.0         0.5 gbm_grid2_model_107  5.069315439854374E8
## 
## ---
##     col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 149             1.0       0.01                 0.99         1     10.0        0.75 gbm_grid2_model_146 3.4271307547685847E9
## 150             1.0       0.01                 0.99         1      5.0        0.75  gbm_grid2_model_24 3.4271307547685847E9
## 151             0.8       0.01                 0.99         1     10.0        0.75  gbm_grid2_model_41  3.427939673958527E9
## 152             0.8       0.01                 0.99         1     10.0         1.0  gbm_grid2_model_27 3.4288680514316573E9
## 153             1.0       0.01                 0.99         1      5.0         1.0 gbm_grid2_model_131 3.4293746592550063E9
## 154             1.0       0.01                  1.0         3      1.0        0.75 gbm_grid2_model_154  5.038339642534549E9</pre><p>透過Random Grid Search所得到的最佳模型的交叉驗證RMSE為$21,792。雖然沒有full grid search找到的好($21,554)，但通常兩者所找到模型的效果已是差不多的。</p><pre class="crayon-plain-tag"># Grab the model_id for the top model, chosen by validation error
best_model_id &lt;- grid_perf@model_ids[[1]]
best_model &lt;- h2o.getModel(best_model_id)

# Now let’s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)</pre><p></p><pre class="crayon-plain-tag">## H2ORegressionMetrics: gbm
## ** Reported on validation data. **
## 
## MSE:  474895176
## RMSE:  21792.09
## MAE:  13738.25
## RMSLE:  0.1245287
## Mean Residual Deviance :  474895176</pre><p>一旦我們找到了最佳模型後，就可以用所有的訓練資料再重新訓練一個模型。我們使用從full grid search所得到的最佳模型的參數組合並使用5-fold cross validation來估計穩健的誤差。</p><pre class="crayon-plain-tag"># train final model
h2o.final &lt;- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 5,
  ntrees = 10000,
  learn_rate = 0.01,
  learn_rate_annealing = 1,
  max_depth = 3,
  min_rows = 10,
  sample_rate = 0.75,
  col_sample_rate = 1,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
)</pre><p></p><pre class="crayon-plain-tag"># model stopped after xx trees
h2o.final@parameters$ntrees</pre><p></p><pre class="crayon-plain-tag">## [1] 8660</pre><p></p><pre class="crayon-plain-tag"># cross validated RMSE
h2o.rmse(h2o.final, xval = TRUE)</pre><p></p><pre class="crayon-plain-tag">## [1] 23214.71</pre><p></p>
<h4>Visualization</h4>
<h5>Variable importance</h5>
<p>h2o套件有提供內建的變數重要性繪圖功能。該函式只有一個衡量變數重要性的方法-relative importance，一種衡量每一個變數在每一個模型中，平均能對loss function造成多少影響。能對損失函數帶來最大平均影響者的變數被當作最重要的變數，且其他變數的重要性也是相對於最重要變數所計算而得的數值。另外，vip套件亦可繪製h2o物件的變數重要性圖。</p><pre class="crayon-plain-tag">h2o.varimp_plot(h2o.final, num_of_features = 10)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-63-1.png" alt="gradient boosting machines, GBM" /></p>
<h5>Partial dependence plots</h5>
<p>我們亦可像之前一樣使用vip套件繪製PDP和ICE圖型來了解不同解釋變數邊際變動下對目標變數造成的影響。我們只需要透過一段專用函數，將投入的資料(newdata)轉換成h2o物件(as.h2o)，並將預測的結果在轉換為data frame型態，在當成pred.fun的參數投入。</p><pre class="crayon-plain-tag">pfun &lt;- function(object, newdata) {
  as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]]
}

pdp &lt;- h2o.final %&gt;%
  partial(
    pred.var = "Gr_Liv_Area", 
    pred.fun = pfun, # 研究一下
    grid.resolution = 20, 
    train = ames_train
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")</pre><p></p><pre class="crayon-plain-tag">ice &lt;- h2o.final %&gt;%
  partial(
    pred.var = "Gr_Liv_Area", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = ames_train,
    ice = TRUE
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")</pre><p></p><pre class="crayon-plain-tag">gridExtra::grid.arrange(pdp, ice, nrow = 1)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-64-1.png" alt="gradient boosting machines, GBM" /></p>
<p>h2o並沒有提供內建的ICE曲線繪圖功能，但是他可以繪製平均邊際效益(標準的PDP圖)外加衡量不確定性的一個標準誤差的PDP圖。</p><pre class="crayon-plain-tag">h2o.partialPlot(h2o.final, data = train.h2o, cols = "Overall_Qual")</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-65-1.png" alt="gradient boosting machines, GBM" /></p><pre class="crayon-plain-tag">## PartialDependence: Partial Dependence Plot of model GBM_model_R_1554713367949_5 on column 'Overall_Qual'
##      Overall_Qual mean_response stddev_response std_error_mean_response
## 1   Above_Average 173439.744454    59206.888258             1307.342580
## 2         Average 169032.396430    58675.968674             1295.619387
## 3   Below_Average 165991.718878    61198.281190             1351.314368
## 4       Excellent 227862.381764    66306.055141             1464.098718
## 5            Fair 161119.245285    62126.019654             1371.799687
## 6            Good 185468.115459    62195.668588             1373.337600
## 7            Poor 156505.758628    63260.388765             1396.847601
## 8  Very_Excellent 228227.228142    66731.640843             1473.496042
## 9       Very_Good 206801.916781    64639.301392             1427.295261
## 10      Very_Poor 151217.679576    64366.405585             1421.269471</pre><p>但不幸的事，h2o的函數會把類別變數的levels以字母排序的方式繪出，而pdp()函式則是以他們所指定的level順序繪出，使推理更加直觀。</p><pre class="crayon-plain-tag">pdp &lt;- h2o.final %&gt;%
  partial(
    pred.var = "Overall_Qual", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = as.data.frame(ames_train)
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice &lt;- h2o.final %&gt;%
  partial(
    pred.var = "Overall_Qual", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = as.data.frame(ames_train),
    ice = TRUE
    ) %&gt;%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-66-1.png" alt="gradient boosting machines, GBM" /></p>
<h5>LIME</h5>
<p>LIME套件亦有提供內建的函數來處理h2o物件。</p><pre class="crayon-plain-tag"># apply LIME
explainer &lt;- lime(ames_train, h2o.final)
explanation &lt;- explain(local_obs, explainer, n_features = 5)
plot_features(explanation)</pre><p><img src="/wp-content/uploads/2019/04/unnamed-chunk-67-1.png" alt="gradient boosting machines, GBM" /></p>
<h4>Predicting</h4>
<p>最後，我們可以使用h2o.predict()或predict()兩種方式來進行模型對新資料的預測，並使用h2o.performance()來衡量模型模型套用在測試資料集的成效。驗證結果的RMSE為$20,198，跟gbm和xgboost是類似的($21~22K)。</p><pre class="crayon-plain-tag"># convert test set to h2o object
test.h2o &lt;- as.h2o(ames_test)

# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)</pre><p></p><pre class="crayon-plain-tag">## H2ORegressionMetrics: gbm
## 
## MSE:  407897003
## RMSE:  20196.46
## MAE:  12679.15
## RMSLE:  0.1008391
## Mean Residual Deviance :  407897003</pre><p></p><pre class="crayon-plain-tag"># predict with h2o.predict
h2o.predict(h2o.final, newdata = test.h2o)</pre><p></p><pre class="crayon-plain-tag">##    predict
## 1 129814.6
## 2 161987.0
## 3 262990.7
## 4 484455.6
## 5 218094.0
## 6 209430.8
## 
## [879 rows x 1 column]</pre><p></p><pre class="crayon-plain-tag"># predict values with predict
predict(h2o.final, test.h2o)</pre><p></p><pre class="crayon-plain-tag">##    predict
## 1 129814.6
## 2 161987.0
## 3 262990.7
## 4 484455.6
## 5 218094.0
## 6 209430.8
## 
## [879 rows x 1 column]</pre><p></p>
<h3>小結</h3>
<ul>
<li>Gradient Boosting Machines (GBM)是一個強大的集成學習演算法，通常具有一流的預測能力。雖然相較於其他演算法它比較不直覺且需要大型運算，但絕對是機器學習工具箱的必備款！</li>
</ul>
<hr />
<p>參考連結：</p>
<ol>
<li><a href="http://uc-r.github.io/gbm_regression" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines (GBM)</a></li>
<li><a href="https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC2-4%E8%AC%9B-%E8%B3%87%E6%96%99%E5%89%8D%E8%99%95%E7%90%86-missing-data-one-hot-encoding-feature-scaling-3b70a7839b4a" target="_blank" rel="noopener noreferrer">類別資料的處理(有序、無序):one-hot encoding</a></li>
<li><a href="https://towardsdatascience.com/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b" target="_blank" rel="noopener noreferrer">Choosing the right Encoding method-Label vs OneHot Encoder</a></li>
</ol>
<hr />
<p>更多Decision Tree相關的統計學習筆記：</p>
<p><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></p>
<p><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a></p>
<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></p>
<p><a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a></p>
<p>更多Regression相關統計學習筆記：</p>
<p><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></p>
<p><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 – 資料探勘與處理 | 統計 R語言</a></p>
<p><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 – 模型建置、診斷與比較 | R語言</a></p>
<p><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 – Ridge, Lasso, Elastic Net | R語言</a></p>
<p>更多Clustering集群分析統計學習筆記：</p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></p>
<p><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a></p>
<p>其他統計學習筆記：</p>
<p><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></p>
<p>這篇文章 <a rel="nofollow" href="/gradient-boosting-machines-gbm/">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/gradient-boosting-machines-gbm/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Random Forests 隨機森林 &#124; randomForest, ranger, h2o &#124; R語言</title>
		<link>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/</link>
					<comments>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Tue, 19 Mar 2019 14:33:31 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[h2o]]></category>
		<category><![CDATA[random forests]]></category>
		<category><![CDATA[randomForest]]></category>
		<category><![CDATA[ranger]]></category>
		<category><![CDATA[Regression Tree]]></category>
		<category><![CDATA[隨機森林]]></category>
		<guid isPermaLink="false">/?p=2802</guid>

					<description><![CDATA[<p>Bagging法綜合多個樹模型結果，可以降低單一樹模型的高變異性並提升預測正確率。但Bagging法中樹與樹之間的相關性會降低模型整體的表現。隨機森林 Rand [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Bagging法</a>綜合多個樹模型結果，可以降低單一樹模型的高變異性並提升預測正確率。但Bagging法中樹與樹之間的相關性會降低模型整體的表現。隨機森林 Random forests 是Bagging修改後的版本，它是由「去相關性」的樹模型所組成的集成演算法，有很不錯的預測正確率且是一個受歡迎、開箱即用的演算法。</p>
<h3>載入所需套件</h3>
<p></p><pre class="crayon-plain-tag">library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # an extremely fast java-based platform
library(dplyr)
library(magrittr)</pre><p>準備資料。</p>
<ul>
<li>使用AmesHousing套件中的Ames Housing Data。</li>
<li>使用resample package中的initial_split()將資料切分成7:3(將參數設定為prop = 0.7)。</li>
<li>再分別用training()和testing()函數將切分好的資料萃取出。</li>
<li>並使用set.seed()來確保資料切分結果是可再現的。</li>
</ul>
<p></p><pre class="crayon-plain-tag"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split &lt;- initial_split(data = AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</pre><p></p>
<h3>Random Forests 概念介紹</h3>
<p>隨機森林模型的基礎概念和Decision Trees和Bagging一樣的(可以參考<a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">決策樹,Decision Trees</a>和<a href="/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/" target="_blank" rel="noopener noreferrer">Bagging</a>)。Bagging Trees模型在演算法中納入了隨機的元素，有效的降低了單一樹模型的高變異性與提升模型預測正確率。然而在bagging中的trees並非所有都是彼此相互獨立的，因為在每一棵樹切分節點時都是考慮所有原始的預測變數。也因爲上述關係，來自不同bootstrapped samples的樹彼此的結構都會有些類似（尤其是在樹的上半部，用來切割的前幾大變數都會非常類似）。</p>
<p>樹的結構相遇的這個特性就稱作tree correlation，它阻礙了Bagging最適地降低預測目標值的變異(variance)。為了更近一步降低變異，我們需要最小化樹與樹之間的相關性。這可以透過注入更多的隨機性到長樹的過程。 Random Forests是透過以下兩步驟來達成的：</p>
<ol>
<li>Bootstrap (拔靴法) : 跟Bagging很類似，每一顆樹都是建立自不同的bootstrapped sample，讓他們稍稍不一樣並稍稍去相關性。</li>
<li>Split-variable randomization (變數切割的隨機性) : 每一次在執行變數切割時，搜尋切割變數的範圍被限縮為隨機的子集合，即隨機挑選m個隸屬於總p個變數的子集合作為切割搜尋變數的範圍。對回歸樹來說，預設使用\(m=\frac{p}{3}\)，是個可經調教的參數。當\(m = p\)的時候，則跟只進行步驟1的結果一樣。</li>
</ol>
<p>因為每棵樹都是來自不同的隨機bootstrapped sample且每一次切割都是隨機挑選變數的子集合，因此樹與樹之間的關聯性會下降地較Bagging更低。</p>
<h4>OOB error vs. test set error</h4>
<p>和Bagging一樣，bootstrap resample法的一個天然好處，就是隨機森林模型可以透過out-of-bag(OOB)的樣本誤差來作為有效與合理近似實驗誤差(test error)。不需要額外產生或犧牲訓練資料集，OOB sample可供作為一個內建的驗證子集合。OOB sample 的存在讓尋找使模型錯誤率趨於穩定的最適樹模型數量(ntree)更有效率。但是OOB error和test error終究還是預期會不太相同。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2804" src="/wp-content/uploads/2019/03/Rplot.png" alt="random forests-隨機森林" width="800" height="500" srcset="/wp-content/uploads/2019/03/Rplot.png 800w, /wp-content/uploads/2019/03/Rplot-300x188.png 300w, /wp-content/uploads/2019/03/Rplot-768x480.png 768w, /wp-content/uploads/2019/03/Rplot-230x144.png 230w, /wp-content/uploads/2019/03/Rplot-350x219.png 350w, /wp-content/uploads/2019/03/Rplot-480x300.png 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>[上圖：Random forest out-of-bag error versus validation error.]</p>
<p>&nbsp;</p>
<p>此外，有很多package都沒有可以追蹤在某一棵樹模型中，哪些是OOB sample哪些不是的功能。這樣在比須比較多個模型的成效時，想要使用相同的驗證資料集來幫每個模型打分數是不可行的。而且，技術上雖然可以對OOB sample計算特定指標(metrics)如root mean squared logarithmic error (RMSLE)，但並非所有package都有內建這樣的運算功能。所以如果你想要比較多個模型的成效或是使用較不傳統的損失函數指標，你可能還是會選擇cross validation。</p>
<h4>Advantages &amp; Disadvantages of Random Forests</h4>
<h5>優點</h5>
<ol>
<li>隨機森林模型通常具有非常好的成效。</li>
<li>非常好的「開箱即用」的模型-不太需要調整什麼參數。</li>
<li>有內建的驗證資料集validation set &#8211; 不須為了額外驗證而犧牲資料。</li>
<li>不需前處理(pre-processing)。</li>
<li>對於離群值的處理是強大的。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>當運算大型資料時會變的非常慢。</li>
<li>雖然模型預測正確率高，但通常無法跟更先進的boosting演算法相比。</li>
<li>較不易解釋。</li>
</ol>
<h3>基本實作</h3>
<p>在R中有超過20種的Random Forests Packages。以下會使用歷史最悠久且最受歡迎randomForest套件來說明示範基本的Random Forests模型實作。但必須注意，當你的資料集變得很大的時候，randomForest回無法很好的擴展到大型資料（即使使用foreach進行平行運算）。此外，為了探索和比較不同模型參數的效果，我們也可找到更多有效的套件。因此，在模型tuning階段，我們會說明如何使用ranger和h2o package來進行更有效率的Random Forests modeling。</p>
<p>randomForest::randomForest可以使用formula或x,y matrix表示的方式來指定模型資料。我們以下以formula的方式來指定模型設定並使用randomForest模型預設參數。</p><pre class="crayon-plain-tag"># for reproduciblity
set.seed(123)

# default RF model
m1 &lt;- randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train
)

m1</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 661089658
##                     % Var explained: 89.8</pre><p>從m1結果來看：</p>
<ul>
<li>randomForest預設會使用500棵樹。</li>
<li>每一次切割(each split)會隨機篩選出\(\frac{Features}{3}=26\)個預測變數作為base。(原始data扣除目標變數Sale_Price的number of features = 80)</li>
<li>m1$mse(regression only)代表的是由OOB sample所計算出的「平均誤差平方(mean squared erre)」向量，即殘差平方和(sum of squared residuals)除上樹的個數(n)。</li>
<li>綜合500棵樹的mse(OOB error)為m1$mse[500] = 6.6108966 × 10<sup>8</sup>。</li>
</ul>
<p>我們可以進一步將不同棵樹所組成的隨機森林模型(ntree)對應的平均誤差(MSE)給繪出：</p><pre class="crayon-plain-tag">plot(m1)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-4-1-3.png" alt="random forests-隨機森林" /></p>
<p>從上圖可以發現，模型平均誤差大概在100棵樹時開始趨於穩定，且平均誤差值降低的速度開始於300棵樹左右時開始變緩。</p>
<p>我們可以進一步找出使得MSE最小的樹的個數。</p><pre class="crayon-plain-tag">which.min(m1$mse)</pre><p></p><pre class="crayon-plain-tag">## [1] 447</pre><p>最適隨機森林的RMSE則為(平均Sale_Price的誤差值)：</p><pre class="crayon-plain-tag">sqrt(m1$mse[which.min(m1$mse)])</pre><p></p><pre class="crayon-plain-tag">## [1] 25648.78</pre><p>如果我們不想要OOB error，randomForest函數亦提供驗證資料集(validation set)來幫助我們計算模型預測正確率。</p>
<p>要計算驗證誤差，首先我們進一步將訓練資料集依據8:2的比例切成訓練和驗證資料集，並分別使用analysis()和assessment()函數萃取出訓練和驗證資料。</p><pre class="crayon-plain-tag"># create training and validation data 
set.seed(123)
valid_split &lt;- initial_split(ames_train, .8)

# training data
ames_train_v2 &lt;- analysis(valid_split)

# validation datas
ames_valid &lt;- assessment(valid_split)

# 將驗證資料整理成x_test和y_test
x_test &lt;- ames_valid[setdiff(names(ames_valid),"Sale_Price")]
y_test &lt;- ames_valid$Sale_Price

# 在randomForest函數中使用x-test和y-test當作驗證資料集的參數
rf_oob_comp &lt;- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train_v2,
  xtest = x_test,
  ytest = y_test
)

rf_oob_comp</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train_v2,      xtest = x_test, ytest = y_test) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 667486651
##                     % Var explained: 89.17
##                        Test set MSE: 841004412
##                     % Var explained: 89.16</pre><p>可以看到模型結果多了Test set MSE，和原始的OOB MES不太一樣。我們將不同顆樹組成的模型所對應的OOB error和test error萃取出來並畫出誤差隨著樹的數量的變化圖，並比較兩者的差距。</p><pre class="crayon-plain-tag"># extract OOB &amp; validation errors
oob &lt;- sqrt(rf_oob_comp$mse)
validation &lt;- sqrt(rf_oob_comp$test$mse)

data.frame(
  ntrees = 1:rf_oob_comp$ntree,
  OOB.error = oob,
  Test.error = validation
) %&gt;% 
  gather(key = metric, value = RMSE, 2:3) %&gt;% 
  ggplot(aes(x = ntrees, y = RMSE, color = metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-8-1-3.png" alt="random forests-隨機森林" /></p>
<p>Random Forests是其中一個很好的「開箱即用」的演算法之一。基本上不需要調整什麼參數(tuning)，模型的預測能力就可ㄧ有很好的成效。</p>
<p>舉例來說，從上圖中，我們不需要調整參數就可以得到小於$30K的RMSE，比完整tuning後的<a href="/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/" target="_blank" rel="noopener noreferrer">Bagging模</a>RMSE降低超過$6K；比完整tuning的<a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">elastic net模型</a>RMSE降低超過$2K(沒有將目標變數取log的elastic net model版本請參考以下)。而我們還可以進一步透過參數調整tuning將Random Forests模型的預測正確性優化。</p><pre class="crayon-plain-tag"># elastic net
library(caret)
ames_train_x &lt;- model.matrix(object = Sale_Price ~ ., data =  ames_train)[, -1]
ames_train_y &lt;- ames_train$Sale_Price

# ames_test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1]
# ames_test_y &lt;- ames_test$Sale_Price

train_control &lt;- trainControl(method = "cv", number = 10)

caret_mod &lt;- train(
  x = ames_train_x, 
  y = ames_train_y,
  method = "glmnet", 
  prePro = c("center","scale","zv","nzv"),
  trControl = train_control,
  tuneLength = 10
)

min(caret_mod$results$RMSE)</pre><p></p><pre class="crayon-plain-tag">## [1] 32268.14</pre><p></p>
<h4>tuning</h4>
<p>Random Forests在tuning上非常簡單，因為只有幾個tuning parameters。通常tuning模型一開始最主要的考量點就是每一次分割所用來挑選的潛在變數名單，另外就是幾個需要注意的hyperparameters，包括如下：(這些hyperparameters在不同package的命名可能有所不同)</p>
<ul>
<li>ntree : number of trees。我們希望有足夠的樹來穩定模型的誤差，但過多的樹會是沒效率且沒必要的，特別是遇到大型資料集的時候。</li>
<li>mtry : 每次在決定切割變數時，所隨機抽樣的潛在變數清單數量。當mtry = p(即所有特徵變數數量)，Random Forests的結果就會和bagging一樣。而當mtry = 1，會造就每一次split所使用的變數completely random，每個變數都有機會但會造成非常偏差的結果。</li>
<li>sampsize : 訓練每棵樹模型的樣本數大小。預設是使用63.25%訓練資料集的比例，因為這個是獨立觀察值出現在bootstrapped sample的期望機率值。較低的樣本數大小雖然會降低訓練時間，但可能會產生不必要的偏差。增加樣本數大小可以提升模型正確率，但有可能會產生overfitting(因為會增加模型變異性)。所以一般來說，我們校正此樣本大小參數時會使用60-80%的比例。</li>
<li>nodesize : 末梢(葉)節點最小觀察資料個數。用來控制樹模型的複雜度。小的葉節點大小允許更深更複雜的樹模型，大的葉節點大小則會產生叫淺的樹模型。這又是一種「偏差v.s.變異度(bias -variance)」的權衡，當樹長得越深時，模型變異性愈高(有過度配適的風險)，而當樹長得越潛時則會有較多偏差(有沒辦法完整捕捉資料中的模式跟關係)。</li>
<li>maxnode : 內部節點最大個數值。另一種控制模型複雜度的變數，內部節點樹越多則會長出更深的樹，內部節點越少則產生越淺的樹。</li>
</ul>
<h4>Initial tuning with randomForest</h4>
<p>如果一開始只針對mtry進行校正，可以使用randomForest::tuneRF來進行簡易快速的評估。tunRF會從指定的mtry值開始，並每次增加給定的間距，直到模型OOB error降低的幅度開始小於特定幅度為止。</p>
<p>比如說，以下想知道mtry從5開始，每間隔相加1.5，所得到到的OOB error，直到OOB error改善的不度不超過1%為止。</p>
<p>因為tuneRF需要使用x,y形式指定資料，因此首先我們先使用setdiff()函數將變數名稱依據目標變數與預測變數分開。</p>
<p>模型跑完後會自動將每個mtry值所對應的OOB error值繪出。我們可以發現，當mtry &gt; 22後，OOB開始不再下降，最適mtry水準值約與features/3 = 80/3 = 26差不多。</p><pre class="crayon-plain-tag"># 篩選出預測變數名稱
features &lt;- setdiff(x = names(ames_train), y = "Sale_Price")

# 固定不同mtry參數值的模型所使用的隨機OOB sample一樣的
set.seed(123)

m2 &lt;- tuneRF(x = ames_train[features], y = ames_train$Sale_Price, mtryStart = 5, ntreeTry = 500 ,stepFactor = 1.5, improve = 0.01, trace = FALSE)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-10-1-4.png" alt="random forests-隨機森林" /></p><pre class="crayon-plain-tag">plot(m2)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-10-2-4.png" alt="random forests-隨機森林" /></p>
<h4>Full grid search with ranger</h4>
<p>如果想要套用綜合mtry以及其他參數的hyperparameter組合，我們需要建立一個grid並使用loop迴圈的方式，去測試每一個hyperparameter的組合和模型的成效。但因為randomForest()函數無法有效的將運算擴展至大型數據運算，因此我們會使用以c++執行的ranger()函數來解決。</p>
<p>我們可先使用Sys.time()稍稍比較tuneRF和ranger執行一種隨機森林模型所需的時間。</p><pre class="crayon-plain-tag">system.time(
  ames_randomForest &lt;- randomForest(
    formula = Sale_Price ~ ., 
    data = ames_train, 
    ntree = 500, 
    mtry = floor(length(features)/3)
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
## 111.957   0.735 169.989</pre><p></p><pre class="crayon-plain-tag">system.time(
  ames_ranger &lt;- ranger(
    formula   = Sale_Price ~ ., 
    data      = ames_train, 
    num.trees = 500,
    mtry      = floor(length(features) / 3)
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  10.367   0.174   5.665</pre><p>由以上結果我們可以看到，同樣是執行一次隨機森林，ranger()所需的時間僅約6秒，而randomForests則需要169秒。</p>
<p>為了進行grid search，我們首先先建立一個hyperparameters的grid，由許多不同的mtry, minimum node size,和 sample size所組成。總共會有96種組合。</p><pre class="crayon-plain-tag"># hyperparameter grid search
hyper_grid &lt;- expand.grid(
  mtry = seq(20, 30, by = 2),
  node_size = seq(3, 9, by = 2),
  sample_size = c(0.55, 0.632, 0.7, 0.8),
  OOB_RMSE = 0
)

# total number of combinations
nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 96</pre><p>我們使用loop迴圈，一一帶入不同hyperparameters到ranger()函數中，並固定每一次randomForests的的森林樹木數量為500(因為從前面經驗我們知道500棵樹即足夠使OOB error趨於穩定並收斂)。另外每一次randomForests執行時，我們也固定隨機亂數種子，讓同樣sample_size參數值所對應的抽樣樣本可以相同，凸顯其他參數變化所帶來的效果。</p><pre class="crayon-plain-tag">for (i in 1:nrow(hyper_grid)) {
  # train model
  model &lt;- ranger(
    formula = Sale_Price ~ .,
    data = ames_train, 
    num.trees = 500, 
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i], 
    sample.fraction = hyper_grid$sample_size[i],
    seed = 123
  )

  # 並將每一此訓練模型的OOB RMSE萃取儲存
  hyper_grid$OOB_RMSE[i] &lt;- sqrt(model$prediction.error)
}

# 我們將結果依序OOB_RMSE由小至大排列，取模型成效前十名印出
hyper_grid %&gt;% 
  dplyr::arrange(OOB_RMSE) %&gt;% 
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    mtry node_size sample_size OOB_RMSE
## 1    20         5         0.8 25918.20
## 2    20         3         0.8 25963.96
## 3    28         3         0.8 25997.78
## 4    22         5         0.8 26041.05
## 5    22         3         0.8 26050.63
## 6    20         7         0.8 26061.72
## 7    26         3         0.8 26069.40
## 8    28         5         0.8 26069.83
## 9    26         7         0.8 26075.71
## 10   20         9         0.8 26091.08</pre><p>從前十名模型成效結果我們可以發現：</p>
<ul>
<li>OOB_RMSE大致落在26K左右。</li>
<li>最適mtry的值落在所有20~30範圍區間。表示mtry在此區間對於OOB_RMSE沒有太大影響。</li>
<li>最適最小節點觀察值數量大約落在3~5。</li>
<li>最適抽樣比例約為0.8。</li>
<li>表示抽樣比例高(~80%)和深度較長(葉節點觀測個數大小3~5)的隨機森林成效較好(OOB RMSE)。</li>
</ul>
<h5>調整變數型態</h5>
<p>雖然我們已知random forests對於原始類別型變數處理效果是不錯的，我們還是進一步來試試將類別變數重新編碼為dummy variables是否能提升random forests的預測表現。</p>
<p>以下，我們使用dummyVars()函數將類別變數重新編碼為虛擬變數。</p><pre class="crayon-plain-tag">to_dum &lt;- dummyVars(formula = ~., data = ames_train, fullRank = FALSE)</pre><p>原始類別預測變數(80)被轉換完後變成353個欄位。</p><pre class="crayon-plain-tag">ames_to_dum &lt;- predict(to_dum, newdata = ames_train) %&gt;% as.data.frame()</pre><p>將資料名稱變成ranger相容的名稱。</p><pre class="crayon-plain-tag">names(ames_to_dum) &lt;- make.names(names = names(ames_to_dum), allow_ = FALSE)</pre><p>建立hyperparameter grid。並將mtry的區間調整為更大範圍。並執行grid search。</p><pre class="crayon-plain-tag">hyper_grid_2 &lt;- expand.grid(
  mtry = seq(50, 200, by = 25),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE  = 0
)

for(i in 1:nrow(hyper_grid_2)){
  model &lt;- ranger(
    formula = Sale.Price ~.,
    data = ames_to_dum, 
    num.trees = 500, 
    mtry = hyper_grid_2$mtry[i],
    min.node.size = hyper_grid_2$node_size[i], 
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed = 123
  )

  hyper_grid_2$OOB_RMSE[i] &lt;- sqrt(model$prediction.error)
}</pre><p></p><pre class="crayon-plain-tag">hyper_grid_2 %&gt;% 
  dplyr::arrange(OOB_RMSE) %&gt;% 
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    mtry node_size sampe_size OOB_RMSE
## 1    50         3        0.8 26981.17
## 2    75         3        0.8 27000.85
## 3    75         5        0.8 27040.55
## 4    75         7        0.8 27086.80
## 5    50         5        0.8 27113.23
## 6   125         3        0.8 27128.26
## 7   100         3        0.8 27131.08
## 8   125         5        0.8 27136.93
## 9   125         3        0.7 27155.03
## 10  200         3        0.8 27171.37</pre><p>由結果可分發線</p>
<ul>
<li>OOB RMSE 落在27K左右，並沒有比類別變數重新編碼前的26K來得好。</li>
<li>將類別變數重新編碼成dummy variables是無法提升模型成效的。</li>
</ul>
<p>所以到目前為至，最適的random forests模型參數分別為mtry = 20, node_size = 5, sample_size = 0.8。我們重複執行100次這個參數設定的模型，來計算此模型的error rate的期望值大小。</p><pre class="crayon-plain-tag">OOB_RMSE &lt;- vector(mode = "numeric", length = 100)

for(i in 1:length(OOB_RMSE)){
  optimal_ranger &lt;- ranger(
    formula         = Sale_Price ~ ., 
    data            = ames_train, 
    num.trees       = 500,
    mtry            = 20,
    min.node.size   = 5,
    sample.fraction = .8,
    importance      = 'impurity'
  )

  OOB_RMSE[i] &lt;- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-19-1-3.png" alt="random forests-隨機森林" /></p>
<p>執行100次random forests的結果後，我們可以觀察到OOB RMSE的期望值約落在26000~26200區間。</p>
<p>另外，我們在在模型參數importance = &#8216;impurity&#8217;。這表示我們是依據節點不純度(node impurity)的改善幅度來衡量每個變數的重要性。變數的重要性是計算每一次使用不同變數切割結點後，總能使MSE下降的程度(跨多棵樹模型)，而那些無法透過該變數切割所降低的模型錯誤率，則被稱作node impurity。而能夠降低越多MSE的變數則被重要性越高。</p>
<p>因此，在每一次節點分割時，我們都會計算每個變數所造成的MSE下降程度，而累積下降MSE幅度最高者，則被認為是較為重要的變數。</p>
<p>我們將變數重要性結果繪出：</p><pre class="crayon-plain-tag">options(scipen = -1)
optimal_ranger$variable.importance %&gt;% 
  as.matrix() %&gt;% 
  as.data.frame() %&gt;% 
  add_rownames() %&gt;% 
  `colnames&lt;-`(c("varname","imp")) %&gt;%
  arrange(desc(imp)) %&gt;% 
  top_n(25,wt = imp) %&gt;% 
  ggplot(mapping = aes(x = reorder(varname, imp), y = imp)) +
  geom_col() +
  coord_flip() +
  ggtitle(label = "Top 25 important variables") +
  theme(
    axis.title = element_blank()
  )</pre><p></p><pre class="crayon-plain-tag">## Warning: Deprecated, use tibble::rownames_to_column() instead.</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-20-1-3.png" alt="random forests-隨機森林" /></p>
<p>由上圖我們可以看到前三名重要變數依序為：Overall_Qual, Gr_Liv_Area, Garage_Cars。</p>
<h4>Full grid search with H2O</h4>
<p>我們已經知道剛剛在執行ranger進行hyperparameter grid計算時，還花滿長一段時間的。雖然ranger在計算上是有效率的，<br />
但是當遇到大型的grid時，我們手寫的loop迴圈會變得非常沒有效率。</p>
<p>而這時候，h2o套件則是一個強大有效率的java-based介面，可以提供平行分布式運算方法。此外，h20還可以提供不同的&#8221;search path&#8221;。有別於一一執行每一種hyperparameter grid組合，h2o可允許不同的最適搜尋路徑來執行，直到模型成效改善達一定程度等search path。使得在tuning模型上更具效率。以下便來介紹如何使用h2o套件執行random forests。</p><pre class="crayon-plain-tag"># start up h2o
h2o.no_progress() # turn off progress bars when creating reports/tutorials)
h2o.init(max_mem_size = "4g")</pre><p></p><pre class="crayon-plain-tag">##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 days 3 hours 
##     H2O cluster timezone:       Asia/Taipei 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.22.1.1 
##     H2O cluster version age:    2 months and 19 days  
##     H2O cluster name:           H2O_started_from_R_peihsuan_qcx617 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   0.04 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        FALSE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20)</pre><p>首先我們先來使用完整grid search path (又叫做full cartesian)，即表示會逐一檢視所有我們所指派的參數組合(hyper_grid.h2o)。<br />
根據hyper_grid.h2o的參數組合，共會有4*3*2 = 24種組合。也因為這邊是採用cartisian法，所以也不會比上面的方法快多少。</p><pre class="crayon-plain-tag"># create feature names
y &lt;- "Sale_Price"
x &lt;- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o &lt;- as.h2o(ames_train)</pre><p></p><pre class="crayon-plain-tag"># 指派參數組合 hyperparameter grid
hyper_grid.h2o &lt;- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 25, by = 2),
  sample_rate = c(.70, .80)
)

# build grid search 
# 以「cartesian」法逐一執行每一個參數組合的隨機森林模型

# 測試24種組合所需的時間
system.time(
grid &lt;- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid",
  x = x, 
  y = y, 
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = list(strategy = "Cartesian")
  )
)</pre><p></p><pre class="crayon-plain-tag">##   user  system elapsed 
##  9.628   3.104 874.954</pre><p></p><pre class="crayon-plain-tag"># 蒐集結果並依照每一種參數組合模型的MSE誤差來排名
# collect the results and sort by our model performance metric of choice
grid_perf &lt;- h2o.getGrid(
  grid_id = "rf_grid", 
  sort_by = "mse", 
  decreasing = FALSE
  )</pre><p></p><pre class="crayon-plain-tag">print(grid_perf)</pre><p></p><pre class="crayon-plain-tag">## H2O Grid Details
## ================
## 
## Grid ID: rf_grid 
## Used hyper parameters: 
##   -  mtries 
##   -  ntrees 
##   -  sample_rate 
## Number of models: 91 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   mtries ntrees sample_rate        model_ids                 mse
## 1     20    400         0.8 rf_grid_model_85 6.073124636667156E8
## 2     26    300         0.8 rf_grid_model_82 6.119220802095695E8
## 3     20    300         0.8 rf_grid_model_79 6.148210280053709E8
## 4     26    500         0.7 rf_grid_model_70 6.154919117655025E8
## 5     26    400         0.8 rf_grid_model_88 6.155747553830479E8
## 
## ---
##    mtries ntrees sample_rate        model_ids                 mse
## 86     24    200        0.55  rf_grid_model_3 6.629919566473577E8
## 87     28    200         0.7 rf_grid_model_53 6.643770708728626E8
## 88     28    300        0.55 rf_grid_model_11 6.644256979260525E8
## 89     30    200        0.55  rf_grid_model_6 6.658494098992392E8
## 90     22    500        0.55 rf_grid_model_20 6.755417850891622E8
## 91     28    200        0.55  rf_grid_model_5 6.786043491214715E8</pre><p>然而我們從上述結果可以注意到，最好的模型的OOB RMSE只有2.464371 × 10<sup>4</sup> (&lt;25K)，比我們先前所校正的模型效果都還更好。<br />
這是由於h2o套件在參數包括「最小節點大小(minimum node size)」、「樹的深度(tree depth)」等都更「慷慨」，比如說h2o預設最小節點大小為1，而ranger和randomForest該參數則都預設為5。</p>
<h4>Random Discrete grid search with H2O</h4>
<p>當遇到參數變很多的情況下，額外增加一個參數將會巨幅拉大grid search執行時間。為了因應這樣的不變，h2o提供了一種叫做「RandomDiscrete」的grid search搜尋路徑，有別於「Cartisian」逐一執行所有組合，「RandomDiscrete」會隨機挑選參數組合，直到達到一定程度的改善幅度、或是超過一定的時間、或是已執行一定數目的模型數（或是以上三種情況的交叉組合）。雖然使用「RandomDiscrete」搜尋可能會錯過最佳的參數組合效果，但基本上他已經能夠調教出不錯的模型了。</p>
<p>比如說，以下範例的參數組同樣為24種，我們設計一個「RandomDiscrete」grid search，停止條件為達成以下任一條件：近10組模型的MSE相較於最佳模性的改善幅度未超過0.5%、執行時間超過600秒(30 min)。</p><pre class="crayon-plain-tag"># hyperparameter grid
hyper_grid.h2o &lt;- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 25, by = 2),
  sample_rate = c(.70, .80)
)

# random grid search criteria
search_criteria &lt;- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 30*60
  )


# build grid search 
system.time(
random_grid &lt;- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = x,
  y = y,
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )
)</pre><p></p><pre class="crayon-plain-tag">##  user  system elapsed 
##  6.721   1.877 713.918</pre><p></p><pre class="crayon-plain-tag"># collect the results and sort by our model performance metric of choice
grid_perf2 &lt;- h2o.getGrid(
  grid_id = "rf_grid2",
  sort_by = "mse",
  decreasing = FALSE
  )</pre><p></p><pre class="crayon-plain-tag">print(grid_perf2)</pre><p></p><pre class="crayon-plain-tag">H2O Grid Details
================

Grid ID: rf_grid2 
Used hyper parameters: 
  -  mtries 
  -  ntrees 
  -  sample_rate 
Number of models: 24 
Number of failed models: 0 

Hyper-Parameter Search Summary: ordered by increasing mse
  mtries ntrees sample_rate         model_ids                 mse
1     20    500         0.8 rf_grid2_model_19 5.996100879211967E8
2     22    500         0.8 rf_grid2_model_24  6.09073686599265E8
3     22    500         0.7  rf_grid2_model_4 6.096855932933546E8
4     24    400         0.8  rf_grid2_model_8 6.132880206896532E8
5     22    400         0.8 rf_grid2_model_13 6.151492899918578E8

---
   mtries ntrees sample_rate         model_ids                 mse
19     20    500         0.7 rf_grid2_model_12 6.347692770119294E8
20     22    300         0.7 rf_grid2_model_21 6.361713674445038E8
21     24    400         0.7  rf_grid2_model_6 6.431105576136292E8
22     20    400         0.7  rf_grid2_model_9   6.4353236575248E8
23     22    200         0.7  rf_grid2_model_5 6.477837564818393E8
24     24    200         0.8  rf_grid2_model_1  6.50234231493715E8</pre><p>檢視「RandomDiscrete」grid search結果我們發現，經「隨機」檢視24組參數組合後，最佳的模型MSE只有2.448694 × 10<sup>4</sup>  (v.s. Cartisian grid search找到的2.464371 × 10<sup>4</sup>已非常接近)。且random discrete法只花了約11分鐘(=713/60)的時間(v.s. complete search的15分鐘(=874 sec / 60)。</p>
<p>一旦找到了最佳模型，我們就可以將模型套用在hold-out test set測試資料上來計算最後的「驗證誤差 test error」。結果顯示驗證RMSE誤差為23K，比elastic nets(32K)和bagging(36K)法低了$10K左右。</p><pre class="crayon-plain-tag"># 根據Cartesian法中，選出MSE最低的model，
# Grab the model_id for the top model, chosen by validation error
best_model_id &lt;- grid_perf2@model_ids[[1]]
best_model &lt;- h2o.getModel(best_model_id)</pre><p></p><pre class="crayon-plain-tag"># Now let’s evaluate the model performance on a test set
ames_test.h2o &lt;- as.h2o(ames_test)</pre><p></p><pre class="crayon-plain-tag">best_model_perf &lt;- h2o.performance(model = best_model, newdata = ames_test.h2o)</pre><p></p><pre class="crayon-plain-tag"># RMSE of best model
h2o.mse(best_model_perf) %&gt;% sqrt()</pre><p></p><pre class="crayon-plain-tag">## [1] 23303.05</pre><p></p>
<h3>預測</h3>
<p>一但我們挑出了我們偏好的模型，就像之前一樣，可以使用predict()函數將模型套用在新的資料集上做預測。我們可以來比較所有模型類別的預測效果(randomForest, ranger, h2o)(隨然呈現的結果有稍稍的不一樣)。另外要注意的就是h2o模型使用資料的格式不太依樣。</p>
<p>randomForest</p><pre class="crayon-plain-tag"># randomForest
pred_randomForest &lt;- predict(ames_randomForest, ames_test)
head(pred_randomForest)</pre><p></p><pre class="crayon-plain-tag">##        1        2        3        4        5        6 
## 128454.9 155459.6 264329.4 382519.7 211966.4 214173.0</pre><p>ranger</p><pre class="crayon-plain-tag"># ranger
pred_ranger &lt;- predict(ames_ranger, ames_test)
head(pred_ranger$predictions)</pre><p></p><pre class="crayon-plain-tag">## [1] 128893.3 154095.1 270183.4 389106.2 222629.6 210352.8</pre><p>h2o</p><pre class="crayon-plain-tag"># h2o
pred_h2o &lt;- predict(best_model, ames_test.h2o)</pre><p></p><pre class="crayon-plain-tag">head(pred_h2o)</pre><p></p><pre class="crayon-plain-tag">##    predict
## 1 119430.6
## 2 152900.0
## 3 278208.3
## 4 283966.7
## 5 225166.7
## 6 200583.3</pre><p></p>
<h3>小結</h3>
<ol>
<li>random forest是一個非常強大的「開箱即用」的演算法。不用太多的參數調教就會有相當不錯的預測能力。</li>
<li>此外，random forest也是模型中對資料前處理要求最少的模型，不太需要做資料轉換，是許多解決預測問題時最快能應用的方法。</li>
<li>以bagging為基礎，並透過(1)每棵樹進行bootstrapped sample 與 (2)節點分割時隨機挑選變數清單來「去除樹模型間的相關性」，能有效提升模型的高變異性與提升預測精準度。</li>
</ol>
<hr />
<p>參考文章連結：</p>
<p><a href="http://uc-r.github.io/random_forests" target="_blank" rel="noopener noreferrer">隨機森林 random forest</a></p>
<p>更多Decision Tree相關的統計學習筆記：</p>
<p><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></p>
<p><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a></p>
<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></p>
<p><a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a></p>
<p>更多Regression相關統計學習筆記：</p>
<p><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></p>
<p><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 – 資料探勘與處理 | 統計 R語言</a></p>
<p><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 – 模型建置、診斷與比較 | R語言</a></p>
<p><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 – Ridge, Lasso, Elastic Net | R語言</a></p>
<p>更多Clustering集群分析統計學習筆記：</p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></p>
<p><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a></p>
<p>其他統計學習筆記：</p>
<p><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></p>
<p>這篇文章 <a rel="nofollow" href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>Regularized Regression &#124; 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net &#124; R語言</title>
		<link>/regularized-regression-ridge-lasso-elastic/</link>
					<comments>/regularized-regression-ridge-lasso-elastic/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Fri, 04 Jan 2019 05:38:32 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[elastic net]]></category>
		<category><![CDATA[general linear regression]]></category>
		<category><![CDATA[lasso]]></category>
		<category><![CDATA[multicollinearity]]></category>
		<category><![CDATA[overfitting]]></category>
		<category><![CDATA[regression]]></category>
		<category><![CDATA[regularization]]></category>
		<category><![CDATA[regularized regression]]></category>
		<category><![CDATA[ridge]]></category>
		<guid isPermaLink="false">/?p=2359</guid>

					<description><![CDATA[<p>在線性回歸模型中，為了最佳化目標函式(最小化誤差平方和)，資料需符合許多假設，才能得到不偏回歸係數，使得模型變異量最低。可現實中數據非常可能有多個特徵變數，使得 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/regularized-regression-ridge-lasso-elastic/">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>在線性回歸模型中，為了最佳化目標函式(最小化誤差平方和)，資料需符合許多假設，才能得到不偏回歸係數，使得模型變異量最低。可現實中數據非常可能有多個特徵變數，使得模型假設不成立而產生過度配適問題，這時則需透過正規化法(regularized regression)來控制回歸係數，藉此降低模型變異以及樣本外誤差。</p>
<h3>Regularized Regression</h3>
<h3>載入實作所需的套件</h3>
<p></p><pre class="crayon-plain-tag">library(rsample)  # data splitting 
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting</pre><p>資料準備：Data 使用 AmesHousing package中的 Ames Housing data</p><pre class="crayon-plain-tag"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split &lt;- initial_split(data = AmesHousing::make_ames(),prop = 0.7, strata = "Sale_Price")
ames_train &lt;- training(ames_split)
ames_test &lt;- testing(ames_split)</pre><p></p>
<h3>為何需要資料正規化(Regularization)?</h3>
<p>我們知道，OLS(Ordinary Least Squares)最小平方和線性回歸的最佳化目標函式就是尋找一個平面，使得預測與實際值的誤差平方和(Sum of Squared Error, SSE)最小化（如下圖，紫色點代表實際觀測值，粉紅色為預測平面，黑色實線則表示實際與預測值得殘差）。</p>
<p><img src="/wp-content/uploads/2019/01/unnamed-chunk-170-1.png" alt="Regularized Regression" /></p>
<p>線性回歸的目標函式：</p>
<p>\[minimize\bigg \{ SSE = \sum_{i=1}^n (y_{i} &#8211; \hat{y}_{i})^2 \bigg \}\]</p>
<p>而要最佳化目標函式，資料必須符合以下幾個基本假設：</p>
<ul>
<li>線性關係</li>
<li>變數成常態分配</li>
<li>變數間無自相關</li>
<li>殘差變異同質性</li>
<li>觀測值個數(n)需大於特徵個數(p)(n&gt;p)</li>
<li>模型不能有共線性問題(否則估計回歸係數會有問題)</li>
</ul>
<p>但現實中，數據往往存在許多特徵變數(p很大)，隨著特徵變數量增加，許多基本假設不再成立，以至於我們必須用替代方法來解決線性預測問題。具體來說，當特徵變數量增加(p增加)，我們常會遇到的三個主要問題包括：</p>
<h4>1. Multicollinearity 多元共線性</h4>
<p>當特徵變數個數增加(p增加)，我們就有越高的機會捕捉到存在共線性的變數。而當模型存在共線性時，回歸係數項就會變得非常不穩定(high variance, 高變異)。<br />
舉例來說，我們先從多達81個特徵變數中:</p>
<ol>
<li>找出相關係數絕對值高於0.6的變數組合。</li>
<li>找出哪些變數與自變數(Sale_Price)具有高度相關性(相關係數絕對值大於 0.6)。</li>
</ol>
<p>首先我們先計算出每對變數間的相關係數和P-value的data frame</p><pre class="crayon-plain-tag">library(Hmisc) # rcorr()函數
data &lt;- as.data.frame(AmesHousing::make_ames())
# 使用rcorr()產生Matrix of correlations and P-values
res2 &lt;- rcorr(as.matrix(data[,sapply(data, is.numeric)]))

# 將相關係數與p-value矩陣轉換成data frame的函數
flattenCorrMatrix &lt;- function(cormat, pmat) {
  ut &lt;- upper.tri(cormat) # Lower and Upper Triangular Part of a Matrix
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

cor_table &lt;- flattenCorrMatrix(res2$r, res2$P)
head(cor_table)</pre><p></p><pre class="crayon-plain-tag">##            row         column        cor                     p
## 1 Lot_Frontage       Lot_Area 0.13686214 0.0000000000001005862
## 2 Lot_Frontage     Year_Built 0.02613050 0.1573419061953282849
## 3     Lot_Area     Year_Built 0.02325850 0.2081737048985332628
## 4 Lot_Frontage Year_Remod_Add 0.06950923 0.0001662509781792387
## 5     Lot_Area Year_Remod_Add 0.02168222 0.2406818444157394765
## 6   Year_Built Year_Remod_Add 0.61209525 0.0000000000000000000</pre><p>列出相關係數絕對值高於0.6的變數組合</p><pre class="crayon-plain-tag">cor_table %&gt;% filter(abs(cor) &gt; 0.6) %&gt;% arrange(desc(cor))</pre><p></p><pre class="crayon-plain-tag">##              row         column       cor p
## 1    Garage_Cars    Garage_Area 0.8898660 0
## 2    Gr_Liv_Area  TotRms_AbvGrd 0.8077721 0
## 3  Total_Bsmt_SF   First_Flr_SF 0.8004287 0
## 4    Gr_Liv_Area     Sale_Price 0.7067799 0
## 5  Bedroom_AbvGr  TotRms_AbvGrd 0.6726472 0
## 6  Second_Flr_SF    Gr_Liv_Area 0.6552512 0
## 7    Garage_Cars     Sale_Price 0.6475616 0
## 8    Garage_Area     Sale_Price 0.6401383 0
## 9  Total_Bsmt_SF     Sale_Price 0.6325288 0
## 10   Gr_Liv_Area      Full_Bath 0.6303208 0
## 11  First_Flr_SF     Sale_Price 0.6216761 0
## 12    Year_Built Year_Remod_Add 0.6120953 0
## 13 Second_Flr_SF      Half_Bath 0.6116337 0</pre><p>列出與目標變數Sale_Price有高度相關(相關係數絕對值高於0.5)的變數組合</p><pre class="crayon-plain-tag">cor_table %&gt;% filter(row == "Sale_Price" | column == "Sale_Price") %&gt;% filter(abs(round(cor,digits = 2)) &gt;= 0.5) %&gt;% arrange(desc(cor))</pre><p></p><pre class="crayon-plain-tag">##               row     column       cor p
## 1     Gr_Liv_Area Sale_Price 0.7067799 0
## 2     Garage_Cars Sale_Price 0.6475616 0
## 3     Garage_Area Sale_Price 0.6401383 0
## 4   Total_Bsmt_SF Sale_Price 0.6325288 0
## 5    First_Flr_SF Sale_Price 0.6216761 0
## 6      Year_Built Sale_Price 0.5584261 0
## 7       Full_Bath Sale_Price 0.5456039 0
## 8  Year_Remod_Add Sale_Price 0.5329738 0
## 9    Mas_Vnr_Area Sale_Price 0.5021960 0
## 10  TotRms_AbvGrd Sale_Price 0.4954744 0</pre><p>我們取具有高相關性的兩變數Gr_Liv_Area、TotRms_AbvGrd來說明。兩變數相關係數高達0.81。其中，Gr_Liv_Area和TotRms_AbvGrd皆分別與目標變數具有高度相關(相關係數分別為：cor = 0.71 and cor = 0.50)。</p>
<p>我們將兩變數投入線性模型進行配適，並比較各自投入線性回歸的係數。</p>
<p>模型1: 投入兩高相關性變數</p><pre class="crayon-plain-tag">lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)    Gr_Liv_Area  TotRms_AbvGrd  
##       49953.6          137.3       -11788.2</pre><p>模型2: 單獨使用Gr_Liv_Area進行回歸的結果。</p><pre class="crayon-plain-tag">lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)
## 
## Coefficients:
## (Intercept)  Gr_Liv_Area  
##       17797          108</pre><p>模型3: 單獨使用TotRms_AbvGrd進行回歸的結果。</p><pre class="crayon-plain-tag">lm(Sale_Price ~ TotRms_AbvGrd, data = ames_train)</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
## lm(formula = Sale_Price ~ TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)  TotRms_AbvGrd  
##         26820          23731</pre><p>可以發現：</p>
<ul>
<li>兩高相關變數同時進行回歸配適時，會得到Gr_Liv_Area與目標變數為正相關而TotRms_AbvGrd與目標變數為負相關的係數結果。</li>
<li>單獨投入回歸配適時，Gr_Liv_Area和TotRms_AbvGrd都變成正向係數。</li>
</ul>
<p>以上係數正負號有問題的結果，就是模型存在共線性問題時常遇到的結果。具高度相關的變數係數會過度膨脹(over-inflated)且變得很不穩定(fluctuate significantly)。係數項大幅波動的結果，就是過度配適(overfitting)，也就是說，在權衡“bias-variance”(即誤差v.s模型變異度)階段中具模型有很高的變異度。雖然我們可以在建模後(事後)，使用VIF(variance inflation factors)變異數膨脹因子來檢查哪些變數有共線性並移除，但仍不太清楚要移除哪個變數好，或者是擔心移除變數會讓模型失去有價值的訊號。</p>
<h4>2. Insufficient Solution 解決方案不充分</h4>
<p>當特徵個數(p)超過觀測個數(n)(p&gt;n)時，OLS(最小平方法)回歸解矩陣是不可逆的(solution matrix is not invertible)。這代表(1)最小平方估計參數解不是唯一。會存在無限的可用的解，但這些解大多都過度配適資料。(2)在大多數的情況下，這些結果在計算上是不可行的(computationally infeasible)。</p>
<p>因此，只能透過移除特徵變數直到(n&gt;p)再將資料投入最小平方回歸模型進行配適。雖然可以透過人工的方式事前處理特徵變數過多的問題，但可能很麻煩且容易出錯。</p>
<h4>3. Interpretability 可解釋性</h4>
<p>當我們的特徵變數個數量很大時，我們會希望識別出具有最強解釋效果的較小子集合(Subsetting)。通常我們會偏好透過變數選取(feature selection)的方法來解決。其中一個變數選取法叫做「hard thresholding feature selection(硬閾值特徵選取)」，可以透過線性模型選取(linear model selection)來進行（best subsets &amp; stepwise regression)，但這個方法通常計算上效率較低也不好擴展，而且是直接透過增加或減少特徵變數的方式來進行模型比較。另一個方法叫做「soft thresholding feature selection(軟閾值特徵選取)」，此法將慢慢的將特徵效果推向0。</p>
<h3>Regularized Regression 正規化回歸</h3>
<p>當遇到以上問題，一個替代OLS回歸的方法就是透過「正規化回歸 regularized regression」(又稱作penalized models或shrinkage method)來對回歸係數做管控。正規化回歸模型會對回歸係數大小做出約束，並逐漸的將回歸係數壓縮到零。而對回歸係數的限制將有助於降低係數的幅度和波動，並降低模型的變異。</p>
<p>正規化回歸的目標函式與OLS回歸類似，但多了一個懲罰參數(penalty parameter, P)：</p>
<p>\[minimize\bigg \{ SSE + P \bigg \}\]</p>
<p>而常見的懲罰係數有兩種(分別對應到ridge回歸模型 &amp; lasso回歸模型)，效果是類似的。懲罰係數將會限制回歸係數的大小，除非該變數可以使誤差平方和(SSE)降低對應水準，該特徵係數才會上升。以下就來進一步介紹兩種最常見的正規化回歸法。</p>
<h3>Ridge Regression</h3>
<p>Ridge Regression透過將懲罰參數\(\lambda \sum_{j=1}^p \beta_{j}^2\)加入目標函式中。也因為該參數為對係數做出二階懲罰，故又稱為L2 Penalty懲罰參數。</p>
<p>\[minimize\bigg \{ SSE +\lambda \sum_{j=1}^p \beta_{j}^2 \bigg \}\]</p>
<p>而L2懲罰參數的值可以透過「tuninig parameter, \(\lambda\)」來控制。當\(\lambda \rightarrow 0\)，L2懲罰參數就跟OLS回歸一樣，目標函式只有最小化SSE；而當\(\lambda \rightarrow \infty\)時，懲罰效果最大，迫使所有係數都趨近於0。(如下圖範例所示，係數隨著\(\lambda\)由0變化到821(log(821) = 6.7)，係數逐漸被ridge法正規化的過程。)</p>
<p><img src="/wp-content/uploads/2019/01/unnamed-chunk-177-1.png" alt="Regularized Regression, Ridge" /></p>
<p>由上圖我們可以觀察到：</p>
<ul>
<li>部分特徵係數會波動，直到\(\log(\lambda) \simeq 0\)才逐漸穩定開始收斂至0。這表示存在多重共線性，唯有透過\(\log(\lambda) &gt; 0\)校正參數來限制係數，以降低模型變異和誤差。</li>
</ul>
<p>但如何決定最適的shrinkage的程度(校正參數\(\lambda\))來最小化模型誤差呢？我們會透過以下的R程式語言實作來說明。</p>
<h4>實作ridge regression using R</h4>
<p>這邊主要會使用glmnet套件來執行。因為glmnet函數沒使用formula參數，故我們需要將資料分成x和y來帶入glmnet函數參數值，且x參數須為matrix型態。而我們將會使用model.matrix()函數來將質性變數轉換成dummy variables虛擬變量（如果資料維度很大的時候，則可參考更有效率的處理函數 Matrix::sparse.model.matrix）(並將第一行的截距項忽略)。另外，我們也將目標變數進行log轉換(因為目標變數分佈有偏)。</p>
<p>step 1: 將資料分成預測變數矩陣與目標變數，並替目標變數進行log轉換</p><pre class="crayon-plain-tag">ames_train_x &lt;- model.matrix(object = Sale_Price ~ ., data =  ames_train)[, -1]
ames_train_y &lt;- log(ames_train$Sale_Price)

ames_test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1]
ames_test_y &lt;- log(ames_test$Sale_Price)</pre><p>查看質化變數經轉換成虛擬變量後，我們特徵變數維度從原始81個維度，增加為307個維度（不含截距項）。</p><pre class="crayon-plain-tag"># What is the dimension of of your feature matrix?
dim(ames_train_x)</pre><p></p><pre class="crayon-plain-tag">## [1] 2054  307</pre><p>step 2: 執行ridge model</p>
<p>我們使用glmnet::glmnet()來建立Ridge模型，並使用參數alpha來指定說要使用哪種懲罰參數，alpha = 0為Ridge，alpha = 1為lasso，或是 0 \(\leq\) alpha \(\leq\) 1為elastic net。</p>
<p>glmnet會在執行時，主要會做兩件事：</p>
<ol>
<li>預測變數在投入正規化回歸模型(regularized regression)前是需要被標準化(standardized)。而glmnet函數則會幫你處理標準化這件事。如果你已在使用glmnet前標準化預測變數，則可將參數設定為standaradize = FALSE。</li>
<li>glmnet會跨非常大的\(\lambda\)區間來執行ridge model。如下圖所示：</li>
</ol>
<p></p><pre class="crayon-plain-tag"># Apply Ridge regression to ames data
ames_ridge &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

plot(ames_ridge, xvar = "lambda")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-181-1.png" alt="Regularized Regression, Ridge" /></p>
<p>我們可以用以下方式看\(\lambda\)的區間。glmnet預設為使用100組\(\lambda\)，當然你也可以自行指定\(\lambda\)區間，但絕大多數時候是不太需要去調整的。</p><pre class="crayon-plain-tag">ames_ridge$lambda</pre><p></p><pre class="crayon-plain-tag">##   [1] 279.10348741 254.30870285 231.71661862 211.13155287 192.37520764
##   [6] 175.28512441 159.71327708 145.52478975 132.59676852 120.81723707
##  [11] 110.08416672 100.30459276  91.39380920  83.27463509  75.87674603
##  [16]  69.13606504  62.99420758  57.39797580  52.29889783  47.65280789
##  [21]  43.41946378  39.56219829  36.04760164  32.84523206  29.92735217
##  [26]  27.26868869  24.84621355  22.63894441  20.62776299  18.79524938
##  [31]  17.12553123  15.60414624  14.21791689  12.95483634  11.80396439
##  [36]  10.75533273   9.79985861   8.92926618   8.13601479   7.41323366
##  [41]   6.75466241   6.15459682   5.60783940   5.10965440   4.65572679
##  [46]   4.24212485   3.86526617   3.52188658   3.20901188   2.92393211
##  [51]   2.66417804   2.42749981   2.21184742   2.01535299   1.83631458
##  [56]   1.67318146   1.52454063   1.38910464   1.26570041   1.15325908
##  [61]   1.05080672   0.95745595   0.87239820   0.79489675   0.72428031
##  [66]   0.65993724   0.60131024   0.54789149   0.49921832   0.45486914
##  [71]   0.41445982   0.37764035   0.34409183   0.31352366   0.28567108
##  [76]   0.26029285   0.23716915   0.21609970   0.19690199   0.17940976
##  [81]   0.16347149   0.14894914   0.13571691   0.12366019   0.11267456
##  [86]   0.10266486   0.09354440   0.08523417   0.07766220   0.07076291
##  [91]   0.06447653   0.05874861   0.05352954   0.04877413   0.04444117
##  [96]   0.04049314   0.03689584   0.03361811   0.03063157   0.02791035</pre><p>我們亦可透過coef()函數去查看每個\(\lambda\)懲罰係數模型產生的係數。glmnet會儲存每個模型的所有係數，並按照\(\lambda\)由大到小排列。<br />
但由於預測變數實在太多，我們只挑Gr_Liv_Area和TotRms_AbvGrd兩個變數分別在\(lamda\)為最大(279.1034874)和最小值(0.0279103)的係數值。</p>
<p>對應到最小\(\lambda\)值的係數。</p><pre class="crayon-plain-tag"># coefficients for the smallest lambda parameters
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 100]</pre><p></p><pre class="crayon-plain-tag">##   Gr_Liv_Area TotRms_AbvGrd 
##  0.0001004011  0.0096383231</pre><p>對應到最大\(\lambda\)值的係數。可以看到當\(\lambda\)懲罰係數很大時，係數幾乎都被壓縮逼近到0。</p><pre class="crayon-plain-tag"># coefficients for the largest lambda parameters
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 1]</pre><p></p><pre class="crayon-plain-tag">##                                      Gr_Liv_Area 
## 0.0000000000000000000000000000000000000005551202 
##                                    TotRms_AbvGrd 
## 0.0000000000000000000000000000000000001236183840</pre><p>雖然看到了懲罰係數\(\lambda\)對係數限制的效果，但到目前為止我們還不了解懲罰限制式對模型的改善程度。</p>
<h4>Tuning</h4>
<p>\(\lambda\)是一個用來校正參數，用來避免模型對訓練資料集產生過度配適的情況。然而，為了找出最適的\(\lambda\)，我們會需要利用cross-validation來協助。我們可以透過cv.glmnet()函數來執行k-fold cross validation，預設k=10。</p><pre class="crayon-plain-tag"># Apply CV Ridge regression to ames data
ames_ridge &lt;- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

# plot result
plot(ames_ridge)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-185-1.png" alt="Regularized Regression, Ridge" /></p>
<p>上圖為執行10-fold cross validation，不同\(\lambda\)值所對應的MSE(mean squared error)的結果。我們可以就MSE變化的區間發現，其實改善幅度有限；但是我們發現，當我們對係數使用\(\log(\lambda) \geq 0\)的懲罰係數，則MSE就會大幅上升。圖上方的數字(299)表示模型中的變數數目。因為Ridge Model不會直接強制讓變數係數變成0，所以模型中的變數數目維持不變。(但在lasso和elastic模型中就不大一樣)。</p>
<p>圖中的第一和第二條垂直虛線則分別代表：</p>
<ul>
<li>對應最小MSE的\(\lambda\)</li>
<li>對應最小MSE一個標準差內的最大\(\lambda\)</li>
</ul>
<p>我們可以透過以下指令查看兩者的值：</p>
<p>最小MSE值</p><pre class="crayon-plain-tag">min(ames_ridge$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02207459</pre><p>對應最小MSE的\(\lambda\)</p><pre class="crayon-plain-tag">ames_ridge$lambda.min</pre><p></p><pre class="crayon-plain-tag">## [1] 0.1489491</pre><p>對應最小MSE的\(\log(\lambda)\)</p><pre class="crayon-plain-tag">log(ames_ridge$lambda.min)</pre><p></p><pre class="crayon-plain-tag">## [1] -1.90415</pre><p>距離最小MSE一個標準差內的MSE值</p><pre class="crayon-plain-tag">ames_ridge$cvm[ames_ridge$lambda == ames_ridge$lambda.1se]</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02474565</pre><p>對應距離最小MSE一個標準差內的\(\lambda\)</p><pre class="crayon-plain-tag">ames_ridge$lambda.1se</pre><p></p><pre class="crayon-plain-tag">## [1] 0.6013102</pre><p>對應距離最小MSE一個標準差內的\(\log(\lambda)\)</p><pre class="crayon-plain-tag">log(ames_ridge$lambda.1se)</pre><p></p><pre class="crayon-plain-tag">## [1] -0.5086443</pre><p>在lasso和elastic模型中，使用對應距離最小MSE一個標準差內的\(\lambda\)是很明顯的結果。但在ridge模型中，我們可先用視覺化圖表來衡量。</p>
<p>我們將所有對應不同\(\lambda\)的係數值繪出，並以紅色垂直虛線表示對應最小MSE一個標準差內\(\lambda\)。</p><pre class="crayon-plain-tag">ames_ridge_min &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

{
  plot(ames_ridge_min, xvar = "lambda")
  abline(v = log(ames_ridge$lambda.1se), col = "red", lty = "dashed")
  }</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-192-1.png" alt="Regularized Regression, Ridge" /></p>
<p>由上圖我們可以看到，在極大化預測精準度的同時，我們可以對係數做出多少限制。</p>
<h4>Advantages &amp; Disadvantages</h4>
<h5>優點</h5>
<ol>
<li>實質上，Ridge模型會將具有相關性的變數推向彼此，並避免使得其中一個有極大正係數另一個有極大負係數的情況。</li>
<li>此外，許多不相干的變數係數會被逼近為0(不會等於0)。表示我們可以降低我們資料集中的雜訊，幫助我們更清楚的識別出模型中真正的訊號(signals)。</li>
</ol>
<p>比如說以下我們來看對應\(\lambda\)值為最小MSE一個標準差的模型中，Top 25個具有影響力的變數分別有哪些。</p><pre class="crayon-plain-tag">coef(ames_ridge, s = "lambda.1se") %&gt;% # 308 x 1 sparse Matrix of class "dgCMatrix"
  as.matrix() %&gt;% 
  as.data.frame() %&gt;% 
  add_rownames(var = "var") %&gt;% 
  `colnames&lt;-`(c("var","coef")) %&gt;%
  filter(var != "(Intercept)") %&gt;%  #剔除截距項
  top_n(25, wt = coef) %&gt;% 
  ggplot(aes(coef, reorder(var, coef))) +
  geom_point() +
  ggtitle("Top 25 influential variables") +
  xlab("Coefficient") +
  ylab(NULL)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-193-1.png" alt="Regularized Regression, Ridge" /></p>
<h5>缺點</h5>
<ol>
<li>然而，Ridge模型會保留所有變數。假如你覺得需要保留所有變數並將較無影響力的變數雜訊給減弱並最小化共線性，則模型是好的。</li>
<li>Ridge 模型是不具有變數挑選(feature selection)功能的。假如妳需要更近一步減少資料中的訊號(signals)並尋找subset來解釋，則lasso模型會更適合。</li>
</ol>
<h3>Lasso Regression</h3>
<p>Lasso的全名為Least absolute shrinkage and selection operator(<a href="https://www.jstor.org/stable/2346178?seq=1#page_scan_tab_contents" target="_blank" rel="noopener noreferrer"> Tibshirani, 1996</a>)。是Ridge模型外的另一個選擇，並在目標函式的限制式有所調整。有別於二階懲罰(L2 Penalty)，Lasso模型在目標函式中所使用的是一階懲罰式(L1 Penalty)\(\lambda \sum_{j=1}^p |\beta_{j}|\)。</p>
<p>\[ minimize \bigg \{ SSE + \lambda \sum_{j=1}^p |\beta_{j}| \bigg\} \]</p>
<p>不像Ridge模型只會將係數逼近到接近零（但不會真的是0），Lasso模型則真的會將係數推進成0(如下圖)。因此，Lasso模型不僅能使用正規化(regulariztion)來優化模型，亦可以自動執行變數篩選(Feature selection)。</p>
<p><img src="/wp-content/uploads/2019/01/unnamed-chunk-194-1.png" alt="Regularized Regression, Lasso" /></p>
<p>從上圖我們可以看到，在\(\log(\lambda) = -6\)時，所有8個變數（圖表上方數字）都包還在模型內，而當在\(\log(\lambda) = -3\)時只剩下6個變數，最後當在\(\log(\lambda) = -1\)時，只剩2個變數被保留在模型內。因此，當遇到資料變數非常多時，Lasso模型是可以幫你識別並挑選出有最強（也最一致）訊號的變數。</p>
<h4>使用R實作Lasso模型</h4>
<p>建置Lasso模型的方法跟Ridge模型作法一樣，只需將glmnet()函數中alpha參數改設定為1。</p><pre class="crayon-plain-tag">ames_lasso &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso, xvar = "lambda")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-195-1.png" alt="Regularized Regression, Lasso" /></p>
<p>從上圖中可以看到大約是在\(\log(\lambda) \rightarrow -6\)的時候，被保留在模型中的變數數目大量下降。而且，當\(\log(\lambda) = -10 \rightarrow \lambda = 0\)，在沒有懲罰限制式(跟OLS模型一樣)時，數個變數的係數值都非常大，表示可能變數與變數間存在高度相關而導致他們的係數變得極大。當我們對模型做出限制，這些資料中為雜訊的變數係數就會被推進成0。</p>
<p>同樣的，如何選擇最適的懲罰係數\(\lambda\)亦和Ridge模型一樣透過執行cross validation來看不同\(\lambda\)對應的MSE值，並找出使MSE發生最小值(或在一個標準差之內)的\(\lambda\)來決定。</p>
<h4>Tuning</h4>
<p>為了找出最適的\(\lambda\)，我們跟Ridge模型一樣，使用cv.glmnet()函數來看不同\(\lambda\)對應的MSE值，並將alpha參數設定為1。</p><pre class="crayon-plain-tag">ames_lasso &lt;- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-196-1.png" alt="Regularized Regression, Lasso" /></p>
<p>從上圖可以發現，我們可以透過採用大約\( -6 \leq \log(\lambda) \leq -4\)的懲罰係數區間來最小化模型的MSE。而且在該懲罰係數區間，不僅最小化模型的MSE，同時也壓縮模型內被保留下來的變數數目至大約\(131 \geq p \geq 63\)的區間。</p>
<p>同樣的，我們可以透過以下方法來取得最小MSE和一個標準差內的MSE以及對應的\(\lambda\)。</p>
<p>minimum MSE</p><pre class="crayon-plain-tag">min(ames_lasso$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02309004</pre><p>對應最小MSE的\(\log(\lambda)\)</p><pre class="crayon-plain-tag">log(ames_lasso$lambda.min)</pre><p></p><pre class="crayon-plain-tag">## [1] -5.555725</pre><p>within 1 S.E. of minimum MSE</p><pre class="crayon-plain-tag">ames_lasso$cvm[ames_lasso$lambda == ames_lasso$lambda.1se]</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02626365</pre><p>對應到最小MSE一個標準差內的最大\(\log(\lambda)\)</p><pre class="crayon-plain-tag">ames_lasso$lambda.1se</pre><p></p><pre class="crayon-plain-tag">## [1] 0.01295484</pre><p>此時在Lasso模型的例子中，使用對應到最小MSE一個標準差內的最大\(\lambda\)當作懲罰係數的優勢是更明顯的。假設我們使用對應最小MSE的\(\lambda\)當作懲罰係數，那麼我們可以將原始變數集個數從307降到小於150左右。但考量到MSE也會有些變化性(variability)，因此我們可以合理假設，可以使用稍微更高的懲罰係數來達成類似的MSE程度，其\(\lambda\)所對應的變數數目約為小於70的水準左右。如果你分析的目的志在說明與詮釋預測變數，這樣懲罰係數的選擇應會有很大的幫助。</p><pre class="crayon-plain-tag">ames_lasso_min &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

{
  plot(ames_lasso_min, xvar = "lambda")
  abline(v = log(ames_lasso$lambda.min), col = "red", lty = "dashed")
  abline(v = log(ames_lasso$lambda.1se), col = "red", lty = "dashed")
}</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-201-1.png" alt="Regularized Regression, Lasso" /></p>
<h4>Advantages and Disadvantages</h4>
<h5>優點</h5>
<ol>
<li>與Ridge模型一樣，Lasso模型亦會將具有相關性的變數推向彼此，並避免使得其中一個有極大正係數另一個有極大負係數的情況。</li>
<li>與Ridge模型最大的差別，就是Lasso會將不具影響力的變數係數變成0，自動進行變數篩選(Feature selection)。這樣的處理方式簡化並自動化識別出那些對模型預測正確性有高度影響力的變數。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>然而，時常在我們移除變數的同時也會犧牲掉模型的正確性。所以為了得到Lasso產生的更清楚與簡潔的模型結果，我們也會降低模型的正確性。</li>
</ol>
<p>一般來說，Ridge和Lasso模型所產生的最小MSE不會有太大差別(如下結果所示)。所以除非你單純只看最小化MSE的結果，實質上他們兩的差異並不顯著。</p><pre class="crayon-plain-tag"># minimum Ridge MSE
min(ames_ridge$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02207459</pre><p></p><pre class="crayon-plain-tag"># minimum Ridge MSE
min(ames_lasso$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02309004</pre><p></p>
<h3>Elastic Net</h3>
<p>涵蓋Ridge和Lasso兩個模型的就是Elastic Net模型(<a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2005.00503.x" target="_blank" rel="noopener noreferrer">Zou and Hasie,005</a>)，該模型綜合了兩個懲罰限制式。</p>
<p>\[ minimize \bigg \{ SSE + \lambda_{1} \sum_{j=1}^p \beta_{j}^2 + \lambda_{2} \sum_{j=1}^p |\beta_{j}| \bigg \} \]</p>
<p>雖然Lasso模型會執行變數挑選，但一個源自於懲罰參數的結果就是，通常當兩個高度相關的變數的係數在被逼近成為0的過程中，可能一個會完全變成0但另為一個仍保留在模型中。此外，這種一個在內、一個在外的處理方法不是很有系統。相對的，Ridge模型的懲罰參數就稍具效率一點，可以有系統的<br />
將高相關性變數的係數一起降低。於是，Elastic Net模型的優勢就在於，它綜合了Ridge Penalty達到有效正規化優勢以及Lasso Penalty能夠進行變數挑選優勢。</p>
<h4>使用R實作 Elastic Net</h4>
<p>跟Ridge和Lasso一樣是使用glmnet()，並調整介於0~1之間的alpha參數。當alpha = 0.5時，Ridge和Lasso的組合是平均的，而當alpha\(\rightarrow\)0時，會有較多的Ridge Penalty權重，而當alpha\(\rightarrow\)1時，則會有較多的Lasso Penalty權重。</p><pre class="crayon-plain-tag">lasso    &lt;- glmnet(ames_train_x, ames_train_y, alpha = 1.0) 
elastic1 &lt;- glmnet(ames_train_x, ames_train_y, alpha = 0.25) 
elastic2 &lt;- glmnet(ames_train_x, ames_train_y, alpha = 0.75) 
ridge    &lt;- glmnet(ames_train_x, ames_train_y, alpha = 0.0)

par(mfrow = c(2,2), mar = c(4,2,4,2), + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1) \n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = 0.75) \n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = 0.25) \n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0) \n\n")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-204-1.png" alt="Regularized Regression, Elastic Net" /></p>
<h4>tuning</h4>
<p>在Ridge和Lasso模型中，\(\lambda\)是我們主要調整的參數，然而在Elastic Net模型中，我們會需要調教\(\lambda\)和alpha兩個參數。</p>
<p>首先，我們先建立一個fold_id，用來固定每一次建模每筆資料所在的fold id(1 ~ nfold)，即每次建模使用的CV folds都是相同的(如果單純只使用nfolds = 10，雖然每次建模都會進行10-fold的CV，但每一次資料所在的fold id卻是不一樣的)。(*在前的Ridge和Lasso的例子中，因為只會run一次CV.glmnet，來看不同\(\lambda\)值交叉驗證的MSE值，因此只需指定nfold數即可；但在Elastic Net模型中，會根據不同alpha(i.e., Ridge和Lasso權重佔比)來跑不同的cv.glmnet，為了確保模型結果不受亂數fold if結果影響，故固定每一次資料所在的fold id）</p>
<p>接著我們再建立一個tuning grid用來搜羅從0-1的區間的alpha值以及對應的空欄位值（包括最小MSE和一個標準差內的MSE以及兩者分別所對應的\(\lambda\)值）後續將用來儲存不同alpha參數跑出的CV模型結果。</p><pre class="crayon-plain-tag"># maintain the same folds across all models
fold_id &lt;- sample(x = 1:10, size = length(ames_train_y), replace = TRUE)

# search across a range of alphas
tuning_grid &lt;- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)</pre><p>接著，我們便可以開始迭代不同的alpha值(不同的Ridge &amp; Lasso權重組成)，套用CV Elastic Net，萃取出每一個alpha值對應的模型結果，包括最小MSE以及一個標準差內的MSE、以及兩者分別所對應的\(\lambda\)值。</p><pre class="crayon-plain-tag">for(i in seq_along(tuning_grid$alpha)){
  # fit CV model for each alpha value
  fit &lt;- cv.glmnet(x = ames_train_x, y = ames_train_y, alpha = tuning_grid$alpha[i],foldid = fold_id)

  # extract MSE and lambda values
  tuning_grid$mse_min[i]    &lt;- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    &lt;- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] &lt;- fit$lambda.min
  tuning_grid$lambda_1se[i] &lt;- fit$lambda.1se
}

tuning_grid</pre><p></p><pre class="crayon-plain-tag">## # A tibble: 11 x 5
##    alpha mse_min mse_1se lambda_min lambda_1se
##    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1   0    0.0217  0.0247    0.124       0.601 
##  2   0.1  0.0217  0.0251    0.0292      0.108 
##  3   0.2  0.0219  0.0257    0.0146      0.0590
##  4   0.3  0.0221  0.0257    0.0107      0.0393
##  5   0.4  0.0222  0.0258    0.00802     0.0295
##  6   0.5  0.0223  0.0258    0.00642     0.0236
##  7   0.6  0.0223  0.0264    0.00535     0.0216
##  8   0.7  0.0223  0.0265    0.00458     0.0185
##  9   0.8  0.0224  0.0265    0.00401     0.0162
## 10   0.9  0.0224  0.0265    0.00357     0.0144
## 11   1    0.0226  0.0262    0.00292     0.0118</pre><p>接著我們每一個alpha值跑出的模型中，最適\(\lambda\)值所對應的「最小MSE\(\pm\)1個標準差內的MSE區間」繪出，由下圖我們可以發現，<br />
正負一個標準差內的MSE都落在相近的正確率區間(即不同alpha水準值間，模型正確率沒有太大差別)。因此，我們可以選取alpha = 1的Lasso模型來重重倚賴他的變數挑選功能，並合理假設模型正確率不會有損失(因為alpha從1變化到0正確率都沒有什麼變化)。</p><pre class="crayon-plain-tag">tuning_grid %&gt;%
  mutate(se = mse_1se - mse_min) %&gt;% # 計算1個SE的距離
  ggplot(aes(alpha, mse_min)) + # 繪製不同alpha參數下，cv所得的最小MSE值
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
  ggtitle("MSE ± one standard error")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-207-1.png" alt="Regularized Regression, Elastic Net" /></p>
<h4>優缺點</h4>
<h5>優點</h5>
<ol>
<li>Elastic Model的優勢就是它綜合了Ridge Penalty的有效的正規化過程以及Lasso Penalty的變數篩選功能。讓我們能夠控制變數共線性的問題，能夠在p&gt;n時執行回歸，並降低資料中過多的雜訊，以幫助我們將具有影響力的變數獨立出來且維持住模型正確率。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>然而，Elastic Net，以及一般的regularization models，依舊有假設預測變數和目標變數需具有線性關係。雖然我們可以結合non-additive models(一種無母數回歸模型，non-parametric regression)交互作用，但當資料變數很多的時候，會是非常繁瑣與困難的。因此，當非線性關係存在時，可能考慮使用非線性迴歸的方法。</li>
</ol>
<h3>Predicting</h3>
<p>一旦決定好最適的模型後，我們可以使用predict()函數，來將模型套用在新的資料集上做預測。唯一要注意的是，你需要提供predict()s參數，來指定你要的\(\lambda\)值。</p>
<p>比如說以下我們想要建立一個Lasso模型，並使用對應最小MSE的\(\lambda\)值來做預測。</p><pre class="crayon-plain-tag"># create a lasso model
cv_lasso &lt;- cv.glmnet(x = ames_train_x, y = ames_train_y, alpha = 1)
min(cv_lasso$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02243528</pre><p></p><pre class="crayon-plain-tag">pred &lt;- predict(cv_lasso,newx = ames_test_x, s = cv_lasso$lambda.min)
mean((ames_test_y - pred)^2) #計算MSE</pre><p></p><pre class="crayon-plain-tag">## [1] 0.01483042</pre><p>我們預測結果的平均誤差平方為0.01483，比cv MSE的(0.02244)還來的更低些。</p>
<h3>使用其他Package來實作: caret &amp; h20</h3>
<p>glmnet不是唯一能夠處理Regularized Regression的套件。常用的幾種套件包括caret和h20。以下僅簡單介紹caret套件的執行方法。</p>
<p>caret</p><pre class="crayon-plain-tag"># load caret package
library(caret)

train_control &lt;- trainControl(method = "cv", number = 10)

caret_mod &lt;- train(
  x = ames_train_x, 
  y = ames_train_y,
  method = "glmnet", 
  prePro = c("center","scale","zv","nzv"),
  trControl = train_control,
  tuneLength = 10
)

head(caret_mod$results)</pre><p></p><pre class="crayon-plain-tag">##   alpha       lambda      RMSE  Rsquared       MAE     RMSESD RsquaredSD
## 1   0.1 0.0001289530 0.1621572 0.8442073 0.1056494 0.02614671 0.04570000
## 2   0.1 0.0002978982 0.1621441 0.8442298 0.1056370 0.02616059 0.04573050
## 3   0.1 0.0006881835 0.1619713 0.8445118 0.1053968 0.02646506 0.04638065
## 4   0.1 0.0015897932 0.1617801 0.8448180 0.1050524 0.02695754 0.04747639
## 5   0.1 0.0036726286 0.1615155 0.8453152 0.1045659 0.02754710 0.04877829
## 6   0.1 0.0084842486 0.1610666 0.8462117 0.1039097 0.02810395 0.04988037
##         MAESD
## 1 0.005227902
## 2 0.005231369
## 3 0.005283405
## 4 0.005294270
## 5 0.005145444
## 6 0.005222816</pre><p></p>
<h3>小結</h3>
<ol>
<li>鑑於傳統一般線性回歸模型沒有挑選變數之功能，且又遇到變數特徵數量非常大的時候，會容易使得模型假設不成立並發生模型過度配適的問題（變異度高），為降低此變異和樣本外誤差，可以使用Regularized Regression。</li>
<li>常見的Regularized Regression方法包括，Ridge(alpha = 0)、Lasso(alpha=1)和Elastic Net(\(0\leq alpha \leq 1\))。他們分別透過二階的Ridge Penalty和一階的Lasso Penalty或綜合兩種Penalty(並以alpha設定Ridge &amp; Lasso Penalty的權重)來對傳統OLS的目標函示的加上係數懲罰限制式，唯有該係數能夠降低的SSE幅度夠大才能增加其的係數大小。</li>
<li>Ridge會有系統的將干擾變數係數逼近0(但不會等於0)，而Lasso則會將較無影響力的變數係數變成0。前者正規化過程較有效(即同為高度相關的變數，最終不會產生一個係數為零一個係數不為零的情況)，但從頭到尾都會保留模型中所有變數；後者則具有變數挑選的功能，但正規化過程較不具系統性。而Elastic Net則綜合上述兩者的優勢，同時兼顧有效的正規化過程以及變數挑選功能。</li>
<li>但不管是Ridge, Lasso還是Elastic Net，這些一般性線性回歸模型都收限於「預測變數需與目標變數成線性關係」之假設，若假設不成立，仍得考慮非線性的回歸模型。</li>
<li>此學習筆記僅介紹Regularized Regrssion的基本概念。Regularized Regression方法亦延伸出其他parametric generalized linear models（包括logistic regression, multinomial, poisson, support vector machines）。此外亦存在許多其他可替代方法如Least Angle Regression和The Bayesian Lasso。</li>
</ol>
<hr />
<p><strong>更多統計學習筆記：</strong></p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p><strong>參考文章連結：</strong></p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
<li><a href="http://uc-r.github.io/regularized_regression" target="_blank" rel="noopener noreferrer">正規化回歸 Regularized Regression</a></li>
<li><a href="http://rpubs.com/skydome20/R-Note18-Subsets_Shrinkage_Methods" target="_blank" rel="noopener noreferrer">R筆記 – (18) Subsets &amp; Shrinkage Regression (Stepwise &amp; Lasso)</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/regularized-regression-ridge-lasso-elastic/">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/regularized-regression-ridge-lasso-elastic/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
			</item>
		<item>
		<title>Regression Tree &#124; 迴歸樹, Bagging, Bootstrap Aggregation &#124; R語言</title>
		<link>/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/</link>
					<comments>/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Wed, 02 Jan 2019 14:06:06 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[Bagging]]></category>
		<category><![CDATA[Bootstrap Aggregation]]></category>
		<category><![CDATA[ensemble learning]]></category>
		<category><![CDATA[Regression Tree]]></category>
		<category><![CDATA[集成學習]]></category>
		<guid isPermaLink="false">/?p=2322</guid>

					<description><![CDATA[<p>有別於「分類」樹(classification tree)是用來找尋「最能區分標籤資料類別」的一系列變數，「迴歸」樹(regression tree)則是用來找 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>有別於「分類」樹(classification tree)是用來找尋「最能區分標籤資料類別」的一系列變數，「迴歸」樹(regression tree)則是用來找尋「最能區分目標連續變數相近度」的一系列變數。迴歸樹投入變數可以式任何資料型態(與分類樹一樣)，唯一差別是迴歸樹的目標變數是連續型變數。</p>
<p>不管是哪種型態的決策樹，單一決策樹模型的結果不穩定度高（high variance），預測能力也較弱，也因此我們多半會搭配使用bootstrap aggregating(or Bagging)(一種集成學習法ensemble learning)，綜合多顆決策樹的預測結果，降低單一決策樹的變異度或不穩定性，避免overfitting過度配適的問題。而諸多更複雜的決策數模型也是由此衍伸，如隨機森林(Random Forest)和(Gradient Boosting Machine)。</p>
<p>而此篇學習筆記將會實作Regression Tree以及bagging。</p>
<h3>Regression Tree</h3>
<h3>載入實作所需的套件</h3>
<p>其中決策樹模型會使用到的套件就是rpart(分類決策樹也是使用該套件)。</p><pre class="crayon-plain-tag">library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging</pre><p>而範例資料則是使用AmesHousing package中的Ames Housing數據。</p>
<p>將資料分成70%訓練集，30%測試集：</p><pre class="crayon-plain-tag">set.seed(123)
ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</pre><p></p>
<h3>決定切點 (Deciding on splits)</h3>
<ol>
<li>決策樹的分枝演算法會由上而下進行，貪心的切割出最完整的樹。而每一個分裂點的選取，都會去檢視每一個投入變數的值切分的結果，找出使得切分後兩群(R1, R2)的組內誤差平方和(SSE, sums of squares error)最小的變數和切點。目標函示如下：<br />
\[minimize\space \bigg\{SSE = \sum_{i \in R1}(y_{i} &#8211; c_{1})^2 + \sum_{i \in R2}(y_{i} &#8211; c_{2})^2 \bigg\}\]</li>
<li>分枝演算法會反覆在各個切分後的子群聚中執行，直到達到停止切割的條件。</li>
<li>而這一棵經過貪心分枝結果的決策樹，會長的非常大，複雜度高。即便可能可以很好的預測訓練資料集，但很可能已過度配適(overfit)，當將預測規則套用在新資料集，預測能力不穩定度高。</li>
</ol>
<h3>Cost complexity criterion</h3>
<p>為了優化決策樹在新資料上的預測能力穩定度，我們會需要在訓練結果的複雜度、深度與預測穩定度取得平衡。</p>
<p>要找到這樣的平衡，我們的方法就是先長出一棵完整和最複雜的樹，並加入樹的複雜度作為目標函式的懲罰因子(cost complexity parameter,\(\alpha\))，並以此為目標來修剪出最適的決策樹模型。加入懲罰因子限制是的目標函數如下：</p>
<p>\[minimize \bigg \{ SSE + \alpha |T| \bigg \}\]</p>
<p>其中，T代表的是決策樹的終端節點(Terminal nodes)數量，\(\alpha\)則為懲罰參數，cost complexity parameter。</p>
<p>在不同給定的懲罰參數\(\alpha\)值下，我們可以找出修剪後最小的一棵樹，使得使得上述加入懲罰限制式的懲罰誤差(penalized error)(\(SSE + \alpha |T|\))最小(the lowest penalized error)。(以上概念與regularized regression的懲罰限制式很像。)</p>
<p>懲罰參數越小，會傾向得到較大較複雜的修剪後決策樹，而懲罰參數越大，修剪後決策樹則會較小較精簡。</p>
<p>因此，上述加入懲罰的誤差目標函示可以確保說，樹繼續變複雜的前提就是SSE的下降幅度要高於複雜度的懲罰成本(cost complexity penalty)。</p>
<p>而要怎麼決定最適的懲罰參數(\(\alpha\))，我們會評估多組對應不同水準\(\alpha\)值的模型組合，並使用交叉驗證法來計算每一個\(\alpha\)值的交叉驗證誤差(cross-validated error, X-val error)，來尋找使得交叉驗證誤差最小的的最適\(\alpha\)值(optimal \(\alpha\))和最適的決策樹子集合(optimal subtree)。</p>
<p>「交叉驗證誤差(X-Val error)」：</p>
<ul>
<li>此處的「誤差」就是加入懲罰因子的「懲罰誤差」\(SSE + \alpha |T|\)，只是是透過k-fold交叉驗證所計算出來的，所以亦稱做「交叉驗證誤差(X-val error)」。</li>
<li>也因為此誤差是計算在<strong>測試資料集</strong>與真實值的誤差，故亦可稱作<strong>Predicted Residual sum of squares(PRESS)</strong>，跟<strong>SSE</strong>不太一樣，SSE是使用<strong>訓練資料</strong>本身來計算與真實資料的誤差，因此SSE通常會比PRESS來的小。</li>
</ul>
<h3>優缺點</h3>
<p>迴歸樹的優點包括：</p>
<ol>
<li>結果很好解釋。</li>
<li>可以快速產生預測規則（每一次分割只需找出最適的變數與切點，沒有太複雜的計算）。</li>
<li>可以從樹的長相與順序得知變數的重要性，越先被拿來切割的變數，該變數所能降低的SSE幅度越大。</li>
<li>如果模型預測途中遇到遺失值，雖然不能從上而下將該筆資料列分類，但我們可以在遇遺失值的節點處，由下而上取每個節點的平均值回溯估計該遺失值。</li>
<li>決策樹模型提供一個非線性的&#8221;jagged&#8221;(參差不齊的) response平面，所以他可以配適真實不是很平滑(smooth)的回歸平面。而如果真實回歸平面是平滑的，則片段的常數(piece-wise constant)可以任意逼近它。</li>
<li>存在許多快速、可靠的演算法來學習這些樹模型。</li>
</ol>
<p>而迴歸樹的幾個缺點包括：</p>
<ol>
<li>單一迴歸樹的變異數大，預測能力不穩定（使用訓練資料的子集合即能大大改變樹的末梢節點長相。）</li>
<li>也因為單一迴歸樹的變異數大，預測精準度也不太佳。</li>
</ol>
<h3>使用R來執行基本的迴歸樹演算法</h3>
<p>我們可以使用rpart()來執行迴歸樹演算法，並使用rpart.plot()來繪製模型結果。方式大致與分類樹相同，唯一差別在於method參數要設定為&#8221;anova&#8221;。</p><pre class="crayon-plain-tag">m1 &lt;- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = "anova"
  )</pre><p>我們可以執行m1來看一下模型訓練結果，會詳細說明每一次切割所使用的變數和切點，以及切割前後的資料筆數變化。</p><pre class="crayon-plain-tag">m1</pre><p></p><pre class="crayon-plain-tag">## n= 2051 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 2051 13299200000000 181620.20  
##    2) Overall_Qual=Very_Poor,Poor,Fair,Below_Average,Average,Above_Average,Good 1699  4001092000000 156147.10  
##      4) Neighborhood=North_Ames,Old_Town,Edwards,Sawyer,Mitchell,Brookside,Iowa_DOT_and_Rail_Road,South_and_West_of_Iowa_State_University,Meadow_Village,Briardale,Northpark_Villa,Blueste 1000  1298629000000 131787.90  
##        8) Overall_Qual=Very_Poor,Poor,Fair,Below_Average 195   173369900000  98238.33 *
##        9) Overall_Qual=Average,Above_Average,Good 805   852605100000 139914.80  
##         18) First_Flr_SF&amp;lt; 1150.5 553   302338400000 129936.80 *
##         19) First_Flr_SF&amp;gt;=1150.5 252   374390700000 161810.90 *
##      5) Neighborhood=College_Creek,Somerset,Northridge_Heights,Gilbert,Northwest_Ames,Sawyer_West,Crawford,Timberland,Northridge,Stone_Brook,Clear_Creek,Bloomington_Heights,Veenker,Green_Hills 699  1260199000000 190995.90  
##       10) Gr_Liv_Area&amp;lt; 1477.5 300   247261100000 164045.20 *
##       11) Gr_Liv_Area&amp;gt;=1477.5 399   631199000000 211259.60  
##         22) Total_Bsmt_SF&amp;lt; 1004.5 232   164042700000 192946.30 *
##         23) Total_Bsmt_SF&amp;gt;=1004.5 167   281257000000 236700.80 *
##    3) Overall_Qual=Very_Good,Excellent,Very_Excellent 352  2874510000000 304571.10  
##      6) Overall_Qual=Very_Good 254   885511300000 273369.50  
##       12) Gr_Liv_Area&amp;lt; 1959.5 155   325667700000 247662.30 *
##       13) Gr_Liv_Area&amp;gt;=1959.5 99   297033800000 313618.30 *
##      7) Overall_Qual=Excellent,Very_Excellent 98  1100817000000 385440.30  
##       14) Gr_Liv_Area&amp;lt; 1990 42    78801640000 325358.30 *
##       15) Gr_Liv_Area&amp;gt;=1990 56   756691700000 430501.80  
##         30) Neighborhood=College_Creek,Edwards,Timberland,Veenker 8   115305100000 281887.50 *
##         31) Neighborhood=Old_Town,Somerset,Northridge_Heights,Northridge,Stone_Brook 48   435248600000 455270.80  
##           62) Total_Bsmt_SF&amp;lt; 1433 12    31430660000 360094.20 *
##           63) Total_Bsmt_SF&amp;gt;=1433 36   258880600000 486996.40 *</pre><p>我們可以將上述模型結果用rpart.plot()函數來進行視覺化呈現。該函數參數可以進階調整圖形視覺畫呈現方式，可參考<a href="http://www.milbo.org/rpart-plot/prp.pdf" target="_blank" rel="noopener noreferrer">這篇文件</a>。</p><pre class="crayon-plain-tag">rpart.plot(m1)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-76-1.png" alt="plot of chunk unnamed-chunk-76" /></p>
<p>從上圖可以觀察到：</p>
<ul>
<li>每一個節點的觀測值個數和目標變數平均值，顏色越深表示目標變數平均值越大。</li>
<li>該完整的決策樹共12個葉節點(leaf nodes or terminal nodes)以及11個內部節點(internal nodes)。</li>
</ul>
<p>此外，我們也可以使用plotcp()函數，看一下在rpart()修樹過程中，套用不同懲罰參數(\(\alpha\))值，使用預設10-fold cross validation (k=10)，計算hold-out資料與真實資料的誤差(懲罰誤差or交叉驗證誤差)變化圖。</p><pre class="crayon-plain-tag">plotcp(m1)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-77-1.png" alt="plot of chunk unnamed-chunk-77" /></p>
<p>由上圖可以發現：</p>
<ul>
<li>下x軸為cp or cost complexity parameter (\(\alpha\))，上x軸為末梢節點數(number of terminal nodes, |T|)，y軸為交叉驗證誤差(X-val relative error)。</li>
<li>隨著懲罰係數減少，X-val error降低幅度呈遞減趨勢，直到cp達到預設的0.01則停止。</li>
<li>而根據專家建議，通常可以接受使用<strong>與最小X-val error相距一個標準差以內</strong>所對應的Tree Size(|T|)來作為修樹的最佳大小<strong>（1-SE rule）</strong>。比如說上圖中，水平虛線通過minimum X-val error &#8211; 1 SE的誤差值約對應在cp = 0.015和T = 9，因此我們亦可以選擇T = 9。</li>
</ul>
<p>為了進一步說明為何要選擇T=12（或T=9 如果你採用 1 -SE rule)。我們可以將參數最小cp值設定為0，即沒有任何懲罰條件存在的狀況下，讓決策樹長到最大最完整。</p><pre class="crayon-plain-tag">m2 &lt;- rpart(
    formula = Sale_Price ~ .,
    data    = ames_train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

{plotcp(m2)
  abline(v = 12,lty = "dashed", col = "red")}</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-78-1.png" alt="plot of chunk unnamed-chunk-78" /></p>
<p>由上圖我們可以發現，當T&gt;12後，即使樹繼續長大，但交叉驗證誤差下降幅度遞減，因此我們可以果斷將樹修剪到最精簡T=12且誤差也是最小的程度。</p>
<p>我們也可以看一下詳細的cp值和對應的X-Val error。rpart()預設的懲罰參數為cp=0.1，並執行一連串由大到小的\(\alpha\)值來計算誤差，直到cp=0.1所對應最小樹的大小為12(or 11 splits)，最小交叉驗證誤差為0.262(xerror)。</p><pre class="crayon-plain-tag">m1$cptable</pre><p></p><pre class="crayon-plain-tag">##            CP nsplit rel error    xerror       xstd
## 1  0.48300624      0 1.0000000 1.0017486 0.05769371
## 2  0.10844747      1 0.5169938 0.5189120 0.02898242
## 3  0.06678458      2 0.4085463 0.4126655 0.02832854
## 4  0.02870391      3 0.3417617 0.3608270 0.02123062
## 5  0.02050153      4 0.3130578 0.3325157 0.02091087
## 6  0.01995037      5 0.2925563 0.3228913 0.02127370
## 7  0.01976132      6 0.2726059 0.3175645 0.02115401
## 8  0.01550003      7 0.2528446 0.3096765 0.02117779
## 9  0.01397824      8 0.2373446 0.2857729 0.01902451
## 10 0.01322455      9 0.2233663 0.2833382 0.01936841
## 11 0.01089820     10 0.2101418 0.2687777 0.01917474
## 12 0.01000000     11 0.1992436 0.2621273 0.01957837</pre><p></p>
<h3>Tuning 修樹</h3>
<p>除了使用懲罰參數cost complexity parameter (\(\alpha\))來限制樹的大小，我們也常透過以下參數來修樹：</p>
<ul>
<li>minsplit : 分裂前至少/最小所需的資料筆數。預設為20筆。如果將此數值調小，可讓末梢節點(terminal nodes)即使僅有少數一些資料，也可以產生對應的預測值。</li>
<li>maxdepth : 從根節點(root nodes)到葉節點(terminal nodes)間的最大內部節點數量上限。預設為30，可以長出還滿大一棵樹。</li>
</ul>
<p>rpart()函數中，是使用control這個參數來設定一系列的參數水準值(hyperparameter setting)。<br />
比如說，我們想要建立一顆minsplit = 10和maxdepth = 12的決策樹模型。可以透過以下方式：</p><pre class="crayon-plain-tag">m3 &lt;-
  rpart(formula = Sale_Price ~ .,
        data = ames_train,
        method = "anova",
        control = list(minsplit = 10, maxdepth = 12, xval = 10))

m3$cptable</pre><p></p><pre class="crayon-plain-tag">##            CP nsplit rel error    xerror       xstd
## 1  0.48300624      0 1.0000000 1.0007911 0.05768347
## 2  0.10844747      1 0.5169938 0.5192042 0.02900726
## 3  0.06678458      2 0.4085463 0.4140423 0.02835387
## 4  0.02870391      3 0.3417617 0.3556013 0.02106960
## 5  0.02050153      4 0.3130578 0.3251197 0.02071312
## 6  0.01995037      5 0.2925563 0.3151983 0.02095032
## 7  0.01976132      6 0.2726059 0.3106164 0.02101621
## 8  0.01550003      7 0.2528446 0.2913458 0.01983930
## 9  0.01397824      8 0.2373446 0.2750055 0.01725564
## 10 0.01322455      9 0.2233663 0.2677136 0.01714828
## 11 0.01089820     10 0.2101418 0.2506827 0.01561141
## 12 0.01000000     11 0.1992436 0.2480154 0.01583340</pre><p>雖然這個參數設定的方法挺管用，但如果想要評估不同參數水準組合的模型效果，手動調整就不具效率，此時，可以使用grid search的方法，來自動執行與比較不同參數水準值組合的效果並依據此來選擇最適合的模型參數設定。</p>
<p>執行grid search之前，我們先建立一個hyperparameter grid。舉例來說，想測試minsplit落在5-20區間和maxdepth落在8~15之間的參數組合(因為原始模型顯示最是模型節點數目為12)。這樣一來總共就會有16*8=128種參數組合的模型。</p><pre class="crayon-plain-tag"># 建立grid
hyper_grid &lt;- expand.grid(
  minsplit = seq(5,20,1),
  maxdepth = seq(8,15,1)
)
head(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">##   minsplit maxdepth
## 1        5        8
## 2        6        8
## 3        7        8
## 4        8        8
## 5        9        8
## 6       10        8</pre><p>來看grid中的總共組合數。</p><pre class="crayon-plain-tag">nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 128</pre><p>接著使用loop迴圈來一一建立這128種不同參數組合的模型。</p><pre class="crayon-plain-tag">models &lt;- list()

for(i in 1:nrow(hyper_grid)){
  minsplit &lt;- hyper_grid$minsplit[i]
  maxdepth &lt;- hyper_grid$maxdepth[i]

  models[[i]] &lt;- rpart(
    formula = Sale_Price ~., 
    data = ames_train, 
    method = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
  )
}</pre><p>接著，我們在撰寫一個函式來將每一組模型的最小交叉驗證誤差（x-val error)和對應的\(\alpha\)值取出，加到grid的資料集中。</p><pre class="crayon-plain-tag"># create the function to extract minimum error and associated alpha
# get CP
get_cp &lt;- function(x){
  min &lt;- which.min(x$cptable[,"xerror"]) # 回傳發生最小xerror的列index
  cp &lt;- x$cptable[min, "CP"]
}

get_min_error &lt;- function(x){
  min &lt;- which.min(x$cptable[,"xerror"])
  xerror &lt;- x$cptable[min, "xerror"]
}

# 新增每組cp產生模型的cp和minXerror兩個欄位，並依照交叉驗證誤差由小到大排列，取前五個組合
hyper_grid %&gt;% 
  mutate(
    cp = purrr::map_dbl(models,get_cp),
    error = purrr::map_dbl(models,get_min_error)
  ) %&gt;% 
  arrange(error) %&gt;% 
  top_n(-5, wt = error)</pre><p></p><pre class="crayon-plain-tag">##   minsplit maxdepth        cp     error
## 1        5       13 0.0108982 0.2421256
## 2        6        8 0.0100000 0.2453631
## 3       12       10 0.0100000 0.2454067
## 4        8       13 0.0100000 0.2459588
## 5       19        9 0.0100000 0.2460173</pre><p>可發現hyperparameter參數組選出的最適模型的交叉驗證誤差較原始模型改善了一些(m1 的最小交叉驗證誤差 0.262 v.s. 最適模型的 0.242)。</p>
<p>我們將這組得到最小交叉誤差的模型套用在test data進行預測。</p><pre class="crayon-plain-tag">optimal_tree &lt;- rpart(
  formula = Sale_Price ~ .,
  data = ames_train,
  method = "anova",
  control = list(minsplit = 5, maxdepth = 13, cp = 0.01)
)

pred &lt;- predict(optimal_tree, newdata = ames_test)
RMSE(pred = pred, obs = ames_test$Sale_Price)</pre><p></p><pre class="crayon-plain-tag">## [1] 39145.39</pre><p>我們預測的Root-Mean-Squared-Error 平均誤差平方和開根號(均方根誤差)為39145。</p>
<h3>Bagging</h3>
<h4>Bagging概念介紹</h4>
<p>就像之前提到的，單一棵決策樹有變異度高(high variance)的問題。雖然修剪樹規則可以降低變異度，但其實有更有效的方法來大副降低變異度並提升決策樹的預測效果，其中一個方法就是Boostrap Aggregation (或稱Bagging)(概念來自於 <a href="https://link.springer.com/article/10.1023%2FA%3A1018054314350" target="_blank" rel="noopener noreferrer">Breiman, 1996</a>)。</p>
<p>Bagging整合和平均多組模型的預測結果。透過平均每組模型的預測結果，<br />
可以有效降低來自單一模型的變異度，並且避免過度配適overfitting。基本上Bagging可以分為三個基本步驟：</p>
<ol>
<li>從訓練資料集(training data)中產生m組 bootstrap samples。Bootstrapped sample可以讓我們創造出有些差異的資料集，且這些資料集都是與原始訓練資料擁有相同分佈的。</li>
<li>對每一個bootstrapped sample訓練一組單一且未修剪的決策樹。</li>
<li>將每一組模型的結果平均，產生一個整體平均的預測值。</li>
</ol>
<p><img loading="lazy" class="alignnone size-full wp-image-2333" src="/wp-content/uploads/2019/01/Untitled-presentation.jpg" alt="Regression Tree, Bagging" width="960" height="720" srcset="/wp-content/uploads/2019/01/Untitled-presentation.jpg 960w, /wp-content/uploads/2019/01/Untitled-presentation-300x225.jpg 300w, /wp-content/uploads/2019/01/Untitled-presentation-768x576.jpg 768w, /wp-content/uploads/2019/01/Untitled-presentation-830x623.jpg 830w, /wp-content/uploads/2019/01/Untitled-presentation-230x173.jpg 230w, /wp-content/uploads/2019/01/Untitled-presentation-350x263.jpg 350w, /wp-content/uploads/2019/01/Untitled-presentation-480x360.jpg 480w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>Bagging的概念可以套用在任何回歸和分類模型上；然而Bagging主要還是對於具有高變異度的模型較有效果。比如說，較為穩定的有母數模型(parametric model)如線性回歸和multi-adaptive regression splines模型，使用Bagging所能改善的預測能力幅度都較小。</p>
<p>Bagging的一個好處為，平均來說，一個Bootstrap sample會包含63%(2/3)的訓練資料集，剩餘約33%(1/3)的訓練資料不再bootstrapped sample內。我們稱這一包不在bootstrapped sample內的資料為out-of-bag (OOB) sample。我們可以透過這個OOB sample來衡量模型的準確度，產生一個自然而然的交叉驗證過程。</p>
<h4>Bagging with ipred</h4>
<p>配飾bagged tree model是簡單的。我們改使用ipred::bagging來建立模型。並將參數coob設定為TRUE來表示我們將使用OOB sample來估計模型錯誤率。</p><pre class="crayon-plain-tag"># make bootstrapping reproducible
set.seed(123)

bagged_m1 &lt;- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  coob = TRUE
)

bagged_m1</pre><p></p><pre class="crayon-plain-tag">## 
## Bagging regression trees with 25 bootstrap replications 
## 
## Call: bagging.data.frame(formula = Sale_Price ~ ., data = ames_train, 
##     coob = TRUE)
## 
## Out-of-bag estimate of root mean squared error:  36543.37</pre><p>我們可以發現，經由bagged後的決策數模型錯誤率將近下降了快2602(from 39145 to 36543)。</p>
<p>需要注意的一點就是，一般來說，越多樹模型效果越好。當我們加入更多樹模型，就可以平均更多高變異度的單一樹模型。隨著模型的數量增加，我們可以得到大幅降低的變異度（以及錯誤率），並且直到某一數量水準值為止，變異度下降的幅度開始趨緩收斂，即可得到建立穩定模型所需的最適的樹數量。從經驗上來說，通常不太會用到超過50棵樹來穩定模型的誤差。</p>
<p>bagging預設會產生25組bootstrap sample和樹模型，但有時候我們可能需要更多顆樹模型。我們可以觀察樹的多寡和誤差的變化。我們觀察10~50棵樹在穩定模型誤差的效果變化。</p><pre class="crayon-plain-tag"># assess 10-50 bagged trees
ntree &lt;- 10:50

# create empty vector to store OOB RMSE values
rmse &lt;- vector(mode = "numeric",length = length(ntree))

for(i in seq_along(ntree)){
  # reproducibility: 固定bootstrap亂數結果
  set.seed(123)

  # perform bagged model
  model &lt;- bagging(
    formula = Sale_Price ~.,
    data = ames_train,
    coob = TRUE,
    nbagg = ntree[i]
  )

  # get OOB error
  rmse[i] &lt;- model$err
}

# 將不同樹數量與誤差的圖繪出
{plot(x = ntree, y = rmse,type = "l",lwd = 2)
abline(v = 25, col = "red", lty = "dashed")}</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-87-1.png" alt="plot of chunk unnamed-chunk-87" /></p>
<p>我們可以發現差不多在樹數量為25時誤差水準趨於穩定。所以如果單純僅加入更多樹不太能在優化模型的錯誤率。</p>
<h4>Bagging with caret</h4>
<p>使用ipred::bagging來進行bagging是簡單的，但使用caret來進行bagging會有更多好處如下：</p>
<ol>
<li>他可以更簡單的進行cross validation。雖然我們可以使用OOB error，但使用cross validation可以提供強大的真實test error的誤差期望值。</li>
<li>我們可以跨多個bagged trees來衡量變數的重要性。</li>
</ol>
<p>我們來建立一個10-fold cross validation的模型。</p><pre class="crayon-plain-tag"># Specify 10-fold cross validation
ctrl &lt;- trainControl(method = "cv", number = 10)

# CV bagged model
bagged_cv &lt;- train(
  Sale_Price ~.,
  data = ames_train,
  method = "treebag", 
  trControl = ctrl,
  importance = TRUE
)


# assess result
bagged_cv</pre><p></p><pre class="crayon-plain-tag">## Bagged CART 
## 
## 2051 samples
##   80 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 1846, 1845, 1847, 1845, 1846, 1847, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   36477.25  0.8001783  24059.85</pre><p>我們看到，交叉驗證的RMSE為36477。</p>
<p>我們亦可以看前20名重要變數分別為哪些。</p><pre class="crayon-plain-tag"># plot most important variables
plot(varImp(bagged_cv),20)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-89-1.png" alt="plot of chunk unnamed-chunk-89" /></p>
<p>在回歸模型中，衡量變數的重要性為根據該變數切割(split)所能讓總SSE減少的量，並綜合平均m顆樹的效果。有最大影響SSE平均下降幅度的變數會被認為是最重要的變數。而上圖中重要性的值(importance value)則是每個變數平均使SSE下降程度相較於最重要變數的相對百分比(值的區間為0-100)。</p>
<p>同時，我們亦來比較將此模型套用在測試資料集的錯誤率(v.s. 交叉驗證錯誤率)。</p><pre class="crayon-plain-tag">pred &lt;- predict(object = bagged_cv,newdata = ames_test)
RMSE(pred = pred,obs = ames_test$Sale_Price)</pre><p></p><pre class="crayon-plain-tag">## [1] 35262.59</pre><p>可以發現cross validation(36477)和套用在test set(out of sample)的估計錯誤率(estimated error)(35263)是非常相近的。</p>
<p>而在之後的學習筆記中，會看從bagging概念所延伸的模型(如隨機森林Random Forest和GBMs)可以如何更好的改善模型錯誤率。</p>
<h3>小結</h3>
<ol>
<li>決策樹是一個非常直覺的模型演算法，有許多好處，但卻有高變異度(high variance)的問題。雖然可以透過更改修剪樹的規則(如cp,minsplit,maxdepth)來改善誤差和過度配飾(overfitting)，但程度有限。</li>
<li>能有效解決高變異度的問題可以透過Bagging(Bootstrap Aggregation)法，綜合和平均多棵樹模型的預測值，降低單一棵樹模型所產生的高變異度，並提升模型預側能力（降低錯誤率）。隨著樹的數量的增加，Bagging模型錯誤率會在某一最適值下趨於收斂與穩定。</li>
<li>評估Bagging模型錯誤率常用的方法可以是簡單的OOB error或是更強大的cross-validation error，兩者分別可以透過inpred::bagging和caret::train來得到。而caret::train 另外的好處就是可以計算出跨bagged tree所計算出來的變數重要性。</li>
<li>Bagging的概念可以套用在任何回歸或分類演算法上，但對於本身就非常穩定的有母數模型如linear regression等效果就沒有這麼大。</li>
<li>Bagging概念也是更複雜模強大模型如隨機森林(Random Forest)和Gradient Boosting Machines (GBMs)的延伸。</li>
</ol>
<hr />
<p>更多統計模型學習筆記：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考連結：</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
<li><a href="http://uc-r.github.io/regression_trees" target="_blank" rel="noopener noreferrer">Regression Trees</a></li>
<li><a href="https://rpubs.com/skydome20/R-Note16-Ensemble_Learning" target="_blank" rel="noopener noreferrer">R筆記 – (16) Ensemble Learning(集成學習)</a></li>
<li><a href="http://www.milbo.org/rpart-plot/prp.pdf" target="_blank" rel="noopener noreferrer">Plotting rpart trees with the rpart.plot package</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>資料標準化(Data Scaling)對複回歸分析(Mutiple Regression)的影響 &#124; R統計</title>
		<link>/data-scaling-multiple-linear-r-%e8%b3%87%e6%96%99%e6%a8%99%e6%ba%96%e5%8c%96/</link>
					<comments>/data-scaling-multiple-linear-r-%e8%b3%87%e6%96%99%e6%a8%99%e6%ba%96%e5%8c%96/#respond</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Tue, 25 Sep 2018 13:19:38 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[data scale]]></category>
		<category><![CDATA[data scaling]]></category>
		<category><![CDATA[multiple regression]]></category>
		<category><![CDATA[多元回歸分析]]></category>
		<category><![CDATA[複回歸分析]]></category>
		<category><![CDATA[資料標準化]]></category>
		<guid isPermaLink="false">/?p=1813</guid>

					<description><![CDATA[<p>在進行多元線性回歸分時，會遇到多變數彼此單位標準不一致的情況，如果想要比較回歸方程式不同解釋變數的估計參數彼此間的大小關係時，若沒有進行-資料標準化-之處理，是 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/data-scaling-multiple-linear-r-%e8%b3%87%e6%96%99%e6%a8%99%e6%ba%96%e5%8c%96/">資料標準化(Data Scaling)對複回歸分析(Mutiple Regression)的影響 | R統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>在進行多元線性回歸分時，會遇到多變數彼此單位標準不一致的情況，如果想要比較回歸方程式不同解釋變數的估計參數彼此間的大小關係時，若沒有進行-資料標準化-之處理，是不能直接拿來做比較的。為了比較每一個解釋變數變動一單位對目標變數之影響大小，我們需事前將目標變數與解釋變數進行-資料標準化-處理。</p>
<h4>多元線性回歸方程式</h4>
<p>$$y=\beta_{0} + \beta_{1}*x_{1} + &#8230; + \beta_{i}*x_{i}+\epsilon_{i}$$</p>
<h4>解釋變數\(X_{j}\)的參數估計值\(\hat{\beta}_{j}\)之含義</h4>
<p>解釋變數\(X_{j}\)的參數估計值\(\hat{\beta}_{j}\)之含義為：在其他解釋變數不變下，解釋變數\(X_{j}\)變動一個衡量單位，目標變數將變動\(\hat{\beta}_{j}\)個衡量單位。即</p>
<p>$$\frac{dy}{dx_{j}}=\beta_{j}$$</p>
<p>若要比較不同解釋變數對目標變數之影響大小(\(\hat{\beta}_{j}\))，我們必須進行標準化回歸，即將目標變數與預測變數一併標準化。</p>
<h4>標準化回歸</h4>
<p>標準化回歸就將目標變數和預測變數分別減去各自的平均數再除以各自的標準差，即計算z-score，再將標準化的目標與解釋變數投入回歸模型。</p>
<p>$$Z=\frac{x-\bar{x}}{s_{x}}$$</p>
<h4>step 1: Data scaling 資料標準化</h4>
<p>在R程式語言中，我們則可以使用scale()函數。scale()函數預設將資料center到資料平均值，並除以標準差(standard deviation)來scale資料。</p>
<p>我們使用R內建mtcars資料集來進行示範。</p><pre class="crayon-plain-tag">data("mtcars")

df &lt;- data.frame(mpg = mtcars$mpg, hp = mtcars$hp, disp = mtcars$disp, wt = mtcars$wt)
summary(df)

#      mpg              hp             disp             wt       
# Min.   :10.40   Min.   : 52.0   Min.   : 71.1   Min.   :1.513  
# 1st Qu.:15.43   1st Qu.: 96.5   1st Qu.:120.8   1st Qu.:2.581  
# Median :19.20   Median :123.0   Median :196.3   Median :3.325  
# Mean   :20.09   Mean   :146.7   Mean   :230.7   Mean   :3.217  
# 3rd Qu.:22.80   3rd Qu.:180.0   3rd Qu.:326.0   3rd Qu.:3.610  
# Max.   :33.90   Max.   :335.0   Max.   :472.0   Max.   :5.424</pre><p>我們將資料進行標準化。</p><pre class="crayon-plain-tag">scale_df &lt;- scale(df)
summary(scale_df)
#      mpg                hp               disp               wt         
# Min.   :-1.6079   Min.   :-1.3810   Min.   :-1.2879   Min.   :-1.7418  
# 1st Qu.:-0.7741   1st Qu.:-0.7320   1st Qu.:-0.8867   1st Qu.:-0.6500  
# Median :-0.1478   Median :-0.3455   Median :-0.2777   Median : 0.1101  
# Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
# 3rd Qu.: 0.4495   3rd Qu.: 0.4859   3rd Qu.: 0.7688   3rd Qu.: 0.4014  
# Max.   : 2.2913   Max.   : 2.7466   Max.   : 1.9468   Max.   : 2.2553</pre><p>並使用attribute()檢視各變數所使用的center和scale。</p><pre class="crayon-plain-tag">attributes(scale_df)

# $dim
# [1] 32  4
# 
# $dimnames
# $dimnames[[1]]
# NULL
# 
# $dimnames[[2]]
# [1] "mpg"  "hp"   "disp" "wt"  
# 
# 
# $`scaled:center`
#       mpg        hp      disp        wt 
#  20.09062 146.68750 230.72188   3.21725 
# 
# $`scaled:scale`
#         mpg          hp        disp          wt 
#   6.0269481  68.5628685 123.9386938   0.9784574</pre><p>可以發現center和scale分別皆為預設的變數平均值與標準差。</p><pre class="crayon-plain-tag">apply(X = df,MARGIN = 2,FUN = mean)
#      mpg        hp      disp        wt 
# 20.09062 146.68750 230.72188   3.21725
apply(X = df,MARGIN = 2,FUN = sd)
#       mpg          hp        disp          wt 
# 6.0269481  68.5628685 123.9386938   0.9784574</pre><p></p>
<h4>step 2: 進行標準化回歸</h4>
<ul>
<li><span style="text-decoration: underline;"><strong>標準化回歸模型中的各項估計參數</strong>通常被稱作為<span style="color: #9f6ad4; text-decoration: underline;">\(\beta\)係數「Beta Coefficents」</span></span>。<span style="text-decoration: underline;">其估計值的大小可直接被拿來進行多元變數影響力之比較</span>。</li>
<li>標準化模型中的解釋變數都是標準化後的Z-score，<span style="text-decoration: underline;">是以各變數的<span style="color: #9f6ad4; text-decoration: underline;">樣本標準差</span>為衡量單位</span>，因此每一個解釋變數的變動一單位的程度都是相同的。</li>
<li><span style="color: #9f6ad4;">解釋變數\(X_{j}\)的\(\beta_{j}\)係數<span style="color: #333333;">代表著</span>：其他解釋變數不變的情況下，解釋變數\(X_{j}\)變動一個單位（一個樣本標準差，\(s_{x}\)），目標變數將變動\(\beta_{j}\)個樣本標準差，即變動\(\beta_{j}\times s_{y}\)。</span></li>
<li>標準化回歸方程式：<br />
$$y^*=\beta_{0}^* + \beta_{1}^**x_{1}^* + &#8230; + \beta_{i}^**x_{i}^*+\epsilon_{i}^*$$<br />
其中，<br />
$$y^*=\frac{y-\bar{y}}{s_{y}},\space x^*=\frac{x-\bar{x}}{s_{x}},\space \beta_{j}^*=\frac{\beta_{j}*s_{x}}{s_{y}}$$</li>
</ul>
<p></p><pre class="crayon-plain-tag">mod_scale &lt;- lm(formula = mpg ~ ., data = as.data.frame(scale_df))
summary(mod_scale)
# Call:
#   lm(formula = mpg ~ ., data = scale_df)
# 
# Residuals:
#      Min       1Q   Median       3Q      Max 
# -0.64562 -0.27212 -0.02854  0.17607  0.97245 
# 
# Coefficients:
#               Estimate Std. Error t value Pr(&gt;|t|)   
# (Intercept)  3.568e-17  7.740e-02   0.000  1.00000   
# hp          -3.544e-01  1.301e-01  -2.724  0.01097 * 
# disp        -1.927e-02  2.128e-01  -0.091  0.92851   
# wt          -6.171e-01  1.731e-01  -3.565  0.00133 **
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Residual standard error: 0.4379 on 28 degrees of freedom
# Multiple R-squared:  0.8268,	Adjusted R-squared:  0.8083 
# F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11</pre><p>我們同時執行未標準化回歸的結果。</p><pre class="crayon-plain-tag">mod &lt;- lm(formula = mpg ~ ., data = df)
summary(mod)

# Call:
#   lm(formula = mpg ~ ., data = df)
# 
# Residuals:
#    Min     1Q Median     3Q    Max 
# -3.891 -1.640 -0.172  1.061  5.861 
# 
# Coefficients:
#              Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept) 37.105505   2.110815  17.579  &lt; 2e-16 ***
# hp          -0.031157   0.011436  -2.724  0.01097 *  
# disp        -0.000937   0.010350  -0.091  0.92851    
# wt          -3.800891   1.066191  -3.565  0.00133 ** 
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Residual standard error: 2.639 on 28 degrees of freedom
# Multiple R-squared:  0.8268,	Adjusted R-squared:  0.8083 
# F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11</pre><p>可發現:</p>
<ul>
<li>mod_scale和mod<span style="color: #9f6ad4;">兩個模型的Multiple R-squared(0.8268)、Adjusted R-squared(0.8083) 、F-statistic(44.57 on 3 and 28 DF)、P-value(8.65e-11)等顯著性結果都相同</span>。</li>
<li><span style="color: #9f6ad4;">資料標準化僅改變了「估計參數(estimate)」和「標準誤(std. error)」的部分</span>。</li>
</ul>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h4>Step 3: 比較標準化前後回歸係數差異</h4>
<p>我們根據\(\beta_{j}^*=\frac{\beta_{j}*s_{x}}{s_{y}}\)的關係式來重新計算標準化前後估計參數間的關係。</p>
<p>先查看兩組模型的估計參數如下：</p><pre class="crayon-plain-tag">mod_scale$coefficients
#  (Intercept)            hp          disp            wt 
# 3.567690e-17 -3.544385e-01 -1.926874e-02 -6.170635e-01 

mod$coefficients
# (Intercept)            hp          disp            wt 
# 37.1055052690 -0.0311565508 -0.0009370091 -3.8008905826</pre><p>查看data scaling各變數的平均數與標準差值。</p><pre class="crayon-plain-tag"># attr() : Get or set specific attributes of an object.
attr(scale_df,'scaled:center')
#      mpg        hp      disp        wt 
# 20.09062 146.68750 230.72188   3.21725 
attr(scale_df,'scaled:scale')
#       mpg          hp        disp          wt 
# 6.0269481  68.5628685 123.9386938   0.9784574</pre><p>根據\(\beta_{j}^*=\frac{\beta_{j}*s_{x}}{s_{y}}\)，使用未標準化估係數計算標準化係數如下：</p><pre class="crayon-plain-tag">(coeff_scale &lt;- mod$coefficients*attr(scale_df,'scaled:scale')/attr(scale_df,'scaled:scale')[1])
# (Intercept)          hp        disp          wt 
# 37.10550527 -0.35443851 -0.01926874 -0.61706350</pre><p>或是將標準化係數還原如下：</p><pre class="crayon-plain-tag">(coeff_orig &lt;- mod_scale$coefficients/attr(scale_df,'scaled:scale')*attr(scale_df,'scaled:scale')[1])
#  (Intercept)            hp          disp            wt 
# 3.567690e-17 -3.115655e-02 -9.370091e-04 -3.800891e+00</pre><p></p>
<h4>結論</h4>
<ul>
<li>常用的資料標準化方法：使用平均值作為中心點，並使用樣本標準差作為衡量單位。<br />
$$Z=\frac{x-\bar{x}}{s_{x}}$$</li>
<li>資料標準化僅影響各預測變數的<span style="text-decoration: underline;">係數</span>與<span style="text-decoration: underline;">標準誤</span>標準化前後之大小，其中<span style="text-decoration: underline;">係數標準化前(\(\beta\))後(\(\beta^*\))</span>之關係如下：<br />
$$\beta_{j}^*=\frac{\beta_{j}\times s_{x}}{s_{y}}$$</li>
<li>資料標準化並<span style="color: #9f6ad4;"><strong>不會</strong></span>對<span style="text-decoration: underline;">各係數顯著性</span>和<span style="text-decoration: underline;">模型顯著性</span>與<span style="text-decoration: underline;">調整後R平方等統計量</span>有所影響。</li>
<li>資料標準化的目的是讓各變數的估計參數大小可以彼此比較，並以此推論各解釋變數的影響力。<span style="text-decoration: underline;">並非建模必要步驟</span>。</li>
</ul>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<hr />
<p>更多統計模型學習筆記連結：</p>
<p><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e5%9b%9e%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">線性回歸模型</a></p>
<p><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e5%9b%9e%e6%ad%b8/" target="_blank" rel="noopener noreferrer">羅吉斯回歸模型part1</a></p>
<p><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e5%9b%9e%e6%ad%b8/" target="_blank" rel="noopener noreferrer">羅吉斯回歸模型part2</a></p>
<p><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">決策樹/隨機森林</a></p>
<p><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">階層式分群法</a></p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">切割式分群法</a></p>
<hr />
<p>參考連結：</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
<li><a href="http://www2.kobe-u.ac.jp/~kawabat/ch06.pdf">http://www2.kobe-u.ac.jp/~kawabat/ch06.pdf</a></li>
<li><a href="http://web.nchu.edu.tw/~finmyc/stat13p.pdf">http://web.nchu.edu.tw/~finmyc/stat13p.pdf</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/data-scaling-multiple-linear-r-%e8%b3%87%e6%96%99%e6%a8%99%e6%ba%96%e5%8c%96/">資料標準化(Data Scaling)對複回歸分析(Mutiple Regression)的影響 | R統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/data-scaling-multiple-linear-r-%e8%b3%87%e6%96%99%e6%a8%99%e6%ba%96%e5%8c%96/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Principal Components Analysis (PCA) &#124; 主成份分析 &#124; R 統計</title>
		<link>/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/</link>
					<comments>/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Mon, 10 Sep 2018 12:04:27 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[PCA]]></category>
		<category><![CDATA[principal components analysis]]></category>
		<category><![CDATA[主成分分析]]></category>
		<category><![CDATA[維度縮減]]></category>
		<category><![CDATA[降低維度]]></category>
		<guid isPermaLink="false">/?p=1346</guid>

					<description><![CDATA[<p>主成份分析(principal components analysis, PCA)的應用非常廣泛，可以簡化資料維度資訊，用最精簡的主成份特徵來解釋目標變數的最大 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>主成份分析(principal components analysis, PCA)的應用非常廣泛，可以簡化資料維度資訊，用最精簡的主成份特徵來解釋目標變數的最大變異，避免共線性與過度配適等問題。而主成份分析的計算過程會使用到線性代數中的<a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F">特徵值與特徵向量</a>技術。本學習筆記會介紹主成份分析的基礎以及R套件的函數用法。</p>
<h3>Principal Components Analysis 主成份分析簡介</h3>
<ul>
<li>主成分分析屬於非監督是式學習法，即處理一組沒有回應變數Y(目標變數)的一群X變數(\(X_{1},X_{2},&#8230;,X_{n}\))，即沒標籤的資料集(unlabeled data)。</li>
<li>主成份分析的主要目的為：降低資料維度。一方面可以避免共線性問題發生，一方面別是當資料維度非常大量時，可以減少運算並避免過度配適等問題產生。</li>
</ul>
<h4>本篇文章主要會分成以下幾個部分</h4>
<ol>
<li>載入所需套件</li>
<li>資料準備</li>
<li>主成份分析</li>
<li>萃取主成分</li>
<li>R語言內建的PCA函數</li>
</ol>
<h3>1. 載入所需套件</h3>
<p></p><pre class="crayon-plain-tag">library(tidyverse)  # data manipulation and visualization
library(gridExtra)  # plot arrangement</pre><p></p>
<h3>2. 資料準備</h3>
<p>我們主要使用R內建數據USArrests。</p><pre class="crayon-plain-tag">data("USArrests")
head(USArrests, 10)

#             Murder Assault UrbanPop Rape
# Alabama       13.2     236       58 21.2
# Alaska        10.0     263       48 44.5
# Arizona        8.1     294       80 31.0
# Arkansas       8.8     190       50 19.5
# California     9.0     276       91 40.6
# Colorado       7.9     204       78 38.7
# Connecticut    3.3     110       77 11.1
# Delaware       5.9     238       72 15.8
# Florida       15.4     335       80 31.9
# Georgia       17.4     211       60 25.8</pre><p></p>
<h3>3. Principal Components Analysis 主成份分析</h3>
<ul>
<li>目的為使用較少的變數來解釋最多的變異。</li>
<li>當遇上大量變數時，可以透過主成份分析將資料維度有效縮減，以利後續資料探勘與分析工作。</li>
<li>我們希望透過主成份分析來達成：<span style="color: #9f6ad4;">運用最少的代表性資料維度</span>來有效摘要大部分的資料資訊或變異（能捕捉到越多資料變異越好）。</li>
</ul>
<h4>principal components analysis 主成份分析計算</h4>
<p>假設我們的dataset由n個觀測值和p個維度所組成，而<span style="color: #9f6ad4;">主成份分析(PCA)所找出的維度將是<span style="text-decoration: underline;">p個維度的線性組合</span></span>。</p>
<p>第一個主成份(\(Z_{1}\))到第p個主成份(\(Z_{p}\))可透過以下公式來表示：</p>
<p>$$Z_{1}=\phi_{11}X_{1}+\phi_{21}X_{2}+&#8230;+\phi_{p1}X_{p}$$</p>
<p>&#8230;</p>
<p>$$Z_{p}=\phi_{1p}X_{1}+\phi_{2p}X_{2}+&#8230;+\phi_{pp}X_{p}$$</p>
<p>其中，</p>
<ul>
<li>\(\phi_{ji}\)稱為每一個主成分\(Z_{i}\)的<strong><span style="color: #9f6ad4;">負荷向量(loading vector)</span></strong>(由\(\phi_{1i},\phi_{2i},&#8230;,\phi_{pi}\)所組成)。為標準化數值，不同主成分間的負荷向量可互相比較。</li>
<li>每個主成份\(Z_{i}\)的\(\phi_{ji}\)是透過最大化對應主成份\(Z_{i}\)的的解釋變異而得。</li>
<li>\(\sum_{j=1}^p\phi_{ji}^2=1\)，表示各主成分由不同比重的原始維度所組成。</li>
<li>每個主成分的<span style="color: #9f6ad4;">負荷向量\(\phi_{ji}\)(loading vector)都是相互垂直的向量(\(\phi_{1}\perp \phi_{2} \perp &#8230;. \perp \phi_{p}\))</span>，即表示<span style="color: #9f6ad4;">各主成分彼此間的解釋變異為互相獨立的</span>。</li>
</ul>
<p>我們可以使用線性代數的技術來計算每一個主成分的負荷向量(loading vector)：</p>
<ol>
<li>資料標準化。</li>
<li>計算個原始維度兩兩間的共變異矩陣(covariance matrix)。其中，共變異數計算公式如下：<br />
$$cov(X,Y)=\frac{1}{N-1}\sum_{i=1}^N(x_{i}-\bar{x})(y_{i}-\bar{y})$$</li>
<li>計算共變異矩陣的特徵值(eigenvalues)與特徵向量(eigenvectors)。<span style="color: #9f6ad4;">而對應由大到小排列的特徵值的特徵向量，依序分別為解釋最大變異的主成分\(Z_{1}\)到解釋變異最小的主成分\(Z_{p}\)的負荷向量</span>。</li>
<li>萃取主成分。</li>
</ol>
<p>Step 1: 資料維度標準化</p>
<p>主成份分析的第一步就是進行資料的標準化，標準化的好處是，可以消除不同指標間的單位度量衡差異。待分析出不同主成份後，可以互相比較不同主成份間的平均值。我們將各維度（欄）進行標準化(centering)。＊而scale的方式就是 (x &#8211; mean(x)) / sd(x)。</p><pre class="crayon-plain-tag">scaled_df &lt;- apply(USArrests, 2, scale)
head(scaled_df)

#          Murder   Assault   UrbanPop         Rape
# [1,] 1.24256408 0.7828393 -0.5209066 -0.003416473
# [2,] 0.50786248 1.1068225 -1.2117642  2.484202941
# [3,] 0.07163341 1.4788032  0.9989801  1.042878388
# [4,] 0.23234938 0.2308680 -1.0735927 -0.184916602
# [5,] 0.27826823 1.2628144  1.7589234  2.067820292
# [6,] 0.02571456 0.3988593  0.8608085  1.864967207</pre><p>step 2: 我們可以使用cov()函數來計算兩兩變數的共變異數矩陣\(p\times p\)如下。</p><pre class="crayon-plain-tag">arrests.cov &lt;- cov(scaled_df)
arrests.cov
#              Murder   Assault   UrbanPop      Rape
# Murder   1.00000000 0.8018733 0.06957262 0.5635788
# Assault  0.80187331 1.0000000 0.25887170 0.6652412
# UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412
# Rape     0.56357883 0.6652412 0.41134124 1.0000000</pre><p>step 3: 計算矩陣的特徵值(eigen values)與特徵向量(eigen vectors)。eigen()函數產出的物件包含兩部分：</p>
<ul>
<li>$values：排序過後的特徵值。</li>
<li>$vectors：對應不同特徵值的特徵向量。</li>
</ul>
<p></p><pre class="crayon-plain-tag">arrests.eigen &lt;- eigen(arrests.cov)
arrests.eigen
# eigen() decomposition
# $values
# [1] 2.4802416 0.9897652 0.3565632 0.1734301
# 
# $vectors
#            [,1]       [,2]       [,3]        [,4]
# [1,] -0.5358995  0.4181809 -0.3412327  0.64922780
# [2,] -0.5831836  0.1879856 -0.2681484 -0.74340748
# [3,] -0.2781909 -0.8728062 -0.3780158  0.13387773
# [4,] -0.5434321 -0.1673186  0.8177779  0.08902432</pre><p>step 4: 我們將萃取首兩大主成分1和主成分2作為我們說明的例子。</p><pre class="crayon-plain-tag"># Extract the loadings
(phi &lt;- arrests.eigen$vectors[,1:2])
#            [,1]       [,2]
# [1,] -0.5358995  0.4181809
# [2,] -0.5831836  0.1879856
# [3,] -0.2781909 -0.8728062
# [4,] -0.5434321 -0.1673186</pre><p>因為R計算的特徵向量預設為指向負值，且我們知道<span style="color: #9f6ad4;">特徵向量是獨特的值，只要是在同一方向上，不管是正向還是負向<span style="text-decoration: underline;">純量縮放</span>，都是相同的</span>。為了方便說明，我們將特徵向量乘上純量-1。</p><pre class="crayon-plain-tag">phi &lt;- -phi
row.names(phi) &lt;- c("Murder", "Assault", "UrbanPop", "Rape")
colnames(phi) &lt;- c("PC1", "PC2")
phi
#                PC1        PC2
# Murder   0.5358995 -0.4181809
# Assault  0.5831836 -0.1879856
# UrbanPop 0.2781909  0.8728062
# Rape     0.5434321  0.1673186</pre><p>檢視以上萃取的主成分向量，我們可以做出以下推論：</p>
<ol>
<li>主成分1(PC1): 代表各大犯罪（包含謀殺、突擊、強姦）的犯罪發生率因子。</li>
<li>主成分2(PC2): 代表為城市化水平因子。</li>
</ol>
<p>有了對主成分的基本了解後，<span style="color: #9f6ad4;">我們將<span style="text-decoration: underline;">各觀測值</span><strong>投影</strong>到<span style="text-decoration: underline;">各主成分向量上</span>，並計算<span style="text-decoration: underline;">各觀測值</span>的<strong><span style="text-decoration: underline;">各主成分分數(principal component scores)</span></strong></span>。</p><pre class="crayon-plain-tag"># Calculate Principal Components scores
PC1 &lt;- as.matrix(scaled_df) %*% phi[,1] # %*% 表示矩陣乘法，此處為nxp矩陣乘上px1的向量，得到nx1的向量。
PC2 &lt;- as.matrix(scaled_df) %*% phi[,2]

# Create data frame with Principal Components scores
PC &lt;- data.frame(State = row.names(USArrests), PC1, PC2)
head(PC)

#        State        PC1        PC2
# 1    Alabama  0.9756604 -1.1220012
# 2     Alaska  1.9305379 -1.0624269
# 3    Arizona  1.7454429  0.7384595
# 4   Arkansas -0.1399989 -1.1085423
# 5 California  2.4986128  1.5274267
# 6   Colorado  1.4993407  0.9776297</pre><p>接著，我們將各國對應的主成分維度繪製成二維的平面圖。</p><pre class="crayon-plain-tag"># Plot Principal Components for each State
par(family="黑體-繁 中黑")
ggplot(PC, aes(PC1, PC2)) + 
  modelr::geom_ref_line(h = 0) +
  modelr::geom_ref_line(v = 0) +
  geom_text(aes(label = State), size = 3) +
  xlab("PC1: rate of serious crime, 重大犯罪發生率") + 
  ylab("PC2: urbanization, 都市化程度") + 
  ggtitle("First Two Principal Components of USArrests Data")+
  theme(text=element_text(family="黑體-繁 中黑"))</pre><p></p>
<ul>
<li>在PC1重大犯罪率維度上，Florida, Navada, California具有高重大犯罪率，而North Dakota, Vermont的重大犯罪率則較低。</li>
<li>在PC2都市化程度維度上，Hawaii, New Jersey為高度都市化城市，而North California, Mississippi的都市化程度則較低。</li>
<li>而接近中心點的城市如Indiana, Virginia則表示在兩維度表現皆為平均值。</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-1388" src="/wp-content/uploads/2018/09/Rplot01-1.jpeg" alt="principal components analysis, PCA, 主成份分析" width="700" height="661" srcset="/wp-content/uploads/2018/09/Rplot01-1.jpeg 700w, /wp-content/uploads/2018/09/Rplot01-1-300x283.jpeg 300w, /wp-content/uploads/2018/09/Rplot01-1-230x217.jpeg 230w, /wp-content/uploads/2018/09/Rplot01-1-350x331.jpeg 350w, /wp-content/uploads/2018/09/Rplot01-1-480x453.jpeg 480w" sizes="(max-width: 700px) 100vw, 700px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>4. 萃取主成分</h3>
<p>透過以上計算過程我們能夠產出p個主成分向量，但是卻會有以下疑問：</p>
<ul>
<li>該如何決定萃取的主成分數目？</li>
<li>萃取的主成分又構成資料變異的多少比例？</li>
</ul>
<p>以下我們將介紹常用的指標<span style="color: #9f6ad4;"><strong><span style="text-decoration: underline;">PVE(The Proportion of Variance Explained)</span></strong></span>以及以利用該指標所繪出的資訊曲線圖，來輔助我們判斷每一個主成分解釋變異比例以及合適的萃取數。</p>
<h4>The Proportion of Variance Explained (PVE)</h4>
<p>第m個主成分解釋的變異量佔比計算公式如下：</p>
<p>$$PVE=\frac{\sum_{i=1}^n(\sum_{j=1}^p\phi_{jm}x_{ij})^2} {\sum_{j=1}^p\sum_{i=1}^n x_{ij}^2}$$</p>
<p>即為<strong><span style="color: #9f6ad4;">第m個主成分的特徵值除上所有主成分特徵值加總</span></strong>。</p><pre class="crayon-plain-tag">PVE &lt;- arrests.eigen$values / sum(arrests.eigen$values)
round(PVE, 2)
# [1] 0.62 0.25 0.09 0.04</pre><p>從上面計算結果，我們可知第一個主成分解釋變異佔比為62%，第二主成分解釋變異占比為25%。而首兩個主成分總解釋變異便佔了87%。</p>
<p>我們可繪出各主成分所對應的解釋變異佔比以及累積佔比圖。</p><pre class="crayon-plain-tag"># PVE (aka scree) plot
PVEplot &lt;-
  qplot(c(1:4), PVE) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("PVE") +
  ggtitle("Scree Plot") +
  ylim(0, 1)


# Cumulative PVE plot
cumPVE &lt;- 
  qplot(c(1:4), cumsum(PVE)) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab(NULL) + 
  ggtitle("Cumulative Scree Plot") +
  ylim(0,1)

grid.arrange(PVEplot, cumPVE, ncol = 2)</pre><p><img loading="lazy" class="alignnone size-large wp-image-1394" src="/wp-content/uploads/2018/09/Rplot02-1024x967.jpeg" alt="principal components analysis, PCA, 主成份分析" width="1024" height="967" srcset="/wp-content/uploads/2018/09/Rplot02-1024x967.jpeg 1024w, /wp-content/uploads/2018/09/Rplot02-300x283.jpeg 300w, /wp-content/uploads/2018/09/Rplot02-768x725.jpeg 768w, /wp-content/uploads/2018/09/Rplot02-830x784.jpeg 830w, /wp-content/uploads/2018/09/Rplot02-230x217.jpeg 230w, /wp-content/uploads/2018/09/Rplot02-350x330.jpeg 350w, /wp-content/uploads/2018/09/Rplot02-480x453.jpeg 480w, /wp-content/uploads/2018/09/Rplot02.jpeg 1200w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<h4>決定主成分萃取數</h4>
<p>會視不同的分析目的和情形而定。一般來說，<strong><span style="color: #9f6ad4;">會根據每個主成分解釋變異百分比變化圖或累積圖，找出解釋絕大部分變異的最少主成分</span></strong>，約為線條出現「肘彎」轉折點處。比如說根據累積圖（上圖右），我們發現第三和第四主成分解釋的變異都有限，不如前兩個主成分，因此我們挑選解釋變異佔比前兩大主成分，兩主成分總構成約87%的變異。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>5. R語言內建的PCA函數</h3>
<p>在前面幾個步驟，我們為了熟悉主成分的計算邏輯，手動計算資料的共變異矩陣與矩陣對應的特徵值與特徵向量，但並不適合複雜的PCA運算。這邊將介紹一個R簡化PCA運算常用的套件: stats套件中的prcomp()函數。</p>
<p>prcomp()函數可以被用來快速處理上述幾個PCA相關運算。幾個重要參數包括：</p>
<ul>
<li>center: 預設值為TRUE。表示將資料平均值置中為0。</li>
<li>scale: 預設值為TRUE。表示將資料標準差壓縮為1。</li>
</ul>
<p></p><pre class="crayon-plain-tag">pca_result &lt;- prcomp(x = USArrests, center = TRUE, scale = TRUE)
pca_result

# Standard deviations (1, .., p=4):
# [1] 1.5748783 0.9948694 0.5971291 0.4164494
# 
# Rotation (n x k) = (4 x 4):
#                 PC1        PC2        PC3         PC4
# Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
# Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
# UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
# Rape     -0.5434321 -0.1673186  0.8177779  0.08902432

names(pca_result)
# [1] "sdev"     "rotation" "center"   "scale"    "x"</pre><p>我們可將pca_result結果全部列出或列出list名稱names(pca_result)或透過pca_result$xxx單獨呼叫list物件，對照了解以下資訊。</p>
<p>sdev: 表示每個主成分的標準差。</p><pre class="crayon-plain-tag">pca_result$sdev
# [1] 1.5748783 0.9948694 0.5971291 0.4164494</pre><p>rotation: 每個主成分的負荷向量(loading vector)。也因為R預設特徵向量指向負向，我們調整將特徵向量乘上純量-1。經調整後，便和我們先前手動計算的結果相同了。</p><pre class="crayon-plain-tag">pca_result$rotation
#                 PC1        PC2        PC3         PC4
# Murder   -0.5358995  0.4181809 -0.3412327  0.64922780
# Assault  -0.5831836  0.1879856 -0.2681484 -0.74340748
# UrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773
# Rape     -0.5434321 -0.1673186  0.8177779  0.08902432

pca_result$rotation &lt;- -pca_result$rotation
pca_result$rotation
#                PC1        PC2        PC3         PC4
# Murder   0.5358995 -0.4181809  0.3412327 -0.64922780
# Assault  0.5831836 -0.1879856  0.2681484  0.74340748
# UrbanPop 0.2781909  0.8728062  0.3780158 -0.13387773
# Rape     0.5434321  0.1673186 -0.8177779 -0.08902432</pre><p>center, scale: 每一個主成分在資料標準化前的平均數與標準差。</p><pre class="crayon-plain-tag">pca_result$center
# Murder  Assault UrbanPop     Rape 
#  7.788  170.760   65.540   21.232 
pca_result$scale
#   Murder   Assault  UrbanPop      Rape 
# 4.355510 83.337661 14.474763  9.366385</pre><p>另外，我們也可以<span style="text-decoration: underline;"><strong>從結果取得每一個國家（每一個觀察值）的主成分分數</strong></span>。（<span style="color: #9f6ad4;">但記得，如同特徵向量之處理，主成分分數也要乘上純量-1</span>) 。</p><pre class="crayon-plain-tag">pca_result$x &lt;- - pca_result$x
head(pca_result$x)
#                   PC1        PC2         PC3          PC4
# Alabama     0.9756604 -1.1220012  0.43980366 -0.154696581
# Alaska      1.9305379 -1.0624269 -2.01950027  0.434175454
# Arizona     1.7454429  0.7384595 -0.05423025  0.826264240
# Arkansas   -0.1399989 -1.1085423 -0.11342217  0.180973554
# California  2.4986128  1.5274267 -0.59254100  0.338559240
# Colorado    1.4993407  0.9776297 -1.08400162 -0.001450164</pre><p>如果要印出二維主成分圖，可以使用stat套件中的biplot()函數。常用參數包括：</p>
<ul>
<li>choice: 預設為使用第一和第二欄來繪製(choice = 1:2)。如果想要調整繪製x,y軸，比如說繪製第三和第四主成分則可將參數設定為choices = 3:4。</li>
</ul>
<p></p><pre class="crayon-plain-tag">biplot(x = pca_result) #default為第一和第二欄</pre><p>跟先前比較不同之處，圖中的標記的紅色向量分別表示各變數對主成分的作用的方向。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1396" src="/wp-content/uploads/2018/09/Rplot03-2.jpeg" alt="principal components analysis, PCA, 主成份分析" width="707" height="700" srcset="/wp-content/uploads/2018/09/Rplot03-2.jpeg 707w, /wp-content/uploads/2018/09/Rplot03-2-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot03-2-300x297.jpeg 300w, /wp-content/uploads/2018/09/Rplot03-2-230x228.jpeg 230w, /wp-content/uploads/2018/09/Rplot03-2-350x347.jpeg 350w, /wp-content/uploads/2018/09/Rplot03-2-480x475.jpeg 480w" sizes="(max-width: 707px) 100vw, 707px" /></p>
<p>另外，我們也可以算出每個主成分解釋變異量。</p><pre class="crayon-plain-tag">(VE &lt;- pca_result$sdev^2)
# [1] 2.4802416 0.9897652 0.3565632 0.1734301</pre><p>計算PVE。結果跟先前手動計算相同。</p><pre class="crayon-plain-tag">PVE &lt;- VE / sum(VE)
round(PVE, 2)
## [1] 0.62 0.25 0.09 0.04</pre><p></p>
<h3>總結</h3>
<p>主成分分析的應用非常廣泛，不僅可搭配迴歸、羅吉斯回歸、分群演算法一併使用，來有效降低維度。</p>
<p>或是說，在查看分群結果時，我們可透過fviz_cluster()或plot.kmeans()函數將多維資料呈現在二維平面用的即是主成分分析方法(請參考「<a href="/%e7%b5%b1%e8%a8%88-r%e8%aa%9e%e8%a8%80-%e5%88%86%e7%be%a4%e5%88%86%e6%9e%90-clustering-partitional-clustering-%e5%88%87%e5%89%b2%e5%bc%8f%e5%88%86%e7%be%a4-k-means%e3%80%81k-medoid-2/" target="_blank" rel="noopener noreferrer">分群分析 Clustering | Partitional Clustering</a>」)。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<hr />
<p>更多統計模型筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考:</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Partitional Clustering 切割式分群 &#124; Kmeans, Kmedoid &#124; Clustering 資料分群</title>
		<link>/partitional-clustering-kmeans-kmedoid/</link>
					<comments>/partitional-clustering-kmeans-kmedoid/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Fri, 07 Sep 2018 14:25:59 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[average silhouette width]]></category>
		<category><![CDATA[clustering]]></category>
		<category><![CDATA[elbow method]]></category>
		<category><![CDATA[gap statistic]]></category>
		<category><![CDATA[kmeans]]></category>
		<category><![CDATA[kmedoid]]></category>
		<category><![CDATA[optimal number of clusters]]></category>
		<category><![CDATA[partitional clustering]]></category>
		<category><![CDATA[最佳分群數]]></category>
		<guid isPermaLink="false">/?p=1251</guid>

					<description><![CDATA[<p>Partitional Clustering, 切割式分群，屬於資料分群屬的一種方法。資料分群屬於非監督式學習，所處理的資料是沒有正確答案/標籤/目標變數可參考 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/partitional-clustering-kmeans-kmedoid/">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Partitional Clustering, 切割式分群，屬於資料分群屬的一種方法。資料分群屬於非監督式學習，所處理的資料是沒有正確答案/標籤/目標變數可參考的。常見的切割式分群演算法包括kmeans, kmedoid。本篇將介紹分「分割式分群法」的實作與特色。</p>
<h3>資料分群簡介</h3>
<ul>
<li>資料分群是一個將資料分割成數個子集合的方法，主要目的包括：
<ul>
<li>找出資料中相近的<span style="color: #9f6ad4;">群聚(clusters)</span>，</li>
<li>找出各群的<span style="color: #9f6ad4;">代表點</span>。代表點可以是群聚中的<span style="color: #9f6ad4;">中心點(centroids)</span>或是<span style="color: #9f6ad4;">原型(prototypes)</span>。</li>
</ul>
</li>
<li>透過各群的代表點，可以達到幾個目標：
<ul>
<li>資料壓縮</li>
<li>降低雜訊</li>
<li>降低計算量</li>
</ul>
</li>
<li> 資料分群將<span style="color: #9f6ad4;">依據資料自身屬性計算彼此間的相似度</span>（物以類聚），而<span style="color: #9f6ad4;">「相似度」</span>主要有兩種型態：
<ul>
<li>「Compactness」：目標是讓子集合間差異最大化，子集合內差異最小化。如階層式分群和K-means分群。</li>
<li>「Connectedness」：目標是將可串連在一起的個體分成一群。如譜分群(Spectral Clustering)。</li>
</ul>
</li>
<li>資料分群屬於<span style="color: #9f6ad4;">非監督式學習法(Unsupervised Learning)</span>，即資料沒有標籤(unlabeled data)或沒有標準答案，<span style="color: #9f6ad4;">無法透過所謂的目標變數(response variable)來做分類之訓練</span>。也因為資料沒有標籤之緣故，與監督式學習法和強化式學習法不同，<span style="color: #9f6ad4;">非監督式學習法無法衡量演算法的正確率</span>。</li>
</ul>
<p><span style="color: #333333;"><span style="text-decoration: underline;">資料分群系列文</span>章會依序介紹以下幾種常見的分群方法：</span></p>
<ol>
<li><a href="/%e7%b5%b1%e8%a8%88-r%e8%aa%9e%e8%a8%80-%e5%88%86%e7%be%a4%e5%88%86%e6%9e%90-clustering-hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a41/">階層式分群(hierarchical clustering)</a>
<ol>
<li>聚合式階層分群法 Agglomerative Hierarchical Clustering</li>
<li>分裂式階層分群法 Divisive Hierarchical Clustering</li>
<li>最佳分群群數(Determining Optimal Clusters)</li>
</ol>
</li>
<li><a href="/%e7%b5%b1%e8%a8%88-r%e8%aa%9e%e8%a8%80-%e5%88%86%e7%be%a4%e5%88%86%e6%9e%90-clustering-partitional-clustering-%e5%88%87%e5%89%b2%e5%bc%8f%e5%88%86%e7%be%a4-k-means%e3%80%81k-medoid-2/">切割式分群(partitional clustering)</a>
<ol>
<li>K-means</li>
<li>K-medoid</li>
<li>最佳分群群數(Determining Optimal Clusters)</li>
</ol>
</li>
<li>譜分群(Spectral Clustering)</li>
</ol>
<h3>2.切割式分群(partitional clustering)</h3>
<p>我們主要會cover以下步驟以完成K-Means &amp; K-Medoid分群分析：</p>
<ul>
<li>Step 1: 載入所需套件(Packages)</li>
<li>Step 2: 資料準備</li>
<li>Step 3: 衡量群聚距離</li>
<li>Step 4: K-Means分群</li>
<li>Step 5: K-Medoid分群</li>
<li>Step 6: 決定最適分群數目
<ul>
<li>Elbow Method</li>
<li>Average silhouette Width</li>
<li>Gap Statistic</li>
</ul>
</li>
</ul>
<h4>Step 1: 載入所需套件(Packages)</h4>
<p></p><pre class="crayon-plain-tag">library(dplyr)
library(magrittr) #pipelines
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms &amp; visualization</pre><p></p>
<h4>Step 2: 資料準備</h4>
<p>我們使用R內建資料集USArrests。</p><pre class="crayon-plain-tag"># 資料預處理：(1)遺失值處理(2)資料標準化(平均數為0，標準差為1)
inputData &lt;- 
  USArrests %&gt;% 
  na.omit() %&gt;% # 忽略遺失值
  scale() # 資料標準化
head(inputData)

#                Murder   Assault   UrbanPop         Rape
# Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
# Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
# Arizona    0.07163341 1.4788032  0.9989801  1.042878388
# Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
# California 0.27826823 1.2628144  1.7589234  2.067820292
# Colorado   0.02571456 0.3988593  0.8608085  1.864967207</pre><p></p>
<h4>Step 3: 衡量群聚距離</h4>
<p>分類演算法將依據<span style="color: #9f6ad4;"><strong>資料個體</strong>兩兩間之距離</span>作為<span style="text-decoration: underline;">分群基礎</span>。而不同距離衡量也將影響分類結果。我們根據<span style="text-decoration: underline;">不同距離定義</span>計算所謂的<span style="color: #9f6ad4;"><strong>「相異度/距離矩陣(dissimilarity/distance matrix)」</strong></span><strong>，作為後續分類基礎。</strong>傳統計算距離常用的方法包括：(1)歐式距離(Euclidean Distance)(2)曼哈頓距離(Manhattan Distance)。定義分別如下：</p>
<p>歐式距離(Euclidean Distance)</p>
<p>$$d_{euc}(x,y)=\sqrt{\sum_{i=1}^n(x_{i}-y_{i})^2}$$</p>
<p>曼哈頓距離(Manhattan Distance)</p>
<p>$$d_{man}(x,y)=\sum_{i=1}^n|(x_{i}-y_{i})|$$</p>
<p>其中，x和y分別代表長度為n的向量。</p>
<p>幾個R裡面<span style="text-decoration: underline;">計算</span>和<span style="text-decoration: underline;">視覺化</span><span style="color: #9f6ad4;">資料個體間「相異度/距離矩陣」</span>的函數包括：</p>
<ul>
<li>stat套件中的dist(method=&#8230;)函數：參數method預設為&#8221;euclidean&#8221;。其他可選擇的方法包括：&#8221;maximum&#8221;, &#8220;manhattan&#8221;, &#8220;canberra&#8221;, &#8220;binary&#8221;, &#8220;minkowski&#8221;。</li>
<li>factoextra套件中的get_dist()函數：參數method預設為&#8221;euclidean&#8221;。<span style="color: #9f6ad4;">和dist()函數不同的是，他支援correlation-based distance measures</span>包括&#8221;pearson&#8221;, &#8220;kendall&#8221;和 &#8220;spearman&#8221; （*可根據資料屬性來選擇適合的距離計算方法，比如說，<span style="text-decoration: underline;">correlation-based distances常被運用在基因表達數據分析(gene expression data analyses)</span>）。</li>
<li>factoextra套件中fviz_dist()函數：則可將相異度（距離）矩陣(使用get_dist()產生的物件）計算結果視覺化。</li>
</ul>
<p>以下我們就使用get_dist()計算資料個體間兩兩距離(使用預設歐式距離法)，並使用fviz_dist()將相異/距離矩陣結果視覺化。</p><pre class="crayon-plain-tag">distance &lt;- get_dist(x = inputData)
fviz_dist(dist.obj = distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))</pre><p><img loading="lazy" class="alignnone size-full wp-image-1280" src="/wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1.jpeg" alt="partitional clustering" width="1000" height="996" srcset="/wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1.jpeg 1000w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-830x827.jpeg 830w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-480x478.jpeg 480w" sizes="(max-width: 1000px) 100vw, 1000px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h4>Step 4: K-Means分群</h4>
<h5>K-Means簡介</h5>
<ul>
<li>分群演算法中最出名的即為K-Means演算法。</li>
<li>是最簡單也最常使用的分群演算法。</li>
<li>K-Means會根據一些距離的測量將觀測值分成數個組別。</li>
<li><span style="color: #9f6ad4;">需要事前指定分群數目</span>。</li>
<li>目的是將極大化群內相似度，和最大化群間相異度。</li>
<li>指定群聚的平均值作為中心點(centroid)。</li>
<li><span style="color: #9f6ad4;">投入變數以連續變數為佳</span><span style="color: #9f6ad4;">（kmeans(x = &#8230;只能為數值矩陣&#8230;)）。</span></li>
</ul>
<h5>K-Means基礎概念</h5>
<ul>
<li>指定分群數k，並最小化k群組內變異總和(total intra-cluster variation or within-cluster variation)。</li>
<li>K-Means的演算法有許多，而其中標準的演算法是由Hartigan教授所提出的。Hartigan教授將<span style="color: #9f6ad4;">群聚內變異總和</span>定義為：<span style="color: #9f6ad4;">各資料點距離群中心的歐式距離平方加總</span>。<br />
$$W(C_{k})=\sum_{x_{i}\in C_{k}}(x_{i}-\mu_{k})^2$$<br />
其中，</p>
<ul>
<li>\(x_{i}\)代表群聚\(C_{k}\)中的資料點。</li>
<li>\(\mu_{k}\)代表群聚\(C_{k}\)中資料點的平均值所指定的中心點。</li>
</ul>
</li>
<li>而所有群內變異總和(total within-cluster variation)為：<br />
$$total\space within-cluster\space variation\space = \sum_{i=1}^kW(C_{k})=\sum_{i=1}^k\sum_{x_{i}\in C_{k}}(x_{i}-\mu_{k})^2$$<br />
total within-cluster variation是用來衡量群聚的緊緻度(Compactness)，我們希望這個值越小越好。</li>
</ul>
<h5>K-Means演算法</h5>
<ol>
<li>由User事先指定分群數目k。</li>
<li>演算法<span style="color: #9f6ad4;">隨機從資料集中挑選k個資料點當作初始中心點(initial centers)</span>。</li>
<li>各資料點將依據<span style="text-decoration: underline;">距離初始中心的的歐式距離</span>遠近分派指定給最近的群聚\(C_{k}\)(cluster assignment)。</li>
<li>各群聚\(C_{k}\)<span style="color: #9f6ad4;">重新計算所有群聚中資料點的平均值作為新的中心點（centroid update)</span>。並重新檢測所有資料點是否位在與其最近的中心點的群聚中。</li>
<li>重複第3~4步驟，來最小化各群聚的變異總和，直到<span style="text-decoration: underline; color: #9f6ad4;">各群聚組合趨於穩定與收斂（不再變動）(reach convergence)</span>，或<span style="text-decoration: underline; color: #9f6ad4;">已達最大迭代次數（R的kmeans函數預設為最大迭代次數為10）</span>才停止。</li>
</ol>
<h5>使用R套件stats中的kmeans函數執行K-Means分群演算法</h5>
<p>kmeans()函數幾個重要參數說明：</p>
<ul>
<li>centers: 指定分群數目k或指定初始中心點數目k。</li>
<li>nstart: 因為初始中心點是隨機選取，所以可以透過nstart參數多嘗試幾組隨機初始值，並選擇回傳最好的初始中心點分群結果。</li>
</ul>
<p>我們預設將資料分成2群(centers = 2)，並隨機執行25次初始中心點挑選與分群結果。我們可以使用str()結構函數看分群結果回傳值。其中：</p>
<ul>
<li>$cluster表示資料被指定的分群結果。</li>
<li>$center表示群聚中心點矩陣。</li>
<li>$totss表示total sum of squares。</li>
<li>$withininss: 表示within-cluster sum of squares，即單一群聚內總變異。</li>
<li>$tot.withinss: 表示total within-cluster sum of squares，即所有群聚內變異加總(sum($withinss))。</li>
<li>$size: 每群聚內資料個數。</li>
</ul>
<p></p><pre class="crayon-plain-tag">set.seed(101)
k_clust &lt;- kmeans(inputData, centers = 2, nstart = 25)
str(k_clust)

# List of 9
# $ cluster     : Named int [1:50] 2 2 2 1 2 2 1 1 2 2 ...
#  ..- attr(*, "names")= chr [1:50] "Alabama" "Alaska" "Arizona" "Arkansas" ...
# $ centers     : num [1:2, 1:4] -0.67 1.005 -0.676 1.014 -0.132 ...
#  ..- attr(*, "dimnames")=List of 2
#  .. ..$ : chr [1:2] "1" "2"
#  .. ..$ : chr [1:4] "Murder" "Assault" "UrbanPop" "Rape"
# $ totss       : num 196
# $ withinss    : num [1:2] 56.1 46.7
# $ tot.withinss: num 103
# $ betweenss   : num 93.1
# $ size        : int [1:2] 30 20
# $ iter        : int 1
# $ ifault      : int 0
# - attr(*, "class")= chr "kmeans"</pre><p>或是將分群結果印出。可以分別得知以下資訊：</p>
<ul>
<li>各群聚大小（資料個數）。</li>
<li>群聚在各維度的中心點（平均值）(2x4的矩陣)。</li>
<li>各資料列（列名稱）被分類到的群聚結果。</li>
<li>各群聚的變異總和。</li>
<li>可使用的成分(available components)，即我們可以透過kmeans產生的物件+$&#8230;取得的資訊。比如說<span style="color: #9f6ad4;">k_clust$cluster</span>即得得到各列分群結果。</li>
</ul>
<p></p><pre class="crayon-plain-tag">k_clust 

# K-means clustering with 2 clusters of sizes 30, 20
# 
# Cluster means:
#      Murder    Assault   UrbanPop       Rape
# 1 -0.669956 -0.6758849 -0.1317235 -0.5646433
# 2  1.004934  1.0138274  0.1975853  0.8469650
# 
# Clustering vector:
#        Alabama         Alaska        Arizona       Arkansas     California       Colorado    Connecticut       Delaware 
#              2              2              2              1              2              2              1              1 
#        Florida        Georgia         Hawaii          Idaho       Illinois        Indiana           Iowa         Kansas 
#              2              2              1              1              2              1              1              1 
#       Kentucky      Louisiana          Maine       Maryland  Massachusetts       Michigan      Minnesota    Mississippi 
#              1              2              1              2              1              2              1              2 
#       Missouri        Montana       Nebraska         Nevada  New Hampshire     New Jersey     New Mexico       New York 
#              2              1              1              2              1              1              2              2 
# North Carolina   North Dakota           Ohio       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina 
#              2              1              1              1              1              1              1              2 
#   South Dakota      Tennessee          Texas           Utah        Vermont       Virginia     Washington  West Virginia 
#              1              2              2              1              1              1              1              1 
#      Wisconsin        Wyoming 
#              1              1 
# 
# Within cluster sum of squares by cluster:
# [1] 56.11445 46.74796
# (between_SS / total_SS =  47.5 %)
# 
# Available components:
#   
# [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"        
# [8] "iter"         "ifault"</pre><p>我們可以進一步使用factoextra套件中的fviz_cluster()函數來將分群結果視覺化。(*值得注意的是，當變數維度&gt;2，<strong><span style="color: #9f6ad4;">fviz_cluster會使用主成分分析</span></strong>，找出最主要的兩個主成分來作為圖形的橫軸與縱軸。括號內的百分比則分別表示主成分解釋變異佔比。更多<span style="color: #9f6ad4;"><strong>主成分分析PCA</strong></span>請參考<a href="/%e7%b5%b1%e8%a8%88-r%e8%aa%9e%e8%a8%80-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90-principal-components-analysis-pca/" target="_blank" rel="noopener noreferrer">「主成分分析(PCA)」</a>)</p><pre class="crayon-plain-tag">fviz_cluster(k_clust, data = inputData)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1287" src="/wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result.jpeg" alt="partitional clustering" width="1000" height="996" srcset="/wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result.jpeg 1000w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-830x827.jpeg 830w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-480x478.jpeg 480w" sizes="(max-width: 1000px) 100vw, 1000px" /></p>
<p>或者是，使用傳統的成對資料（分類結果,原始資料）繪製散佈圖來檢視分群結果。</p><pre class="crayon-plain-tag">inputData %&gt;%
  as_tibble() %&gt;%
  mutate(cluster = k_clust$cluster, #新增分群結果
         state = row.names(USArrests) #將列名稱指定為原始資料標籤
         ) %&gt;%
  ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) + #使用ggplot套件繪圖（指定x,y軸），標記國家名稱，並依據分群結果上色
  geom_text()</pre><p><img loading="lazy" class="alignnone size-full wp-image-1288" src="/wp-content/uploads/2018/09/Rplot03-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot03-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot03-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot03-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot03-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot03-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot03-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot03-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>但因為通常資料維度會不止兩類，我們可以考慮使用useful套件中的plot.kmeans()函數，該<span style="color: #9f6ad4;">函數可以將多維尺度調整以將資料投影到二維空間</span>。然而這個效果似乎與factoextra套件中的fviz_cluster()函數無異，只不過還是factoextra套件中的fviz_cluster()函數更優！</p><pre class="crayon-plain-tag">library(useful)
plot.kmeans(x = k_clust, data = inputData)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1289" src="/wp-content/uploads/2018/09/Rplot04-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot04-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot04-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot04-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot04-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot04-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot04-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot04-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>也因為我們一開始需要預設分群數目k值。我們打算試試不同初始中心點數目的效果差異。</p><pre class="crayon-plain-tag"># 嘗試多種k的分群效果
set.seed(101)
k_clust &lt;- kmeans(inputData, centers = 2, nstart = 25)
k_clust_3 &lt;- kmeans(inputData, centers = 3, nstart = 25)
k_clust_4 &lt;- kmeans(inputData, centers = 4, nstart = 25)
k_clust_5 &lt;- kmeans(inputData, centers = 5, nstart = 25)


# plots to compare
p1 &lt;- fviz_cluster(k_clust, geom = "point", data = inputData) + ggtitle("k = 2")
p2 &lt;- fviz_cluster(k_clust_3, geom = "point",  data = inputData) + ggtitle("k = 3")
p3 &lt;- fviz_cluster(k_clust_4, geom = "point",  data = inputData) + ggtitle("k = 4")
p4 &lt;- fviz_cluster(k_clust_5, geom = "point",  data = inputData) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2) # Arrange multiple grobs on a page (將不會影響到par()中參數設定)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1290" src="/wp-content/uploads/2018/09/Rplot05-1.jpeg" alt="partitional clustering" width="1000" height="996" srcset="/wp-content/uploads/2018/09/Rplot05-1.jpeg 1000w, /wp-content/uploads/2018/09/Rplot05-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot05-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot05-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot05-1-830x827.jpeg 830w, /wp-content/uploads/2018/09/Rplot05-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot05-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot05-1-480x478.jpeg 480w" sizes="(max-width: 1000px) 100vw, 1000px" /></p>
<p>但僅憑上述嘗試，我們還是很難決定最佳群數該設定為多少。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h4>Step 5: K-Medoid分群</h4>
<p><strong>（可以依情況決定是否將Step 4的kmeans()法取代）</strong></p>
<ul>
<li>使用K-Means演算法的兩個限制為：
<ul>
<li><span style="color: #9f6ad4;">不能處理類別變數資料 （kmeans(x = &#8230;只能為數值矩陣&#8230;)）</span></li>
<li><span style="color: #9f6ad4;">容易受離群值影響</span></li>
</ul>
</li>
<li>使用K-Mediods演算法時，中心點將選選擇群內某個觀測值，而非群內平均值，就像中位數一樣，較不易受離群值所影響。是K-Means更強大的版本。</li>
<li>K-Medoids最常使用的演算法為PAM(Partitioning Around Medoid, 分割環繞物件法）。</li>
<li><span style="color: #ff0000;">K-Medoids比K-Means更強大之處在於他最小化相異度加總值，而非僅是歐式距離平方和</span>。<span style="color: #ff0000;">(The goal is to find k representative objects which minimize the sum of the dissimilarities of the observations to their closest representative object. )</span></li>
<li>但遇到資料量較大時，K-Medoid法所需要的記憶體和運算時間都是龐大的，此時可以另外考慮clara(Clustering Large Applications)函數。</li>
</ul>
<p>我們可以使用cluster套件中的pam()函數來執行。幾個重要參數包括：</p>
<ul>
<li><span style="color: #9f6ad4;">x: 可以為數值矩陣、data frame、甚是是<strong>相異度矩陣(dissimilarity matrix)</strong></span>。
<ul>
<li>若為numeric matrix or data frame須確保變數只能為數值。</li>
<li>若為dissimalirity matrix，可以是透過daisy()或dist()計算個體間距離的結果。
<ul>
<li>其中daisy()可以處理類別行變數的距離矩陣計算，須設定參數metric = c(&#8220;gower&#8221;)。</li>
<li>而傳統dist()則可以指定使用method = &#8220;euclidean&#8221; 或 &#8220;manhattan&#8221;。</li>
</ul>
</li>
</ul>
</li>
<li>k: 指定分群數目。</li>
<li>diss: TRUE/FALSE。投入的x是否為dissmilarity matrix。</li>
<li>metric: 如果投入的x為matrix或data frame，需指定計算dissimilarity matrix所用的距離衡量方式，可為&#8221;euclidean&#8221; 或 &#8220;manhattan&#8221;。若投入的x已是dissimilarity matrix，則忽略該參數。</li>
</ul>
<p></p><pre class="crayon-plain-tag">kmedoid.cluster &lt;- pam(x = inputData, k=3) 

# 分群結果視覺化
fviz_cluster(kmedoid.cluster, data = inputData,main = 'K-Medoid')</pre><p><img loading="lazy" class="alignnone size-full wp-image-1339" src="/wp-content/uploads/2018/09/Rplot12-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot12-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot12-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot12-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot12-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot12-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot12-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot12-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>並且我們可以直接利用K-Medoid產生的物件繪製silhouette plot。</p><pre class="crayon-plain-tag"># 繪製側影圖
plot(kmedoid.cluster,which.plots = 2)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1340" src="/wp-content/uploads/2018/09/Rplot13-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot13-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot13-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot13-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot13-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot13-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot13-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot13-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h4>Step 6: 決定最適分群數目</h4>
<p>以下為3個常見幫助我們決定最佳分群數的方法：</p>
<ol>
<li>Elbow Method（亦稱做Hartigan法）</li>
<li>Average Silhouette method（側影圖法）</li>
<li>Gap statistic（Gap統計量，預測-觀測）</li>
</ol>
<h4>Elbow Method</h4>
<p>根據切割式分群的目的：最小化<strong><span style="color: #9f6ad4;">各群群內變異加總</span></strong> （<span style="color: #9f6ad4;"><strong>total within-cluster variation</strong></span> 或 <span style="color: #9f6ad4;"><strong>total within-cluster sum of square，簡稱wss</strong></span>）。我們依序計算k = 1 ~ k=n各種分群結果的群內變異加總值，並將之與對應的k值繪製成圖(x軸為k值，y軸為<span style="color: #9f6ad4;">wss</span>)。並找出曲線彎曲（如膝蓋彎曲）處對應的k值，即各群群內變異加總值趨於收斂的轉折點，作為最佳的分群數目。</p><pre class="crayon-plain-tag">set.seed(123)

# function to compute total within-cluster sum of square 
wss &lt;- function(k) {
  kmeans( x = inputData, centers =  k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values &lt;- 1:15

# extract wss for 2-15 clusters
wss_values &lt;- map_dbl(k.values, wss)

plot(k.values, wss_values,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")</pre><p>由下圖可觀察到，wss約在k=4時出現轉折彎曲，即wss下降幅度開始變小（趨於穩定）。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1295" src="/wp-content/uploads/2018/09/Rplot06-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot06-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot06-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot06-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot06-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot06-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot06-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot06-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>而幸好這樣複雜的計算與繪圖過程，我們都可以透過factoextra套件中的fviz_nbclust()函數來達成。便可將上述數行程式碼縮減到一行指令。</p><pre class="crayon-plain-tag">set.seed(123)
fviz_nbclust(x = inputData,FUNcluster = kmeans, method = "wss")</pre><p><img loading="lazy" class="alignnone size-full wp-image-1296" src="/wp-content/uploads/2018/09/Rplot07-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot07-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot07-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot07-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot07-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot07-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot07-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot07-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h4>Average Silhouette Method</h4>
<ul>
<li>簡單來說，<span style="color: #9f6ad4;">average silhouette method</span>衡量<span style="color: #9f6ad4;">各分群結果的品質 ((\(C_{1} \sim C_{k})\)都會有average silhouette width)</span>。</li>
<li>該指標是透過計算各群聚中，各個物件的<a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" target="_blank" rel="noopener noreferrer">silhouette width</a>，並取群內silhouette with平均值而得。</li>
<li>每一群聚中，<span style="color: #9f6ad4;"><span style="text-decoration: underline;">各物件的silhouette with</span>衡量的是<span style="text-decoration: underline;">該物件是否被很好的歸類在合適的群聚</span></span>。
<ul>
<li><span style="color: #333333;"><strong>若silhouette with若為正數且值越大，則表示該觀測植被很好的分派到合適的群聚。</strong></span></li>
<li><span style="color: #333333;"><strong>若silhouette with值很小或甚至為負數，則表示該觀測值的分群結果不是很合適</strong></span>。</li>
</ul>
</li>
<li><span style="text-decoration: underline; color: #9f6ad4;">整個群聚</span>的<span style="text-decoration: underline; color: #9f6ad4;">average silhouette width</span>則是將各觀測值的silhouette with取平均所計算而得。<span style="color: #9f6ad4;"><strong>整個群聚的average silhouette with越大表示分群做得越好</strong></span>。</li>
<li>最後，再將所有k群聚的average silhouette with取平均，<span style="color: #9f6ad4;">得出 每一個對應k群分群結果的average silhouette width</span>。</li>
<li>於是我們可以得到k=1~k=n不同分群結果的average silhouette width。而其中<strong><span style="color: #9f6ad4;">極大化average silhouette width的k值即為最佳分群數目</span></strong>。</li>
</ul>
<p>我們先來計算與視覺化，當k=5各群的average silhouette width以及整體分群結果的average silhouette width。</p>
<p>首先我們使用cluster套件中的silhouette()函數來計算每個觀測值得silhouette width。（初始計算結果並沒有各群的平均silhouette with）</p><pre class="crayon-plain-tag">km.res &lt;- kmeans(x = inputData, centers = 5, nstart = 25)
ss &lt;- silhouette(km.res$cluster, dist(inputData))
#       cluster neighbor    sil_width
#  [1,]       3        2  0.448730181
#  [2,]       4        3  0.054455351
#  [3,]       4        1  0.432969809
#  [4,]       2        3  0.228329605
#  [5,]       4        1  0.448896074
#  [6,]       4        2  0.310290004
#  [7,]       1        2  0.279323646
#  [8,]       1        2 -0.026200144
#  [9,]       4        3  0.246185137
# [10,]       3        4  0.426879411
# [11,]       1        2  0.254134176
# [12,]       5        2  0.239121931
# [13,]       4        1  0.260389832
# [14,]       2        1  0.364248885
# [15,]       5        2  0.512603957
# [16,]       2        1  0.260830275
# [17,]       2        5  0.305423442
# [18,]       3        4  0.375176086
# [19,]       5        2  0.548907843
# [20,]       4        3  0.073438587
# [21,]       1        2  0.425221521
# [22,]       4        3  0.408874055
# [23,]       5        2  0.140353445
# [24,]       3        2  0.522765736
# [25,]       2        4  0.143376681
# [26,]       2        5  0.121906504
# [27,]       2        5  0.083351231
# [28,]       4        3  0.427993699
# [29,]       5        2  0.545620065
# [30,]       1        2  0.331314848
# [31,]       4        3  0.317371390
# [32,]       4        1  0.361348047
# [33,]       3        2  0.430379777
# [34,]       5        2  0.548983948
# [35,]       1        2 -0.016788571
# [36,]       2        1  0.246846452
# [37,]       2        1  0.143477706
# [38,]       1        2 -0.007681734
# [39,]       1        2  0.341420835
# [40,]       3        2  0.566811560
# [41,]       5        2  0.424934521
# [42,]       3        2  0.226342361
# [43,]       4        3  0.178417378
# [44,]       1        2  0.296027252
# [45,]       5        2  0.430596483
# [46,]       2        1  0.436958480
# [47,]       1        2 -0.014964942
# [48,]       5        2  0.349650799
# [49,]       5        1  0.295667125
# [50,]       2        1  0.403192800
# attr(,"Ordered")
# [1] FALSE
# attr(,"call")
# silhouette.default(x = km.res$cluster, dist = dist(inputData))
# attr(,"class")
# [1] "silhouette"</pre><p>如果要計算<span style="color: #9f6ad4;">整體分群結果的average silhouette width</span>，我們可以將結果的第三欄取平均值。</p><pre class="crayon-plain-tag">mean(ss[, 3])
# [1] 0.3030781</pre><p>如果想要計算與視覺化<span style="color: #9f6ad4;">各群的average silhouette width</span>，我們可以使用plot()函數。</p><pre class="crayon-plain-tag">plot(ss)</pre><p>從下圖我們可以觀察到：</p>
<ul>
<li>j = 1 ~ 5。表示有五個群聚\(C_{j}\)。</li>
<li>\(n_{j}\):各群觀測值數。</li>
<li>\(ave_{i\in C_{j}}s_{i}\): 群聚\(C_{j}\)的average silhouette width。</li>
<li>\(S_{i}\): 各觀測值的silhouette width。</li>
<li>圖最下方的average silhouette width則為整體分群結果的值。</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-1310" src="/wp-content/uploads/2018/09/Rplot08-1.jpeg" alt="partitional clustering" width="800" height="796" srcset="/wp-content/uploads/2018/09/Rplot08-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot08-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot08-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot08-1-768x764.jpeg 768w, /wp-content/uploads/2018/09/Rplot08-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot08-1-350x348.jpeg 350w, /wp-content/uploads/2018/09/Rplot08-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>有了以上單一k群結果的silhouette計算概念後，我們進一步比較不同k值（假設k=2 ~ 15)的分群結果品質。</p><pre class="crayon-plain-tag"># function to compute average silhouette for k clusters
avg_sil &lt;- function(k) {
  km.res &lt;- kmeans(x = inputData, centers = k, nstart = 25)
  ss &lt;- silhouette(km.res$cluster, dist(inputData))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values &lt;- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values &lt;- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
     type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of clusters K",
     ylab = "Average Silhouettes")</pre><p>由下圖我們可以比較出，分兩群的結果最佳，分四群的結果次之。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1311" src="/wp-content/uploads/2018/09/Rplot09-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot09-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot09-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot09-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot09-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot09-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot09-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot09-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>像Elbow Method一樣，我們也可以使用<span style="color: #9f6ad4;">fviz_nbclust函數</span>將上述結果的程式碼指令濃縮成一行並繪出。<span style="color: #9f6ad4;">只需要將參數method從&#8221;</span>wss<span style="color: #9f6ad4;">&#8220;改成</span>&#8220;silhouette<span style="color: #9f6ad4;">&#8220;即可</span>！</p><pre class="crayon-plain-tag">fviz_nbclust(inputData, kmeans, method = "silhouette")</pre><p><img loading="lazy" class="alignnone size-full wp-image-1312" src="/wp-content/uploads/2018/09/Rplot10-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot10-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot10-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot10-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot10-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot10-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot10-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot10-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h4>Gap statistic</h4>
<ul>
<li>Gap Statistic法可以應用在任何分群演算法上（比如說分裂式分群或階層式分群）。</li>
<li><strong>Gap Statistic(k): </strong>比較不同k水準值下，<span style="text-decoration: underline; color: #9f6ad4;">實際觀測值分群結果的群內總變異</span><span style="text-decoration: underline;"><span style="color: #9f6ad4; text-decoration: underline;">(\(W_{k}\))</span></span>與<span style="text-decoration: underline;"><span style="color: #9f6ad4; text-decoration: underline;">自助抽樣法B回產生樣本分群結果期望群內總變異(\(\overline{W_{k}}\))</span></span>，即衡量<span style="color: #9f6ad4;"><span style="text-decoration: underline;">觀測</span>和<span style="text-decoration: underline;">預期</span>之間的差距</span>。</li>
<li><strong>群內總變異的期望值(\(\overline{W_{k}}\)): </strong>使用<span style="color: #9f6ad4;">自助抽樣法(Bootstap)</span>進行B回重新抽樣，計算k水準值下重新抽樣樣本分群結果的群內總變異(\(W_{ki}\sim W_{kB}\))，並計算 k水準值下的群內總變異期望值，即<br />
$$E(W_{k})=mean(W_{k})=\overline{W_{k}}=\frac{1}{B}\sum_{i=1}^B\log (W_{ki})$$</li>
<li><span style="color: #333333;"><strong>群內總變異的<span style="color: #9f6ad4;">標準差Standard Deviation(\(Sd(W_{k})\))</span>:<br />
</strong>$$Sd(W_{k})=\sqrt{\frac{1}{B}\sum_{i=1}^B(\log  (W_{ki}) &#8211; \overline{W_{k}})^2 }$$</span></li>
<li><strong>群內總變異樣本平均值的<span style="color: #9f6ad4;">標準誤Standard Error(\(s_{k}\)): </span></strong>即樣本平均值(\(\overline{W_{k}}\))與真實母體平均值(\(\mu_{k}\))的變異。<span style="color: #ff0000;"><strong><br />
</strong></span>$$s_{k} = sd(k)\times \sqrt{1+\frac{1}{B}}$$</li>
<li><span style="color: #ff0000;">自助抽樣將模擬蒙地卡羅的採樣過程，也就是說每一個變數\(x_{i}\)，會依據他們的最大最小值範圍[\(\min(x_{i}),\max(x_{i})\)]，來重新計算標準化後的值。</span></li>
<li>Gap Statistic則是計算在不同k水準值下，觀測與預期值的總群內變異差異，公式如下：<br />
$$Gap(k) =\overline{W_{k}} -{W_{k}}=\frac{1}{B}\sum_{i=1}^B\log (W_{ki}) &#8211; W_{k}$$</li>
<li> 並且我們選擇使Gap(k)最大化的最小的k值，以符合：<br />
$$Gap(k)\geq Gap(k+1)-s_{k+1}$$<br />
即表示<span style="text-decoration: underline;">k群的分群結構</span>和<span style="text-decoration: underline;">虛無假設的uniform分配（沒有分群）</span>有<span style="text-decoration: underline;">很大的差距</span>。</li>
</ul>
<p>我們可以透過cluster套件中的clusGap()函數來計算不同k水平值對應的該統計量(Gap)以及標準誤(Standard Error, SE)。也因為我們要找到使最大化Gap(k)的最小k值，所以記得調整參數method=&#8221;firstmax&#8221;。將結果印出後，我們可看到每個對應k水平值的：</p>
<ul>
<li>logW: 觀測值群內總變異。</li>
<li>E.logW: 群內總變異期望值。</li>
<li>gap: E.logW &#8211; logW統計量。</li>
<li>SE.sim: 標準誤(simulation standard error)。</li>
</ul>
<p></p><pre class="crayon-plain-tag"># compute gap statistic
set.seed(123)
gap_stat &lt;- clusGap(x = inputData, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")

# Clustering Gap statistic ["clusGap"] from call:
# clusGap(x = inputData, FUNcluster = kmeans, K.max = 10, B = 50,     nstart = 25)
# B=50 simulated reference sets, k = 1..10; spaceH0="scaledPCA"
#   --&gt; Number of clusters (method 'firstmax'): 4
#            logW   E.logW       gap     SE.sim
#   [1,] 3.458369 3.638250 0.1798804 0.03653200
#   [2,] 3.135112 3.371452 0.2363409 0.03394132
#   [3,] 2.977727 3.235385 0.2576588 0.03635372
#   [4,] 2.826221 3.120441 0.2942199 0.03615597
#   [5,] 2.738868 3.020288 0.2814197 0.03950085
#   [6,] 2.669860 2.933533 0.2636730 0.03957994
#   [7,] 2.598748 2.855759 0.2570109 0.03809451
#   [8,] 2.531626 2.784000 0.2523744 0.03869283
#   [9,] 2.468162 2.716498 0.2483355 0.03971815
#  [10,] 2.394884 2.652241 0.2573567 0.04104674</pre><p>我們進一步使用fviz_gap_stat()函數將上面gap統計量計算結果視覺化。而根據Gap(k)統計量，最佳分群數為4組。</p><pre class="crayon-plain-tag">fviz_gap_stat(gap_stat)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1333" src="/wp-content/uploads/2018/09/Rplot11-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot11-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot11-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot11-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot11-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot11-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot11-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot11-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h3>總結</h3>
<ul>
<li>影響切割式分群的幾個參數包括：<span style="color: #9f6ad4;">隨機起始中心點的選擇、中心點定義、以及目標式最小化「距離」衡量定義</span>。
<ul>
<li>中心點：
<ul>
<li>kmeans的中心點是群內的平均值，易受離群值影響。</li>
<li>kmedoid的中心點則是實際的點（類似中位數），不易受極端值影響。</li>
</ul>
</li>
<li>目標式：
<ul>
<li>kmeans是以最小化群內各點到中心點的「歐式距離」為目標<br />
(minimize the sum of squared euclidean distances from points to the assigned cluster)</li>
<li>kmedoid是以最小化群內各點至中心點的「變異度」為目標<br />
(minimize the sum of the dissimilarities of the observations to their closest representative object)</li>
</ul>
</li>
</ul>
</li>
<li>因為切割式分群需使用者決定<span style="color: #9f6ad4;">最適的群數</span>，因此必須依據分群目的，綜合考量不同的方法（elbow method, average silhouette width, gap statistic)計算出的不同k值對應的組內變異加總值，來做決定。</li>
<li>由於目標式最小化的標的不同，雖然kmedoid效果會比kmeans更強大，但因為kmedoid需計算n^2的相異度矩陣，會耗費時間和運算記憶體，因此如果遇到大型數據時，可以使用clara()。pam()目前有觀測值上限(<i>n &lt;= 65536)。</i></li>
</ul>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<hr />
<p>更多統計模型筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/partitional-clustering-kmeans-kmedoid/">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/partitional-clustering-kmeans-kmedoid/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
			</item>
		<item>
		<title>Hierarchical Clustering 階層式分群 &#124; Clustering 資料分群 &#124; R 統計</title>
		<link>/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/</link>
					<comments>/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Wed, 05 Sep 2018 07:43:45 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[clustering]]></category>
		<category><![CDATA[hierarchical clustering]]></category>
		<category><![CDATA[分群]]></category>
		<category><![CDATA[分群演算法]]></category>
		<category><![CDATA[資料分群]]></category>
		<category><![CDATA[階層式分群]]></category>
		<guid isPermaLink="false">/?p=1157</guid>

					<description><![CDATA[<p>Hierarchical Clustering, 屬於資料分群的一種方法。資料分群屬於非監督式學習，處理的資料是沒有正確答案/標籤/目標變數可參考的。常見的分群 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Hierarchical Clustering, 屬於資料分群的一種方法。資料分群屬於非監督式學習，處理的資料是沒有正確答案/標籤/目標變數可參考的。常見的分群方法包括著名的kmeans, hierarchical clustering，分別使用不同分群演算邏輯。本篇將介紹階層式分群的實作與特色。</p>
<h3>資料分群簡介</h3>
<ul>
<li>資料分群是一個將資料分割成數個子集合的方法，主要目的包括：
<ul>
<li>找出資料中相近的<span style="color: #9f6ad4;">群聚(clusters)</span>，</li>
<li>找出各群的<span style="color: #9f6ad4;">代表點</span>。代表點可以是群聚中的<span style="color: #9f6ad4;">中心點(centroids)</span>或是<span style="color: #9f6ad4;">原型(prototypes)</span>。</li>
</ul>
</li>
<li>透過各群的代表點，可以達到幾個目標：
<ul>
<li>資料壓縮</li>
<li>降低雜訊</li>
<li>降低計算量</li>
</ul>
</li>
<li> 資料分群將<span style="color: #9f6ad4;">依據資料自身屬性計算彼此間的相似度</span>（物以類聚），而<span style="color: #9f6ad4;">「相似度」</span>主要有兩種型態：
<ul>
<li>「Compactness」：目標是讓子集合間差異最大化，子集合內差異最小化。如階層式分群和K-means分群。</li>
<li>「Connectedness」：目標是將可串連在一起的個體分成一群。如譜分群(Spectral Clustering)。</li>
</ul>
</li>
<li>資料分群屬於<span style="color: #9f6ad4;">非監督式學習法(Unsupervised Learning)</span>，即資料沒有標籤(unlabeled data)或沒有標準答案，<span style="color: #9f6ad4;">無法透過所謂的目標變數(response variable)來做分類之訓練</span>。也因為資料沒有標籤之緣故，與監督式學習法和強化式學習法不同，<span style="color: #9f6ad4;">非監督式學習法無法衡量演算法的正確率</span>。</li>
</ul>
<p><span style="color: #333333;"><span style="text-decoration: underline;">資料分群系列文</span>章會依序介紹以下幾種常見的分群方法：</span></p>
<ol>
<li><a href="/%e7%b5%b1%e8%a8%88-r%e8%aa%9e%e8%a8%80-%e5%88%86%e7%be%a4%e5%88%86%e6%9e%90-clustering-hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a41/">階層式分群(hierarchical clustering)</a>
<ol>
<li>聚合式階層分群法 Agglomerative Hierarchical Clustering</li>
<li>分裂式階層分群法 Divisive Hierarchical Clustering</li>
<li>最佳分群群數(Determining Optimal Clusters)</li>
</ol>
</li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">切割式分群(partitional clustering)</a>
<ol>
<li>K-means</li>
<li>K-medoid</li>
<li>最佳分群群數(Determining Optimal Clusters)</li>
</ol>
</li>
<li>譜分群(Spectral Clustering)</li>
</ol>
<h3>1. 階層式分群(Hierarchical clustering)</h3>
<ul>
<li>階層分群法的概念是在分群中建立分群，並不需要預先設定分群數。</li>
<li>產生的分群結果為一目瞭然的樹狀結構圖（又稱作<span style="color: #9f6ad4;">dendrogram</span>）。</li>
<li>群數(number of clusters)可由大變小(divisive hierarchical clustering)，或是由小變大(agglomerative hierarchical clustering)，透過群聚反覆的分裂和合併後，在選取最佳的群聚數。</li>
<li>階層式分群兩種演算法：
<ul>
<li>當採行<span style="color: #9f6ad4;">聚合法</span>，又稱作AGNES(Agglomerative Nesting)，資料會由樹狀結構的底部開始開始逐次合併 (<span style="color: #9f6ad4;">bottom-up</span>)，在R語言中所使用的函數為<span style="color: #9f6ad4;">hclust()[in stat package]或agnes[in cluster package]</span>，<span style="color: #9f6ad4;">擅於</span><span style="color: #9f6ad4;">處理與識別小規模群聚</span></li>
<li>當採行<span style="color: #9f6ad4;">分裂法，<span style="color: #333333;">又稱作DIANA(Divisive Analysis)</span></span>，資料則會由樹狀結構的頂部開始逐次分裂(<span style="color: #9f6ad4;">top-down</span>)，在R語言中使用的套件為<span style="color: #9f6ad4;">diana()[in cluster package]</span>，<span style="color: #9f6ad4;">擅於處理與識別大規模群聚</span>。</li>
</ul>
</li>
<li>階層分群可被用運用<span style="color: #9f6ad4;">數值與類別</span>資料。</li>
</ul>
<h3><span style="color: #333333;">1-1. 聚合式階層群聚法（AGNES, bottom-up）</span></h3>
<p>我們使用Iris資料集-經典的鳶尾花資料來進行聚合式階層群聚法之說明。(<span style="color: #9f6ad4;">*注意，資料必須先預處理遺失值的部分，比如說使用na.omit(data)。</span>)</p><pre class="crayon-plain-tag">head(iris)

#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
# 1          5.1         3.5          1.4         0.2  setosa
# 2          4.9         3.0          1.4         0.2  setosa
# 3          4.7         3.2          1.3         0.2  setosa
# 4          4.6         3.1          1.5         0.2  setosa
# 5          5.0         3.6          1.4         0.2  setosa
# 6          5.4         3.9          1.7         0.4  setosa</pre><p>因為分群法為非監督式學習法，專處理<span style="color: #9f6ad4;">無標籤(</span>un-labeled<span style="color: #9f6ad4;">)<span style="color: #333333;">或</span>無標準答案</span>的資料分群，故我們將分類結果欄位Species從我們的inputData刪除。</p><pre class="crayon-plain-tag">inputData &lt;- iris[,-5]
head(inputData)

#   Sepal.Length Sepal.Width Petal.Length Petal.Width
# 1          5.1         3.5          1.4         0.2
# 2          4.9         3.0          1.4         0.2
# 3          4.7         3.2          1.3         0.2
# 4          4.6         3.1          1.5         0.2
# 5          5.0         3.6          1.4         0.2
# 6          5.4         3.9          1.7         0.4</pre><p>在進行聚合式群聚法前，我們先簡單介紹幾個hclust()函數中的重要參數：</p>
<ul>
<li>d : 由dist()函數計算出來<span style="color: #9f6ad4;">資料兩兩間的相異度矩陣(dissimilarity matrix)</span>，即兩兩資料間的距離矩陣。</li>
<li>method: <span style="color: #9f6ad4;">群(clusters)</span><strong>聚合</strong>或<strong>連結</strong>的方式。包括：single(單一）、complete（完整）、average（平均）、Ward&#8217;s（華德）和 centroid（中心）等法。其中又以average(平均)聚合方法被認為是最適合的。不同方法對階層分群結果亦有極大影響。</li>
</ul>
<p>R語言hclust()套件中提供<span style="color: #9f6ad4;">群聚距離演算法</span>來<span style="color: #9f6ad4;">衡量兩群聚的不相似度</span>，最常見的幾種演算法如下：</p>
<ul>
<li>單一連結聚合演算法(single-linkage agglomerative algorithm): 群與群的距離定義為不同群聚中<span style="color: #9f6ad4;">最近</span>的兩個點的距離。傾向產生較於緊緻(compact)的群聚。<br />
$$d(C_{i},C_{j})=\min_{a\in C_{i},b\in C_{j}}d(a,b)$$</li>
<li>完整連結聚合演算法(complete-linkage agglomerative algorithm): 群與群的距離定義為不同群聚中<span style="color: #9f6ad4;">最遠</span>的兩個點的距離。傾向產生較於長(long)、鬆散(loose)的群聚。<br />
$$d(C_{i},C_{j})=\max_{a\in C_{i},b\in C_{j}}d(a,b)$$</li>
<li>平均連結聚合演算法(average-linkage agglomerative algorithm): 群與群的距離定義為不同群聚中<span style="color: #9f6ad4;">各點與各點距離總和的平均</span>。<br />
$$d(C_{i},C_{j})=\sum_{a\in C_{i},b\in C_{j}}\frac{d(a,b)}{|C_{i}||C_{j}|}$$<br />
其中，\(|C_{i}|\)和\(|C_{j}|\)分別為群聚\(C_{i}\)和\(C_{j}\)的大小。</li>
<li>中心連結聚合演算法(centroid-linkage agglomerative algorithm): 群與群的距離定義為<span style="color: #9f6ad4;">不同群聚中心點之間的距離</span>。<br />
$$d(A,B)= d(\bar{X}_{A},\bar{X}_{B})=\|\bar{X}_{A}-\bar{X}_{B}\|^2$$</li>
<li>華德最小變異法(Ward&#8217;s Minimum Variance): 最小化各群聚內變異加總(minimize the total within-cluster variance)。主要用來尋找緊湊球型的群聚。<span style="color: #9f6ad4;">反覆比較每對資料<strong>合併後的群內總變異數的增量</strong>，並找增量最小的組別優先合併</span>。越早合併的子集表示其間的相似度越高。<span style="color: #9f6ad4;">而使用華德最小變異法的前提為，初始各點資料距離必須是歐式距離的平方和(Squared Euclidean Distance)</span>。在R套件hclust將參數設為method = &#8220;ward.D2&#8243;(不是&#8221;ward.D&#8221;)。（<a href="https://en.wikipedia.org/wiki/Ward%27s_method">華德法參考文獻</a>）</li>
</ul>
<p>我們先來試試，不同的資料點間距離矩陣計算之效果。即<span style="color: #9f6ad4;">調整dist()中參數method</span>。</p>
<ul>
<li>歐式距離（以二維空間為例）：<span style="color: #9f6ad4;">最常用也是最重要的距離計算法</span>。<br />
$$d(P_{1},P_{2})=\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}$$</li>
<li>曼哈頓距離（以二維空間為例）：<br />
$$d(P_{1},P_{2})=|x_{1}-x_{2}|+|y_{1}-y_{2}|$$</li>
</ul>
<p></p><pre class="crayon-plain-tag"># 由於階層式分群是依據個體間的「距離」來計算彼此的相似度。
# 我們會先使用dist()函數，來計算所有資料個體間的「距離矩陣(Distance Matrix)」
# 而「距離」的算法又有：(1)歐式距離(2)曼哈頓距離

E.dist &lt;- dist(x = inputData, method = "euclidean")
M.dist &lt;- dist(x = inputData, method = "manhattan")

# 讓圖形以一行兩欄的排版方式呈現，如要還原請用dev.off()
par(mfrow=c(1,2))

# 將以上資料間距離作為參數投入階層式分群函數：hclust()
# 使用歐式距離進行分群
h.E.cluster &lt;- hclust(E.dist)
plot(h.E.cluster, xlab="歐式距離",family="黑體-繁 中黑")

# 使用曼哈頓距離進行分群
h.M.cluster &lt;- hclust(M.dist) 
plot(h.M.cluster, xlab="曼哈頓距離", family="黑體-繁 中黑")</pre><p>可發現不同資料距離計算會有不同分群結果。</p>
<ul>
<li><span style="color: #9f6ad4;">Height</span>表示<span style="color: #9f6ad4;">n-1組</span>實數值。各值為<span style="color: #9f6ad4;">使用特定聚合方法計算出來的標準數值</span>。</li>
</ul>
<p><img loading="lazy" class="alignnone size-large wp-image-1221" src="/wp-content/uploads/2018/09/Rplot01-1024x469.jpeg" alt="hierarchical clustering" width="1024" height="469" srcset="/wp-content/uploads/2018/09/Rplot01-1024x469.jpeg 1024w, /wp-content/uploads/2018/09/Rplot01-300x137.jpeg 300w, /wp-content/uploads/2018/09/Rplot01-768x352.jpeg 768w, /wp-content/uploads/2018/09/Rplot01-830x380.jpeg 830w, /wp-content/uploads/2018/09/Rplot01-230x105.jpeg 230w, /wp-content/uploads/2018/09/Rplot01-350x160.jpeg 350w, /wp-content/uploads/2018/09/Rplot01-480x220.jpeg 480w, /wp-content/uploads/2018/09/Rplot01.jpeg 1308w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>比較不同歐式距離搭配不同聚合演算法的分群結果。</p><pre class="crayon-plain-tag">dev.off()
par(mfrow= c(3,2),family="黑體-繁 中黑")
plot(hclust(E.dist, method="single"),xlab = "最近聚合法:single-linkage")   # 最近法
plot(hclust(E.dist, method="complete"), xlab = "最遠聚合法:complete-linkage")  # 最遠法
plot(hclust(E.dist, method="average"), xlab = "平均聚合法: average-linkage")  # 平均法
plot(hclust(E.dist, method="centroid"), xlab = "中心法: centroid-linkage") # 中心法
plot(hclust(E.dist, method="ward.D2"), xlab = "華德法: Ward's Method")  # 華德法</pre><p>從下圖可以發現群聚數結構的幾個特徵：</p>
<ul>
<li>Single-linkage法具有「大者恆大」之效果。</li>
<li>而 complete linkage 和 average linkage 比較具有「齊頭並進」之效果。</li>
</ul>
<p><img loading="lazy" class="alignnone size-large wp-image-1261" src="/wp-content/uploads/2018/09/Rplot07-1024x1024.jpeg" alt="hierarchical clustering" width="1024" height="1024" srcset="/wp-content/uploads/2018/09/Rplot07-1024x1024.jpeg 1024w, /wp-content/uploads/2018/09/Rplot07-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot07-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot07-768x768.jpeg 768w, /wp-content/uploads/2018/09/Rplot07-830x830.jpeg 830w, /wp-content/uploads/2018/09/Rplot07-230x230.jpeg 230w, /wp-content/uploads/2018/09/Rplot07-350x350.jpeg 350w, /wp-content/uploads/2018/09/Rplot07-480x480.jpeg 480w, /wp-content/uploads/2018/09/Rplot07.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>接著，我們聚焦在採用<span style="color: #9f6ad4;">歐式距離</span>搭配華德最小變異聚合演算法。可透過設定<span style="color: #9f6ad4;">hclust()參數method=&#8221;ward.D2&#8243;</span>。</p><pre class="crayon-plain-tag">dev.off()
par(family="黑體-繁 中黑")
plot(hclust(E.dist, method="ward.D2"), xlab = "華德法: Ward's Method")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1236" src="/wp-content/uploads/2018/09/Rplot02_Eu_Ward-1024x683.jpeg" alt="hierarchical clustering" width="1024" height="683" srcset="/wp-content/uploads/2018/09/Rplot02_Eu_Ward-1024x683.jpeg 1024w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-300x200.jpeg 300w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-768x512.jpeg 768w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-830x553.jpeg 830w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-230x153.jpeg 230w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-350x233.jpeg 350w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-480x320.jpeg 480w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>以上結果也可以使用anges()函數來執行。此函數跟hclust()很類似，不同之處是該函數能另外計算<span style="color: #9f6ad4;">聚合係數(agglomerative coefficient)</span>。<span style="color: #9f6ad4;">聚合係數是衡量群聚結構被辨識的程度，聚合係數越接近1代表有堅固的群聚結構(strong clustering structure)</span>。在這個使用歐式距離搭配華德連結演算法的群聚係數有高達99%的表現。</p><pre class="crayon-plain-tag"># Compute with agnes
library(cluster)
hc2 &lt;- agnes(E.dist, method = "ward")
# Agglomerative coefficient
hc2$ac
## [1] 0.9908772</pre><p>我們可以用聚合係數來比較多組分群連結演算法的效果。此範例中又以華德法表現最好。</p><pre class="crayon-plain-tag"># methods to assess
m &lt;- c( "average", "single", "complete", "ward")
names(m) &lt;- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac &lt;- function(x) {
  agnes(E.dist, method = x)$ac
}

map_dbl(m, ac) #Apply a function to each element of a vector 
#   average    single  complete      ward 
# 0.9300174 0.8493364 0.9574622 0.9908772</pre><p>若遇將agnes()產生的樹狀圖繪出可使用函數pltree()。可以發現結果大致上與hclust()結果差不多。</p><pre class="crayon-plain-tag">dev.off()
hc2 &lt;- agnes(E.dist, method = "ward")
pltree(hc2, cex = 0.6, hang = -1, main = "Dendrogram of agnes")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1264" src="/wp-content/uploads/2018/09/Rplot08-1024x614.jpeg" alt="hierarchical clustering" width="1024" height="614" srcset="/wp-content/uploads/2018/09/Rplot08-1024x614.jpeg 1024w, /wp-content/uploads/2018/09/Rplot08-300x180.jpeg 300w, /wp-content/uploads/2018/09/Rplot08-768x461.jpeg 768w, /wp-content/uploads/2018/09/Rplot08-830x498.jpeg 830w, /wp-content/uploads/2018/09/Rplot08-230x138.jpeg 230w, /wp-content/uploads/2018/09/Rplot08-350x210.jpeg 350w, /wp-content/uploads/2018/09/Rplot08-480x288.jpeg 480w, /wp-content/uploads/2018/09/Rplot08.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>對階層分群結果產生的樹狀結構進行<span style="color: #9f6ad4;">截枝</span>可以<span style="color: #9f6ad4;">將觀測值分成數群</span>。<span style="color: #9f6ad4;">截肢方法</span>有兩種：</p>
<ol>
<li>指定所要的分群數: rect.hclust(k=&#8230;)、cutree(k=&#8230;)</li>
<li>指定截枝的位置: rect.hclust(h=&#8230;)</li>
</ol>
<p>比如說，指定將資料分成3群與13群。</p><pre class="crayon-plain-tag">h.E.Ward.cluster &lt;- hclust(E.dist, method="ward.D2")
plot(h.E.Ward.cluster)
rect.hclust(tree =h.E.Ward.cluster, k = 3, border = "red")
rect.hclust(tree =h.E.Ward.cluster, k = 13, border = "blue")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1237" src="/wp-content/uploads/2018/09/Rplot03-1024x683.jpeg" alt="hierarchical clustering" width="1024" height="683" srcset="/wp-content/uploads/2018/09/Rplot03-1024x683.jpeg 1024w, /wp-content/uploads/2018/09/Rplot03-300x200.jpeg 300w, /wp-content/uploads/2018/09/Rplot03-768x512.jpeg 768w, /wp-content/uploads/2018/09/Rplot03-830x553.jpeg 830w, /wp-content/uploads/2018/09/Rplot03-230x153.jpeg 230w, /wp-content/uploads/2018/09/Rplot03-350x233.jpeg 350w, /wp-content/uploads/2018/09/Rplot03-480x320.jpeg 480w, /wp-content/uploads/2018/09/Rplot03.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>比如說，指定截枝高度分別為4和10，可分別將資料分成5組跟3組。</p><pre class="crayon-plain-tag">h.E.Ward.cluster &lt;- hclust(E.dist, method="ward.D2")
plot(h.E.Ward.cluster)
rect.hclust(tree =h.E.Ward.cluster, h = 4, border = "red")
rect.hclust(tree =h.E.Ward.cluster, h = 10, border = "blue")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1238" src="/wp-content/uploads/2018/09/Rplot04-1024x683.jpeg" alt="hierarchical clustering" width="1024" height="683" srcset="/wp-content/uploads/2018/09/Rplot04-1024x683.jpeg 1024w, /wp-content/uploads/2018/09/Rplot04-300x200.jpeg 300w, /wp-content/uploads/2018/09/Rplot04-768x512.jpeg 768w, /wp-content/uploads/2018/09/Rplot04-830x553.jpeg 830w, /wp-content/uploads/2018/09/Rplot04-230x153.jpeg 230w, /wp-content/uploads/2018/09/Rplot04-350x233.jpeg 350w, /wp-content/uploads/2018/09/Rplot04-480x320.jpeg 480w, /wp-content/uploads/2018/09/Rplot04.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>如果要將資料標記上分群結果，可使用cutree()，並指定參數k為所欲截枝群樹。</p><pre class="crayon-plain-tag">h.E.Ward.cluster &lt;- hclust(E.dist, method="ward.D2")
cut.h.cluster &lt;- cutree(tree = h.E.Ward.cluster, k = 3)
cut.h.cluster

#   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2
#  [56] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3
# [111] 3 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 2 3 3 3 2 3 3 3 2 3 3 3 2 3 3 2</pre><p>分群結果和實際結果比較。</p><pre class="crayon-plain-tag">table(cut.h.cluster, iris$Species)

# cut.h.cluster setosa versicolor virginica
#             1     50          0         0
#             2      0         49        15
#             3      0          1        35</pre><p>將資料分群的混淆矩陣視覺化。</p><pre class="crayon-plain-tag">plot(table(iris$Species, cut.h.cluster), main = "Confusion Matrix for Species Clustering", xlab = "Species", ylab = "Cluster")</pre><p>可以發現，屬於setosa種類的都被很好的分到第一群，屬於versicolor主要被分到第二群，但virginica似乎沒有分得很好。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1257" src="/wp-content/uploads/2018/09/Rplot05.jpeg" alt="hierarchical clustering" width="800" height="722" srcset="/wp-content/uploads/2018/09/Rplot05.jpeg 800w, /wp-content/uploads/2018/09/Rplot05-300x271.jpeg 300w, /wp-content/uploads/2018/09/Rplot05-768x693.jpeg 768w, /wp-content/uploads/2018/09/Rplot05-230x208.jpeg 230w, /wp-content/uploads/2018/09/Rplot05-350x316.jpeg 350w, /wp-content/uploads/2018/09/Rplot05-480x433.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>如果回頭看一下原始資料分佈長相，可以發現versicolor和virginica其實部分資料靠得很近，因此兩種Species被分到第二和第三群也是合理的。</p><pre class="crayon-plain-tag">ggplot(data = iris,mapping = aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(aes(col = Species))</pre><p><img loading="lazy" class="alignnone size-full wp-image-1247" src="/wp-content/uploads/2018/09/Rplot06.jpeg" alt="hierarchical clustering" width="800" height="722" srcset="/wp-content/uploads/2018/09/Rplot06.jpeg 800w, /wp-content/uploads/2018/09/Rplot06-300x271.jpeg 300w, /wp-content/uploads/2018/09/Rplot06-768x693.jpeg 768w, /wp-content/uploads/2018/09/Rplot06-230x208.jpeg 230w, /wp-content/uploads/2018/09/Rplot06-350x316.jpeg 350w, /wp-content/uploads/2018/09/Rplot06-480x433.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>1-2. 分裂式階層群聚法（DIANA, top-down）</p>
<p>我們使用R內建的USArrests資料集來進行說明。<span style="color: #9f6ad4;">進行分群前，我們先將資料預處理，包括遺失值忽略以及標準化(0~1)</span>。</p><pre class="crayon-plain-tag">head(USArrests)
#            Murder Assault UrbanPop Rape
# Alabama      13.2     236       58 21.2
# Alaska       10.0     263       48 44.5
# Arizona       8.1     294       80 31.0
# Arkansas      8.8     190       50 19.5
# California    9.0     276       91 40.6
# Colorado      7.9     204       78 38.7

inputData &lt;- 
  USArrests %&gt;% 
  na.omit() %&gt;% 
  scale() # scaling/standardizing the data</pre><p>diana()函數運作方式很類似agnes()，<span style="color: #9f6ad4;">只差diana沒有method參數</span>。</p><pre class="crayon-plain-tag"># compute divisive hierarchical clustering
diana_clust &lt;- diana(inputData)

# Divise coefficient; amount of clustering structure found
diana_clust$dc
## [1] 0.8514345

# plot dendrogram
pltree(diana_clust, cex = 0.6, hang = -1, main = "Dendrogram of diana")</pre><p>由分裂式階層分群演算法產出的樹狀圖結果如下：</p>
<ul>
<li>葉節點（末梢節點）代表個別資料點。</li>
<li>垂直座標軸的Height代表群聚間的(不)相似度((dis)similarity)， 群聚的高度(height)越高，代表觀測值間越不相似（組內變異越大）。<span style="text-decoration: underline;">需要注意的是，要總結兩個觀測值的相似度，只能用<span style="color: #9f6ad4; text-decoration: underline;">兩觀測值何時被第一次合併的群聚高度</span>來做判斷，而不能以水平軸的距離來評估</span>。</li>
</ul>
<p><img loading="lazy" class="alignnone size-large wp-image-1266" src="/wp-content/uploads/2018/09/Rplot09-1024x683.jpeg" alt="hierarchical clustering" width="1024" height="683" srcset="/wp-content/uploads/2018/09/Rplot09-1024x683.jpeg 1024w, /wp-content/uploads/2018/09/Rplot09-300x200.jpeg 300w, /wp-content/uploads/2018/09/Rplot09-768x512.jpeg 768w, /wp-content/uploads/2018/09/Rplot09-830x553.jpeg 830w, /wp-content/uploads/2018/09/Rplot09-230x153.jpeg 230w, /wp-content/uploads/2018/09/Rplot09-350x233.jpeg 350w, /wp-content/uploads/2018/09/Rplot09-480x320.jpeg 480w, /wp-content/uploads/2018/09/Rplot09.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>另外，我們可以透過縱軸的群聚高度來決定分群數目。我們可以使用cutree()函數來得到各觀測值被分類到的群。</p><pre class="crayon-plain-tag"># Cut diana() tree into 4 groups
diana_clust &lt;- diana(inputData)
group &lt;- cutree(diana_clust, k = 4)
group
# [1] 1 2 2 3 2 2 3 3 2 1 3 4 2 3 4 3 4 1 4 2 3 2 4 1 2 4 4 2 4 3 2 2 1 4 3 3 3 3 3 1 4 1 2 3 4 3 3 4 4 3</pre><p>我們使用factoextra套件中的fviz_clusterc函數來將分群結果視覺化。</p><pre class="crayon-plain-tag">library(factoextra)
fviz_cluster(list(data = inputData, cluster = group))</pre><p><img loading="lazy" class="alignnone size-full wp-image-1267" src="/wp-content/uploads/2018/09/Rplot10.jpeg" alt="hierarchical clustering" width="800" height="796" srcset="/wp-content/uploads/2018/09/Rplot10.jpeg 800w, /wp-content/uploads/2018/09/Rplot10-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot10-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot10-768x764.jpeg 768w, /wp-content/uploads/2018/09/Rplot10-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot10-350x348.jpeg 350w, /wp-content/uploads/2018/09/Rplot10-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h3>1-3. 決定階層式分群最佳群聚數</h3>
<h4>1-3-1. Elbow Method</h4>
<p>使用函數fviz_nbclust()，並將參數FUN指定為hcut(表階層式分群法)。wss代表組內平方誤差。</p><pre class="crayon-plain-tag">fviz_nbclust(inputData, FUN = hcut, method = "wss")</pre><p>由結果圖可知根據Elbow法，最佳分群數目為4群。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1270" src="/wp-content/uploads/2018/09/Rplot11.jpeg" alt="hierarchical clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot11.jpeg 800w, /wp-content/uploads/2018/09/Rplot11-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot11-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot11-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot11-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot11-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot11-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h4>1-3-2. Average Silhouette Method</h4>
<p>將method參數改為silhouette。</p><pre class="crayon-plain-tag">fviz_nbclust(x = inputData,FUNcluster = hcut, method = "silhouette")</pre><p>根據Average Silhouette Width，最佳分群數目為2 群。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1271" src="/wp-content/uploads/2018/09/Rplot12.jpeg" alt="hierarchical clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot12.jpeg 800w, /wp-content/uploads/2018/09/Rplot12-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot12-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot12-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot12-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot12-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot12-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h4>1-3-3. Gap Statistic Method</h4>
<p>使用clusGap()函數。</p><pre class="crayon-plain-tag">gap_stat &lt;- clusGap(x = inputData,FUNcluster = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)</pre><p>根據Gap(k)統計量，最佳分群數為3群。(＊\(Gap(k)\geq Gap(k+1) &#8211; s_{k+1}\))</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1272" src="/wp-content/uploads/2018/09/Rplot13.jpeg" alt="hierarchical clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot13.jpeg 800w, /wp-content/uploads/2018/09/Rplot13-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot13-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot13-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot13-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot13-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot13-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h3>總結</h3>
<ul>
<li>階層分群主要受：<span style="text-decoration: underline;">資料個體兩兩間距離矩陣衡量方法</span>與<span style="text-decoration: underline;">群與群連結方法所影響</span>。</li>
<li> 階層分群優點包括：
<ul>
<li>概念簡單且樹狀圖(<span style="color: #9f6ad4;">dendrogram</span>)結果一目瞭然。</li>
<li>只需要資料點間距離即可產出分群結果。</li>
<li>可以應用在數值或<span style="color: #9f6ad4;">類別</span>資料。</li>
</ul>
</li>
<li><span style="color: #9f6ad4;">但缺點就是運算速度，以及不適用處理大量資料</span>。運算速度方面，可以考慮其他R套件如fastcluster中的hclust函數，執行上會比一般hclust更有效率。</li>
</ul>
<hr />
<p>更多統計模型筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考:</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/feed/</wfw:commentRss>
			<slash:comments>6</slash:comments>
		
		
			</item>
		<item>
		<title>Tree Surrogate &#124; Tree Surrogate Variables in CART &#124; R 統計</title>
		<link>/decision-tree-surrogate-in-cart/</link>
					<comments>/decision-tree-surrogate-in-cart/#respond</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Sun, 02 Sep 2018 14:33:42 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[tree surrogate]]></category>
		<category><![CDATA[樹替代]]></category>
		<category><![CDATA[決策樹空值填補]]></category>
		<category><![CDATA[空值填補]]></category>
		<category><![CDATA[空值預測]]></category>
		<guid isPermaLink="false">/?p=1039</guid>

					<description><![CDATA[<p>Tree Surrogate 樹替代是決策樹CART演算法裡面內建的處理遺失值的一個很棒的演算法。只要資料列有目標變數搭配只少一個未遺失的特徵值，即可進行遺失值 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-surrogate-in-cart/">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Tree Surrogate 樹替代是決策樹CART演算法裡面內建的處理遺失值的一個很棒的演算法。只要資料列有目標變數搭配只少一個未遺失的特徵值，即可進行遺失值的預測，計算預測遺失值的surrogate variable list來作為替代值順位。</p>
<h3>CART決策樹模型遺失值預測法 Tree Surrogate 簡介</h3>
<ul>
<li>在使用rpart()(Recursive Partitioning Tree)建模時，我們不用特別針對遺失值進行處理。</li>
<li>只要觀測值有y值，和至少一個未遺失的x值，則可投入模型。</li>
<li>在遇到觀測資料有遺失值的情況，會使用Surrogate Variable List中的代理變數順位最高者來預測遺失變數值</li>
</ul>
<h3>Tree Surrogate Variables</h3>
<table>
<colgroup>
<col />
<col />
<col />
<col />
<col /></colgroup>
<tbody>
<tr bgcolor="#ddd">
<td class="">
<div>ID</div>
</td>
<td class="">
<div>A</div>
</td>
<td class="">
<div>B</div>
</td>
<td class="">
<div><span style="color: #0000ff;">C</span></div>
</td>
<td class="">
<div>T</div>
</td>
</tr>
<tr>
<td class="">
<div>1</div>
</td>
<td class="">
<div>T</div>
</td>
<td class="">
<div>?</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>Y</div>
</td>
</tr>
<tr>
<td class="">
<div>2</div>
</td>
<td class="">
<div>?</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>?</div>
</td>
<td class="">
<div>N</div>
</td>
</tr>
<tr>
<td class="">
<div>3</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>Y</div>
</td>
</tr>
<tr>
<td class="">
<div>4</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>T</div>
</td>
<td class="">
<div>T</div>
</td>
<td class="">
<div>N</div>
</td>
</tr>
</tbody>
</table>
<p>其中：</p>
<ul>
<li>T欄為目標變數</li>
<li>A,B,C欄為預測變數</li>
<li>而根據Gini Index，C會被選中為決策樹變數</li>
</ul>
<p>對於C欄位有遺失值的觀測資料列，CART決策樹會使用計算出來的代理變數(Surrogate Variables)來預測C值。</p>
<p>計算C欄位Surrogate Variables的方法如下：依據計算除了C以外的預測變數作為Surrogate Variables的錯誤率並比較排序。</p>
<p>Step 1: C is missing but B has value</p>
<div>首先計算B作為Surrogate Variable的錯誤率：</div>
<div></div>
<table>
<colgroup>
<col />
<col /></colgroup>
<tbody>
<tr bgcolor="#ddd">
<td class="">
<div>B</div>
</td>
<td class="">
<div>C</div>
</td>
</tr>
<tr>
<td class="">
<div>F</div>
</td>
<td class="">
<div>F</div>
</td>
</tr>
<tr>
<td class="">
<div>T</div>
</td>
<td class="">
<div>T</div>
</td>
</tr>
</tbody>
</table>
<div></div>
<div>使用決策樹模型產生用B預測C的邏輯：</div>
<ul>
<li>
<div>B=T -&gt; C=T</div>
</li>
<li>
<div>B=F -&gt; C=F</div>
</li>
<li>
<div>Error rate = 0%</div>
</li>
</ul>
<div>
<div>Step 2: C is missing but A has value</div>
</div>
<div>計算Ａ作為Surrogate Variable預測C值得錯誤率：</div>
<div></div>
<div>
<table>
<colgroup>
<col />
<col /></colgroup>
<tbody>
<tr bgcolor="#ddd">
<td class="">A</td>
<td class="">C</td>
</tr>
<tr>
<td class="">
<div>T</div>
</td>
<td class="">
<div>F</div>
</td>
</tr>
<tr>
<td class="">
<div>F</div>
</td>
<td class="">
<div>F</div>
</td>
</tr>
<tr>
<td class="">
<div>F</div>
</td>
<td class="">
<div>T</div>
</td>
</tr>
</tbody>
</table>
<p>使用決策樹模型產生用A預測C的邏輯：</p>
<ul>
<li>
<div>A=T -&gt; C=F</div>
</li>
<li>
<div>A=F -&gt; C=T</div>
</li>
<li>
<div>Erro rate = 33%</div>
</li>
</ul>
<div>
<div>總結以上，預測C欄位各Surrogate Variables的預測錯誤率分別為：</div>
</div>
<ul>
<li>
<div>A 有33% 的預測錯誤率</div>
</li>
<li>
<div>B 有0％的預測錯誤率</div>
</li>
<li>
<div>Naive隨機則有50%的預測錯誤率 (blind rule)</div>
</li>
<li>若錯誤率比Naive隨機預測結果還高者，會從Surrogate Variables list給移除</li>
</ul>
<div>因此，Surrogate Variables的順位為：B,A,Naive</div>
<ul>
<li>
<div>以ID2為例，C值會被指派為（根據B）F</div>
</li>
</ul>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>更多rpart()函數對surrogate variable說明可參考：<a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf" target="_blank" rel="noopener noreferrer">https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf</a></p>
<hr />
<p>更多統計模型筆記連結：</p>
<p><a href="/linear_regression_using_airquality_dataset/" target="_blank" rel="noopener noreferrer">線性回歸模型</a></p>
<p><a href="/logistic_regression_part1/" target="_blank" rel="noopener noreferrer">羅吉斯回歸模型part1</a></p>
<p><a href="/logistic_regression_part2/" target="_blank" rel="noopener noreferrer">羅吉斯回歸模型part2</a></p>
<p style="text-align: left;"><a href="/decision_tree_cart/" target="_blank" rel="noopener noreferrer">決策樹/隨機森林</a></p>
<p><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">random forests</a></p>
<p><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines (GBMs)</a></p>
<p><a href="/hierarchical-clustering/" target="_blank" rel="noopener noreferrer">階層式分群法</a></p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">分割式分群法</a></p>
</div>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-surrogate-in-cart/">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/decision-tree-surrogate-in-cart/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Decision Tree 決策樹 &#124; CART, Conditional Inference Tree, RandomForest</title>
		<link>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/</link>
					<comments>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Fri, 31 Aug 2018 06:08:33 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[cart]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[prune]]></category>
		<category><![CDATA[random forest]]></category>
		<category><![CDATA[rpart]]></category>
		<category><![CDATA[tree surrogate]]></category>
		<category><![CDATA[決策樹]]></category>
		<category><![CDATA[隨機森林]]></category>
		<guid isPermaLink="false">/?p=1026</guid>

					<description><![CDATA[<p>Decision Tree 決策樹模型是一個不受資料分配限制的模型，模型結果以樹狀呈現，簡單易懂，解釋性極高，且模型同時兼具變數挑選與遺失值填補的機制，並能處理 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Decision Tree 決策樹模型是一個不受資料分配限制的模型，模型結果以樹狀呈現，簡單易懂，解釋性極高，且模型同時兼具變數挑選與遺失值填補的機制，並能處理分類與回歸問題，是一個廣泛被使用的模型。另外，以決策樹為基礎集成學習而成的隨機森林，更能有效降低模型的錯誤率與並解決過度配適等問題的著名機器學習法之一。</p>
<h3>Decision Tree 決策樹簡介</h3>
<ul>
<li><span style="color: #9f6ad4;">決策樹</span>是一個多功能的機器學習演算法，不僅可以進行<span style="color: #9f6ad4;">分類</span>亦可進行<span style="color: #9f6ad4;">回歸</span>任務。</li>
<li>可以配適複雜的資料集，是個強大的演算法。</li>
<li>屬於<span style="color: #9f6ad4;">無母數回歸</span>方法(<span style="color: #9f6ad4;">non-parametric</span>)：對資料長相的要求不像回歸模型（有母數法，parametric）嚴格，不需要假設資料的線性關係與常態分佈。<br />
(無母數介紹請參考)</li>
<li>決策樹演算法也是隨機森林演算法的基礎(隨機森林也是至今具潛力的演算法之一)。</li>
<li>有諸多演算法，常見的包括CART, CHAID。</li>
<li>決策樹可以用來建立非線性模型，通常被用在迴歸，也可以用在對於遞迴預測變數最二元分類。</li>
</ul>
<h4>補充-無母數統計：</h4>
<ol>
<li>適用於母體分佈情況未知、小樣本、母體分佈不為常態或不易轉換為常態，對資料長相的要求小。</li>
<li>無母數統計推論時所使用的<span style="color: #9f6ad4;">樣本統計量</span>分配通常與母體分配無關，不需要使用樣本統計量去推論母體中位數、適合度、獨立性、隨機性。</li>
<li>無母數統計又稱作「不受分配限制統計法」(distribution-free)。</li>
</ol>
<h4>常見的決策樹演算法比較</h4>
<table style="height: 116px; width: 100%; border-collapse: collapse; background-color: #ffffff;" border="1">
<tbody>
<tr style="height: 23px;" bgcolor="#ddd">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">演算法</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">資料屬性</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">分割規則</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">修剪樹規則</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">ID3</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;">
<div><span style="color: #000000;">Entropy,</span></div>
<div><span style="color: #000000;">Gain Ratio</span></div>
</td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Predicted Error Rate</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">C4.5</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Gain Ratio</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Predicted Error Rate</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">CHAID</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;">
<div><span style="color: #000000;">Chi-Square Test</span></div>
</td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">No Pruning</span></td>
</tr>
<tr style="height: 24px;">
<td style="width: 25%; height: 24px;"><span style="color: #000000;">CART</span></td>
<td style="width: 25%; height: 24px;"><span style="color: #000000;">離散與連續型</span></td>
<td style="width: 25%; height: 24px;"><span style="color: #000000;">Gini Index</span></td>
<td class="" style="width: 25%; height: 24px;">
<div><span style="color: #000000;">Entire Error Rate</span></div>
<div><span style="color: #000000;">(Training and Predicted)</span></div>
</td>
</tr>
</tbody>
</table>
<h4>決策樹挑選變數常用的測量值</h4>
<p>常見的資訊量（衡量資料<span style="color: #9f6ad4;">純度</span>）：</p>
<ul>
<li><strong>Entropy (熵)</strong>:<br />
$$I_{H}(t)=-\sum_{i=1}^c p(i|t) \log_{2} p(i|t)$$<br />
其中，H代表Homogeneity(同質性)。<br />
<span style="color: #9f6ad4;">當Entropy=0表示completely homogeneous(pure)，而當Entropy=1則表示資料為50%-50%之組成，是不純的</span><span style="color: #9f6ad4;">(impurity)</span>。</li>
<li><strong>Gini Impurity (Gini不純度)</strong>:<br />
$$I_{G}(t) =\sum_{i=1}^c p(i|t)(1-p(i|t)) = 1-\sum_{i=1}^c p(i|t)^2$$<br />
其中，G則代表Gini Impurity。</li>
</ul>
<p>決定切割點的測量值：</p>
<ul>
<li><strong>Information Gain (資訊增益)</strong>: 則衡量節點切割前後資料純度的變化。<span style="color: #9f6ad4;">節點的選擇，當選IG值越大的變數為佳</span>。<br />
$$IG = Info(D) &#8211; Info_{A}(D)$$<br />
其中，\(Info(D)\)為原始資料純度，而\(Info_{\space A}(D)\)則表示使用A規則切割後的資料純度。<br />
$$Info_{\space A}(D)=\sum_{j=1}^m \frac{N_{j}}{N_{p}} Info(D_{j})$$<br />
當m=2，即為二元分類時，<br />
$$IG = Info(D) &#8211; \frac{N_{left}}{N_{p}} Info(D_{left}) &#8211; \frac{N_{right}}{N_{p}} Info(D_{right})$$</li>
</ul>
<h4>資料與分析問題</h4>
<ul>
<li>Data: 鐵達尼資料集包含13個變數與1309筆觀測值。</li>
<li>Problem: 我們想分析與預測具有什麼樣特徵的乘客，比較有機會在冰山撞船後可以存活下來。</li>
<li>Method: 使用CART(Classification and Regression Tree)決策樹模型來找出重要解釋變數。</li>
</ul>
<h4>訓練與視覺化決策樹，我們將進行以下步驟：</h4>
<ol>
<li>載入資料</li>
<li>資料探勘</li>
<li>資料前處理</li>
<li>產生訓練與測試資料集</li>
<li>建置模型</li>
<li>進行預測</li>
<li>衡量模型表現</li>
<li>修剪樹(Post-pruning)</li>
<li>K-Fold Cross Validation</li>
<li>模型比較(1)：條件推論樹(Conditional Inference Tree)</li>
<li>模型比較(2)：隨機森林(Random Forest)</li>
</ol>
<h3>Step1: 載入資料</h3>
<p></p><pre class="crayon-plain-tag"># 從google drive shareable link 讀入csv檔案
# https://drive.google.com/file/d/1S7S-siBGkMR3YUVAbaTkfS1CxOji_Ngd/view?usp=sharing
id &lt;- "1S7S-siBGkMR3YUVAbaTkfS1CxOji_Ngd" # google file ID
inputData &lt;- read.csv(sprintf("https://docs.google.com/uc?id=%s&amp;export=download", id))
head(inputData)

#   pclass survived                                            name    sex     age sibsp parch ticket     fare
# 1      1        1                   Allen, Miss. Elisabeth Walton female 29.0000     0     0  24160 211.3375
# 2      1        1                  Allison, Master. Hudson Trevor   male  0.9167     1     2 113781 151.5500
# 3      1        0                    Allison, Miss. Helen Loraine female  2.0000     1     2 113781 151.5500
# 4      1        0            Allison, Mr. Hudson Joshua Creighton   male 30.0000     1     2 113781 151.5500
# 5      1        0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000     1     2 113781 151.5500
# 6      1        1                             Anderson, Mr. Harry   male 48.0000     0     0  19952  26.5500
#     cabin embarked                       home.dest
# 1      B5        S                    St Louis, MO
# 2 C22 C26        S Montreal, PQ / Chesterville, ON
# 3 C22 C26        S Montreal, PQ / Chesterville, ON
# 4 C22 C26        S Montreal, PQ / Chesterville, ON
# 5 C22 C26        S Montreal, PQ / Chesterville, ON
# 6     E12        S                    New York, NY

tail(inputData)
#      pclass survived                      name    sex  age sibsp parch ticket    fare cabin embarked home.dest
# 1304      3        0     Yousseff, Mr. Gerious   male   NA     0     0   2627 14.4583              C          
# 1305      3        0      Zabour, Miss. Hileni female 14.5     1     0   2665 14.4542              C          
# 1306      3        0     Zabour, Miss. Thamine female   NA     1     0   2665 14.4542              C          
# 1307      3        0 Zakarian, Mr. Mapriededer   male 26.5     0     0   2656  7.2250              C          
# 1308      3        0       Zakarian, Mr. Ortin   male 27.0     0     0   2670  7.2250              C          
# 1309      3        0        Zimmerman, Mr. Leo   male 29.0     0     0 315082  7.8750              S</pre><p>我們可以發現數據是經過排列過的，因為這樣會嚴重影響到我們後續隨機產生訓練與測試資料集，所以我們必須將資料重新隨機排列。</p>
<p>使用sample()隨機產生一組數列index。</p><pre class="crayon-plain-tag">shuffle_index &lt;- sample(x = 1:nrow(inputData))
head(shuffle_index)</pre><p>並將隨機數列index指派給titanic資料集。即可觀察到資料已無排序。</p><pre class="crayon-plain-tag">inputData &lt;- inputData[shuffle_index,]
head(inputData)

#      pclass survived                                   name    sex age sibsp parch     ticket   fare cabin
# 632       3        0            Andersson, Mr. Johan Samuel   male  26     0     0     347075  7.775      
# 526       2        0                       Pain, Dr. Alfred   male  23     0     0     244278 10.500      
# 822       3        0              Goldsmith, Mr. Frank John   male  33     1     1     363291 20.525      
# 485       2        1           Lemore, Mrs. (Amelia Milley) female  34     0     0 C.A. 34260 10.500   F33
# 627       3        0 Andersson, Miss. Ida Augusta Margareta female  38     4     2     347091  7.775      
# 1183      3        1       Salkjelsvik, Miss. Anna Kristine female  21     0     0     343120  7.650      
#      embarked                         home.dest
# 632         S                      Hartford, CT
# 526         S                      Hamilton, ON
# 822         S Strood, Kent, England Detroit, MI
# 485         S                       Chicago, IL
# 627         S      Vadsbro, Sweden Ministee, MI
# 1183        S</pre><p></p>
<h3>Step2: 資料探勘</h3>
<p>使用summary()摘要基礎統計。</p><pre class="crayon-plain-tag">summary(inputData)

#        pclass         survived                                   name          sex           age         
# Min.   :1.000   Min.   :0.000   Connolly, Miss. Kate            :   2   female:466   Min.   : 0.1667  
# 1st Qu.:2.000   1st Qu.:0.000   Kelly, Mr. James                :   2   male  :843   1st Qu.:21.0000  
# Median :3.000   Median :0.000   Abbing, Mr. Anthony             :   1                Median :28.0000  
# Mean   :2.295   Mean   :0.382   Abbott, Master. Eugene Joseph   :   1                Mean   :29.8811  
# 3rd Qu.:3.000   3rd Qu.:1.000   Abbott, Mr. Rossmore Edward     :   1                3rd Qu.:39.0000  
# Max.   :3.000   Max.   :1.000   Abbott, Mrs. Stanton (Rosa Hunt):   1                Max.   :80.0000  
#                                 (Other)                         :1301                NA's   :263      
#          sibsp            parch            ticket          fare                     cabin      embarked
# Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000                  :1014    :  2   
# 1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896   C23 C25 C27    :   6   C:270   
# Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454   B57 B59 B63 B66:   5   Q:123   
# Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295   G6             :   5   S:914   
# 3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275   B96 B98        :   4           
# Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329   C22 C26        :   4           
#                                  (Other) :1261   NA's   :1         (Other)        : 271           
#                home.dest  
# :564  
# New York, NY        : 64  
# London              : 14  
# Montreal, PQ        : 10  
# Cornwall / Akron, OH:  9  
# Paris, France       :  9  
# (Other)             :639</pre><p>使用str()查看資料結構。</p><pre class="crayon-plain-tag">str(inputData)
# 'data.frame':	1309 obs. of  12 variables:
# $ pclass   : int  3 2 3 2 3 3 1 3 3 2 ...
# $ survived : int  0 0 0 1 0 1 1 0 1 1 ...
# $ name     : Factor w/ 1307 levels "Abbing, Mr. Anthony",..: 41 920 459 703 36 1068 864 949 1092 516 ...
# $ sex      : Factor w/ 2 levels "female","male": 2 2 2 1 1 1 1 2 1 1 ...
# $ age      : num  26 23 33 34 38 21 23 NA NA 7 ...
# $ sibsp    : int  0 0 1 0 4 0 1 0 0 0 ...
# $ parch    : int  0 0 1 0 2 0 0 0 0 2 ...
# $ ticket   : Factor w/ 929 levels "110152","110413",..: 453 194 584 767 468 410 574 415 388 783 ...
# $ fare     : num  7.78 10.5 20.52 10.5 7.78 ...
# $ cabin    : Factor w/ 187 levels "","A10","A11",..: 1 1 1 183 1 1 134 1 1 1 ...
# $ embarked : Factor w/ 4 levels "","C","Q","S": 4 4 4 4 4 4 2 4 3 4 ...
# $ home.dest: Factor w/ 370 levels "","?Havana, Cuba",..: 154 150 317 64 345 1 191 1 1 165 ...</pre><p>我們可初步發現：</p>
<ol>
<li>pclass(座艙等級)和survuved(生存與否)應由int轉換成factor變數</li>
<li><span style="color: #9f6ad4;">類別水準數過多</span>的變數：<span style="color: #9f6ad4;">name</span>(1307 levels),<span style="color: #9f6ad4;">ticket</span>(929 levels), <span style="color: #9f6ad4;">cabin</span>(187 levels), <span style="color: #9f6ad4;">home.dest</span>(370 levels)<span style="color: #9f6ad4;">應予以排除</span>。</li>
<li>排除以上變數後，存在許多遺失值(NA value)的變數有：age(263), fare(1)。<span style="color: #9f6ad4;">但由於CART決策樹rpart()演算法中，預設會刪除y遺失的資料列，並保留至少有一個預測變數未遺失的觀察資料列，並使用Surrogate Variables來預測遺失特徵值。因此我們不會特別處理遺失值的部分</span>。(*更多決策樹遺失值預測請參考<a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate in CART</a>)</li>
</ol>
<h3>Step3: 資料前處理</h3>
<p>根據資料探勘結果，要處理的項目如下：</p>
<ol>
<li>移除變數<span style="color: #9f6ad4;">name</span>(1307 levels),<span style="color: #9f6ad4;">ticket</span>(929 levels), <span style="color: #9f6ad4;">cabin</span>(187 levels), <span style="color: #9f6ad4;">home.dest</span></li>
<li>將變數pclass(座艙等級)和survuved(生存與否)轉換為factor變數。</li>
</ol>
<p></p><pre class="crayon-plain-tag">library(dplyr)

clean_inputData &lt;- 
  inputData %&gt;% 
  # Drop variables
  select(-c(home.dest, cabin, name, ticket)) %&gt;% 
  #Convert to factor level
  mutate(pclass = factor(pclass, levels = c(1, 2, 3), labels = c('Upper', 'Middle', 'Lower')),
         survived = factor(survived, levels = c(0, 1), labels = c('No', 'Yes')))

glimpse(clean_inputData)

# Observations: 1,309
# Variables: 8
# $ pclass   &lt;fct&gt; Lower, Middle, Lower, Middle, Lower, Lower, Upper, Lower, Lower, Middle, Lower, Upper, Upper, Middle, Lower, Lower, Lower, Upper, Lower, Lower,...
# $ survived &lt;fct&gt; No, No, No, Yes, No, Yes, Yes, No, Yes, Yes, No, Yes, No, Yes, No, No, No, Yes, No, Yes, No, Yes, Yes, Yes, No, Yes, No, No, No, Yes, No, Yes, ...
# $ sex      &lt;fct&gt; male, male, male, female, female, female, female, male, female, female, male, female, male, female, male, male, female, female, male, female, m...
# $ age      &lt;dbl&gt; 26.0, 23.0, 33.0, 34.0, 38.0, 21.0, 23.0, NA, NA, 7.0, 1.0, 16.0, 58.0, 24.0, 33.0, NA, NA, 36.0, 36.0, 19.0, 19.0, 25.0, 51.0, 4.0, 40.0, 18.0...
# $ sibsp    &lt;int&gt; 0, 0, 1, 0, 4, 0, 1, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 8, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ parch    &lt;int&gt; 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 3, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ fare     &lt;dbl&gt; 7.7750, 10.5000, 20.5250, 10.5000, 7.7750, 7.6500, 113.2750, 7.7750, 7.7792, 26.2500, 46.9000, 57.9792, 113.2750, 27.0000, 7.7750, 8.0500, 7.75...
# $ embarked &lt;fct&gt; S, S, S, S, S, S, C, S, Q, S, S, C, C, S, S, S, Q, C, S, S, S, C, S, S, S, C, S, S, S, S, S, C, C, S, S, S, S, S, Q, S, S, S, S, Q, Q, S, C, S,...</pre><p></p>
<h3>Step4: 產生訓練與測試資料集</h3>
<p>為了確保兩組資料集中生還比例不要差異太大，我們會先將資料依據目標變數(survived)分成兩組(No, Yes)，再進行隨機切割成80%訓練組跟20%測試組。</p><pre class="crayon-plain-tag">input_ones &lt;- clean_inputData[which(clean_inputData$survived == 'Yes'),]
input_zeros &lt;- clean_inputData[which(clean_inputData$survived == 'No'), ]
set.seed(100)
input_ones_training_row &lt;- sample(1:nrow(input_ones),0.8*nrow(input_ones))
input_zeros_training_row &lt;- sample(1:nrow(input_zeros),0.8*nrow(input_zeros))

training_ones &lt;- input_ones[input_ones_training_row,]
training_zeros &lt;- input_zeros[input_zeros_training_row,]
trainingData &lt;- rbind(training_ones, training_zeros)

# 產生測試資料集
test_ones &lt;- input_ones[-input_ones_training_row,]
test_zeros &lt;- input_zeros[-input_zeros_training_row,]
testData &lt;- rbind(test_ones, test_zeros)</pre><p>檢查切割完的資料集大小與目標變數的分佈比例：</p>
<ul>
<li>原始資料列1309被隨機切割為80%訓練資料集(1047筆)與20%測試資料集(262筆)。</li>
<li>發現訓練及測試資料集的目標變數survived比例都是38%。差異在1%以內。</li>
</ul>
<p></p><pre class="crayon-plain-tag">dim(trainingData)
# [1] 1047    8
dim(testData)
# [1] 262   8

# 確認兩資料是隨機的
prop.table(table(trainingData$survived))
#        No       Yes 
# 0.6179561 0.3820439 

prop.table(table(testData$survived))
#        No       Yes 
# 0.6183206 0.3816794</pre><p></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step5: 建置模型</h3>
<p>我們使用CART(Classification and Regression Tree)決策樹演算法-rpart()。</p>
<ul>
<li>rpart為遞迴分割法(<span style="color: #9f6ad4;">R</span>ecursive <span style="color: #9f6ad4;">Part</span>itioning Tree)的縮寫。</li>
<li>對所有參數和分割點進行評估。</li>
<li>最佳選擇是使分割後的組內資料更為「一致(pure)」。
<ul>
<li>「一致(pure)」是指組內資料的應變數取直變異較小。</li>
</ul>
</li>
<li><span style="color: #9f6ad4;">使用Gini值測量資料的「一致(pure)」性(Homogeneity)</span>。</li>
<li>建模過程分為兩階段(2 stages)：
<ul>
<li>先長出最複雜的樹(grow the complex/full tree)。(直到Leaf size樹葉內的觀測個數少於5個或是模型沒有優化的空間為止）</li>
<li>再使用交叉驗證(Cross Validation)來修剪樹(Prune)。並尋找<span style="color: #9f6ad4;">使估計風險值(estimate of risk)參數(complexity parameter)</span>最小值的決策樹。</li>
</ul>
</li>
</ul>
<p>rpart()參數設定：</p>
<ul>
<li>method分成 “anova”、”poisson”、”class”和”exp”。當目標變數為factor時，我們將其設定為&#8221;class&#8221;。</li>
<li>control: 通常會使用rpart.control()另外作設定(事前修樹，pre-prune)。</li>
<li>na.action: <a href="https://www.rdocumentation.org/packages/rpart/versions/4.1-13/topics/rpart">預設為<span style="color: #9f6ad4;">na.rpart</span></a>，即使用CART演算法中的<a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">surrogate variables</a>做預測。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(rpart)
library(rpart.plot)

fit &lt;- rpart(formula = survived ~ ., data = trainingData, method = 'class')
# arguments:
# method: 
# - "class" for a classification tree (y is a factor) 			
# - "anova" for a regression tree</pre><p>使用rpart.plot()檢視決策樹規則。</p><pre class="crayon-plain-tag">rpart.plot(fit, extra= 106)</pre><p><span style="color: #9f6ad4;">節點顏色越綠越深，代表該節點(條件下)的survived機率越高（目標事件發生機率越高）</span>。</p>
<p>每個Node節點上的數值分別代表:</p>
<ul>
<li>預測類別(0,1)</li>
<li>預測目標類別的機率(1的機率)</li>
<li>節點中觀測資料個數佔比</li>
</ul>
<p><img loading="lazy" class="alignnone wp-image-1032" src="/wp-content/uploads/2018/08/Rplot01-1.jpeg" alt="Decision Tree " width="648" height="609" /></p>
<p>將決策樹規則使用rpart.rules()印出。</p><pre class="crayon-plain-tag">rpart.rules(x = fit,cover = TRUE)
# survived                                                                                         cover
#     0.06 when sex is   male                             &amp; age &lt;  9.5              &amp; sibsp &gt;= 3      2%
#     0.07 when sex is female &amp; pclass is           Lower              &amp; fare &gt;= 23                   3%
#     0.17 when sex is   male                             &amp; age &gt;= 9.5                               61%
#     0.58 when sex is female &amp; pclass is           Lower              &amp; fare &lt;  23                  14%
#     0.90 when sex is   male                             &amp; age &lt;  9.5              &amp; sibsp &lt;  3      2%
#     0.93 when sex is female &amp; pclass is Upper or Middle                                            19%</pre><p>可發現規則依照survived比例（目標事件發生機率）由低到高排序。cover則代表該節點觀測資料個數占比。</p>
<p>檢視交叉驗證(cross-validation)的不同cp值(complexity parameter)下的錯誤率。</p>
<p><span style="color: #9f6ad4;">cp值代表的是每一個規則（切割）所能改善模型適合度的程度(cross validation relative error, or X-val relative error)。可以發現每一個新的規則的cp呈遞減趨勢。且rpart()預設cp=0.01，即代表如果該規則（切割）沒有達到至少0.01的模型適合度改善，則停止。</span>(*<a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf" target="_blank" rel="noopener noreferrer">rpart函數對complexity parameter的說明</a>)</p><pre class="crayon-plain-tag">printcp(x = fit) 

# Classification tree:
#   rpart(formula = survived ~ ., data = trainingData, method = "class")
# 
# Variables actually used in tree construction:
#   [1] age    fare   pclass sex    sibsp 
# 
# Root node error: 400/1047 = 0.38204
# 
# n= 1047 
# 
#      CP nsplit rel error xerror     xstd
# 1 0.425      0     1.000  1.000 0.039305
# 2 0.030      1     0.575  0.575 0.033492
# 3 0.020      3     0.515  0.530 0.032507
# 4 0.010      5     0.475  0.510 0.032040</pre><p>將模型的cp table畫出。<span style="color: #9f6ad4;">可以觀察到，隨著模型的複雜度（成本）增加，所能改善的模型適合度的空間降低(X-val relative error)</span>。</p><pre class="crayon-plain-tag">plotcp(x = fit)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1034" src="/wp-content/uploads/2018/08/Rplot02-1.jpeg" alt="Decision Tree " width="816" height="771" srcset="/wp-content/uploads/2018/08/Rplot02-1.jpeg 816w, /wp-content/uploads/2018/08/Rplot02-1-300x283.jpeg 300w, /wp-content/uploads/2018/08/Rplot02-1-768x726.jpeg 768w, /wp-content/uploads/2018/08/Rplot02-1-230x217.jpeg 230w, /wp-content/uploads/2018/08/Rplot02-1-350x331.jpeg 350w, /wp-content/uploads/2018/08/Rplot02-1-480x454.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<h3>Step6: 進行預測</h3>
<p>使用predict()將訓練好的模型套用在測試資料集上。</p><pre class="crayon-plain-tag">predicted &lt;- predict(object = fit,newdata = testData,type = 'class')
# 參數說明：
# type: Type of prediction			
# - 'class': for classification			
# - 'prob': to compute the probability of each class			
# - 'vector': Predict the mean response at the node level</pre><p></p>
<h3>Step7: 衡量模型表現</h3>
<p>由於預測結果為類別型(0,1)，故我們以Confusion Matrix為基礎，來計算以下幾個常用指標：</p>
<ul>
<li>Accuracy/Misclassification Rate</li>
<li>Precision</li>
<li>Sensitivity(or Recall)</li>
<li>Specificity</li>
</ul>
<p>計算Confusion Matrix（數據左欄位預測值，上方列為真實值）</p><pre class="crayon-plain-tag">tbl &lt;- table(predicted,testData$survived)
tbl
# predicted  No Yes
#       No  140  28
#       Yes  22  72</pre><p>計算模型的正確率Accuracy</p><pre class="crayon-plain-tag"># Accuracy
accuracy &lt;- sum(diag(tbl)) / sum(tbl)
accuracy
# [1] 0.8091603</pre><p>可以發現<span style="color: #9f6ad4;">未修剪的模型</span>對測試資料的<span style="color: #9f6ad4;">預測正確率高達近81％</span>。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step8: 修剪樹(Post-Pruning)</h3>
<p>一般來說，修剪樹可以分為事前與事後。</p>
<ul>
<li><strong>事前</strong>：透過<span style="color: #9f6ad4;">rpart.control()</span>來調整重要參數，包括：
<ul>
<li><strong>minsplit</strong>：每一個node最少要幾個觀測值，預設為20。</li>
<li><strong>minbucket</strong>：在末端的node上(Leaf,樹葉)最少要幾個觀測值，預設為round(minsplit/3)。</li>
<li><strong>cp</strong>：complexity parameter。決定當新規則加入，改善模型相對誤差(x-val relative value)的程度如沒有大於cp值，則不加入該規則。<span style="color: #9f6ad4;">預設為0.01</span>。</li>
<li><strong>maxdepth</strong>：決策樹的深度，建議不超過6層。</li>
</ul>
</li>
<li><strong>事後</strong>：則是透過prune(x = , cp = )來設定。</li>
</ul>
<p>我們這邊採用post-pruning法。並選擇讓交叉驗證中相對誤差改變量最小的cp值。</p><pre class="crayon-plain-tag"># prune tree ：
fit.prune &lt;- prune(fit, cp = fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])</pre><p>將依據cp門檻值修剪後的樹規則繪出。</p><pre class="crayon-plain-tag"># plot the pruned tree 
rpart.plot(fit.prune, extra= 106, tweak = 1.1, shadow.col = "gray", branch.lty = 3, roundint = TRUE)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1035" src="/wp-content/uploads/2018/08/Rplot03_prune.jpeg" alt="Decision Tree " width="816" height="771" srcset="/wp-content/uploads/2018/08/Rplot03_prune.jpeg 816w, /wp-content/uploads/2018/08/Rplot03_prune-300x283.jpeg 300w, /wp-content/uploads/2018/08/Rplot03_prune-768x726.jpeg 768w, /wp-content/uploads/2018/08/Rplot03_prune-230x217.jpeg 230w, /wp-content/uploads/2018/08/Rplot03_prune-350x331.jpeg 350w, /wp-content/uploads/2018/08/Rplot03_prune-480x454.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>查看prune tree預測正確率。</p><pre class="crayon-plain-tag">tbl_prune &lt;- table(predicted = predicted.prune, actuals = testData$survived)

tbl_prune   
#          actuals
# predicted  No Yes
#       No  140  28
#       Yes  22  72

# Accuracy
accuracy &lt;- sum(diag(tbl_prune)) / sum(tbl_prune)
accuracy

# [1] 0.8091603</pre><p>可以發現pruned tree 和full tree兩者長得一樣，Accuracy也相同。<span style="color: #9f6ad4;">原因在於，因為在建立full tree時，預設cp=0.01，跟prune()使用的cp值是相同的</span>。</p>
<h3>Step 9: K-Fold Cross Validation</h3>
<p>為了確保模型無過度配適(overfitting)和預測準度的穩定性，我們使用k-fold cross validation(k=10)重新抽樣樣本進行模型驗證。理想中，交叉驗證後的平均正確率應與prune tree相近。</p>
<p>其中必須注意的是，因為資料中有遺失值觀測值，且train()函數中<span style="color: #9f6ad4;">參數na.action預設值為na.fail(即遇到有遺失值程序會失敗）</span>，故必須<span style="color: #0000ff;"><span style="color: #9f6ad4;">將設定調整為na.pass(不採取任何動作)或na.omit(忽略有遺失值的觀測值)</span><span style="color: #333333;">，方能正常執行函數指令。</span></span></p><pre class="crayon-plain-tag">library(caret)
library(e1071)
# 選則resampling的方法
train_control &lt;- trainControl(method = "cv",number = 10) # k = 10
# specify the model 
train_control.model &lt;- train(survived ~ ., data = trainingData, method = 'rpart',na.action = na.pass, trControl = train_control)
train_control.model

# CART 
# 
# 1047 samples
#    7 predictor
#    2 classes: 'No', 'Yes' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 943, 942, 942, 943, 942, 943, ... 
# Resampling results across tuning parameters:
#   
#   cp     Accuracy   Kappa    
#   0.020  0.8041850  0.5738715
#   0.030  0.7851282  0.5357307
#   0.425  0.6676099  0.1752418
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.02.</pre><p>進行10次交叉驗證的平均正確率為80.4%，與修剪後的樹模型正確率80.91%沒有太大差異（差異百分比在1%以內）。表示模型沒有overfitting的問題。</p>
<p>如果將參數na.action調整為na.rpart（使用CART中的代理變數surrogate variables來預測)。</p><pre class="crayon-plain-tag">train_control.model.2 &lt;- train(survived ~ ., data = trainingData, method = 'rpart',na.action = na.rpart, trControl = train_control)
train_control.model.2
# CART 
# 
# 1047 samples
# 7 predictor
# 2 classes: 'No', 'Yes' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 942, 943, 942, 943, 942, 942, ... 
# Resampling results across tuning parameters:
#   
#   cp     Accuracy   Kappa    
# 0.020  0.8042308  0.5735154
# 0.030  0.7880037  0.5452657
# 0.425  0.6685714  0.1862793
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.02.</pre><p>進行10次交叉驗證的平均正確率亦約為80.4%。</p>
<h3>Step 10: 模型比較(1)-條件推論樹(Conditional Inference Tree)</h3>
<ul>
<li>R的party套件提供<span style="color: #9f6ad4;">無母數回歸(non-parametric regression)樹模型</span>，可處理名目(nominal)、尺度(ordinal)、數值(numeric)、設限(censored)、多變量(multivariate)資料型態。</li>
<li>你可以使用ctree(formula, data = )函數來產生分類或回歸樹模型，樹模型類型會根據目標變數型態而有所不同。</li>
<li>ctree()透過統計檢驗來決定預測變數與分割點之選擇。
<ul>
<li>先假設所有預測變數與目標變數獨立(Null Hypothesis)。</li>
<li>進行<span style="color: #9f6ad4;">卡方獨立檢定(Chi-Square Test)</span>。</li>
<li>檢驗<span style="color: #9f6ad4;">p-value</span>小於threshold(ex: 0.05)則拒絕虛無假設，表示預測變數與目標變數具有顯著相關性，加入模型。</li>
<li>將相關性最強的變數選做第一次分割的變數。</li>
<li>繼續在各自子資料集進行分割變數計算與選擇。</li>
</ul>
</li>
<li>因為樹是<span style="color: #9f6ad4;">根據統計量顯著與否</span>來判斷規則之必要性，<span style="color: #9f6ad4;">因此與rpart()不同，ctree()是不需要剪枝的(prune)</span>。</li>
<li>另參數na.action預設為na.pass (即不採取任何動作)。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(party)
fit_ctree &lt;- ctree(survived ~ ., data = trainingData)
plot(fit_ctree)
predicted.ctree &lt;- predict(object = fit_ctree, newdata = testData)

tbl_ctree &lt;-table(predicted = predicted.ctree, actuals = testData$survived)
tbl_ctree

#          actuals
# predicted  No Yes
#       No  140  29
#       Yes  22  71

# Accuracy
accuracy &lt;- sum(diag(tbl_ctree)) / sum(tbl_ctree)
accuracy
# [1] 0.8053435</pre><p>可以發現模型準確率為80.5%，跟rpart演算法的k-fold交叉驗證的平均正確率80.4%沒有差太多。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step 11: 模型比較(2)-隨機森林(Random Forest)</h3>
<ul>
<li>隨機森林是一個<span style="color: #9f6ad4;">集成學習法(ensemble learning)</span>，意思是將幾個建立好的模型結果整合在一起，以提升預測準確率。</li>
<li>由集成學習法建立的模型<span style="color: #9f6ad4;">較能不容易發生過度配適的問題</span>，雖然提供較好的預測，但在推論和解釋度方面就會有所限制。</li>
<li>隨機森林由好幾個決策樹所組成，而不同決策樹是由不同隨機抽取的預測變數形成的。</li>
<li>而且特別的是，<span style="text-decoration: underline;">隨機森林不止對列(Row)進行抽樣，亦對行(Column)進行抽樣</span>，因此所產生的子集資料，其實是行與列同時抽樣後的結果。</li>
<li>對<span style="color: #9f6ad4;">列抽樣</span>，可以部分解決因<span style="color: #9f6ad4;">類別不平衡(Class Imbalance)</span>對預測帶來的問題；而對<span style="color: #9f6ad4;">行抽樣</span>，則可解決部分因<span style="color: #9f6ad4;">共線性(collinearity)</span>對預測造成的問題。<br />
(若是探討對「變數解釋性」的影響，則需要用 Lasso和Stepwise來解決)。</li>
<li>我們可用R裡面randomForest套件中的<span style="color: #9f6ad4;">randomForest()函數</span>來建立隨機森林。</li>
<li>參數na.action預設為na.fail (即遇到遺失值則停止執行)。因為資料集中有遺失觀測值，故必須將之調整為na.omit。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(randomForest)
set.seed(101)
fit.rf &lt;- randomForest(survived ~ ., data = trainingData, na.action = na.omit)</pre><p>檢視模型訓練結果。</p>
<ul>
<li>Number of trees: 隨機森林由500棵隨機生成的決策樹所組成。</li>
<li><span class="bash">利用<span style="color: #9f6ad4;">OOB(Out Of Bag)</span>運算出來的</span><span style="color: #9f6ad4;">錯誤率</span>為<span style="color: #0000ff;"><span style="color: #9f6ad4;">18.82%</span><span style="color: #333333;">。</span></span></li>
</ul>
<p></p><pre class="crayon-plain-tag"># Call:
#   randomForest(formula = survived ~ ., data = trainingData, na.action = na.omit) 
# Type of random forest: classification
# Number of trees: 500
# No. of variables tried at each split: 2
# 
# OOB estimate of  error rate: 18.82%
# Confusion matrix:
#      No Yes class.error
# No  463  44  0.08678501
# Yes 116 227  0.33819242</pre><p>自己驗證與計算較精確的OOB estimate正確率Accuracy為81.17%。</p><pre class="crayon-plain-tag">tbl.rf &lt;- fit.rf$confusion[,c(1,2)]
accuracy &lt;- sum(diag(tbl.rf)) / sum(tbl.rf)
accuracy
# [1] 0.8117647</pre><p>另外，我們將「<span style="color: #9f6ad4;">增加每一顆決策樹，整體誤差的改變量</span>」繪出，以輔助決策「需要多少顆決策樹，整體誤差才會趨於穩定」。</p>
<ul>
<li>當為分類樹時(classification tree)
<ul>
<li>誤差為OOB(out-of-bag) Erro Rates。</li>
<li><span style="color: #9f6ad4;">黑色實線表示整體的OOB error rate，而其他顏色虛線表示各類別的OOB Error Rate</span>。</li>
</ul>
</li>
<li>當為回歸樹時(regression tree)
<ul>
<li>誤差為OOB(out-of-bag) MSE。</li>
<li><span style="color: #9f6ad4;">只會有一條黑色實線代表整體的OOB MSE</span>。</li>
</ul>
</li>
</ul>
<p></p><pre class="crayon-plain-tag">plot(fit.rf)</pre><p>從圖中幾條線可以觀察到：</p>
<ul>
<li>整體錯誤率（黑色實線）隨著決策樹數量上升，下降到約18%並趨於穩定。</li>
<li>實際類別為Yes的錯誤率（綠色虛線）隨著決策樹數量的上升，下降到約33.8%並趨於穩定。</li>
<li>實際類別為No的錯誤率（紅色虛線）隨著決策樹數量的上升，下降到約8.6%並趨於穩定。</li>
<li>而「<span style="color: #9f6ad4;">最佳決策樹數目(ntree)</span><span style="color: #0000ff;">」</span>，約100多棵樹即足夠使誤差趨於穩定（不需要到500棵樹）。</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-1103" src="/wp-content/uploads/2018/08/Rplot04_rf_plot.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot04_rf_plot.jpeg 816w, /wp-content/uploads/2018/08/Rplot04_rf_plot-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot04_rf_plot-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot04_rf_plot-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot04_rf_plot-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot04_rf_plot-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>另外一個隨機森林中一個重要參數：mtry，表示每一個樹節點(node)在進行切割時(split)隨機抽樣的變數數量。可使用tuneRF()來調整mtry的值。</p><pre class="crayon-plain-tag">trainingData_naomit &lt;- na.omit(trainingData)
tuneRF(x = trainingData_naomit[,-2], y = trainingData_naomit[,2])


# mtry = 2  OOB error = 19.53% 
# Searching left ...
# mtry = 1 	OOB error = 20.71% 
# -0.06024096 0.05 
# Searching right ...
# mtry = 4 	OOB error = 21.53% 
# -0.1024096 0.05 
#       mtry  OOBError
# 1.OOB    1 0.2070588
# 2.OOB    2 0.1952941
# 4.OOB    4 0.2152941</pre><p>可以發現在mtry=2時，誤差最小。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1111" src="/wp-content/uploads/2018/08/Rplot05_tuneRF.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot05_tuneRF.jpeg 816w, /wp-content/uploads/2018/08/Rplot05_tuneRF-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot05_tuneRF-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot05_tuneRF-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot05_tuneRF-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot05_tuneRF-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>randomForest中類別樹預設的mtry=sqrt(p)，其中p代表x變數的數目。因為原始隨機森林模型預設值跟tuneRF建議的值相同，故我們另外不調整。</p><pre class="crayon-plain-tag">fit.rf$mtry
# [1] 2</pre><p>看每個x變數的重要性(<span style="color: #9f6ad4;">the mean decrease in Gini index</span>)，即看哪個變數對<span style="color: #9f6ad4;">損失函數Loss Function</span>最有貢獻。(*randomForest參數importance預設值為False，僅會產生the mean decrease in Gini index，如果要產生其他指標如mean decrease in accuracy，要將其改為TRUE。)</p><pre class="crayon-plain-tag">round(importance(fit.rf),2) # importance of each predictor
# or
# round(fit.rf$importance, 2)

#          MeanDecreaseGini
# pclass              27.55
# sex                100.67
# age                 55.09
# sibsp               13.89
# parch               13.40
# fare                59.79
# embarked            11.55</pre><p>將變數重要性（貢獻度）繪出。</p><pre class="crayon-plain-tag">varImpPlot(fit.rf)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1115" src="/wp-content/uploads/2018/08/Rplot06_varImpPlot.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot06_varImpPlot.jpeg 816w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>最後將調整好的模型應用在testData並評估正確性。</p><pre class="crayon-plain-tag">predicted.rf &lt;- predict(object = fit.rf, newdata = testData)
tbl.rf &lt;- table(predicted = predicted.rf, actuals = testData$survived)
accuracy &lt;- sum(diag(tbl.rf)) / sum(tbl.rf)
accuracy
# [1] 0.8307692</pre><p>隨機森林的預測準確率為83%，較原先的決策樹(accuracy = 80%)改善約4%。</p>
<h3>總結</h3>
<ul>
<li>Decision Tree 決策樹模型具有簡單易懂的樹狀邏輯圖，可解釋度高。</li>
<li>Decision Tree 決策樹對於資料的要求低，沒有常態分配與線性關係的假設，不受資料分配限制。</li>
<li>Decision Tree 決策樹的<span style="color: #9f6ad4;">有變數篩選機制</span>。
<ul>
<li>rpart演算法進行Gini Index檢定，並計算complexity parameter來進行變數篩選。</li>
<li>ctree演算法進行chi-square檢定，檢驗各投入變數是否與目標變數相關並計算p-value來看相關性的顯著效果。</li>
</ul>
</li>
<li>Decision Tree 決策樹亦<span style="color: #9f6ad4;">有空值填補機制</span> &#8211; <a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate</a>。
<ul>
<li>rpart演算法na.action預設為na.rpart。(更多有關決策樹遺失值預測請參考 <a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate in CART</a>)</li>
<li><a href="https://www.rdocumentation.org/packages/partykit/versions/1.2-2/topics/ctree">ctree演算法na.action預設為na.pass</a>，可以將其改為na.rpart。</li>
</ul>
</li>
<li>Decision Tree 演算法<span style="color: #9f6ad4;">rpart和ctree皆能處理連續型(continuous)與類別型(categorical)變數之切割</span>。</li>
<li>由Decision Tree 決策樹衍伸出的集成學習法「<span style="color: #9f6ad4;">隨機森林 random forest</span>」可以有效降低模型的錯誤率、解決過度配適、透過反覆抽樣解決類別不平衡與共線性問題。</li>
</ul>
<hr />
<p>更多<span style="color: #9f6ad4;">統計模型</span>筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>學習筆記參考連結：</p>
<ol>
<li><a href="https://www.guru99.com/r-decision-trees.html" target="_blank" rel="noopener noreferrer">Decision Tree in R with Example </a></li>
<li><a href="https://www.statmethods.net/advstats/cart.html">Tree-Based Models</a></li>
<li><a href="https://rstudio-pubs-static.s3.amazonaws.com/275285_90aaf9a2a64d43a5846a86dbcde8eba9.html">R_programming &#8211; (8)決策樹(Decision Tree)</a></li>
<li><a href="https://dzone.com/articles/decision-trees-and-pruning-in-r">Decision Trees and Pruning in R</a></li>
<li><a href="https://statinfer.com/203-3-10-pruning-a-decision-tree-in-r/">Pruning a Decision Tree in R</a></li>
<li><a href="https://www.edureka.co/blog/decision-trees/">How To Create A Perfect Decision Tree</a></li>
<li><a href="https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda">[資料分析&amp;機器學習] 第3.5講 : 決策樹(Decision Tree)以及隨機森林(Random Forest)介紹</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
	</channel>
</rss>
