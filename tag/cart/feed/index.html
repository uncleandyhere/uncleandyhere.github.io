<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>cart &#8211; 果醬珍珍•JamJam</title>
	<atom:link href="/tag/cart/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>健忘女孩Jam的學習筆記和生活雜記</description>
	<lastBuildDate>Fri, 03 Jul 2020 02:33:57 +0000</lastBuildDate>
	<language>zh-TW</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Decision Tree 決策樹 &#124; CART, Conditional Inference Tree, RandomForest</title>
		<link>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/</link>
					<comments>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Fri, 31 Aug 2018 06:08:33 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[cart]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[prune]]></category>
		<category><![CDATA[random forest]]></category>
		<category><![CDATA[rpart]]></category>
		<category><![CDATA[tree surrogate]]></category>
		<category><![CDATA[決策樹]]></category>
		<category><![CDATA[隨機森林]]></category>
		<guid isPermaLink="false">/?p=1026</guid>

					<description><![CDATA[<p>Decision Tree 決策樹模型是一個不受資料分配限制的模型，模型結果以樹狀呈現，簡單易懂，解釋性極高，且模型同時兼具變數挑選與遺失值填補的機制，並能處理 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Decision Tree 決策樹模型是一個不受資料分配限制的模型，模型結果以樹狀呈現，簡單易懂，解釋性極高，且模型同時兼具變數挑選與遺失值填補的機制，並能處理分類與回歸問題，是一個廣泛被使用的模型。另外，以決策樹為基礎集成學習而成的隨機森林，更能有效降低模型的錯誤率與並解決過度配適等問題的著名機器學習法之一。</p>
<h3>Decision Tree 決策樹簡介</h3>
<ul>
<li><span style="color: #9f6ad4;">決策樹</span>是一個多功能的機器學習演算法，不僅可以進行<span style="color: #9f6ad4;">分類</span>亦可進行<span style="color: #9f6ad4;">回歸</span>任務。</li>
<li>可以配適複雜的資料集，是個強大的演算法。</li>
<li>屬於<span style="color: #9f6ad4;">無母數回歸</span>方法(<span style="color: #9f6ad4;">non-parametric</span>)：對資料長相的要求不像回歸模型（有母數法，parametric）嚴格，不需要假設資料的線性關係與常態分佈。<br />
(無母數介紹請參考)</li>
<li>決策樹演算法也是隨機森林演算法的基礎(隨機森林也是至今具潛力的演算法之一)。</li>
<li>有諸多演算法，常見的包括CART, CHAID。</li>
<li>決策樹可以用來建立非線性模型，通常被用在迴歸，也可以用在對於遞迴預測變數最二元分類。</li>
</ul>
<h4>補充-無母數統計：</h4>
<ol>
<li>適用於母體分佈情況未知、小樣本、母體分佈不為常態或不易轉換為常態，對資料長相的要求小。</li>
<li>無母數統計推論時所使用的<span style="color: #9f6ad4;">樣本統計量</span>分配通常與母體分配無關，不需要使用樣本統計量去推論母體中位數、適合度、獨立性、隨機性。</li>
<li>無母數統計又稱作「不受分配限制統計法」(distribution-free)。</li>
</ol>
<h4>常見的決策樹演算法比較</h4>
<table style="height: 116px; width: 100%; border-collapse: collapse; background-color: #ffffff;" border="1">
<tbody>
<tr style="height: 23px;" bgcolor="#ddd">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">演算法</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">資料屬性</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">分割規則</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">修剪樹規則</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">ID3</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;">
<div><span style="color: #000000;">Entropy,</span></div>
<div><span style="color: #000000;">Gain Ratio</span></div>
</td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Predicted Error Rate</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">C4.5</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Gain Ratio</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Predicted Error Rate</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">CHAID</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;">
<div><span style="color: #000000;">Chi-Square Test</span></div>
</td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">No Pruning</span></td>
</tr>
<tr style="height: 24px;">
<td style="width: 25%; height: 24px;"><span style="color: #000000;">CART</span></td>
<td style="width: 25%; height: 24px;"><span style="color: #000000;">離散與連續型</span></td>
<td style="width: 25%; height: 24px;"><span style="color: #000000;">Gini Index</span></td>
<td class="" style="width: 25%; height: 24px;">
<div><span style="color: #000000;">Entire Error Rate</span></div>
<div><span style="color: #000000;">(Training and Predicted)</span></div>
</td>
</tr>
</tbody>
</table>
<h4>決策樹挑選變數常用的測量值</h4>
<p>常見的資訊量（衡量資料<span style="color: #9f6ad4;">純度</span>）：</p>
<ul>
<li><strong>Entropy (熵)</strong>:<br />
$$I_{H}(t)=-\sum_{i=1}^c p(i|t) \log_{2} p(i|t)$$<br />
其中，H代表Homogeneity(同質性)。<br />
<span style="color: #9f6ad4;">當Entropy=0表示completely homogeneous(pure)，而當Entropy=1則表示資料為50%-50%之組成，是不純的</span><span style="color: #9f6ad4;">(impurity)</span>。</li>
<li><strong>Gini Impurity (Gini不純度)</strong>:<br />
$$I_{G}(t) =\sum_{i=1}^c p(i|t)(1-p(i|t)) = 1-\sum_{i=1}^c p(i|t)^2$$<br />
其中，G則代表Gini Impurity。</li>
</ul>
<p>決定切割點的測量值：</p>
<ul>
<li><strong>Information Gain (資訊增益)</strong>: 則衡量節點切割前後資料純度的變化。<span style="color: #9f6ad4;">節點的選擇，當選IG值越大的變數為佳</span>。<br />
$$IG = Info(D) &#8211; Info_{A}(D)$$<br />
其中，\(Info(D)\)為原始資料純度，而\(Info_{\space A}(D)\)則表示使用A規則切割後的資料純度。<br />
$$Info_{\space A}(D)=\sum_{j=1}^m \frac{N_{j}}{N_{p}} Info(D_{j})$$<br />
當m=2，即為二元分類時，<br />
$$IG = Info(D) &#8211; \frac{N_{left}}{N_{p}} Info(D_{left}) &#8211; \frac{N_{right}}{N_{p}} Info(D_{right})$$</li>
</ul>
<h4>資料與分析問題</h4>
<ul>
<li>Data: 鐵達尼資料集包含13個變數與1309筆觀測值。</li>
<li>Problem: 我們想分析與預測具有什麼樣特徵的乘客，比較有機會在冰山撞船後可以存活下來。</li>
<li>Method: 使用CART(Classification and Regression Tree)決策樹模型來找出重要解釋變數。</li>
</ul>
<h4>訓練與視覺化決策樹，我們將進行以下步驟：</h4>
<ol>
<li>載入資料</li>
<li>資料探勘</li>
<li>資料前處理</li>
<li>產生訓練與測試資料集</li>
<li>建置模型</li>
<li>進行預測</li>
<li>衡量模型表現</li>
<li>修剪樹(Post-pruning)</li>
<li>K-Fold Cross Validation</li>
<li>模型比較(1)：條件推論樹(Conditional Inference Tree)</li>
<li>模型比較(2)：隨機森林(Random Forest)</li>
</ol>
<h3>Step1: 載入資料</h3>
<p></p><pre class="crayon-plain-tag"># 從google drive shareable link 讀入csv檔案
# https://drive.google.com/file/d/1S7S-siBGkMR3YUVAbaTkfS1CxOji_Ngd/view?usp=sharing
id &lt;- "1S7S-siBGkMR3YUVAbaTkfS1CxOji_Ngd" # google file ID
inputData &lt;- read.csv(sprintf("https://docs.google.com/uc?id=%s&amp;export=download", id))
head(inputData)

#   pclass survived                                            name    sex     age sibsp parch ticket     fare
# 1      1        1                   Allen, Miss. Elisabeth Walton female 29.0000     0     0  24160 211.3375
# 2      1        1                  Allison, Master. Hudson Trevor   male  0.9167     1     2 113781 151.5500
# 3      1        0                    Allison, Miss. Helen Loraine female  2.0000     1     2 113781 151.5500
# 4      1        0            Allison, Mr. Hudson Joshua Creighton   male 30.0000     1     2 113781 151.5500
# 5      1        0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000     1     2 113781 151.5500
# 6      1        1                             Anderson, Mr. Harry   male 48.0000     0     0  19952  26.5500
#     cabin embarked                       home.dest
# 1      B5        S                    St Louis, MO
# 2 C22 C26        S Montreal, PQ / Chesterville, ON
# 3 C22 C26        S Montreal, PQ / Chesterville, ON
# 4 C22 C26        S Montreal, PQ / Chesterville, ON
# 5 C22 C26        S Montreal, PQ / Chesterville, ON
# 6     E12        S                    New York, NY

tail(inputData)
#      pclass survived                      name    sex  age sibsp parch ticket    fare cabin embarked home.dest
# 1304      3        0     Yousseff, Mr. Gerious   male   NA     0     0   2627 14.4583              C          
# 1305      3        0      Zabour, Miss. Hileni female 14.5     1     0   2665 14.4542              C          
# 1306      3        0     Zabour, Miss. Thamine female   NA     1     0   2665 14.4542              C          
# 1307      3        0 Zakarian, Mr. Mapriededer   male 26.5     0     0   2656  7.2250              C          
# 1308      3        0       Zakarian, Mr. Ortin   male 27.0     0     0   2670  7.2250              C          
# 1309      3        0        Zimmerman, Mr. Leo   male 29.0     0     0 315082  7.8750              S</pre><p>我們可以發現數據是經過排列過的，因為這樣會嚴重影響到我們後續隨機產生訓練與測試資料集，所以我們必須將資料重新隨機排列。</p>
<p>使用sample()隨機產生一組數列index。</p><pre class="crayon-plain-tag">shuffle_index &lt;- sample(x = 1:nrow(inputData))
head(shuffle_index)</pre><p>並將隨機數列index指派給titanic資料集。即可觀察到資料已無排序。</p><pre class="crayon-plain-tag">inputData &lt;- inputData[shuffle_index,]
head(inputData)

#      pclass survived                                   name    sex age sibsp parch     ticket   fare cabin
# 632       3        0            Andersson, Mr. Johan Samuel   male  26     0     0     347075  7.775      
# 526       2        0                       Pain, Dr. Alfred   male  23     0     0     244278 10.500      
# 822       3        0              Goldsmith, Mr. Frank John   male  33     1     1     363291 20.525      
# 485       2        1           Lemore, Mrs. (Amelia Milley) female  34     0     0 C.A. 34260 10.500   F33
# 627       3        0 Andersson, Miss. Ida Augusta Margareta female  38     4     2     347091  7.775      
# 1183      3        1       Salkjelsvik, Miss. Anna Kristine female  21     0     0     343120  7.650      
#      embarked                         home.dest
# 632         S                      Hartford, CT
# 526         S                      Hamilton, ON
# 822         S Strood, Kent, England Detroit, MI
# 485         S                       Chicago, IL
# 627         S      Vadsbro, Sweden Ministee, MI
# 1183        S</pre><p></p>
<h3>Step2: 資料探勘</h3>
<p>使用summary()摘要基礎統計。</p><pre class="crayon-plain-tag">summary(inputData)

#        pclass         survived                                   name          sex           age         
# Min.   :1.000   Min.   :0.000   Connolly, Miss. Kate            :   2   female:466   Min.   : 0.1667  
# 1st Qu.:2.000   1st Qu.:0.000   Kelly, Mr. James                :   2   male  :843   1st Qu.:21.0000  
# Median :3.000   Median :0.000   Abbing, Mr. Anthony             :   1                Median :28.0000  
# Mean   :2.295   Mean   :0.382   Abbott, Master. Eugene Joseph   :   1                Mean   :29.8811  
# 3rd Qu.:3.000   3rd Qu.:1.000   Abbott, Mr. Rossmore Edward     :   1                3rd Qu.:39.0000  
# Max.   :3.000   Max.   :1.000   Abbott, Mrs. Stanton (Rosa Hunt):   1                Max.   :80.0000  
#                                 (Other)                         :1301                NA's   :263      
#          sibsp            parch            ticket          fare                     cabin      embarked
# Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000                  :1014    :  2   
# 1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896   C23 C25 C27    :   6   C:270   
# Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454   B57 B59 B63 B66:   5   Q:123   
# Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295   G6             :   5   S:914   
# 3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275   B96 B98        :   4           
# Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329   C22 C26        :   4           
#                                  (Other) :1261   NA's   :1         (Other)        : 271           
#                home.dest  
# :564  
# New York, NY        : 64  
# London              : 14  
# Montreal, PQ        : 10  
# Cornwall / Akron, OH:  9  
# Paris, France       :  9  
# (Other)             :639</pre><p>使用str()查看資料結構。</p><pre class="crayon-plain-tag">str(inputData)
# 'data.frame':	1309 obs. of  12 variables:
# $ pclass   : int  3 2 3 2 3 3 1 3 3 2 ...
# $ survived : int  0 0 0 1 0 1 1 0 1 1 ...
# $ name     : Factor w/ 1307 levels "Abbing, Mr. Anthony",..: 41 920 459 703 36 1068 864 949 1092 516 ...
# $ sex      : Factor w/ 2 levels "female","male": 2 2 2 1 1 1 1 2 1 1 ...
# $ age      : num  26 23 33 34 38 21 23 NA NA 7 ...
# $ sibsp    : int  0 0 1 0 4 0 1 0 0 0 ...
# $ parch    : int  0 0 1 0 2 0 0 0 0 2 ...
# $ ticket   : Factor w/ 929 levels "110152","110413",..: 453 194 584 767 468 410 574 415 388 783 ...
# $ fare     : num  7.78 10.5 20.52 10.5 7.78 ...
# $ cabin    : Factor w/ 187 levels "","A10","A11",..: 1 1 1 183 1 1 134 1 1 1 ...
# $ embarked : Factor w/ 4 levels "","C","Q","S": 4 4 4 4 4 4 2 4 3 4 ...
# $ home.dest: Factor w/ 370 levels "","?Havana, Cuba",..: 154 150 317 64 345 1 191 1 1 165 ...</pre><p>我們可初步發現：</p>
<ol>
<li>pclass(座艙等級)和survuved(生存與否)應由int轉換成factor變數</li>
<li><span style="color: #9f6ad4;">類別水準數過多</span>的變數：<span style="color: #9f6ad4;">name</span>(1307 levels),<span style="color: #9f6ad4;">ticket</span>(929 levels), <span style="color: #9f6ad4;">cabin</span>(187 levels), <span style="color: #9f6ad4;">home.dest</span>(370 levels)<span style="color: #9f6ad4;">應予以排除</span>。</li>
<li>排除以上變數後，存在許多遺失值(NA value)的變數有：age(263), fare(1)。<span style="color: #9f6ad4;">但由於CART決策樹rpart()演算法中，預設會刪除y遺失的資料列，並保留至少有一個預測變數未遺失的觀察資料列，並使用Surrogate Variables來預測遺失特徵值。因此我們不會特別處理遺失值的部分</span>。(*更多決策樹遺失值預測請參考<a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate in CART</a>)</li>
</ol>
<h3>Step3: 資料前處理</h3>
<p>根據資料探勘結果，要處理的項目如下：</p>
<ol>
<li>移除變數<span style="color: #9f6ad4;">name</span>(1307 levels),<span style="color: #9f6ad4;">ticket</span>(929 levels), <span style="color: #9f6ad4;">cabin</span>(187 levels), <span style="color: #9f6ad4;">home.dest</span></li>
<li>將變數pclass(座艙等級)和survuved(生存與否)轉換為factor變數。</li>
</ol>
<p></p><pre class="crayon-plain-tag">library(dplyr)

clean_inputData &lt;- 
  inputData %&gt;% 
  # Drop variables
  select(-c(home.dest, cabin, name, ticket)) %&gt;% 
  #Convert to factor level
  mutate(pclass = factor(pclass, levels = c(1, 2, 3), labels = c('Upper', 'Middle', 'Lower')),
         survived = factor(survived, levels = c(0, 1), labels = c('No', 'Yes')))

glimpse(clean_inputData)

# Observations: 1,309
# Variables: 8
# $ pclass   &lt;fct&gt; Lower, Middle, Lower, Middle, Lower, Lower, Upper, Lower, Lower, Middle, Lower, Upper, Upper, Middle, Lower, Lower, Lower, Upper, Lower, Lower,...
# $ survived &lt;fct&gt; No, No, No, Yes, No, Yes, Yes, No, Yes, Yes, No, Yes, No, Yes, No, No, No, Yes, No, Yes, No, Yes, Yes, Yes, No, Yes, No, No, No, Yes, No, Yes, ...
# $ sex      &lt;fct&gt; male, male, male, female, female, female, female, male, female, female, male, female, male, female, male, male, female, female, male, female, m...
# $ age      &lt;dbl&gt; 26.0, 23.0, 33.0, 34.0, 38.0, 21.0, 23.0, NA, NA, 7.0, 1.0, 16.0, 58.0, 24.0, 33.0, NA, NA, 36.0, 36.0, 19.0, 19.0, 25.0, 51.0, 4.0, 40.0, 18.0...
# $ sibsp    &lt;int&gt; 0, 0, 1, 0, 4, 0, 1, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 8, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ parch    &lt;int&gt; 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 3, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ fare     &lt;dbl&gt; 7.7750, 10.5000, 20.5250, 10.5000, 7.7750, 7.6500, 113.2750, 7.7750, 7.7792, 26.2500, 46.9000, 57.9792, 113.2750, 27.0000, 7.7750, 8.0500, 7.75...
# $ embarked &lt;fct&gt; S, S, S, S, S, S, C, S, Q, S, S, C, C, S, S, S, Q, C, S, S, S, C, S, S, S, C, S, S, S, S, S, C, C, S, S, S, S, S, Q, S, S, S, S, Q, Q, S, C, S,...</pre><p></p>
<h3>Step4: 產生訓練與測試資料集</h3>
<p>為了確保兩組資料集中生還比例不要差異太大，我們會先將資料依據目標變數(survived)分成兩組(No, Yes)，再進行隨機切割成80%訓練組跟20%測試組。</p><pre class="crayon-plain-tag">input_ones &lt;- clean_inputData[which(clean_inputData$survived == 'Yes'),]
input_zeros &lt;- clean_inputData[which(clean_inputData$survived == 'No'), ]
set.seed(100)
input_ones_training_row &lt;- sample(1:nrow(input_ones),0.8*nrow(input_ones))
input_zeros_training_row &lt;- sample(1:nrow(input_zeros),0.8*nrow(input_zeros))

training_ones &lt;- input_ones[input_ones_training_row,]
training_zeros &lt;- input_zeros[input_zeros_training_row,]
trainingData &lt;- rbind(training_ones, training_zeros)

# 產生測試資料集
test_ones &lt;- input_ones[-input_ones_training_row,]
test_zeros &lt;- input_zeros[-input_zeros_training_row,]
testData &lt;- rbind(test_ones, test_zeros)</pre><p>檢查切割完的資料集大小與目標變數的分佈比例：</p>
<ul>
<li>原始資料列1309被隨機切割為80%訓練資料集(1047筆)與20%測試資料集(262筆)。</li>
<li>發現訓練及測試資料集的目標變數survived比例都是38%。差異在1%以內。</li>
</ul>
<p></p><pre class="crayon-plain-tag">dim(trainingData)
# [1] 1047    8
dim(testData)
# [1] 262   8

# 確認兩資料是隨機的
prop.table(table(trainingData$survived))
#        No       Yes 
# 0.6179561 0.3820439 

prop.table(table(testData$survived))
#        No       Yes 
# 0.6183206 0.3816794</pre><p></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step5: 建置模型</h3>
<p>我們使用CART(Classification and Regression Tree)決策樹演算法-rpart()。</p>
<ul>
<li>rpart為遞迴分割法(<span style="color: #9f6ad4;">R</span>ecursive <span style="color: #9f6ad4;">Part</span>itioning Tree)的縮寫。</li>
<li>對所有參數和分割點進行評估。</li>
<li>最佳選擇是使分割後的組內資料更為「一致(pure)」。
<ul>
<li>「一致(pure)」是指組內資料的應變數取直變異較小。</li>
</ul>
</li>
<li><span style="color: #9f6ad4;">使用Gini值測量資料的「一致(pure)」性(Homogeneity)</span>。</li>
<li>建模過程分為兩階段(2 stages)：
<ul>
<li>先長出最複雜的樹(grow the complex/full tree)。(直到Leaf size樹葉內的觀測個數少於5個或是模型沒有優化的空間為止）</li>
<li>再使用交叉驗證(Cross Validation)來修剪樹(Prune)。並尋找<span style="color: #9f6ad4;">使估計風險值(estimate of risk)參數(complexity parameter)</span>最小值的決策樹。</li>
</ul>
</li>
</ul>
<p>rpart()參數設定：</p>
<ul>
<li>method分成 “anova”、”poisson”、”class”和”exp”。當目標變數為factor時，我們將其設定為&#8221;class&#8221;。</li>
<li>control: 通常會使用rpart.control()另外作設定(事前修樹，pre-prune)。</li>
<li>na.action: <a href="https://www.rdocumentation.org/packages/rpart/versions/4.1-13/topics/rpart">預設為<span style="color: #9f6ad4;">na.rpart</span></a>，即使用CART演算法中的<a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">surrogate variables</a>做預測。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(rpart)
library(rpart.plot)

fit &lt;- rpart(formula = survived ~ ., data = trainingData, method = 'class')
# arguments:
# method: 
# - "class" for a classification tree (y is a factor) 			
# - "anova" for a regression tree</pre><p>使用rpart.plot()檢視決策樹規則。</p><pre class="crayon-plain-tag">rpart.plot(fit, extra= 106)</pre><p><span style="color: #9f6ad4;">節點顏色越綠越深，代表該節點(條件下)的survived機率越高（目標事件發生機率越高）</span>。</p>
<p>每個Node節點上的數值分別代表:</p>
<ul>
<li>預測類別(0,1)</li>
<li>預測目標類別的機率(1的機率)</li>
<li>節點中觀測資料個數佔比</li>
</ul>
<p><img loading="lazy" class="alignnone wp-image-1032" src="/wp-content/uploads/2018/08/Rplot01-1.jpeg" alt="Decision Tree " width="648" height="609" /></p>
<p>將決策樹規則使用rpart.rules()印出。</p><pre class="crayon-plain-tag">rpart.rules(x = fit,cover = TRUE)
# survived                                                                                         cover
#     0.06 when sex is   male                             &amp; age &lt;  9.5              &amp; sibsp &gt;= 3      2%
#     0.07 when sex is female &amp; pclass is           Lower              &amp; fare &gt;= 23                   3%
#     0.17 when sex is   male                             &amp; age &gt;= 9.5                               61%
#     0.58 when sex is female &amp; pclass is           Lower              &amp; fare &lt;  23                  14%
#     0.90 when sex is   male                             &amp; age &lt;  9.5              &amp; sibsp &lt;  3      2%
#     0.93 when sex is female &amp; pclass is Upper or Middle                                            19%</pre><p>可發現規則依照survived比例（目標事件發生機率）由低到高排序。cover則代表該節點觀測資料個數占比。</p>
<p>檢視交叉驗證(cross-validation)的不同cp值(complexity parameter)下的錯誤率。</p>
<p><span style="color: #9f6ad4;">cp值代表的是每一個規則（切割）所能改善模型適合度的程度(cross validation relative error, or X-val relative error)。可以發現每一個新的規則的cp呈遞減趨勢。且rpart()預設cp=0.01，即代表如果該規則（切割）沒有達到至少0.01的模型適合度改善，則停止。</span>(*<a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf" target="_blank" rel="noopener noreferrer">rpart函數對complexity parameter的說明</a>)</p><pre class="crayon-plain-tag">printcp(x = fit) 

# Classification tree:
#   rpart(formula = survived ~ ., data = trainingData, method = "class")
# 
# Variables actually used in tree construction:
#   [1] age    fare   pclass sex    sibsp 
# 
# Root node error: 400/1047 = 0.38204
# 
# n= 1047 
# 
#      CP nsplit rel error xerror     xstd
# 1 0.425      0     1.000  1.000 0.039305
# 2 0.030      1     0.575  0.575 0.033492
# 3 0.020      3     0.515  0.530 0.032507
# 4 0.010      5     0.475  0.510 0.032040</pre><p>將模型的cp table畫出。<span style="color: #9f6ad4;">可以觀察到，隨著模型的複雜度（成本）增加，所能改善的模型適合度的空間降低(X-val relative error)</span>。</p><pre class="crayon-plain-tag">plotcp(x = fit)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1034" src="/wp-content/uploads/2018/08/Rplot02-1.jpeg" alt="Decision Tree " width="816" height="771" srcset="/wp-content/uploads/2018/08/Rplot02-1.jpeg 816w, /wp-content/uploads/2018/08/Rplot02-1-300x283.jpeg 300w, /wp-content/uploads/2018/08/Rplot02-1-768x726.jpeg 768w, /wp-content/uploads/2018/08/Rplot02-1-230x217.jpeg 230w, /wp-content/uploads/2018/08/Rplot02-1-350x331.jpeg 350w, /wp-content/uploads/2018/08/Rplot02-1-480x454.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<h3>Step6: 進行預測</h3>
<p>使用predict()將訓練好的模型套用在測試資料集上。</p><pre class="crayon-plain-tag">predicted &lt;- predict(object = fit,newdata = testData,type = 'class')
# 參數說明：
# type: Type of prediction			
# - 'class': for classification			
# - 'prob': to compute the probability of each class			
# - 'vector': Predict the mean response at the node level</pre><p></p>
<h3>Step7: 衡量模型表現</h3>
<p>由於預測結果為類別型(0,1)，故我們以Confusion Matrix為基礎，來計算以下幾個常用指標：</p>
<ul>
<li>Accuracy/Misclassification Rate</li>
<li>Precision</li>
<li>Sensitivity(or Recall)</li>
<li>Specificity</li>
</ul>
<p>計算Confusion Matrix（數據左欄位預測值，上方列為真實值）</p><pre class="crayon-plain-tag">tbl &lt;- table(predicted,testData$survived)
tbl
# predicted  No Yes
#       No  140  28
#       Yes  22  72</pre><p>計算模型的正確率Accuracy</p><pre class="crayon-plain-tag"># Accuracy
accuracy &lt;- sum(diag(tbl)) / sum(tbl)
accuracy
# [1] 0.8091603</pre><p>可以發現<span style="color: #9f6ad4;">未修剪的模型</span>對測試資料的<span style="color: #9f6ad4;">預測正確率高達近81％</span>。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step8: 修剪樹(Post-Pruning)</h3>
<p>一般來說，修剪樹可以分為事前與事後。</p>
<ul>
<li><strong>事前</strong>：透過<span style="color: #9f6ad4;">rpart.control()</span>來調整重要參數，包括：
<ul>
<li><strong>minsplit</strong>：每一個node最少要幾個觀測值，預設為20。</li>
<li><strong>minbucket</strong>：在末端的node上(Leaf,樹葉)最少要幾個觀測值，預設為round(minsplit/3)。</li>
<li><strong>cp</strong>：complexity parameter。決定當新規則加入，改善模型相對誤差(x-val relative value)的程度如沒有大於cp值，則不加入該規則。<span style="color: #9f6ad4;">預設為0.01</span>。</li>
<li><strong>maxdepth</strong>：決策樹的深度，建議不超過6層。</li>
</ul>
</li>
<li><strong>事後</strong>：則是透過prune(x = , cp = )來設定。</li>
</ul>
<p>我們這邊採用post-pruning法。並選擇讓交叉驗證中相對誤差改變量最小的cp值。</p><pre class="crayon-plain-tag"># prune tree ：
fit.prune &lt;- prune(fit, cp = fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])</pre><p>將依據cp門檻值修剪後的樹規則繪出。</p><pre class="crayon-plain-tag"># plot the pruned tree 
rpart.plot(fit.prune, extra= 106, tweak = 1.1, shadow.col = "gray", branch.lty = 3, roundint = TRUE)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1035" src="/wp-content/uploads/2018/08/Rplot03_prune.jpeg" alt="Decision Tree " width="816" height="771" srcset="/wp-content/uploads/2018/08/Rplot03_prune.jpeg 816w, /wp-content/uploads/2018/08/Rplot03_prune-300x283.jpeg 300w, /wp-content/uploads/2018/08/Rplot03_prune-768x726.jpeg 768w, /wp-content/uploads/2018/08/Rplot03_prune-230x217.jpeg 230w, /wp-content/uploads/2018/08/Rplot03_prune-350x331.jpeg 350w, /wp-content/uploads/2018/08/Rplot03_prune-480x454.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>查看prune tree預測正確率。</p><pre class="crayon-plain-tag">tbl_prune &lt;- table(predicted = predicted.prune, actuals = testData$survived)

tbl_prune   
#          actuals
# predicted  No Yes
#       No  140  28
#       Yes  22  72

# Accuracy
accuracy &lt;- sum(diag(tbl_prune)) / sum(tbl_prune)
accuracy

# [1] 0.8091603</pre><p>可以發現pruned tree 和full tree兩者長得一樣，Accuracy也相同。<span style="color: #9f6ad4;">原因在於，因為在建立full tree時，預設cp=0.01，跟prune()使用的cp值是相同的</span>。</p>
<h3>Step 9: K-Fold Cross Validation</h3>
<p>為了確保模型無過度配適(overfitting)和預測準度的穩定性，我們使用k-fold cross validation(k=10)重新抽樣樣本進行模型驗證。理想中，交叉驗證後的平均正確率應與prune tree相近。</p>
<p>其中必須注意的是，因為資料中有遺失值觀測值，且train()函數中<span style="color: #9f6ad4;">參數na.action預設值為na.fail(即遇到有遺失值程序會失敗）</span>，故必須<span style="color: #0000ff;"><span style="color: #9f6ad4;">將設定調整為na.pass(不採取任何動作)或na.omit(忽略有遺失值的觀測值)</span><span style="color: #333333;">，方能正常執行函數指令。</span></span></p><pre class="crayon-plain-tag">library(caret)
library(e1071)
# 選則resampling的方法
train_control &lt;- trainControl(method = "cv",number = 10) # k = 10
# specify the model 
train_control.model &lt;- train(survived ~ ., data = trainingData, method = 'rpart',na.action = na.pass, trControl = train_control)
train_control.model

# CART 
# 
# 1047 samples
#    7 predictor
#    2 classes: 'No', 'Yes' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 943, 942, 942, 943, 942, 943, ... 
# Resampling results across tuning parameters:
#   
#   cp     Accuracy   Kappa    
#   0.020  0.8041850  0.5738715
#   0.030  0.7851282  0.5357307
#   0.425  0.6676099  0.1752418
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.02.</pre><p>進行10次交叉驗證的平均正確率為80.4%，與修剪後的樹模型正確率80.91%沒有太大差異（差異百分比在1%以內）。表示模型沒有overfitting的問題。</p>
<p>如果將參數na.action調整為na.rpart（使用CART中的代理變數surrogate variables來預測)。</p><pre class="crayon-plain-tag">train_control.model.2 &lt;- train(survived ~ ., data = trainingData, method = 'rpart',na.action = na.rpart, trControl = train_control)
train_control.model.2
# CART 
# 
# 1047 samples
# 7 predictor
# 2 classes: 'No', 'Yes' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 942, 943, 942, 943, 942, 942, ... 
# Resampling results across tuning parameters:
#   
#   cp     Accuracy   Kappa    
# 0.020  0.8042308  0.5735154
# 0.030  0.7880037  0.5452657
# 0.425  0.6685714  0.1862793
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.02.</pre><p>進行10次交叉驗證的平均正確率亦約為80.4%。</p>
<h3>Step 10: 模型比較(1)-條件推論樹(Conditional Inference Tree)</h3>
<ul>
<li>R的party套件提供<span style="color: #9f6ad4;">無母數回歸(non-parametric regression)樹模型</span>，可處理名目(nominal)、尺度(ordinal)、數值(numeric)、設限(censored)、多變量(multivariate)資料型態。</li>
<li>你可以使用ctree(formula, data = )函數來產生分類或回歸樹模型，樹模型類型會根據目標變數型態而有所不同。</li>
<li>ctree()透過統計檢驗來決定預測變數與分割點之選擇。
<ul>
<li>先假設所有預測變數與目標變數獨立(Null Hypothesis)。</li>
<li>進行<span style="color: #9f6ad4;">卡方獨立檢定(Chi-Square Test)</span>。</li>
<li>檢驗<span style="color: #9f6ad4;">p-value</span>小於threshold(ex: 0.05)則拒絕虛無假設，表示預測變數與目標變數具有顯著相關性，加入模型。</li>
<li>將相關性最強的變數選做第一次分割的變數。</li>
<li>繼續在各自子資料集進行分割變數計算與選擇。</li>
</ul>
</li>
<li>因為樹是<span style="color: #9f6ad4;">根據統計量顯著與否</span>來判斷規則之必要性，<span style="color: #9f6ad4;">因此與rpart()不同，ctree()是不需要剪枝的(prune)</span>。</li>
<li>另參數na.action預設為na.pass (即不採取任何動作)。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(party)
fit_ctree &lt;- ctree(survived ~ ., data = trainingData)
plot(fit_ctree)
predicted.ctree &lt;- predict(object = fit_ctree, newdata = testData)

tbl_ctree &lt;-table(predicted = predicted.ctree, actuals = testData$survived)
tbl_ctree

#          actuals
# predicted  No Yes
#       No  140  29
#       Yes  22  71

# Accuracy
accuracy &lt;- sum(diag(tbl_ctree)) / sum(tbl_ctree)
accuracy
# [1] 0.8053435</pre><p>可以發現模型準確率為80.5%，跟rpart演算法的k-fold交叉驗證的平均正確率80.4%沒有差太多。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step 11: 模型比較(2)-隨機森林(Random Forest)</h3>
<ul>
<li>隨機森林是一個<span style="color: #9f6ad4;">集成學習法(ensemble learning)</span>，意思是將幾個建立好的模型結果整合在一起，以提升預測準確率。</li>
<li>由集成學習法建立的模型<span style="color: #9f6ad4;">較能不容易發生過度配適的問題</span>，雖然提供較好的預測，但在推論和解釋度方面就會有所限制。</li>
<li>隨機森林由好幾個決策樹所組成，而不同決策樹是由不同隨機抽取的預測變數形成的。</li>
<li>而且特別的是，<span style="text-decoration: underline;">隨機森林不止對列(Row)進行抽樣，亦對行(Column)進行抽樣</span>，因此所產生的子集資料，其實是行與列同時抽樣後的結果。</li>
<li>對<span style="color: #9f6ad4;">列抽樣</span>，可以部分解決因<span style="color: #9f6ad4;">類別不平衡(Class Imbalance)</span>對預測帶來的問題；而對<span style="color: #9f6ad4;">行抽樣</span>，則可解決部分因<span style="color: #9f6ad4;">共線性(collinearity)</span>對預測造成的問題。<br />
(若是探討對「變數解釋性」的影響，則需要用 Lasso和Stepwise來解決)。</li>
<li>我們可用R裡面randomForest套件中的<span style="color: #9f6ad4;">randomForest()函數</span>來建立隨機森林。</li>
<li>參數na.action預設為na.fail (即遇到遺失值則停止執行)。因為資料集中有遺失觀測值，故必須將之調整為na.omit。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(randomForest)
set.seed(101)
fit.rf &lt;- randomForest(survived ~ ., data = trainingData, na.action = na.omit)</pre><p>檢視模型訓練結果。</p>
<ul>
<li>Number of trees: 隨機森林由500棵隨機生成的決策樹所組成。</li>
<li><span class="bash">利用<span style="color: #9f6ad4;">OOB(Out Of Bag)</span>運算出來的</span><span style="color: #9f6ad4;">錯誤率</span>為<span style="color: #0000ff;"><span style="color: #9f6ad4;">18.82%</span><span style="color: #333333;">。</span></span></li>
</ul>
<p></p><pre class="crayon-plain-tag"># Call:
#   randomForest(formula = survived ~ ., data = trainingData, na.action = na.omit) 
# Type of random forest: classification
# Number of trees: 500
# No. of variables tried at each split: 2
# 
# OOB estimate of  error rate: 18.82%
# Confusion matrix:
#      No Yes class.error
# No  463  44  0.08678501
# Yes 116 227  0.33819242</pre><p>自己驗證與計算較精確的OOB estimate正確率Accuracy為81.17%。</p><pre class="crayon-plain-tag">tbl.rf &lt;- fit.rf$confusion[,c(1,2)]
accuracy &lt;- sum(diag(tbl.rf)) / sum(tbl.rf)
accuracy
# [1] 0.8117647</pre><p>另外，我們將「<span style="color: #9f6ad4;">增加每一顆決策樹，整體誤差的改變量</span>」繪出，以輔助決策「需要多少顆決策樹，整體誤差才會趨於穩定」。</p>
<ul>
<li>當為分類樹時(classification tree)
<ul>
<li>誤差為OOB(out-of-bag) Erro Rates。</li>
<li><span style="color: #9f6ad4;">黑色實線表示整體的OOB error rate，而其他顏色虛線表示各類別的OOB Error Rate</span>。</li>
</ul>
</li>
<li>當為回歸樹時(regression tree)
<ul>
<li>誤差為OOB(out-of-bag) MSE。</li>
<li><span style="color: #9f6ad4;">只會有一條黑色實線代表整體的OOB MSE</span>。</li>
</ul>
</li>
</ul>
<p></p><pre class="crayon-plain-tag">plot(fit.rf)</pre><p>從圖中幾條線可以觀察到：</p>
<ul>
<li>整體錯誤率（黑色實線）隨著決策樹數量上升，下降到約18%並趨於穩定。</li>
<li>實際類別為Yes的錯誤率（綠色虛線）隨著決策樹數量的上升，下降到約33.8%並趨於穩定。</li>
<li>實際類別為No的錯誤率（紅色虛線）隨著決策樹數量的上升，下降到約8.6%並趨於穩定。</li>
<li>而「<span style="color: #9f6ad4;">最佳決策樹數目(ntree)</span><span style="color: #0000ff;">」</span>，約100多棵樹即足夠使誤差趨於穩定（不需要到500棵樹）。</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-1103" src="/wp-content/uploads/2018/08/Rplot04_rf_plot.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot04_rf_plot.jpeg 816w, /wp-content/uploads/2018/08/Rplot04_rf_plot-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot04_rf_plot-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot04_rf_plot-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot04_rf_plot-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot04_rf_plot-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>另外一個隨機森林中一個重要參數：mtry，表示每一個樹節點(node)在進行切割時(split)隨機抽樣的變數數量。可使用tuneRF()來調整mtry的值。</p><pre class="crayon-plain-tag">trainingData_naomit &lt;- na.omit(trainingData)
tuneRF(x = trainingData_naomit[,-2], y = trainingData_naomit[,2])


# mtry = 2  OOB error = 19.53% 
# Searching left ...
# mtry = 1 	OOB error = 20.71% 
# -0.06024096 0.05 
# Searching right ...
# mtry = 4 	OOB error = 21.53% 
# -0.1024096 0.05 
#       mtry  OOBError
# 1.OOB    1 0.2070588
# 2.OOB    2 0.1952941
# 4.OOB    4 0.2152941</pre><p>可以發現在mtry=2時，誤差最小。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1111" src="/wp-content/uploads/2018/08/Rplot05_tuneRF.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot05_tuneRF.jpeg 816w, /wp-content/uploads/2018/08/Rplot05_tuneRF-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot05_tuneRF-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot05_tuneRF-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot05_tuneRF-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot05_tuneRF-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>randomForest中類別樹預設的mtry=sqrt(p)，其中p代表x變數的數目。因為原始隨機森林模型預設值跟tuneRF建議的值相同，故我們另外不調整。</p><pre class="crayon-plain-tag">fit.rf$mtry
# [1] 2</pre><p>看每個x變數的重要性(<span style="color: #9f6ad4;">the mean decrease in Gini index</span>)，即看哪個變數對<span style="color: #9f6ad4;">損失函數Loss Function</span>最有貢獻。(*randomForest參數importance預設值為False，僅會產生the mean decrease in Gini index，如果要產生其他指標如mean decrease in accuracy，要將其改為TRUE。)</p><pre class="crayon-plain-tag">round(importance(fit.rf),2) # importance of each predictor
# or
# round(fit.rf$importance, 2)

#          MeanDecreaseGini
# pclass              27.55
# sex                100.67
# age                 55.09
# sibsp               13.89
# parch               13.40
# fare                59.79
# embarked            11.55</pre><p>將變數重要性（貢獻度）繪出。</p><pre class="crayon-plain-tag">varImpPlot(fit.rf)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1115" src="/wp-content/uploads/2018/08/Rplot06_varImpPlot.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot06_varImpPlot.jpeg 816w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>最後將調整好的模型應用在testData並評估正確性。</p><pre class="crayon-plain-tag">predicted.rf &lt;- predict(object = fit.rf, newdata = testData)
tbl.rf &lt;- table(predicted = predicted.rf, actuals = testData$survived)
accuracy &lt;- sum(diag(tbl.rf)) / sum(tbl.rf)
accuracy
# [1] 0.8307692</pre><p>隨機森林的預測準確率為83%，較原先的決策樹(accuracy = 80%)改善約4%。</p>
<h3>總結</h3>
<ul>
<li>Decision Tree 決策樹模型具有簡單易懂的樹狀邏輯圖，可解釋度高。</li>
<li>Decision Tree 決策樹對於資料的要求低，沒有常態分配與線性關係的假設，不受資料分配限制。</li>
<li>Decision Tree 決策樹的<span style="color: #9f6ad4;">有變數篩選機制</span>。
<ul>
<li>rpart演算法進行Gini Index檢定，並計算complexity parameter來進行變數篩選。</li>
<li>ctree演算法進行chi-square檢定，檢驗各投入變數是否與目標變數相關並計算p-value來看相關性的顯著效果。</li>
</ul>
</li>
<li>Decision Tree 決策樹亦<span style="color: #9f6ad4;">有空值填補機制</span> &#8211; <a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate</a>。
<ul>
<li>rpart演算法na.action預設為na.rpart。(更多有關決策樹遺失值預測請參考 <a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate in CART</a>)</li>
<li><a href="https://www.rdocumentation.org/packages/partykit/versions/1.2-2/topics/ctree">ctree演算法na.action預設為na.pass</a>，可以將其改為na.rpart。</li>
</ul>
</li>
<li>Decision Tree 演算法<span style="color: #9f6ad4;">rpart和ctree皆能處理連續型(continuous)與類別型(categorical)變數之切割</span>。</li>
<li>由Decision Tree 決策樹衍伸出的集成學習法「<span style="color: #9f6ad4;">隨機森林 random forest</span>」可以有效降低模型的錯誤率、解決過度配適、透過反覆抽樣解決類別不平衡與共線性問題。</li>
</ul>
<hr />
<p>更多<span style="color: #9f6ad4;">統計模型</span>筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>學習筆記參考連結：</p>
<ol>
<li><a href="https://www.guru99.com/r-decision-trees.html" target="_blank" rel="noopener noreferrer">Decision Tree in R with Example </a></li>
<li><a href="https://www.statmethods.net/advstats/cart.html">Tree-Based Models</a></li>
<li><a href="https://rstudio-pubs-static.s3.amazonaws.com/275285_90aaf9a2a64d43a5846a86dbcde8eba9.html">R_programming &#8211; (8)決策樹(Decision Tree)</a></li>
<li><a href="https://dzone.com/articles/decision-trees-and-pruning-in-r">Decision Trees and Pruning in R</a></li>
<li><a href="https://statinfer.com/203-3-10-pruning-a-decision-tree-in-r/">Pruning a Decision Tree in R</a></li>
<li><a href="https://www.edureka.co/blog/decision-trees/">How To Create A Perfect Decision Tree</a></li>
<li><a href="https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda">[資料分析&amp;機器學習] 第3.5講 : 決策樹(Decision Tree)以及隨機森林(Random Forest)介紹</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>Logistic Regression 羅吉斯迴歸 &#124; part2 &#8211; 模型建置、診斷與比較 &#124; R語言</title>
		<link>/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/</link>
					<comments>/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Tue, 28 Aug 2018 06:24:35 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[AUC]]></category>
		<category><![CDATA[cart]]></category>
		<category><![CDATA[confusion matrix]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[logistic regression]]></category>
		<category><![CDATA[model diagnostic]]></category>
		<category><![CDATA[neural networks]]></category>
		<category><![CDATA[random forest]]></category>
		<category><![CDATA[ROC]]></category>
		<category><![CDATA[support vector machine]]></category>
		<category><![CDATA[svm]]></category>
		<category><![CDATA[模型診斷]]></category>
		<category><![CDATA[混亂矩陣]]></category>
		<category><![CDATA[羅吉斯回歸]]></category>
		<guid isPermaLink="false">/?p=851</guid>

					<description><![CDATA[<p>Logistic Regression, 羅吉斯回歸模型，適用於預測二元類別目標變數的發生機率(p)，和線性回歸模型類似，與線性回歸主要不同之處在於：(1) 目 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p class=""></p>
<p>Logistic Regression, 羅吉斯回歸模型，適用於<span style="color: #9f6ad4;">預測<span style="text-decoration: underline;">二元類別目標變數</span>的發生機率(p)</span>，和線性回歸模型類似，與線性回歸主要不同之處在於：(1) 目標變數是目標事件發生機率P經過log函數轉換成log odds值才進行線性預測，且(2)羅吉斯回歸的各項參數是透過最大概似法(MLE)進行估計的。</p>
<h3>Logistic Regression part1 前情提要</h3>
<p>「<a href="/logistic_regression_part1/" target="_blank" rel="noopener noreferrer">Logistic Regression &#8211; part1 &#8211; 資料探勘與處理</a>」篇的重點回顧：</p>
<ol>
<li>探索預測變數自身分佈情況以及各預測變數與目標變數之間的關係。</li>
<li>資料預處理，將類別水準直過多的變數進行簡化。</li>
<li>隨機產生訓練與測試資料集。</li>
<li>計算各預測變數的Information Value來初步判斷各預測變數對目標變數的影響程度。</li>
<li>依據IV值初步從13個變數篩選出6個變數包括：RELATIONSHIP, MARITALSTATUS, AGE, EDUCATIONNUM, CAPITALGAIN,OCCUPATION_rep。</li>
</ol>
<h3>分析資料與問題</h3>
<p>Problem: 預測薪資大於50K(ABOVE50K)的影響特徵因素有哪些？</p>
<p>Data: <a href="https://archive.ics.uci.edu/ml/datasets/adult">adult dataset</a> (snapshot)</p>
<p>目標變數(1)：ABOVE50K</p>
<p>預測變數(13)：AGE, WORKCLASS, EDUCATION, EDUCATIONNUM, MARITALSTATUS, OCCUPATION, RELATIONSHIP, RACE, SEX, CAPITALGAIN, CAPITALLOSS, HOURSPERWEEK, NATIVECOUNTRY</p>
<h3>分析步驟</h3>
<p>(part 2 篇會說明步驟6~8的部分)(步驟1~5請參考「<a href="/logistic_regression_part1/" target="_blank" rel="noopener noreferrer">Logistic Regression &#8211; part1 &#8211; 資料探勘與處理</a>」)</p>
<ol>
<li><span style="color: #000000;">資料載入與檢視</span></li>
<li><span style="color: #000000;">資料探勘</span></li>
<li><span style="color: #000000;">資料前處理</span></li>
<li><span style="color: #000000;">產生訓練資料集與測試資料集</span></li>
<li><span style="color: #000000;">計算特徵變數IV值(Information Value)，篩選變數</span></li>
<li><span style="color: #9f6ad4;">訓練模型與預測</span></li>
<li><span style="color: #9f6ad4;">模型診斷與調整</span></li>
<li><span style="color: #9f6ad4;">模型比較(v.s. Machine Learning Methods)</span></li>
</ol>
<h3>6. 訓練模型與預測</h3>
<p>接續「<a href="/logistic_regression_part1/" target="_blank" rel="noopener noreferrer">Logistic Regression &#8211; part1 &#8211; 資料探勘與處理</a>」，我們將近一步針對初步篩選出的6預測變數進行羅基斯回歸模型分析。</p><pre class="crayon-plain-tag">logitMod &lt;- glm(formula = ABOVE50K_y ~ RELATIONSHIP + MARITALSTATUS + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
                data = trainingData,family = binomial(link = "logit"))
summary(logitMod)

# Call:
#   glm(formula = ABOVE50K_y ~ RELATIONSHIP + MARITALSTATUS + AGE + 
#         EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep, family = binomial(link = "logit"), 
#       data = trainingData)
# 
# Deviance Residuals: 
#   Min       1Q   Median       3Q      Max  
# -5.2263  -0.5640  -0.2258  -0.0528   3.8139  
# 
# Coefficients:
#   Estimate Std. Error z value             Pr(&gt;|z|)    
# (Intercept)                         -6.2679390  0.3703477 -16.924 &lt; 0.0000000000000002 ***
#   RELATIONSHIP Not-in-family          -0.2201571  0.3288849  -0.669             0.503237    
# RELATIONSHIP Other-relative         -1.5525876  0.3172511  -4.894     0.00000098869035 ***
#   RELATIONSHIP Own-child              -1.6632915  0.3252302  -5.114     0.00000031507745 ***
#   RELATIONSHIP Unmarried              -0.6486702  0.3428225  -1.892             0.058472 .  
# RELATIONSHIP Wife                   -0.0196633  0.0760145  -0.259             0.795884    
# MARITALSTATUS Married-AF-spouse      2.4570045  0.6658794   3.690             0.000224 ***
#   MARITALSTATUS Married-civ-spouse     1.9165557  0.3331962   5.752     0.00000000881763 ***
#   MARITALSTATUS Married-spouse-absent -0.0649140  0.2672163  -0.243             0.808062    
# MARITALSTATUS Never-married         -0.3668251  0.1008955  -3.636             0.000277 ***
#   MARITALSTATUS Separated             -0.1122524  0.1867781  -0.601             0.547845    
# MARITALSTATUS Widowed               -0.1851853  0.1746133  -1.061             0.288897    
# AGE                                  0.0217164  0.0017943  12.103 &lt; 0.0000000000000002 ***
#   EDUCATIONNUM                         0.3058023  0.0103400  29.575 &lt; 0.0000000000000002 ***
#   CAPITALGAIN                          0.0003108  0.0000118  26.331 &lt; 0.0000000000000002 ***
#   OCCUPATION_repBlue-Collar           -0.4800489  0.0701546  -6.843     0.00000000000777 ***
#   OCCUPATION_repOther/Unknown         -1.3267766  0.1312315 -10.110 &lt; 0.0000000000000002 ***
#   OCCUPATION_repProfessional           0.1688849  0.0799712   2.112             0.034702 *  
#   OCCUPATION_repService               -0.4061912  0.0845737  -4.803     0.00000156457927 ***
#   OCCUPATION_repWhite-Collar           0.2455151  0.0699361   3.511             0.000447 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
# Null deviance: 25162  on 22791  degrees of freedom
# Residual deviance: 15492  on 22772  degrees of freedom
# AIC: 15532
# 
# Number of Fisher Scoring iterations: 7</pre><p>除了摘要模型，我們另外針對共線性進行檢查。發現變數RELATIONSHIP和MARITALSTATUS有共線性。</p><pre class="crayon-plain-tag"># 模型共線性檢查VIF(希望VIF值都在4以下)
library(car)
vif(logitMod)

#                     GVIF Df GVIF^(1/(2*Df))
# RELATIONSHIP   55.704490  5        1.494820
# MARITALSTATUS  56.687207  6        1.399986
# AGE             1.177411  1        1.085086
# EDUCATIONNUM    1.384560  1        1.176673
# CAPITALGAIN     1.018983  1        1.009447
# OCCUPATION_rep  1.531311  5        1.043533</pre><p>因此，我們根據RELATIONSHIP和MARITALSTATUS的IV值，捨棄其中較低者，重新再進行一次模型建置。</p><pre class="crayon-plain-tag">logitMod &lt;- glm(formula = ABOVE50K_y ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
                data = trainingData,family = binomial(link = "logit"))

summary(logitMod)

# Call:
#   glm(formula = ABOVE50K_y ~ RELATIONSHIP + AGE + EDUCATIONNUM + 
#         CAPITALGAIN + OCCUPATION_rep, family = binomial(link = "logit"), 
#       data = trainingData)
# 
# Deviance Residuals: 
#   Min       1Q   Median       3Q      Max  
# -5.2305  -0.5663  -0.2319  -0.0590   3.5991  
# 
# Coefficients:
#   Estimate  Std. Error z value
# (Intercept)                 -4.39303806  0.15173422 -28.952
# RELATIONSHIP Not-in-family  -2.34469608  0.05835820 -40.178
# RELATIONSHIP Other-relative -3.02210032  0.26185701 -11.541
# RELATIONSHIP Own-child      -3.72243110  0.16395790 -22.704
# RELATIONSHIP Unmarried      -2.66495726  0.09783246 -27.240
# RELATIONSHIP Wife           -0.00985696  0.07585158  -0.130
# AGE                          0.02285379  0.00171654  13.314
# EDUCATIONNUM                 0.30497481  0.01032637  29.534
# CAPITALGAIN                  0.00031083  0.00001181  26.312
# OCCUPATION_repBlue-Collar   -0.47777138  0.07013154  -6.813
# OCCUPATION_repOther/Unknown -1.33506791  0.13132122 -10.166
# OCCUPATION_repProfessional   0.16489267  0.07982984   2.066
# OCCUPATION_repService       -0.40645290  0.08448125  -4.811
# OCCUPATION_repWhite-Collar   0.24924877  0.06989076   3.566
# Pr(&gt;|z|)    
# (Intercept)                 &lt; 0.0000000000000002 ***
#   RELATIONSHIP Not-in-family  &lt; 0.0000000000000002 ***
#   RELATIONSHIP Other-relative &lt; 0.0000000000000002 ***
#   RELATIONSHIP Own-child      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Unmarried      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Wife                       0.896606    
# AGE                         &lt; 0.0000000000000002 ***
#   EDUCATIONNUM                &lt; 0.0000000000000002 ***
#   CAPITALGAIN                 &lt; 0.0000000000000002 ***
#   OCCUPATION_repBlue-Collar       0.00000000000959 ***
#   OCCUPATION_repOther/Unknown &lt; 0.0000000000000002 ***
#   OCCUPATION_repProfessional              0.038871 *  
#   OCCUPATION_repService           0.00000150055853 ***
#   OCCUPATION_repWhite-Collar              0.000362 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
# Null deviance: 25162  on 22791  degrees of freedom
# Residual deviance: 15542  on 22778  degrees of freedom
# AIC: 15570
# 
# Number of Fisher Scoring iterations: 7</pre><p>並在重新檢視共線性。可以發現去除MARITALSTATUS後，所有變數的VIF值都小於4了。</p><pre class="crayon-plain-tag">library(car)
vif(logitMod)

#                     GVIF Df GVIF^(1/(2*Df))
# RELATIONSHIP   1.197104  5        1.018153
# AGE            1.089616  1        1.043847
# EDUCATIONNUM   1.388249  1        1.178240
# CAPITALGAIN    1.018159  1        1.009038
# OCCUPATION_rep 1.531428  5        1.043541</pre><p>於是我們使用此模型來預測測試資料集。</p><pre class="crayon-plain-tag">prob &lt;- predict(logitMod, testData, type="response")  # predicted scores</pre><p>為了優化模型正確率，我們使用InformationValue套件中的optimalCutoff函數來尋找最適機率切點。發現最適機率值切點跟預設的0.5一樣。</p><pre class="crayon-plain-tag">library(InformationValue)
optCutOff &lt;- optimalCutoff(actuals = testData$ABOVE50K_y,predictedScores = predicted)[1]
optCutOff
# [1] 0.5</pre><p></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>決定了機率切點後，我們來驗證模型使用測試資料集的預測效果如何。我們會分別計算以下指標：</p>
<ul>
<li>Confusion Matrix
<ul>
<li>misClassification Rate</li>
<li>precision</li>
<li>Sensitivity</li>
<li>Specifisity</li>
</ul>
</li>
<li>Concordance</li>
<li>ROC/AUC</li>
</ul>
<p>Confusion Matrix: 左欄為預測值，上方欄則為實際值。</p><pre class="crayon-plain-tag"># 產生Confusion Matrix
confusionMatrix(actuals = testData$ABOVE50K_y,predictedScores = predicted,threshold = optCutOff)

#      0    1
# 0 6935 1074
# 1  481 1279</pre><p>預測錯誤率(misClassification Rate)</p><pre class="crayon-plain-tag">misClassError(actuals = testData$ABOVE50K_y,predictedScores = predicted,threshold = optCutOff)
# [1] 0.1592</pre><p>預測精準度(Precision，及所有預測為1的事件中，真實亦為1的事件比率）。</p><pre class="crayon-plain-tag">precision(actuals = testData$ABOVE50K_y,predictedScores = predicted,threshold = optCutOff)
# [1] 0.7267045</pre><p>Sensitivity</p>
<p>$$Sensitivity=\frac{\sharp\space Actual\space 1&#8217;s\space and\space Predicted\space as\space 1&#8217;s}{\sharp\space of\space Actual\space 1&#8217;s}$$</p>
<p>又稱Recall(捕捉率），即真實為1，且被正確預測為1的比例。（通常與Precision精準度成反比）</p><pre class="crayon-plain-tag">sensitivity(testData$ABOVE50K_y, predicted, threshold = optCutOff)
# [1] 0.5435614</pre><p>Specificity</p>
<p class="">$$Specificity=\frac{\sharp\space Actual\space 0&#8217;s\space and\space Predicted\space as\space 0&#8217;s}{\sharp\space of\space Actual\space 0&#8217;s}$$</p>
<p>即真實為0，且被正確預測為0的比例。</p><pre class="crayon-plain-tag">specificity(testData$ABOVE50K_y, predicted, threshold = optCutOff)
# [1] 0.9351402</pre><p>Concordance。則為所有預測結果(0,1)成對的機率值中，真實為1的事件，1的機率高於0的機率佔所有成對資料的比例。<span style="color: #9f6ad4;">理想中，該比例越高越好</span>，即表示所有預測(0,1)的機率值，若真實為1，則1的預測機率理應都大於0的預測機率。（discordance則為相反結果的比例，tied則為機率無差別的結果比例，三比例相加應為100%）。</p><pre class="crayon-plain-tag">Concordance(testData$ABOVE50K_y, predicted)

# $Concordance
# [1] 0.8866298
# 
# $Discordance
# [1] 0.1133702
# 
# $Tied
# [1] -0.00000000000000004163336
# 
# $Pairs
# [1] 17449848</pre><p>ROC/AUC。可以發現在ROC曲線下面積為88.85%。理想中，會希望ROC曲線前段越陡越好，後段越緩越好。表示模型整體預測能力的好壞。</p><pre class="crayon-plain-tag">plotROC(actuals = testData$ABOVE50K_y,predictedScores = predicted)</pre><p><img loading="lazy" class="alignnone size-large wp-image-972" src="/wp-content/uploads/2018/08/pic19-1024x928.png" alt="logistic regression" width="960" height="870" srcset="/wp-content/uploads/2018/08/pic19-1024x928.png 1024w, /wp-content/uploads/2018/08/pic19-300x272.png 300w, /wp-content/uploads/2018/08/pic19-768x696.png 768w, /wp-content/uploads/2018/08/pic19-1140x1034.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>總結以上指標數據，可以發現模型預測能力還不錯，錯誤率15.92%都還在能接受範圍內。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>7. 模型診斷與調整</h3>
<p>為了近一步看看模型有無優化空間以及合適度，常見做法包括(1)殘差分析 (2)K-fold Cross Validation (3)Bootstrap (4)StepwiseRegression。我們這邊就以逐步回歸為範例，來看看模型是否有優化空間。</p>
<p>我們同時使用逐步向後/向前法，並比較兩法最後的結果(AIC)是否有差。</p><pre class="crayon-plain-tag">為了進一步簡化模型，我們使用逐步向前/向後法來挑選變數
# 依據每一個變數加入/移除模型後AIC資訊的變化，來判斷變數對模型的效果
# full model設定為我們剛剛建模結果
m_full &lt;- logitMod 
# 基礎只有截距的模型
m_null &lt;- glm(formula = ABOVE50K_y ~ 1, family = binomial(link = "logit"),data = trainingData)

# backward selection 逐步向後法
stepModBack &lt;- step(object = m_full,scope = list(lower = m_null, upper = m_full),direction = "backward",trace = F)
summary(stepModBack) #模型結果的AIC跟原始模型相同為15570

# Call:
#   glm(formula = ABOVE50K_y ~ RELATIONSHIP + AGE + EDUCATIONNUM + 
#         CAPITALGAIN + OCCUPATION_rep, family = binomial(link = "logit"), 
#       data = trainingData)
# 
# Deviance Residuals: 
#   Min       1Q   Median       3Q      Max  
# -5.2305  -0.5663  -0.2319  -0.0590   3.5991  
# 
# Coefficients:
#   Estimate  Std. Error z value
# (Intercept)                 -4.39303806  0.15173422 -28.952
# RELATIONSHIP Not-in-family  -2.34469608  0.05835820 -40.178
# RELATIONSHIP Other-relative -3.02210032  0.26185701 -11.541
# RELATIONSHIP Own-child      -3.72243110  0.16395790 -22.704
# RELATIONSHIP Unmarried      -2.66495726  0.09783246 -27.240
# RELATIONSHIP Wife           -0.00985696  0.07585158  -0.130
# AGE                          0.02285379  0.00171654  13.314
# EDUCATIONNUM                 0.30497481  0.01032637  29.534
# CAPITALGAIN                  0.00031083  0.00001181  26.312
# OCCUPATION_repBlue-Collar   -0.47777138  0.07013154  -6.813
# OCCUPATION_repOther/Unknown -1.33506791  0.13132122 -10.166
# OCCUPATION_repProfessional   0.16489267  0.07982984   2.066
# OCCUPATION_repService       -0.40645290  0.08448125  -4.811
# OCCUPATION_repWhite-Collar   0.24924877  0.06989076   3.566
# Pr(&gt;|z|)    
# (Intercept)                 &lt; 0.0000000000000002 ***
#   RELATIONSHIP Not-in-family  &lt; 0.0000000000000002 ***
#   RELATIONSHIP Other-relative &lt; 0.0000000000000002 ***
#   RELATIONSHIP Own-child      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Unmarried      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Wife                       0.896606    
# AGE                         &lt; 0.0000000000000002 ***
#   EDUCATIONNUM                &lt; 0.0000000000000002 ***
#   CAPITALGAIN                 &lt; 0.0000000000000002 ***
#   OCCUPATION_repBlue-Collar       0.00000000000959 ***
#   OCCUPATION_repOther/Unknown &lt; 0.0000000000000002 ***
#   OCCUPATION_repProfessional              0.038871 *  
#   OCCUPATION_repService           0.00000150055853 ***
#   OCCUPATION_repWhite-Collar              0.000362 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
# Null deviance: 25162  on 22791  degrees of freedom
# Residual deviance: 15542  on 22778  degrees of freedom
# AIC: 15570
# 
# Number of Fisher Scoring iterations: 7


# forward selection 逐步向前法
stepModForward &lt;- step(object = m_null,scope = list(lower = m_null, upper = m_full),direction = "forward",trace = F)
summary(stepModForward) 
# 向前法、向後法模型結果AIC值皆與原始模型相同為15570，且模型皆相同
# 因此我們會確定採用原始模型logigMod

# Call:
#   glm(formula = ABOVE50K_y ~ RELATIONSHIP + EDUCATIONNUM + CAPITALGAIN + 
#         OCCUPATION_rep + AGE, family = binomial(link = "logit"), 
#       data = trainingData)
# 
# Deviance Residuals: 
#   Min       1Q   Median       3Q      Max  
# -5.2305  -0.5663  -0.2319  -0.0590   3.5991  
# 
# Coefficients:
#   Estimate  Std. Error z value
# (Intercept)                 -4.39303806  0.15173422 -28.952
# RELATIONSHIP Not-in-family  -2.34469608  0.05835820 -40.178
# RELATIONSHIP Other-relative -3.02210032  0.26185701 -11.541
# RELATIONSHIP Own-child      -3.72243110  0.16395790 -22.704
# RELATIONSHIP Unmarried      -2.66495726  0.09783246 -27.240
# RELATIONSHIP Wife           -0.00985696  0.07585158  -0.130
# EDUCATIONNUM                 0.30497481  0.01032637  29.534
# CAPITALGAIN                  0.00031083  0.00001181  26.312
# OCCUPATION_repBlue-Collar   -0.47777138  0.07013154  -6.813
# OCCUPATION_repOther/Unknown -1.33506791  0.13132122 -10.166
# OCCUPATION_repProfessional   0.16489267  0.07982984   2.066
# OCCUPATION_repService       -0.40645290  0.08448125  -4.811
# OCCUPATION_repWhite-Collar   0.24924877  0.06989076   3.566
# AGE                          0.02285379  0.00171654  13.314
# Pr(&gt;|z|)    
# (Intercept)                 &lt; 0.0000000000000002 ***
#   RELATIONSHIP Not-in-family  &lt; 0.0000000000000002 ***
#   RELATIONSHIP Other-relative &lt; 0.0000000000000002 ***
#   RELATIONSHIP Own-child      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Unmarried      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Wife                       0.896606    
# EDUCATIONNUM                &lt; 0.0000000000000002 ***
#   CAPITALGAIN                 &lt; 0.0000000000000002 ***
#   OCCUPATION_repBlue-Collar       0.00000000000959 ***
#   OCCUPATION_repOther/Unknown &lt; 0.0000000000000002 ***
#   OCCUPATION_repProfessional              0.038871 *  
#   OCCUPATION_repService           0.00000150055853 ***
#   OCCUPATION_repWhite-Collar              0.000362 ***
#   AGE                         &lt; 0.0000000000000002 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
# Null deviance: 25162  on 22791  degrees of freedom
# Residual deviance: 15542  on 22778  degrees of freedom
# AIC: 15570
# 
# Number of Fisher Scoring iterations: 7</pre><p>我們發現不管是逐步向前或向後法，最後結果都與原始logitMod的結果是一樣的。</p>
<p>而理想中好的模型，偏差平方和(Deviance)應該在+-3之內，我們在此透過繪圖來檢視。</p><pre class="crayon-plain-tag"># 我們建立一個存放模型偏差殘差的資料表
index &lt;- 1:dim(trainingData)[1]
# deviance residuals
dev_resid &lt;- residuals(logitMod)
above50k &lt;- trainingData$ABOVE50K
dff &lt;- data.frame(index,dev_resid,above50k)

ggplot(data = dff, mapping = aes(x = index,y = dev_resid,color = above50k)) +
  geom_point() +
  geom_hline(yintercept = 3,linetype = "dashed", color = "blue") +
  geom_hline(yintercept = -3,linetype = "dashed", color = "blue") +
  labs(title = "Plot of Deviance Residuals") +
  theme(plot.title = element_text(hjust = 0.5))</pre><p>可以發現偏差平方和皆都落在+-3範圍內(藍色虛線)。</p>
<p><img loading="lazy" class="alignnone size-large wp-image-973" src="/wp-content/uploads/2018/08/pic20-1024x928.png" alt="logistic regression" width="960" height="870" srcset="/wp-content/uploads/2018/08/pic20-1024x928.png 1024w, /wp-content/uploads/2018/08/pic20-300x272.png 300w, /wp-content/uploads/2018/08/pic20-768x696.png 768w, /wp-content/uploads/2018/08/pic20-1140x1034.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<h3>8. 模型比較(v.s. Machine Learning Methods)</h3>
<p>我們將傳統羅吉斯回歸預測結果與近期新穎的機器學習法預測結果做比較。</p>
<h4>8-1. 類神經網路Neural Netwoarks(NN)</h4>
<p></p><pre class="crayon-plain-tag">library(nnet)
# A formula of the form class ~ x1 + x2 + ...
# y should be "class"
nn1 &lt;- nnet(as.factor(ABOVE50K_y) ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
            data = trainingData,size = 40 , maxit = 500) 
# maxit: maximum number of iterations. Default 100.
# size: number of units in the hidden layer. Can be zero if there are skip-layer units.

# type只有"raw"=&gt;prob, "class"=&gt;0,1
nn1.predict &lt;- predict(object = nn1,newdata = testData,type = "raw")

misClassError(actuals = testData$ABOVE50K_y,predictedScores = nn1.predict)
# [1] 0.1522 (&lt;0.1592)</pre><p>類神經網路的預測分類錯誤率為15.22%，稍低於羅吉斯回歸的15.92%。</p>
<h4>8-2. 決策樹CART</h4>
<p></p><pre class="crayon-plain-tag">library(rpart)
tree2 &lt;- rpart(formula = as.factor(ABOVE50K_y) ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
               data = trainingData, method = "class",cp = 1e-3)
# 回傳每組預測0,1的機率結果
tree2.pred.prop &lt;- predict(object = tree2, newdata = testData, type = "prob")
# 回傳預測0,1結果
tree2.pred &lt;- predict(object = tree2,newdata = testData, type = "class")

# confusion Matrix
tb2 &lt;- table(tree2.pred, testData$ABOVE50K_y)
tb2
# tree2.pred    0    1
#          0 6965 1022
#          1  451 1331

# missclassification rate
(1022+451) / (6965+1022+451+1331)
# [1] 0.1507831</pre><p>決策樹的預測分類錯誤率為15.07%，低於羅吉斯回歸的15.92%。</p>
<h4>8-3. 隨機森林Random Forest(RF)</h4>
<p></p><pre class="crayon-plain-tag"># 透過產生bootstrapped的決策樹森林來優化模型預測正確率
library(randomForest)
rf3 &lt;- randomForest(as.factor(ABOVE50K_y) ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
                    data = trainingData, n_tree = 100)
rf3.pred.prob &lt;- predict(object = rf3, newdata = testData, type = "prob")
rf3.pred &lt;- predict(rf3, newdata = testData, type = "class")

# confusion matrix
tb3 &lt;- table(rf3.pred, testData$ABOVE50K_y)
tb3

# rf3.pred    0    1
#        0 6908  968
#        1  508 1385

# missclassification rate
(968+508) / (6908+968+508+1385)
# [1] 0.1510902</pre><p>隨機森林的預測分類錯誤率為15.11%，低於羅吉斯回歸的15.92%。</p>
<h4>8-4. 支援向量機Support Vector Machine(SVM)</h4>
<p></p><pre class="crayon-plain-tag">library(kernlab)
svm4 &lt;- ksvm(as.factor(ABOVE50K_y) ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,data = trainingData)
svm4.pred.prob &lt;- predict(svm4,testData, type = "decision")
svm4.pred &lt;- predict(svm4,testData, type = "response")

tb4 &lt;- table(svm4.pred, testData$ABOVE50K_y)
tb4

# svm4.pred    0    1
#         0 6976 1060
#         1  440 1293

# missclassification rate
(440+1060) / (6976+1060+440+1293)
# [1] 0.1535469</pre><p>SVM的預測分類錯誤率為15.35%，亦低於羅吉斯回歸的15.92%。</p>
<p>綜合以上結果，我們摘要傳統羅吉斯回歸與機器學習法的預測結果如下表：</p>
<table style="border-collapse: collapse; width: 100%; height: 115px;" border="1">
<tbody>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">演算法</td>
<td style="width: 50%; height: 23px;">預測錯誤率</td>
</tr>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">傳統羅吉斯回歸</td>
<td style="width: 50%; height: 23px;">15.92%</td>
</tr>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">類神經網路</td>
<td style="width: 50%; height: 23px;">15.22%</td>
</tr>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">決策樹(CART)</td>
<td style="width: 50%; height: 23px;">15.11%</td>
</tr>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">支持向量機(SVM)</td>
<td style="width: 50%; height: 23px;">15.35%</td>
</tr>
</tbody>
</table>
<p>可以發現機器學習法的對所有測試資料的預測錯誤率沒有比傳統羅吉斯回歸差距太大。</p>
<p>Confusion Matrix看的是模型對於<span style="color: #9f6ad4;">所有data</span>的預測正確度(accuracy)，<span style="color: #9f6ad4;">但因為我們的大部分的資料都是&lt;=50K，所以我們更會比較在不同資料使用水平下，模型在正確度的表現</span>。<span style="color: #9f6ad4;">我們在此會使用<span style="text-decoration: underline;">ROC曲線及曲線下面積AUC</span>來看比較五個模型分別在<span style="text-decoration: underline;">不同資料水平下的模型預測正確率(accuracy)表現</span></span>。</p>
<p>先載入套件ROCR。並替所有模型預測結果產生一組放置(X軸)False Positive Rate和(Y軸)True Positive Rate的data frame，以便後續繪製各模型的ROC曲線。</p>
<p>首先是Logistic Regression的部份如下：</p><pre class="crayon-plain-tag">library(ROCR)

# logistic regression:
# 使用ROC評估分類模型前，都會先創造一個prediction物件，用來將輸入資料標準化
# predictions 為使用測試資料集的模型(logitMod)預測結果(probabilities)，
# 而labels為真實測試集的資料類別(category)
# 兩者個維度必須相同
pr &lt;- prediction(predictions = prob, labels = testData$ABOVE50K_y)
# 產生成效物件。所有的預測評估都會使用這個函數。measure為y軸指標，x.measure為x軸指標。
# 這邊我們會放上ROC曲線的x軸和y軸資訊，分別為1-specificity(x.measure)和sensitivity/recall(measure)
prf &lt;- performance(prediction.obj = pr,measure = "tpr", x.measure = 'fpr')
# 建立一個TP rate和FP rate的data frame
dd &lt;- data.frame(FP = prf@x.values[[1]], TP = prf@y.values[[1]])</pre><p>其他機器學習法如法炮製如下：</p><pre class="crayon-plain-tag"># Neural Networks (NN):
pr1 &lt;- prediction(predictions = nn1.predict,labels = testData$ABOVE50K_y)
prf1 &lt;- performance(prediction.obj = pr1,measure = "tpr",x.measure = "fpr")
dd1 &lt;- data.frame(FP = prf1@x.values[[1]], TP = prf1@y.values[[1]])

# CART:
pr2 &lt;- prediction(predictions = tree2.pred.prop[,2], labels = testData$ABOVE50K_y)
prf2 &lt;- performance(prediction.obj = pr2,measure = "tpr", x.measure = "fpr")
dd2 &lt;- data.frame(FP = prf2@x.values[[1]], TP = prf2@y.values[[1]])

# Random Forest (RF):
pr3 &lt;- prediction(predictions = rf3.pred.prob[,2],labels = testData$ABOVE50K_y)
prf3 &lt;- performance(prediction.obj = pr3, measure = "tpr", x.measure = "fpr")
dd3 &lt;- data.frame(FP = prf3@x.values[[1]], TP = prf3@y.values[[1]])

# SVM:
pr4 &lt;- prediction(predictions = svm4.pred.prob,labels = testData$ABOVE50K_y)
prf4 &lt;- performance(prediction.obj = pr4,measure = "tpr", x.measure = "fpr")
dd4 &lt;- data.frame(FP = prf4@x.values[[1]], TP = prf4@y.values[[1]])</pre><p>準備好各模型預測結果dd, dd1, ~ ,dd4的data frame後，我們將他們轉換成ROC曲線。</p><pre class="crayon-plain-tag"># 將以上幾個模型的ROC曲線畫出：
library(ggplot2)
g &lt;- 
  ggplot() +
    geom_line(data = dd,mapping = aes(x = FP, y = TP, color = 'Logistic Regression')) +
    geom_line(data = dd1,mapping = aes(x = FP, y = TP, color = 'Neural Networks')) +
    geom_line(data = dd2,mapping = aes(x = FP, y = TP, color = 'CART')) +
    geom_line(data = dd3,mapping = aes(x = FP, y = TP, color = 'Random Forest')) +
    geom_line(data = dd4,mapping = aes(x = FP, y = TP, color = 'Support Vector Machine')) +
    geom_segment(mapping = aes(x = 0, xend = 1, y = 0, yend = 1)) +
    ggtitle(label = "ROC Curve") +
    labs(x = "False Positive Rate", y = "True Positive Rate")
g +
  scale_color_manual(name = "classifier",values = c('Logistic Regression'='#E69F00', 
                                                    'Neural Networks'='#56B4E9', 'CART'='#009E73', 
                                                    'Random Forest'='#D55E00', 'Support Vector Machine'='#0072B2'))</pre><p>從圖中可以觀察到，<span style="color: #9f6ad4;">模型中以Neural Networks表現最好，及能使用最少的資料，可以捕捉到最多的目標事件1</span>。(理想中，ROC曲線前段越陡越好，後段則越緩越佳）。</p>
<p><img loading="lazy" class="alignnone size-large wp-image-974" src="/wp-content/uploads/2018/08/pic21-1024x928.png" alt="logistic regression" width="960" height="870" srcset="/wp-content/uploads/2018/08/pic21-1024x928.png 1024w, /wp-content/uploads/2018/08/pic21-300x272.png 300w, /wp-content/uploads/2018/08/pic21-768x696.png 768w, /wp-content/uploads/2018/08/pic21-1140x1034.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>我們進一步將ROC曲線下面積AUC計算出來。</p><pre class="crayon-plain-tag"># AUC
auc &lt;- 
  rbind(performance(pr, measure = 'auc')@y.values[[1]],
        performance(pr1, measure = 'auc')@y.values[[1]],
        performance(pr2, measure = 'auc')@y.values[[1]],
        performance(pr3, measure = 'auc')@y.values[[1]],
        performance(pr4, measure = 'auc')@y.values[[1]])

rownames(auc) &lt;- (c('Logistic Regression', 'Neural Networks', 'CART',
                    'Random Forest', 'Support Vector Machine'))
colnames(auc) &lt;- 'Area Under ROC Curve'
round(auc, 4)

#                        Area Under ROC Curve
# Logistic Regression                  0.8868
# Neural Networks                      0.8970
# CART                                 0.8543
# Random Forest                        0.8646
# Support Vector Machine               0.8651</pre><p>可以發現，Neural Networks曲線下面積最高，Logistic Regression則次之。<span style="color: #9f6ad4;">但因為類神經網路有諸多黑箱邏輯，故為了有更多的解釋基礎，一般來說，還是會選擇使用羅吉斯回歸結果來說明</span>。從此範例也可發現，傳統羅吉斯回歸的對此資料集的預測能力並不亞於諸多新穎的機器學習法。</p>
<h3>總結</h3>
<ul>
<li>羅吉斯回歸與回歸模型相似，必須符合許多基本假設，也因此對資料分配有較多要求，資料前處理的需求亦較多（遺失值、離群值、資料分佈）。</li>
<li>回歸模型基本上並不包含變數篩選，所以事前會使用IV(information value)來做變數初步篩選，事後也會使用VIF檢視共線性。</li>
<li>和線性回歸一樣，羅吉斯會將類別型變數轉換成虛擬變數，因此要避免類別水準值過多的情形。</li>
<li>評估模型適合度，需綜合考量<span style="text-decoration: underline;">整體錯誤率(misclassification rate)</span>與在<span style="text-decoration: underline;"><span style="color: #9f6ad4; text-decoration: underline;">不同資料量水平下的捕捉率(ROC/AUC)</span></span>。</li>
<li>羅吉斯回歸適合用來預<span style="color: #9f6ad4;">測類別變數（二元變數）的<span style="text-decoration: underline;"><strong>機率</strong></span></span>。</li>
<li>模型調整的工約80%是在於資料前處理。</li>
</ul>
<hr />
<p>更多模型建置筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考:</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
			</item>
	</channel>
</rss>
