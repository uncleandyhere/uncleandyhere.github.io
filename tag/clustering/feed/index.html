<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>clustering &#8211; 果醬珍珍•JamJam</title>
	<atom:link href="/tag/clustering/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>健忘女孩Jam的學習筆記和生活雜記</description>
	<lastBuildDate>Fri, 03 Jul 2020 02:31:05 +0000</lastBuildDate>
	<language>zh-TW</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Partitional Clustering 切割式分群 &#124; Kmeans, Kmedoid &#124; Clustering 資料分群</title>
		<link>/partitional-clustering-kmeans-kmedoid/</link>
					<comments>/partitional-clustering-kmeans-kmedoid/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Fri, 07 Sep 2018 14:25:59 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[average silhouette width]]></category>
		<category><![CDATA[clustering]]></category>
		<category><![CDATA[elbow method]]></category>
		<category><![CDATA[gap statistic]]></category>
		<category><![CDATA[kmeans]]></category>
		<category><![CDATA[kmedoid]]></category>
		<category><![CDATA[optimal number of clusters]]></category>
		<category><![CDATA[partitional clustering]]></category>
		<category><![CDATA[最佳分群數]]></category>
		<guid isPermaLink="false">/?p=1251</guid>

					<description><![CDATA[<p>Partitional Clustering, 切割式分群，屬於資料分群屬的一種方法。資料分群屬於非監督式學習，所處理的資料是沒有正確答案/標籤/目標變數可參考 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/partitional-clustering-kmeans-kmedoid/">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Partitional Clustering, 切割式分群，屬於資料分群屬的一種方法。資料分群屬於非監督式學習，所處理的資料是沒有正確答案/標籤/目標變數可參考的。常見的切割式分群演算法包括kmeans, kmedoid。本篇將介紹分「分割式分群法」的實作與特色。</p>
<h3>資料分群簡介</h3>
<ul>
<li>資料分群是一個將資料分割成數個子集合的方法，主要目的包括：
<ul>
<li>找出資料中相近的<span style="color: #9f6ad4;">群聚(clusters)</span>，</li>
<li>找出各群的<span style="color: #9f6ad4;">代表點</span>。代表點可以是群聚中的<span style="color: #9f6ad4;">中心點(centroids)</span>或是<span style="color: #9f6ad4;">原型(prototypes)</span>。</li>
</ul>
</li>
<li>透過各群的代表點，可以達到幾個目標：
<ul>
<li>資料壓縮</li>
<li>降低雜訊</li>
<li>降低計算量</li>
</ul>
</li>
<li> 資料分群將<span style="color: #9f6ad4;">依據資料自身屬性計算彼此間的相似度</span>（物以類聚），而<span style="color: #9f6ad4;">「相似度」</span>主要有兩種型態：
<ul>
<li>「Compactness」：目標是讓子集合間差異最大化，子集合內差異最小化。如階層式分群和K-means分群。</li>
<li>「Connectedness」：目標是將可串連在一起的個體分成一群。如譜分群(Spectral Clustering)。</li>
</ul>
</li>
<li>資料分群屬於<span style="color: #9f6ad4;">非監督式學習法(Unsupervised Learning)</span>，即資料沒有標籤(unlabeled data)或沒有標準答案，<span style="color: #9f6ad4;">無法透過所謂的目標變數(response variable)來做分類之訓練</span>。也因為資料沒有標籤之緣故，與監督式學習法和強化式學習法不同，<span style="color: #9f6ad4;">非監督式學習法無法衡量演算法的正確率</span>。</li>
</ul>
<p><span style="color: #333333;"><span style="text-decoration: underline;">資料分群系列文</span>章會依序介紹以下幾種常見的分群方法：</span></p>
<ol>
<li><a href="/%e7%b5%b1%e8%a8%88-r%e8%aa%9e%e8%a8%80-%e5%88%86%e7%be%a4%e5%88%86%e6%9e%90-clustering-hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a41/">階層式分群(hierarchical clustering)</a>
<ol>
<li>聚合式階層分群法 Agglomerative Hierarchical Clustering</li>
<li>分裂式階層分群法 Divisive Hierarchical Clustering</li>
<li>最佳分群群數(Determining Optimal Clusters)</li>
</ol>
</li>
<li><a href="/%e7%b5%b1%e8%a8%88-r%e8%aa%9e%e8%a8%80-%e5%88%86%e7%be%a4%e5%88%86%e6%9e%90-clustering-partitional-clustering-%e5%88%87%e5%89%b2%e5%bc%8f%e5%88%86%e7%be%a4-k-means%e3%80%81k-medoid-2/">切割式分群(partitional clustering)</a>
<ol>
<li>K-means</li>
<li>K-medoid</li>
<li>最佳分群群數(Determining Optimal Clusters)</li>
</ol>
</li>
<li>譜分群(Spectral Clustering)</li>
</ol>
<h3>2.切割式分群(partitional clustering)</h3>
<p>我們主要會cover以下步驟以完成K-Means &amp; K-Medoid分群分析：</p>
<ul>
<li>Step 1: 載入所需套件(Packages)</li>
<li>Step 2: 資料準備</li>
<li>Step 3: 衡量群聚距離</li>
<li>Step 4: K-Means分群</li>
<li>Step 5: K-Medoid分群</li>
<li>Step 6: 決定最適分群數目
<ul>
<li>Elbow Method</li>
<li>Average silhouette Width</li>
<li>Gap Statistic</li>
</ul>
</li>
</ul>
<h4>Step 1: 載入所需套件(Packages)</h4>
<p></p><pre class="crayon-plain-tag">library(dplyr)
library(magrittr) #pipelines
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms &amp; visualization</pre><p></p>
<h4>Step 2: 資料準備</h4>
<p>我們使用R內建資料集USArrests。</p><pre class="crayon-plain-tag"># 資料預處理：(1)遺失值處理(2)資料標準化(平均數為0，標準差為1)
inputData &lt;- 
  USArrests %&gt;% 
  na.omit() %&gt;% # 忽略遺失值
  scale() # 資料標準化
head(inputData)

#                Murder   Assault   UrbanPop         Rape
# Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
# Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
# Arizona    0.07163341 1.4788032  0.9989801  1.042878388
# Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
# California 0.27826823 1.2628144  1.7589234  2.067820292
# Colorado   0.02571456 0.3988593  0.8608085  1.864967207</pre><p></p>
<h4>Step 3: 衡量群聚距離</h4>
<p>分類演算法將依據<span style="color: #9f6ad4;"><strong>資料個體</strong>兩兩間之距離</span>作為<span style="text-decoration: underline;">分群基礎</span>。而不同距離衡量也將影響分類結果。我們根據<span style="text-decoration: underline;">不同距離定義</span>計算所謂的<span style="color: #9f6ad4;"><strong>「相異度/距離矩陣(dissimilarity/distance matrix)」</strong></span><strong>，作為後續分類基礎。</strong>傳統計算距離常用的方法包括：(1)歐式距離(Euclidean Distance)(2)曼哈頓距離(Manhattan Distance)。定義分別如下：</p>
<p>歐式距離(Euclidean Distance)</p>
<p>$$d_{euc}(x,y)=\sqrt{\sum_{i=1}^n(x_{i}-y_{i})^2}$$</p>
<p>曼哈頓距離(Manhattan Distance)</p>
<p>$$d_{man}(x,y)=\sum_{i=1}^n|(x_{i}-y_{i})|$$</p>
<p>其中，x和y分別代表長度為n的向量。</p>
<p>幾個R裡面<span style="text-decoration: underline;">計算</span>和<span style="text-decoration: underline;">視覺化</span><span style="color: #9f6ad4;">資料個體間「相異度/距離矩陣」</span>的函數包括：</p>
<ul>
<li>stat套件中的dist(method=&#8230;)函數：參數method預設為&#8221;euclidean&#8221;。其他可選擇的方法包括：&#8221;maximum&#8221;, &#8220;manhattan&#8221;, &#8220;canberra&#8221;, &#8220;binary&#8221;, &#8220;minkowski&#8221;。</li>
<li>factoextra套件中的get_dist()函數：參數method預設為&#8221;euclidean&#8221;。<span style="color: #9f6ad4;">和dist()函數不同的是，他支援correlation-based distance measures</span>包括&#8221;pearson&#8221;, &#8220;kendall&#8221;和 &#8220;spearman&#8221; （*可根據資料屬性來選擇適合的距離計算方法，比如說，<span style="text-decoration: underline;">correlation-based distances常被運用在基因表達數據分析(gene expression data analyses)</span>）。</li>
<li>factoextra套件中fviz_dist()函數：則可將相異度（距離）矩陣(使用get_dist()產生的物件）計算結果視覺化。</li>
</ul>
<p>以下我們就使用get_dist()計算資料個體間兩兩距離(使用預設歐式距離法)，並使用fviz_dist()將相異/距離矩陣結果視覺化。</p><pre class="crayon-plain-tag">distance &lt;- get_dist(x = inputData)
fviz_dist(dist.obj = distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))</pre><p><img loading="lazy" class="alignnone size-full wp-image-1280" src="/wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1.jpeg" alt="partitional clustering" width="1000" height="996" srcset="/wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1.jpeg 1000w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-830x827.jpeg 830w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot01_dissimilarity_matrix-1-480x478.jpeg 480w" sizes="(max-width: 1000px) 100vw, 1000px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h4>Step 4: K-Means分群</h4>
<h5>K-Means簡介</h5>
<ul>
<li>分群演算法中最出名的即為K-Means演算法。</li>
<li>是最簡單也最常使用的分群演算法。</li>
<li>K-Means會根據一些距離的測量將觀測值分成數個組別。</li>
<li><span style="color: #9f6ad4;">需要事前指定分群數目</span>。</li>
<li>目的是將極大化群內相似度，和最大化群間相異度。</li>
<li>指定群聚的平均值作為中心點(centroid)。</li>
<li><span style="color: #9f6ad4;">投入變數以連續變數為佳</span><span style="color: #9f6ad4;">（kmeans(x = &#8230;只能為數值矩陣&#8230;)）。</span></li>
</ul>
<h5>K-Means基礎概念</h5>
<ul>
<li>指定分群數k，並最小化k群組內變異總和(total intra-cluster variation or within-cluster variation)。</li>
<li>K-Means的演算法有許多，而其中標準的演算法是由Hartigan教授所提出的。Hartigan教授將<span style="color: #9f6ad4;">群聚內變異總和</span>定義為：<span style="color: #9f6ad4;">各資料點距離群中心的歐式距離平方加總</span>。<br />
$$W(C_{k})=\sum_{x_{i}\in C_{k}}(x_{i}-\mu_{k})^2$$<br />
其中，</p>
<ul>
<li>\(x_{i}\)代表群聚\(C_{k}\)中的資料點。</li>
<li>\(\mu_{k}\)代表群聚\(C_{k}\)中資料點的平均值所指定的中心點。</li>
</ul>
</li>
<li>而所有群內變異總和(total within-cluster variation)為：<br />
$$total\space within-cluster\space variation\space = \sum_{i=1}^kW(C_{k})=\sum_{i=1}^k\sum_{x_{i}\in C_{k}}(x_{i}-\mu_{k})^2$$<br />
total within-cluster variation是用來衡量群聚的緊緻度(Compactness)，我們希望這個值越小越好。</li>
</ul>
<h5>K-Means演算法</h5>
<ol>
<li>由User事先指定分群數目k。</li>
<li>演算法<span style="color: #9f6ad4;">隨機從資料集中挑選k個資料點當作初始中心點(initial centers)</span>。</li>
<li>各資料點將依據<span style="text-decoration: underline;">距離初始中心的的歐式距離</span>遠近分派指定給最近的群聚\(C_{k}\)(cluster assignment)。</li>
<li>各群聚\(C_{k}\)<span style="color: #9f6ad4;">重新計算所有群聚中資料點的平均值作為新的中心點（centroid update)</span>。並重新檢測所有資料點是否位在與其最近的中心點的群聚中。</li>
<li>重複第3~4步驟，來最小化各群聚的變異總和，直到<span style="text-decoration: underline; color: #9f6ad4;">各群聚組合趨於穩定與收斂（不再變動）(reach convergence)</span>，或<span style="text-decoration: underline; color: #9f6ad4;">已達最大迭代次數（R的kmeans函數預設為最大迭代次數為10）</span>才停止。</li>
</ol>
<h5>使用R套件stats中的kmeans函數執行K-Means分群演算法</h5>
<p>kmeans()函數幾個重要參數說明：</p>
<ul>
<li>centers: 指定分群數目k或指定初始中心點數目k。</li>
<li>nstart: 因為初始中心點是隨機選取，所以可以透過nstart參數多嘗試幾組隨機初始值，並選擇回傳最好的初始中心點分群結果。</li>
</ul>
<p>我們預設將資料分成2群(centers = 2)，並隨機執行25次初始中心點挑選與分群結果。我們可以使用str()結構函數看分群結果回傳值。其中：</p>
<ul>
<li>$cluster表示資料被指定的分群結果。</li>
<li>$center表示群聚中心點矩陣。</li>
<li>$totss表示total sum of squares。</li>
<li>$withininss: 表示within-cluster sum of squares，即單一群聚內總變異。</li>
<li>$tot.withinss: 表示total within-cluster sum of squares，即所有群聚內變異加總(sum($withinss))。</li>
<li>$size: 每群聚內資料個數。</li>
</ul>
<p></p><pre class="crayon-plain-tag">set.seed(101)
k_clust &lt;- kmeans(inputData, centers = 2, nstart = 25)
str(k_clust)

# List of 9
# $ cluster     : Named int [1:50] 2 2 2 1 2 2 1 1 2 2 ...
#  ..- attr(*, "names")= chr [1:50] "Alabama" "Alaska" "Arizona" "Arkansas" ...
# $ centers     : num [1:2, 1:4] -0.67 1.005 -0.676 1.014 -0.132 ...
#  ..- attr(*, "dimnames")=List of 2
#  .. ..$ : chr [1:2] "1" "2"
#  .. ..$ : chr [1:4] "Murder" "Assault" "UrbanPop" "Rape"
# $ totss       : num 196
# $ withinss    : num [1:2] 56.1 46.7
# $ tot.withinss: num 103
# $ betweenss   : num 93.1
# $ size        : int [1:2] 30 20
# $ iter        : int 1
# $ ifault      : int 0
# - attr(*, "class")= chr "kmeans"</pre><p>或是將分群結果印出。可以分別得知以下資訊：</p>
<ul>
<li>各群聚大小（資料個數）。</li>
<li>群聚在各維度的中心點（平均值）(2x4的矩陣)。</li>
<li>各資料列（列名稱）被分類到的群聚結果。</li>
<li>各群聚的變異總和。</li>
<li>可使用的成分(available components)，即我們可以透過kmeans產生的物件+$&#8230;取得的資訊。比如說<span style="color: #9f6ad4;">k_clust$cluster</span>即得得到各列分群結果。</li>
</ul>
<p></p><pre class="crayon-plain-tag">k_clust 

# K-means clustering with 2 clusters of sizes 30, 20
# 
# Cluster means:
#      Murder    Assault   UrbanPop       Rape
# 1 -0.669956 -0.6758849 -0.1317235 -0.5646433
# 2  1.004934  1.0138274  0.1975853  0.8469650
# 
# Clustering vector:
#        Alabama         Alaska        Arizona       Arkansas     California       Colorado    Connecticut       Delaware 
#              2              2              2              1              2              2              1              1 
#        Florida        Georgia         Hawaii          Idaho       Illinois        Indiana           Iowa         Kansas 
#              2              2              1              1              2              1              1              1 
#       Kentucky      Louisiana          Maine       Maryland  Massachusetts       Michigan      Minnesota    Mississippi 
#              1              2              1              2              1              2              1              2 
#       Missouri        Montana       Nebraska         Nevada  New Hampshire     New Jersey     New Mexico       New York 
#              2              1              1              2              1              1              2              2 
# North Carolina   North Dakota           Ohio       Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina 
#              2              1              1              1              1              1              1              2 
#   South Dakota      Tennessee          Texas           Utah        Vermont       Virginia     Washington  West Virginia 
#              1              2              2              1              1              1              1              1 
#      Wisconsin        Wyoming 
#              1              1 
# 
# Within cluster sum of squares by cluster:
# [1] 56.11445 46.74796
# (between_SS / total_SS =  47.5 %)
# 
# Available components:
#   
# [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"        
# [8] "iter"         "ifault"</pre><p>我們可以進一步使用factoextra套件中的fviz_cluster()函數來將分群結果視覺化。(*值得注意的是，當變數維度&gt;2，<strong><span style="color: #9f6ad4;">fviz_cluster會使用主成分分析</span></strong>，找出最主要的兩個主成分來作為圖形的橫軸與縱軸。括號內的百分比則分別表示主成分解釋變異佔比。更多<span style="color: #9f6ad4;"><strong>主成分分析PCA</strong></span>請參考<a href="/%e7%b5%b1%e8%a8%88-r%e8%aa%9e%e8%a8%80-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90-principal-components-analysis-pca/" target="_blank" rel="noopener noreferrer">「主成分分析(PCA)」</a>)</p><pre class="crayon-plain-tag">fviz_cluster(k_clust, data = inputData)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1287" src="/wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result.jpeg" alt="partitional clustering" width="1000" height="996" srcset="/wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result.jpeg 1000w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-830x827.jpeg 830w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot02_kmeans_clustering_result-480x478.jpeg 480w" sizes="(max-width: 1000px) 100vw, 1000px" /></p>
<p>或者是，使用傳統的成對資料（分類結果,原始資料）繪製散佈圖來檢視分群結果。</p><pre class="crayon-plain-tag">inputData %&gt;%
  as_tibble() %&gt;%
  mutate(cluster = k_clust$cluster, #新增分群結果
         state = row.names(USArrests) #將列名稱指定為原始資料標籤
         ) %&gt;%
  ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) + #使用ggplot套件繪圖（指定x,y軸），標記國家名稱，並依據分群結果上色
  geom_text()</pre><p><img loading="lazy" class="alignnone size-full wp-image-1288" src="/wp-content/uploads/2018/09/Rplot03-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot03-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot03-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot03-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot03-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot03-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot03-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot03-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>但因為通常資料維度會不止兩類，我們可以考慮使用useful套件中的plot.kmeans()函數，該<span style="color: #9f6ad4;">函數可以將多維尺度調整以將資料投影到二維空間</span>。然而這個效果似乎與factoextra套件中的fviz_cluster()函數無異，只不過還是factoextra套件中的fviz_cluster()函數更優！</p><pre class="crayon-plain-tag">library(useful)
plot.kmeans(x = k_clust, data = inputData)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1289" src="/wp-content/uploads/2018/09/Rplot04-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot04-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot04-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot04-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot04-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot04-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot04-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot04-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>也因為我們一開始需要預設分群數目k值。我們打算試試不同初始中心點數目的效果差異。</p><pre class="crayon-plain-tag"># 嘗試多種k的分群效果
set.seed(101)
k_clust &lt;- kmeans(inputData, centers = 2, nstart = 25)
k_clust_3 &lt;- kmeans(inputData, centers = 3, nstart = 25)
k_clust_4 &lt;- kmeans(inputData, centers = 4, nstart = 25)
k_clust_5 &lt;- kmeans(inputData, centers = 5, nstart = 25)


# plots to compare
p1 &lt;- fviz_cluster(k_clust, geom = "point", data = inputData) + ggtitle("k = 2")
p2 &lt;- fviz_cluster(k_clust_3, geom = "point",  data = inputData) + ggtitle("k = 3")
p3 &lt;- fviz_cluster(k_clust_4, geom = "point",  data = inputData) + ggtitle("k = 4")
p4 &lt;- fviz_cluster(k_clust_5, geom = "point",  data = inputData) + ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2) # Arrange multiple grobs on a page (將不會影響到par()中參數設定)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1290" src="/wp-content/uploads/2018/09/Rplot05-1.jpeg" alt="partitional clustering" width="1000" height="996" srcset="/wp-content/uploads/2018/09/Rplot05-1.jpeg 1000w, /wp-content/uploads/2018/09/Rplot05-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot05-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot05-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot05-1-830x827.jpeg 830w, /wp-content/uploads/2018/09/Rplot05-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot05-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot05-1-480x478.jpeg 480w" sizes="(max-width: 1000px) 100vw, 1000px" /></p>
<p>但僅憑上述嘗試，我們還是很難決定最佳群數該設定為多少。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h4>Step 5: K-Medoid分群</h4>
<p><strong>（可以依情況決定是否將Step 4的kmeans()法取代）</strong></p>
<ul>
<li>使用K-Means演算法的兩個限制為：
<ul>
<li><span style="color: #9f6ad4;">不能處理類別變數資料 （kmeans(x = &#8230;只能為數值矩陣&#8230;)）</span></li>
<li><span style="color: #9f6ad4;">容易受離群值影響</span></li>
</ul>
</li>
<li>使用K-Mediods演算法時，中心點將選選擇群內某個觀測值，而非群內平均值，就像中位數一樣，較不易受離群值所影響。是K-Means更強大的版本。</li>
<li>K-Medoids最常使用的演算法為PAM(Partitioning Around Medoid, 分割環繞物件法）。</li>
<li><span style="color: #ff0000;">K-Medoids比K-Means更強大之處在於他最小化相異度加總值，而非僅是歐式距離平方和</span>。<span style="color: #ff0000;">(The goal is to find k representative objects which minimize the sum of the dissimilarities of the observations to their closest representative object. )</span></li>
<li>但遇到資料量較大時，K-Medoid法所需要的記憶體和運算時間都是龐大的，此時可以另外考慮clara(Clustering Large Applications)函數。</li>
</ul>
<p>我們可以使用cluster套件中的pam()函數來執行。幾個重要參數包括：</p>
<ul>
<li><span style="color: #9f6ad4;">x: 可以為數值矩陣、data frame、甚是是<strong>相異度矩陣(dissimilarity matrix)</strong></span>。
<ul>
<li>若為numeric matrix or data frame須確保變數只能為數值。</li>
<li>若為dissimalirity matrix，可以是透過daisy()或dist()計算個體間距離的結果。
<ul>
<li>其中daisy()可以處理類別行變數的距離矩陣計算，須設定參數metric = c(&#8220;gower&#8221;)。</li>
<li>而傳統dist()則可以指定使用method = &#8220;euclidean&#8221; 或 &#8220;manhattan&#8221;。</li>
</ul>
</li>
</ul>
</li>
<li>k: 指定分群數目。</li>
<li>diss: TRUE/FALSE。投入的x是否為dissmilarity matrix。</li>
<li>metric: 如果投入的x為matrix或data frame，需指定計算dissimilarity matrix所用的距離衡量方式，可為&#8221;euclidean&#8221; 或 &#8220;manhattan&#8221;。若投入的x已是dissimilarity matrix，則忽略該參數。</li>
</ul>
<p></p><pre class="crayon-plain-tag">kmedoid.cluster &lt;- pam(x = inputData, k=3) 

# 分群結果視覺化
fviz_cluster(kmedoid.cluster, data = inputData,main = 'K-Medoid')</pre><p><img loading="lazy" class="alignnone size-full wp-image-1339" src="/wp-content/uploads/2018/09/Rplot12-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot12-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot12-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot12-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot12-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot12-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot12-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot12-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>並且我們可以直接利用K-Medoid產生的物件繪製silhouette plot。</p><pre class="crayon-plain-tag"># 繪製側影圖
plot(kmedoid.cluster,which.plots = 2)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1340" src="/wp-content/uploads/2018/09/Rplot13-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot13-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot13-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot13-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot13-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot13-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot13-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot13-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h4>Step 6: 決定最適分群數目</h4>
<p>以下為3個常見幫助我們決定最佳分群數的方法：</p>
<ol>
<li>Elbow Method（亦稱做Hartigan法）</li>
<li>Average Silhouette method（側影圖法）</li>
<li>Gap statistic（Gap統計量，預測-觀測）</li>
</ol>
<h4>Elbow Method</h4>
<p>根據切割式分群的目的：最小化<strong><span style="color: #9f6ad4;">各群群內變異加總</span></strong> （<span style="color: #9f6ad4;"><strong>total within-cluster variation</strong></span> 或 <span style="color: #9f6ad4;"><strong>total within-cluster sum of square，簡稱wss</strong></span>）。我們依序計算k = 1 ~ k=n各種分群結果的群內變異加總值，並將之與對應的k值繪製成圖(x軸為k值，y軸為<span style="color: #9f6ad4;">wss</span>)。並找出曲線彎曲（如膝蓋彎曲）處對應的k值，即各群群內變異加總值趨於收斂的轉折點，作為最佳的分群數目。</p><pre class="crayon-plain-tag">set.seed(123)

# function to compute total within-cluster sum of square 
wss &lt;- function(k) {
  kmeans( x = inputData, centers =  k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values &lt;- 1:15

# extract wss for 2-15 clusters
wss_values &lt;- map_dbl(k.values, wss)

plot(k.values, wss_values,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")</pre><p>由下圖可觀察到，wss約在k=4時出現轉折彎曲，即wss下降幅度開始變小（趨於穩定）。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1295" src="/wp-content/uploads/2018/09/Rplot06-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot06-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot06-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot06-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot06-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot06-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot06-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot06-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>而幸好這樣複雜的計算與繪圖過程，我們都可以透過factoextra套件中的fviz_nbclust()函數來達成。便可將上述數行程式碼縮減到一行指令。</p><pre class="crayon-plain-tag">set.seed(123)
fviz_nbclust(x = inputData,FUNcluster = kmeans, method = "wss")</pre><p><img loading="lazy" class="alignnone size-full wp-image-1296" src="/wp-content/uploads/2018/09/Rplot07-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot07-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot07-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot07-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot07-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot07-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot07-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot07-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h4>Average Silhouette Method</h4>
<ul>
<li>簡單來說，<span style="color: #9f6ad4;">average silhouette method</span>衡量<span style="color: #9f6ad4;">各分群結果的品質 ((\(C_{1} \sim C_{k})\)都會有average silhouette width)</span>。</li>
<li>該指標是透過計算各群聚中，各個物件的<a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" target="_blank" rel="noopener noreferrer">silhouette width</a>，並取群內silhouette with平均值而得。</li>
<li>每一群聚中，<span style="color: #9f6ad4;"><span style="text-decoration: underline;">各物件的silhouette with</span>衡量的是<span style="text-decoration: underline;">該物件是否被很好的歸類在合適的群聚</span></span>。
<ul>
<li><span style="color: #333333;"><strong>若silhouette with若為正數且值越大，則表示該觀測植被很好的分派到合適的群聚。</strong></span></li>
<li><span style="color: #333333;"><strong>若silhouette with值很小或甚至為負數，則表示該觀測值的分群結果不是很合適</strong></span>。</li>
</ul>
</li>
<li><span style="text-decoration: underline; color: #9f6ad4;">整個群聚</span>的<span style="text-decoration: underline; color: #9f6ad4;">average silhouette width</span>則是將各觀測值的silhouette with取平均所計算而得。<span style="color: #9f6ad4;"><strong>整個群聚的average silhouette with越大表示分群做得越好</strong></span>。</li>
<li>最後，再將所有k群聚的average silhouette with取平均，<span style="color: #9f6ad4;">得出 每一個對應k群分群結果的average silhouette width</span>。</li>
<li>於是我們可以得到k=1~k=n不同分群結果的average silhouette width。而其中<strong><span style="color: #9f6ad4;">極大化average silhouette width的k值即為最佳分群數目</span></strong>。</li>
</ul>
<p>我們先來計算與視覺化，當k=5各群的average silhouette width以及整體分群結果的average silhouette width。</p>
<p>首先我們使用cluster套件中的silhouette()函數來計算每個觀測值得silhouette width。（初始計算結果並沒有各群的平均silhouette with）</p><pre class="crayon-plain-tag">km.res &lt;- kmeans(x = inputData, centers = 5, nstart = 25)
ss &lt;- silhouette(km.res$cluster, dist(inputData))
#       cluster neighbor    sil_width
#  [1,]       3        2  0.448730181
#  [2,]       4        3  0.054455351
#  [3,]       4        1  0.432969809
#  [4,]       2        3  0.228329605
#  [5,]       4        1  0.448896074
#  [6,]       4        2  0.310290004
#  [7,]       1        2  0.279323646
#  [8,]       1        2 -0.026200144
#  [9,]       4        3  0.246185137
# [10,]       3        4  0.426879411
# [11,]       1        2  0.254134176
# [12,]       5        2  0.239121931
# [13,]       4        1  0.260389832
# [14,]       2        1  0.364248885
# [15,]       5        2  0.512603957
# [16,]       2        1  0.260830275
# [17,]       2        5  0.305423442
# [18,]       3        4  0.375176086
# [19,]       5        2  0.548907843
# [20,]       4        3  0.073438587
# [21,]       1        2  0.425221521
# [22,]       4        3  0.408874055
# [23,]       5        2  0.140353445
# [24,]       3        2  0.522765736
# [25,]       2        4  0.143376681
# [26,]       2        5  0.121906504
# [27,]       2        5  0.083351231
# [28,]       4        3  0.427993699
# [29,]       5        2  0.545620065
# [30,]       1        2  0.331314848
# [31,]       4        3  0.317371390
# [32,]       4        1  0.361348047
# [33,]       3        2  0.430379777
# [34,]       5        2  0.548983948
# [35,]       1        2 -0.016788571
# [36,]       2        1  0.246846452
# [37,]       2        1  0.143477706
# [38,]       1        2 -0.007681734
# [39,]       1        2  0.341420835
# [40,]       3        2  0.566811560
# [41,]       5        2  0.424934521
# [42,]       3        2  0.226342361
# [43,]       4        3  0.178417378
# [44,]       1        2  0.296027252
# [45,]       5        2  0.430596483
# [46,]       2        1  0.436958480
# [47,]       1        2 -0.014964942
# [48,]       5        2  0.349650799
# [49,]       5        1  0.295667125
# [50,]       2        1  0.403192800
# attr(,"Ordered")
# [1] FALSE
# attr(,"call")
# silhouette.default(x = km.res$cluster, dist = dist(inputData))
# attr(,"class")
# [1] "silhouette"</pre><p>如果要計算<span style="color: #9f6ad4;">整體分群結果的average silhouette width</span>，我們可以將結果的第三欄取平均值。</p><pre class="crayon-plain-tag">mean(ss[, 3])
# [1] 0.3030781</pre><p>如果想要計算與視覺化<span style="color: #9f6ad4;">各群的average silhouette width</span>，我們可以使用plot()函數。</p><pre class="crayon-plain-tag">plot(ss)</pre><p>從下圖我們可以觀察到：</p>
<ul>
<li>j = 1 ~ 5。表示有五個群聚\(C_{j}\)。</li>
<li>\(n_{j}\):各群觀測值數。</li>
<li>\(ave_{i\in C_{j}}s_{i}\): 群聚\(C_{j}\)的average silhouette width。</li>
<li>\(S_{i}\): 各觀測值的silhouette width。</li>
<li>圖最下方的average silhouette width則為整體分群結果的值。</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-1310" src="/wp-content/uploads/2018/09/Rplot08-1.jpeg" alt="partitional clustering" width="800" height="796" srcset="/wp-content/uploads/2018/09/Rplot08-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot08-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot08-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot08-1-768x764.jpeg 768w, /wp-content/uploads/2018/09/Rplot08-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot08-1-350x348.jpeg 350w, /wp-content/uploads/2018/09/Rplot08-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>有了以上單一k群結果的silhouette計算概念後，我們進一步比較不同k值（假設k=2 ~ 15)的分群結果品質。</p><pre class="crayon-plain-tag"># function to compute average silhouette for k clusters
avg_sil &lt;- function(k) {
  km.res &lt;- kmeans(x = inputData, centers = k, nstart = 25)
  ss &lt;- silhouette(km.res$cluster, dist(inputData))
  mean(ss[, 3])
}

# Compute and plot wss for k = 2 to k = 15
k.values &lt;- 2:15

# extract avg silhouette for 2-15 clusters
avg_sil_values &lt;- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
     type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of clusters K",
     ylab = "Average Silhouettes")</pre><p>由下圖我們可以比較出，分兩群的結果最佳，分四群的結果次之。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1311" src="/wp-content/uploads/2018/09/Rplot09-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot09-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot09-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot09-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot09-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot09-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot09-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot09-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>像Elbow Method一樣，我們也可以使用<span style="color: #9f6ad4;">fviz_nbclust函數</span>將上述結果的程式碼指令濃縮成一行並繪出。<span style="color: #9f6ad4;">只需要將參數method從&#8221;</span>wss<span style="color: #9f6ad4;">&#8220;改成</span>&#8220;silhouette<span style="color: #9f6ad4;">&#8220;即可</span>！</p><pre class="crayon-plain-tag">fviz_nbclust(inputData, kmeans, method = "silhouette")</pre><p><img loading="lazy" class="alignnone size-full wp-image-1312" src="/wp-content/uploads/2018/09/Rplot10-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot10-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot10-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot10-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot10-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot10-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot10-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot10-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h4>Gap statistic</h4>
<ul>
<li>Gap Statistic法可以應用在任何分群演算法上（比如說分裂式分群或階層式分群）。</li>
<li><strong>Gap Statistic(k): </strong>比較不同k水準值下，<span style="text-decoration: underline; color: #9f6ad4;">實際觀測值分群結果的群內總變異</span><span style="text-decoration: underline;"><span style="color: #9f6ad4; text-decoration: underline;">(\(W_{k}\))</span></span>與<span style="text-decoration: underline;"><span style="color: #9f6ad4; text-decoration: underline;">自助抽樣法B回產生樣本分群結果期望群內總變異(\(\overline{W_{k}}\))</span></span>，即衡量<span style="color: #9f6ad4;"><span style="text-decoration: underline;">觀測</span>和<span style="text-decoration: underline;">預期</span>之間的差距</span>。</li>
<li><strong>群內總變異的期望值(\(\overline{W_{k}}\)): </strong>使用<span style="color: #9f6ad4;">自助抽樣法(Bootstap)</span>進行B回重新抽樣，計算k水準值下重新抽樣樣本分群結果的群內總變異(\(W_{ki}\sim W_{kB}\))，並計算 k水準值下的群內總變異期望值，即<br />
$$E(W_{k})=mean(W_{k})=\overline{W_{k}}=\frac{1}{B}\sum_{i=1}^B\log (W_{ki})$$</li>
<li><span style="color: #333333;"><strong>群內總變異的<span style="color: #9f6ad4;">標準差Standard Deviation(\(Sd(W_{k})\))</span>:<br />
</strong>$$Sd(W_{k})=\sqrt{\frac{1}{B}\sum_{i=1}^B(\log  (W_{ki}) &#8211; \overline{W_{k}})^2 }$$</span></li>
<li><strong>群內總變異樣本平均值的<span style="color: #9f6ad4;">標準誤Standard Error(\(s_{k}\)): </span></strong>即樣本平均值(\(\overline{W_{k}}\))與真實母體平均值(\(\mu_{k}\))的變異。<span style="color: #ff0000;"><strong><br />
</strong></span>$$s_{k} = sd(k)\times \sqrt{1+\frac{1}{B}}$$</li>
<li><span style="color: #ff0000;">自助抽樣將模擬蒙地卡羅的採樣過程，也就是說每一個變數\(x_{i}\)，會依據他們的最大最小值範圍[\(\min(x_{i}),\max(x_{i})\)]，來重新計算標準化後的值。</span></li>
<li>Gap Statistic則是計算在不同k水準值下，觀測與預期值的總群內變異差異，公式如下：<br />
$$Gap(k) =\overline{W_{k}} -{W_{k}}=\frac{1}{B}\sum_{i=1}^B\log (W_{ki}) &#8211; W_{k}$$</li>
<li> 並且我們選擇使Gap(k)最大化的最小的k值，以符合：<br />
$$Gap(k)\geq Gap(k+1)-s_{k+1}$$<br />
即表示<span style="text-decoration: underline;">k群的分群結構</span>和<span style="text-decoration: underline;">虛無假設的uniform分配（沒有分群）</span>有<span style="text-decoration: underline;">很大的差距</span>。</li>
</ul>
<p>我們可以透過cluster套件中的clusGap()函數來計算不同k水平值對應的該統計量(Gap)以及標準誤(Standard Error, SE)。也因為我們要找到使最大化Gap(k)的最小k值，所以記得調整參數method=&#8221;firstmax&#8221;。將結果印出後，我們可看到每個對應k水平值的：</p>
<ul>
<li>logW: 觀測值群內總變異。</li>
<li>E.logW: 群內總變異期望值。</li>
<li>gap: E.logW &#8211; logW統計量。</li>
<li>SE.sim: 標準誤(simulation standard error)。</li>
</ul>
<p></p><pre class="crayon-plain-tag"># compute gap statistic
set.seed(123)
gap_stat &lt;- clusGap(x = inputData, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
# Print the result
print(gap_stat, method = "firstmax")

# Clustering Gap statistic ["clusGap"] from call:
# clusGap(x = inputData, FUNcluster = kmeans, K.max = 10, B = 50,     nstart = 25)
# B=50 simulated reference sets, k = 1..10; spaceH0="scaledPCA"
#   --&gt; Number of clusters (method 'firstmax'): 4
#            logW   E.logW       gap     SE.sim
#   [1,] 3.458369 3.638250 0.1798804 0.03653200
#   [2,] 3.135112 3.371452 0.2363409 0.03394132
#   [3,] 2.977727 3.235385 0.2576588 0.03635372
#   [4,] 2.826221 3.120441 0.2942199 0.03615597
#   [5,] 2.738868 3.020288 0.2814197 0.03950085
#   [6,] 2.669860 2.933533 0.2636730 0.03957994
#   [7,] 2.598748 2.855759 0.2570109 0.03809451
#   [8,] 2.531626 2.784000 0.2523744 0.03869283
#   [9,] 2.468162 2.716498 0.2483355 0.03971815
#  [10,] 2.394884 2.652241 0.2573567 0.04104674</pre><p>我們進一步使用fviz_gap_stat()函數將上面gap統計量計算結果視覺化。而根據Gap(k)統計量，最佳分群數為4組。</p><pre class="crayon-plain-tag">fviz_gap_stat(gap_stat)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1333" src="/wp-content/uploads/2018/09/Rplot11-1.jpeg" alt="partitional clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot11-1.jpeg 800w, /wp-content/uploads/2018/09/Rplot11-1-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot11-1-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot11-1-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot11-1-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot11-1-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot11-1-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h3>總結</h3>
<ul>
<li>影響切割式分群的幾個參數包括：<span style="color: #9f6ad4;">隨機起始中心點的選擇、中心點定義、以及目標式最小化「距離」衡量定義</span>。
<ul>
<li>中心點：
<ul>
<li>kmeans的中心點是群內的平均值，易受離群值影響。</li>
<li>kmedoid的中心點則是實際的點（類似中位數），不易受極端值影響。</li>
</ul>
</li>
<li>目標式：
<ul>
<li>kmeans是以最小化群內各點到中心點的「歐式距離」為目標<br />
(minimize the sum of squared euclidean distances from points to the assigned cluster)</li>
<li>kmedoid是以最小化群內各點至中心點的「變異度」為目標<br />
(minimize the sum of the dissimilarities of the observations to their closest representative object)</li>
</ul>
</li>
</ul>
</li>
<li>因為切割式分群需使用者決定<span style="color: #9f6ad4;">最適的群數</span>，因此必須依據分群目的，綜合考量不同的方法（elbow method, average silhouette width, gap statistic)計算出的不同k值對應的組內變異加總值，來做決定。</li>
<li>由於目標式最小化的標的不同，雖然kmedoid效果會比kmeans更強大，但因為kmedoid需計算n^2的相異度矩陣，會耗費時間和運算記憶體，因此如果遇到大型數據時，可以使用clara()。pam()目前有觀測值上限(<i>n &lt;= 65536)。</i></li>
</ul>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<hr />
<p>更多統計模型筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/partitional-clustering-kmeans-kmedoid/">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/partitional-clustering-kmeans-kmedoid/feed/</wfw:commentRss>
			<slash:comments>4</slash:comments>
		
		
			</item>
		<item>
		<title>Hierarchical Clustering 階層式分群 &#124; Clustering 資料分群 &#124; R 統計</title>
		<link>/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/</link>
					<comments>/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Wed, 05 Sep 2018 07:43:45 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[clustering]]></category>
		<category><![CDATA[hierarchical clustering]]></category>
		<category><![CDATA[分群]]></category>
		<category><![CDATA[分群演算法]]></category>
		<category><![CDATA[資料分群]]></category>
		<category><![CDATA[階層式分群]]></category>
		<guid isPermaLink="false">/?p=1157</guid>

					<description><![CDATA[<p>Hierarchical Clustering, 屬於資料分群的一種方法。資料分群屬於非監督式學習，處理的資料是沒有正確答案/標籤/目標變數可參考的。常見的分群 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Hierarchical Clustering, 屬於資料分群的一種方法。資料分群屬於非監督式學習，處理的資料是沒有正確答案/標籤/目標變數可參考的。常見的分群方法包括著名的kmeans, hierarchical clustering，分別使用不同分群演算邏輯。本篇將介紹階層式分群的實作與特色。</p>
<h3>資料分群簡介</h3>
<ul>
<li>資料分群是一個將資料分割成數個子集合的方法，主要目的包括：
<ul>
<li>找出資料中相近的<span style="color: #9f6ad4;">群聚(clusters)</span>，</li>
<li>找出各群的<span style="color: #9f6ad4;">代表點</span>。代表點可以是群聚中的<span style="color: #9f6ad4;">中心點(centroids)</span>或是<span style="color: #9f6ad4;">原型(prototypes)</span>。</li>
</ul>
</li>
<li>透過各群的代表點，可以達到幾個目標：
<ul>
<li>資料壓縮</li>
<li>降低雜訊</li>
<li>降低計算量</li>
</ul>
</li>
<li> 資料分群將<span style="color: #9f6ad4;">依據資料自身屬性計算彼此間的相似度</span>（物以類聚），而<span style="color: #9f6ad4;">「相似度」</span>主要有兩種型態：
<ul>
<li>「Compactness」：目標是讓子集合間差異最大化，子集合內差異最小化。如階層式分群和K-means分群。</li>
<li>「Connectedness」：目標是將可串連在一起的個體分成一群。如譜分群(Spectral Clustering)。</li>
</ul>
</li>
<li>資料分群屬於<span style="color: #9f6ad4;">非監督式學習法(Unsupervised Learning)</span>，即資料沒有標籤(unlabeled data)或沒有標準答案，<span style="color: #9f6ad4;">無法透過所謂的目標變數(response variable)來做分類之訓練</span>。也因為資料沒有標籤之緣故，與監督式學習法和強化式學習法不同，<span style="color: #9f6ad4;">非監督式學習法無法衡量演算法的正確率</span>。</li>
</ul>
<p><span style="color: #333333;"><span style="text-decoration: underline;">資料分群系列文</span>章會依序介紹以下幾種常見的分群方法：</span></p>
<ol>
<li><a href="/%e7%b5%b1%e8%a8%88-r%e8%aa%9e%e8%a8%80-%e5%88%86%e7%be%a4%e5%88%86%e6%9e%90-clustering-hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a41/">階層式分群(hierarchical clustering)</a>
<ol>
<li>聚合式階層分群法 Agglomerative Hierarchical Clustering</li>
<li>分裂式階層分群法 Divisive Hierarchical Clustering</li>
<li>最佳分群群數(Determining Optimal Clusters)</li>
</ol>
</li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">切割式分群(partitional clustering)</a>
<ol>
<li>K-means</li>
<li>K-medoid</li>
<li>最佳分群群數(Determining Optimal Clusters)</li>
</ol>
</li>
<li>譜分群(Spectral Clustering)</li>
</ol>
<h3>1. 階層式分群(Hierarchical clustering)</h3>
<ul>
<li>階層分群法的概念是在分群中建立分群，並不需要預先設定分群數。</li>
<li>產生的分群結果為一目瞭然的樹狀結構圖（又稱作<span style="color: #9f6ad4;">dendrogram</span>）。</li>
<li>群數(number of clusters)可由大變小(divisive hierarchical clustering)，或是由小變大(agglomerative hierarchical clustering)，透過群聚反覆的分裂和合併後，在選取最佳的群聚數。</li>
<li>階層式分群兩種演算法：
<ul>
<li>當採行<span style="color: #9f6ad4;">聚合法</span>，又稱作AGNES(Agglomerative Nesting)，資料會由樹狀結構的底部開始開始逐次合併 (<span style="color: #9f6ad4;">bottom-up</span>)，在R語言中所使用的函數為<span style="color: #9f6ad4;">hclust()[in stat package]或agnes[in cluster package]</span>，<span style="color: #9f6ad4;">擅於</span><span style="color: #9f6ad4;">處理與識別小規模群聚</span></li>
<li>當採行<span style="color: #9f6ad4;">分裂法，<span style="color: #333333;">又稱作DIANA(Divisive Analysis)</span></span>，資料則會由樹狀結構的頂部開始逐次分裂(<span style="color: #9f6ad4;">top-down</span>)，在R語言中使用的套件為<span style="color: #9f6ad4;">diana()[in cluster package]</span>，<span style="color: #9f6ad4;">擅於處理與識別大規模群聚</span>。</li>
</ul>
</li>
<li>階層分群可被用運用<span style="color: #9f6ad4;">數值與類別</span>資料。</li>
</ul>
<h3><span style="color: #333333;">1-1. 聚合式階層群聚法（AGNES, bottom-up）</span></h3>
<p>我們使用Iris資料集-經典的鳶尾花資料來進行聚合式階層群聚法之說明。(<span style="color: #9f6ad4;">*注意，資料必須先預處理遺失值的部分，比如說使用na.omit(data)。</span>)</p><pre class="crayon-plain-tag">head(iris)

#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
# 1          5.1         3.5          1.4         0.2  setosa
# 2          4.9         3.0          1.4         0.2  setosa
# 3          4.7         3.2          1.3         0.2  setosa
# 4          4.6         3.1          1.5         0.2  setosa
# 5          5.0         3.6          1.4         0.2  setosa
# 6          5.4         3.9          1.7         0.4  setosa</pre><p>因為分群法為非監督式學習法，專處理<span style="color: #9f6ad4;">無標籤(</span>un-labeled<span style="color: #9f6ad4;">)<span style="color: #333333;">或</span>無標準答案</span>的資料分群，故我們將分類結果欄位Species從我們的inputData刪除。</p><pre class="crayon-plain-tag">inputData &lt;- iris[,-5]
head(inputData)

#   Sepal.Length Sepal.Width Petal.Length Petal.Width
# 1          5.1         3.5          1.4         0.2
# 2          4.9         3.0          1.4         0.2
# 3          4.7         3.2          1.3         0.2
# 4          4.6         3.1          1.5         0.2
# 5          5.0         3.6          1.4         0.2
# 6          5.4         3.9          1.7         0.4</pre><p>在進行聚合式群聚法前，我們先簡單介紹幾個hclust()函數中的重要參數：</p>
<ul>
<li>d : 由dist()函數計算出來<span style="color: #9f6ad4;">資料兩兩間的相異度矩陣(dissimilarity matrix)</span>，即兩兩資料間的距離矩陣。</li>
<li>method: <span style="color: #9f6ad4;">群(clusters)</span><strong>聚合</strong>或<strong>連結</strong>的方式。包括：single(單一）、complete（完整）、average（平均）、Ward&#8217;s（華德）和 centroid（中心）等法。其中又以average(平均)聚合方法被認為是最適合的。不同方法對階層分群結果亦有極大影響。</li>
</ul>
<p>R語言hclust()套件中提供<span style="color: #9f6ad4;">群聚距離演算法</span>來<span style="color: #9f6ad4;">衡量兩群聚的不相似度</span>，最常見的幾種演算法如下：</p>
<ul>
<li>單一連結聚合演算法(single-linkage agglomerative algorithm): 群與群的距離定義為不同群聚中<span style="color: #9f6ad4;">最近</span>的兩個點的距離。傾向產生較於緊緻(compact)的群聚。<br />
$$d(C_{i},C_{j})=\min_{a\in C_{i},b\in C_{j}}d(a,b)$$</li>
<li>完整連結聚合演算法(complete-linkage agglomerative algorithm): 群與群的距離定義為不同群聚中<span style="color: #9f6ad4;">最遠</span>的兩個點的距離。傾向產生較於長(long)、鬆散(loose)的群聚。<br />
$$d(C_{i},C_{j})=\max_{a\in C_{i},b\in C_{j}}d(a,b)$$</li>
<li>平均連結聚合演算法(average-linkage agglomerative algorithm): 群與群的距離定義為不同群聚中<span style="color: #9f6ad4;">各點與各點距離總和的平均</span>。<br />
$$d(C_{i},C_{j})=\sum_{a\in C_{i},b\in C_{j}}\frac{d(a,b)}{|C_{i}||C_{j}|}$$<br />
其中，\(|C_{i}|\)和\(|C_{j}|\)分別為群聚\(C_{i}\)和\(C_{j}\)的大小。</li>
<li>中心連結聚合演算法(centroid-linkage agglomerative algorithm): 群與群的距離定義為<span style="color: #9f6ad4;">不同群聚中心點之間的距離</span>。<br />
$$d(A,B)= d(\bar{X}_{A},\bar{X}_{B})=\|\bar{X}_{A}-\bar{X}_{B}\|^2$$</li>
<li>華德最小變異法(Ward&#8217;s Minimum Variance): 最小化各群聚內變異加總(minimize the total within-cluster variance)。主要用來尋找緊湊球型的群聚。<span style="color: #9f6ad4;">反覆比較每對資料<strong>合併後的群內總變異數的增量</strong>，並找增量最小的組別優先合併</span>。越早合併的子集表示其間的相似度越高。<span style="color: #9f6ad4;">而使用華德最小變異法的前提為，初始各點資料距離必須是歐式距離的平方和(Squared Euclidean Distance)</span>。在R套件hclust將參數設為method = &#8220;ward.D2&#8243;(不是&#8221;ward.D&#8221;)。（<a href="https://en.wikipedia.org/wiki/Ward%27s_method">華德法參考文獻</a>）</li>
</ul>
<p>我們先來試試，不同的資料點間距離矩陣計算之效果。即<span style="color: #9f6ad4;">調整dist()中參數method</span>。</p>
<ul>
<li>歐式距離（以二維空間為例）：<span style="color: #9f6ad4;">最常用也是最重要的距離計算法</span>。<br />
$$d(P_{1},P_{2})=\sqrt{(x_{1}-x_{2})^2+(y_{1}-y_{2})^2}$$</li>
<li>曼哈頓距離（以二維空間為例）：<br />
$$d(P_{1},P_{2})=|x_{1}-x_{2}|+|y_{1}-y_{2}|$$</li>
</ul>
<p></p><pre class="crayon-plain-tag"># 由於階層式分群是依據個體間的「距離」來計算彼此的相似度。
# 我們會先使用dist()函數，來計算所有資料個體間的「距離矩陣(Distance Matrix)」
# 而「距離」的算法又有：(1)歐式距離(2)曼哈頓距離

E.dist &lt;- dist(x = inputData, method = "euclidean")
M.dist &lt;- dist(x = inputData, method = "manhattan")

# 讓圖形以一行兩欄的排版方式呈現，如要還原請用dev.off()
par(mfrow=c(1,2))

# 將以上資料間距離作為參數投入階層式分群函數：hclust()
# 使用歐式距離進行分群
h.E.cluster &lt;- hclust(E.dist)
plot(h.E.cluster, xlab="歐式距離",family="黑體-繁 中黑")

# 使用曼哈頓距離進行分群
h.M.cluster &lt;- hclust(M.dist) 
plot(h.M.cluster, xlab="曼哈頓距離", family="黑體-繁 中黑")</pre><p>可發現不同資料距離計算會有不同分群結果。</p>
<ul>
<li><span style="color: #9f6ad4;">Height</span>表示<span style="color: #9f6ad4;">n-1組</span>實數值。各值為<span style="color: #9f6ad4;">使用特定聚合方法計算出來的標準數值</span>。</li>
</ul>
<p><img loading="lazy" class="alignnone size-large wp-image-1221" src="/wp-content/uploads/2018/09/Rplot01-1024x469.jpeg" alt="hierarchical clustering" width="1024" height="469" srcset="/wp-content/uploads/2018/09/Rplot01-1024x469.jpeg 1024w, /wp-content/uploads/2018/09/Rplot01-300x137.jpeg 300w, /wp-content/uploads/2018/09/Rplot01-768x352.jpeg 768w, /wp-content/uploads/2018/09/Rplot01-830x380.jpeg 830w, /wp-content/uploads/2018/09/Rplot01-230x105.jpeg 230w, /wp-content/uploads/2018/09/Rplot01-350x160.jpeg 350w, /wp-content/uploads/2018/09/Rplot01-480x220.jpeg 480w, /wp-content/uploads/2018/09/Rplot01.jpeg 1308w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>比較不同歐式距離搭配不同聚合演算法的分群結果。</p><pre class="crayon-plain-tag">dev.off()
par(mfrow= c(3,2),family="黑體-繁 中黑")
plot(hclust(E.dist, method="single"),xlab = "最近聚合法:single-linkage")   # 最近法
plot(hclust(E.dist, method="complete"), xlab = "最遠聚合法:complete-linkage")  # 最遠法
plot(hclust(E.dist, method="average"), xlab = "平均聚合法: average-linkage")  # 平均法
plot(hclust(E.dist, method="centroid"), xlab = "中心法: centroid-linkage") # 中心法
plot(hclust(E.dist, method="ward.D2"), xlab = "華德法: Ward's Method")  # 華德法</pre><p>從下圖可以發現群聚數結構的幾個特徵：</p>
<ul>
<li>Single-linkage法具有「大者恆大」之效果。</li>
<li>而 complete linkage 和 average linkage 比較具有「齊頭並進」之效果。</li>
</ul>
<p><img loading="lazy" class="alignnone size-large wp-image-1261" src="/wp-content/uploads/2018/09/Rplot07-1024x1024.jpeg" alt="hierarchical clustering" width="1024" height="1024" srcset="/wp-content/uploads/2018/09/Rplot07-1024x1024.jpeg 1024w, /wp-content/uploads/2018/09/Rplot07-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot07-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot07-768x768.jpeg 768w, /wp-content/uploads/2018/09/Rplot07-830x830.jpeg 830w, /wp-content/uploads/2018/09/Rplot07-230x230.jpeg 230w, /wp-content/uploads/2018/09/Rplot07-350x350.jpeg 350w, /wp-content/uploads/2018/09/Rplot07-480x480.jpeg 480w, /wp-content/uploads/2018/09/Rplot07.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>接著，我們聚焦在採用<span style="color: #9f6ad4;">歐式距離</span>搭配華德最小變異聚合演算法。可透過設定<span style="color: #9f6ad4;">hclust()參數method=&#8221;ward.D2&#8243;</span>。</p><pre class="crayon-plain-tag">dev.off()
par(family="黑體-繁 中黑")
plot(hclust(E.dist, method="ward.D2"), xlab = "華德法: Ward's Method")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1236" src="/wp-content/uploads/2018/09/Rplot02_Eu_Ward-1024x683.jpeg" alt="hierarchical clustering" width="1024" height="683" srcset="/wp-content/uploads/2018/09/Rplot02_Eu_Ward-1024x683.jpeg 1024w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-300x200.jpeg 300w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-768x512.jpeg 768w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-830x553.jpeg 830w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-230x153.jpeg 230w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-350x233.jpeg 350w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward-480x320.jpeg 480w, /wp-content/uploads/2018/09/Rplot02_Eu_Ward.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>以上結果也可以使用anges()函數來執行。此函數跟hclust()很類似，不同之處是該函數能另外計算<span style="color: #9f6ad4;">聚合係數(agglomerative coefficient)</span>。<span style="color: #9f6ad4;">聚合係數是衡量群聚結構被辨識的程度，聚合係數越接近1代表有堅固的群聚結構(strong clustering structure)</span>。在這個使用歐式距離搭配華德連結演算法的群聚係數有高達99%的表現。</p><pre class="crayon-plain-tag"># Compute with agnes
library(cluster)
hc2 &lt;- agnes(E.dist, method = "ward")
# Agglomerative coefficient
hc2$ac
## [1] 0.9908772</pre><p>我們可以用聚合係數來比較多組分群連結演算法的效果。此範例中又以華德法表現最好。</p><pre class="crayon-plain-tag"># methods to assess
m &lt;- c( "average", "single", "complete", "ward")
names(m) &lt;- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac &lt;- function(x) {
  agnes(E.dist, method = x)$ac
}

map_dbl(m, ac) #Apply a function to each element of a vector 
#   average    single  complete      ward 
# 0.9300174 0.8493364 0.9574622 0.9908772</pre><p>若遇將agnes()產生的樹狀圖繪出可使用函數pltree()。可以發現結果大致上與hclust()結果差不多。</p><pre class="crayon-plain-tag">dev.off()
hc2 &lt;- agnes(E.dist, method = "ward")
pltree(hc2, cex = 0.6, hang = -1, main = "Dendrogram of agnes")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1264" src="/wp-content/uploads/2018/09/Rplot08-1024x614.jpeg" alt="hierarchical clustering" width="1024" height="614" srcset="/wp-content/uploads/2018/09/Rplot08-1024x614.jpeg 1024w, /wp-content/uploads/2018/09/Rplot08-300x180.jpeg 300w, /wp-content/uploads/2018/09/Rplot08-768x461.jpeg 768w, /wp-content/uploads/2018/09/Rplot08-830x498.jpeg 830w, /wp-content/uploads/2018/09/Rplot08-230x138.jpeg 230w, /wp-content/uploads/2018/09/Rplot08-350x210.jpeg 350w, /wp-content/uploads/2018/09/Rplot08-480x288.jpeg 480w, /wp-content/uploads/2018/09/Rplot08.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>對階層分群結果產生的樹狀結構進行<span style="color: #9f6ad4;">截枝</span>可以<span style="color: #9f6ad4;">將觀測值分成數群</span>。<span style="color: #9f6ad4;">截肢方法</span>有兩種：</p>
<ol>
<li>指定所要的分群數: rect.hclust(k=&#8230;)、cutree(k=&#8230;)</li>
<li>指定截枝的位置: rect.hclust(h=&#8230;)</li>
</ol>
<p>比如說，指定將資料分成3群與13群。</p><pre class="crayon-plain-tag">h.E.Ward.cluster &lt;- hclust(E.dist, method="ward.D2")
plot(h.E.Ward.cluster)
rect.hclust(tree =h.E.Ward.cluster, k = 3, border = "red")
rect.hclust(tree =h.E.Ward.cluster, k = 13, border = "blue")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1237" src="/wp-content/uploads/2018/09/Rplot03-1024x683.jpeg" alt="hierarchical clustering" width="1024" height="683" srcset="/wp-content/uploads/2018/09/Rplot03-1024x683.jpeg 1024w, /wp-content/uploads/2018/09/Rplot03-300x200.jpeg 300w, /wp-content/uploads/2018/09/Rplot03-768x512.jpeg 768w, /wp-content/uploads/2018/09/Rplot03-830x553.jpeg 830w, /wp-content/uploads/2018/09/Rplot03-230x153.jpeg 230w, /wp-content/uploads/2018/09/Rplot03-350x233.jpeg 350w, /wp-content/uploads/2018/09/Rplot03-480x320.jpeg 480w, /wp-content/uploads/2018/09/Rplot03.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>比如說，指定截枝高度分別為4和10，可分別將資料分成5組跟3組。</p><pre class="crayon-plain-tag">h.E.Ward.cluster &lt;- hclust(E.dist, method="ward.D2")
plot(h.E.Ward.cluster)
rect.hclust(tree =h.E.Ward.cluster, h = 4, border = "red")
rect.hclust(tree =h.E.Ward.cluster, h = 10, border = "blue")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1238" src="/wp-content/uploads/2018/09/Rplot04-1024x683.jpeg" alt="hierarchical clustering" width="1024" height="683" srcset="/wp-content/uploads/2018/09/Rplot04-1024x683.jpeg 1024w, /wp-content/uploads/2018/09/Rplot04-300x200.jpeg 300w, /wp-content/uploads/2018/09/Rplot04-768x512.jpeg 768w, /wp-content/uploads/2018/09/Rplot04-830x553.jpeg 830w, /wp-content/uploads/2018/09/Rplot04-230x153.jpeg 230w, /wp-content/uploads/2018/09/Rplot04-350x233.jpeg 350w, /wp-content/uploads/2018/09/Rplot04-480x320.jpeg 480w, /wp-content/uploads/2018/09/Rplot04.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>如果要將資料標記上分群結果，可使用cutree()，並指定參數k為所欲截枝群樹。</p><pre class="crayon-plain-tag">h.E.Ward.cluster &lt;- hclust(E.dist, method="ward.D2")
cut.h.cluster &lt;- cutree(tree = h.E.Ward.cluster, k = 3)
cut.h.cluster

#   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2
#  [56] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3
# [111] 3 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 2 3 3 3 2 3 3 3 2 3 3 3 2 3 3 2</pre><p>分群結果和實際結果比較。</p><pre class="crayon-plain-tag">table(cut.h.cluster, iris$Species)

# cut.h.cluster setosa versicolor virginica
#             1     50          0         0
#             2      0         49        15
#             3      0          1        35</pre><p>將資料分群的混淆矩陣視覺化。</p><pre class="crayon-plain-tag">plot(table(iris$Species, cut.h.cluster), main = "Confusion Matrix for Species Clustering", xlab = "Species", ylab = "Cluster")</pre><p>可以發現，屬於setosa種類的都被很好的分到第一群，屬於versicolor主要被分到第二群，但virginica似乎沒有分得很好。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1257" src="/wp-content/uploads/2018/09/Rplot05.jpeg" alt="hierarchical clustering" width="800" height="722" srcset="/wp-content/uploads/2018/09/Rplot05.jpeg 800w, /wp-content/uploads/2018/09/Rplot05-300x271.jpeg 300w, /wp-content/uploads/2018/09/Rplot05-768x693.jpeg 768w, /wp-content/uploads/2018/09/Rplot05-230x208.jpeg 230w, /wp-content/uploads/2018/09/Rplot05-350x316.jpeg 350w, /wp-content/uploads/2018/09/Rplot05-480x433.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>如果回頭看一下原始資料分佈長相，可以發現versicolor和virginica其實部分資料靠得很近，因此兩種Species被分到第二和第三群也是合理的。</p><pre class="crayon-plain-tag">ggplot(data = iris,mapping = aes(x = Petal.Length, y = Petal.Width)) +
  geom_point(aes(col = Species))</pre><p><img loading="lazy" class="alignnone size-full wp-image-1247" src="/wp-content/uploads/2018/09/Rplot06.jpeg" alt="hierarchical clustering" width="800" height="722" srcset="/wp-content/uploads/2018/09/Rplot06.jpeg 800w, /wp-content/uploads/2018/09/Rplot06-300x271.jpeg 300w, /wp-content/uploads/2018/09/Rplot06-768x693.jpeg 768w, /wp-content/uploads/2018/09/Rplot06-230x208.jpeg 230w, /wp-content/uploads/2018/09/Rplot06-350x316.jpeg 350w, /wp-content/uploads/2018/09/Rplot06-480x433.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>1-2. 分裂式階層群聚法（DIANA, top-down）</p>
<p>我們使用R內建的USArrests資料集來進行說明。<span style="color: #9f6ad4;">進行分群前，我們先將資料預處理，包括遺失值忽略以及標準化(0~1)</span>。</p><pre class="crayon-plain-tag">head(USArrests)
#            Murder Assault UrbanPop Rape
# Alabama      13.2     236       58 21.2
# Alaska       10.0     263       48 44.5
# Arizona       8.1     294       80 31.0
# Arkansas      8.8     190       50 19.5
# California    9.0     276       91 40.6
# Colorado      7.9     204       78 38.7

inputData &lt;- 
  USArrests %&gt;% 
  na.omit() %&gt;% 
  scale() # scaling/standardizing the data</pre><p>diana()函數運作方式很類似agnes()，<span style="color: #9f6ad4;">只差diana沒有method參數</span>。</p><pre class="crayon-plain-tag"># compute divisive hierarchical clustering
diana_clust &lt;- diana(inputData)

# Divise coefficient; amount of clustering structure found
diana_clust$dc
## [1] 0.8514345

# plot dendrogram
pltree(diana_clust, cex = 0.6, hang = -1, main = "Dendrogram of diana")</pre><p>由分裂式階層分群演算法產出的樹狀圖結果如下：</p>
<ul>
<li>葉節點（末梢節點）代表個別資料點。</li>
<li>垂直座標軸的Height代表群聚間的(不)相似度((dis)similarity)， 群聚的高度(height)越高，代表觀測值間越不相似（組內變異越大）。<span style="text-decoration: underline;">需要注意的是，要總結兩個觀測值的相似度，只能用<span style="color: #9f6ad4; text-decoration: underline;">兩觀測值何時被第一次合併的群聚高度</span>來做判斷，而不能以水平軸的距離來評估</span>。</li>
</ul>
<p><img loading="lazy" class="alignnone size-large wp-image-1266" src="/wp-content/uploads/2018/09/Rplot09-1024x683.jpeg" alt="hierarchical clustering" width="1024" height="683" srcset="/wp-content/uploads/2018/09/Rplot09-1024x683.jpeg 1024w, /wp-content/uploads/2018/09/Rplot09-300x200.jpeg 300w, /wp-content/uploads/2018/09/Rplot09-768x512.jpeg 768w, /wp-content/uploads/2018/09/Rplot09-830x553.jpeg 830w, /wp-content/uploads/2018/09/Rplot09-230x153.jpeg 230w, /wp-content/uploads/2018/09/Rplot09-350x233.jpeg 350w, /wp-content/uploads/2018/09/Rplot09-480x320.jpeg 480w, /wp-content/uploads/2018/09/Rplot09.jpeg 1500w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>另外，我們可以透過縱軸的群聚高度來決定分群數目。我們可以使用cutree()函數來得到各觀測值被分類到的群。</p><pre class="crayon-plain-tag"># Cut diana() tree into 4 groups
diana_clust &lt;- diana(inputData)
group &lt;- cutree(diana_clust, k = 4)
group
# [1] 1 2 2 3 2 2 3 3 2 1 3 4 2 3 4 3 4 1 4 2 3 2 4 1 2 4 4 2 4 3 2 2 1 4 3 3 3 3 3 1 4 1 2 3 4 3 3 4 4 3</pre><p>我們使用factoextra套件中的fviz_clusterc函數來將分群結果視覺化。</p><pre class="crayon-plain-tag">library(factoextra)
fviz_cluster(list(data = inputData, cluster = group))</pre><p><img loading="lazy" class="alignnone size-full wp-image-1267" src="/wp-content/uploads/2018/09/Rplot10.jpeg" alt="hierarchical clustering" width="800" height="796" srcset="/wp-content/uploads/2018/09/Rplot10.jpeg 800w, /wp-content/uploads/2018/09/Rplot10-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot10-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot10-768x764.jpeg 768w, /wp-content/uploads/2018/09/Rplot10-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot10-350x348.jpeg 350w, /wp-content/uploads/2018/09/Rplot10-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h3>1-3. 決定階層式分群最佳群聚數</h3>
<h4>1-3-1. Elbow Method</h4>
<p>使用函數fviz_nbclust()，並將參數FUN指定為hcut(表階層式分群法)。wss代表組內平方誤差。</p><pre class="crayon-plain-tag">fviz_nbclust(inputData, FUN = hcut, method = "wss")</pre><p>由結果圖可知根據Elbow法，最佳分群數目為4群。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1270" src="/wp-content/uploads/2018/09/Rplot11.jpeg" alt="hierarchical clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot11.jpeg 800w, /wp-content/uploads/2018/09/Rplot11-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot11-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot11-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot11-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot11-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot11-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h4>1-3-2. Average Silhouette Method</h4>
<p>將method參數改為silhouette。</p><pre class="crayon-plain-tag">fviz_nbclust(x = inputData,FUNcluster = hcut, method = "silhouette")</pre><p>根據Average Silhouette Width，最佳分群數目為2 群。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1271" src="/wp-content/uploads/2018/09/Rplot12.jpeg" alt="hierarchical clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot12.jpeg 800w, /wp-content/uploads/2018/09/Rplot12-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot12-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot12-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot12-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot12-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot12-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h4>1-3-3. Gap Statistic Method</h4>
<p>使用clusGap()函數。</p><pre class="crayon-plain-tag">gap_stat &lt;- clusGap(x = inputData,FUNcluster = hcut, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)</pre><p>根據Gap(k)統計量，最佳分群數為3群。(＊\(Gap(k)\geq Gap(k+1) &#8211; s_{k+1}\))</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1272" src="/wp-content/uploads/2018/09/Rplot13.jpeg" alt="hierarchical clustering" width="800" height="797" srcset="/wp-content/uploads/2018/09/Rplot13.jpeg 800w, /wp-content/uploads/2018/09/Rplot13-150x150.jpeg 150w, /wp-content/uploads/2018/09/Rplot13-300x300.jpeg 300w, /wp-content/uploads/2018/09/Rplot13-768x765.jpeg 768w, /wp-content/uploads/2018/09/Rplot13-230x229.jpeg 230w, /wp-content/uploads/2018/09/Rplot13-350x349.jpeg 350w, /wp-content/uploads/2018/09/Rplot13-480x478.jpeg 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h3>總結</h3>
<ul>
<li>階層分群主要受：<span style="text-decoration: underline;">資料個體兩兩間距離矩陣衡量方法</span>與<span style="text-decoration: underline;">群與群連結方法所影響</span>。</li>
<li> 階層分群優點包括：
<ul>
<li>概念簡單且樹狀圖(<span style="color: #9f6ad4;">dendrogram</span>)結果一目瞭然。</li>
<li>只需要資料點間距離即可產出分群結果。</li>
<li>可以應用在數值或<span style="color: #9f6ad4;">類別</span>資料。</li>
</ul>
</li>
<li><span style="color: #9f6ad4;">但缺點就是運算速度，以及不適用處理大量資料</span>。運算速度方面，可以考慮其他R套件如fastcluster中的hclust函數，執行上會比一般hclust更有效率。</li>
</ul>
<hr />
<p>更多統計模型筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考:</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/feed/</wfw:commentRss>
			<slash:comments>6</slash:comments>
		
		
			</item>
	</channel>
</rss>
