<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>logistic regression &#8211; 果醬珍珍•JamJam</title>
	<atom:link href="/tag/logistic-regression/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>健忘女孩Jam的學習筆記和生活雜記</description>
	<lastBuildDate>Fri, 03 Jul 2020 02:33:57 +0000</lastBuildDate>
	<language>zh-TW</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Logistic Regression 羅吉斯迴歸 &#124; part2 &#8211; 模型建置、診斷與比較 &#124; R語言</title>
		<link>/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/</link>
					<comments>/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Tue, 28 Aug 2018 06:24:35 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[AUC]]></category>
		<category><![CDATA[cart]]></category>
		<category><![CDATA[confusion matrix]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[logistic regression]]></category>
		<category><![CDATA[model diagnostic]]></category>
		<category><![CDATA[neural networks]]></category>
		<category><![CDATA[random forest]]></category>
		<category><![CDATA[ROC]]></category>
		<category><![CDATA[support vector machine]]></category>
		<category><![CDATA[svm]]></category>
		<category><![CDATA[模型診斷]]></category>
		<category><![CDATA[混亂矩陣]]></category>
		<category><![CDATA[羅吉斯回歸]]></category>
		<guid isPermaLink="false">/?p=851</guid>

					<description><![CDATA[<p>Logistic Regression, 羅吉斯回歸模型，適用於預測二元類別目標變數的發生機率(p)，和線性回歸模型類似，與線性回歸主要不同之處在於：(1) 目 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p class=""></p>
<p>Logistic Regression, 羅吉斯回歸模型，適用於<span style="color: #9f6ad4;">預測<span style="text-decoration: underline;">二元類別目標變數</span>的發生機率(p)</span>，和線性回歸模型類似，與線性回歸主要不同之處在於：(1) 目標變數是目標事件發生機率P經過log函數轉換成log odds值才進行線性預測，且(2)羅吉斯回歸的各項參數是透過最大概似法(MLE)進行估計的。</p>
<h3>Logistic Regression part1 前情提要</h3>
<p>「<a href="/logistic_regression_part1/" target="_blank" rel="noopener noreferrer">Logistic Regression &#8211; part1 &#8211; 資料探勘與處理</a>」篇的重點回顧：</p>
<ol>
<li>探索預測變數自身分佈情況以及各預測變數與目標變數之間的關係。</li>
<li>資料預處理，將類別水準直過多的變數進行簡化。</li>
<li>隨機產生訓練與測試資料集。</li>
<li>計算各預測變數的Information Value來初步判斷各預測變數對目標變數的影響程度。</li>
<li>依據IV值初步從13個變數篩選出6個變數包括：RELATIONSHIP, MARITALSTATUS, AGE, EDUCATIONNUM, CAPITALGAIN,OCCUPATION_rep。</li>
</ol>
<h3>分析資料與問題</h3>
<p>Problem: 預測薪資大於50K(ABOVE50K)的影響特徵因素有哪些？</p>
<p>Data: <a href="https://archive.ics.uci.edu/ml/datasets/adult">adult dataset</a> (snapshot)</p>
<p>目標變數(1)：ABOVE50K</p>
<p>預測變數(13)：AGE, WORKCLASS, EDUCATION, EDUCATIONNUM, MARITALSTATUS, OCCUPATION, RELATIONSHIP, RACE, SEX, CAPITALGAIN, CAPITALLOSS, HOURSPERWEEK, NATIVECOUNTRY</p>
<h3>分析步驟</h3>
<p>(part 2 篇會說明步驟6~8的部分)(步驟1~5請參考「<a href="/logistic_regression_part1/" target="_blank" rel="noopener noreferrer">Logistic Regression &#8211; part1 &#8211; 資料探勘與處理</a>」)</p>
<ol>
<li><span style="color: #000000;">資料載入與檢視</span></li>
<li><span style="color: #000000;">資料探勘</span></li>
<li><span style="color: #000000;">資料前處理</span></li>
<li><span style="color: #000000;">產生訓練資料集與測試資料集</span></li>
<li><span style="color: #000000;">計算特徵變數IV值(Information Value)，篩選變數</span></li>
<li><span style="color: #9f6ad4;">訓練模型與預測</span></li>
<li><span style="color: #9f6ad4;">模型診斷與調整</span></li>
<li><span style="color: #9f6ad4;">模型比較(v.s. Machine Learning Methods)</span></li>
</ol>
<h3>6. 訓練模型與預測</h3>
<p>接續「<a href="/logistic_regression_part1/" target="_blank" rel="noopener noreferrer">Logistic Regression &#8211; part1 &#8211; 資料探勘與處理</a>」，我們將近一步針對初步篩選出的6預測變數進行羅基斯回歸模型分析。</p><pre class="crayon-plain-tag">logitMod &lt;- glm(formula = ABOVE50K_y ~ RELATIONSHIP + MARITALSTATUS + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
                data = trainingData,family = binomial(link = "logit"))
summary(logitMod)

# Call:
#   glm(formula = ABOVE50K_y ~ RELATIONSHIP + MARITALSTATUS + AGE + 
#         EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep, family = binomial(link = "logit"), 
#       data = trainingData)
# 
# Deviance Residuals: 
#   Min       1Q   Median       3Q      Max  
# -5.2263  -0.5640  -0.2258  -0.0528   3.8139  
# 
# Coefficients:
#   Estimate Std. Error z value             Pr(&gt;|z|)    
# (Intercept)                         -6.2679390  0.3703477 -16.924 &lt; 0.0000000000000002 ***
#   RELATIONSHIP Not-in-family          -0.2201571  0.3288849  -0.669             0.503237    
# RELATIONSHIP Other-relative         -1.5525876  0.3172511  -4.894     0.00000098869035 ***
#   RELATIONSHIP Own-child              -1.6632915  0.3252302  -5.114     0.00000031507745 ***
#   RELATIONSHIP Unmarried              -0.6486702  0.3428225  -1.892             0.058472 .  
# RELATIONSHIP Wife                   -0.0196633  0.0760145  -0.259             0.795884    
# MARITALSTATUS Married-AF-spouse      2.4570045  0.6658794   3.690             0.000224 ***
#   MARITALSTATUS Married-civ-spouse     1.9165557  0.3331962   5.752     0.00000000881763 ***
#   MARITALSTATUS Married-spouse-absent -0.0649140  0.2672163  -0.243             0.808062    
# MARITALSTATUS Never-married         -0.3668251  0.1008955  -3.636             0.000277 ***
#   MARITALSTATUS Separated             -0.1122524  0.1867781  -0.601             0.547845    
# MARITALSTATUS Widowed               -0.1851853  0.1746133  -1.061             0.288897    
# AGE                                  0.0217164  0.0017943  12.103 &lt; 0.0000000000000002 ***
#   EDUCATIONNUM                         0.3058023  0.0103400  29.575 &lt; 0.0000000000000002 ***
#   CAPITALGAIN                          0.0003108  0.0000118  26.331 &lt; 0.0000000000000002 ***
#   OCCUPATION_repBlue-Collar           -0.4800489  0.0701546  -6.843     0.00000000000777 ***
#   OCCUPATION_repOther/Unknown         -1.3267766  0.1312315 -10.110 &lt; 0.0000000000000002 ***
#   OCCUPATION_repProfessional           0.1688849  0.0799712   2.112             0.034702 *  
#   OCCUPATION_repService               -0.4061912  0.0845737  -4.803     0.00000156457927 ***
#   OCCUPATION_repWhite-Collar           0.2455151  0.0699361   3.511             0.000447 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
# Null deviance: 25162  on 22791  degrees of freedom
# Residual deviance: 15492  on 22772  degrees of freedom
# AIC: 15532
# 
# Number of Fisher Scoring iterations: 7</pre><p>除了摘要模型，我們另外針對共線性進行檢查。發現變數RELATIONSHIP和MARITALSTATUS有共線性。</p><pre class="crayon-plain-tag"># 模型共線性檢查VIF(希望VIF值都在4以下)
library(car)
vif(logitMod)

#                     GVIF Df GVIF^(1/(2*Df))
# RELATIONSHIP   55.704490  5        1.494820
# MARITALSTATUS  56.687207  6        1.399986
# AGE             1.177411  1        1.085086
# EDUCATIONNUM    1.384560  1        1.176673
# CAPITALGAIN     1.018983  1        1.009447
# OCCUPATION_rep  1.531311  5        1.043533</pre><p>因此，我們根據RELATIONSHIP和MARITALSTATUS的IV值，捨棄其中較低者，重新再進行一次模型建置。</p><pre class="crayon-plain-tag">logitMod &lt;- glm(formula = ABOVE50K_y ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
                data = trainingData,family = binomial(link = "logit"))

summary(logitMod)

# Call:
#   glm(formula = ABOVE50K_y ~ RELATIONSHIP + AGE + EDUCATIONNUM + 
#         CAPITALGAIN + OCCUPATION_rep, family = binomial(link = "logit"), 
#       data = trainingData)
# 
# Deviance Residuals: 
#   Min       1Q   Median       3Q      Max  
# -5.2305  -0.5663  -0.2319  -0.0590   3.5991  
# 
# Coefficients:
#   Estimate  Std. Error z value
# (Intercept)                 -4.39303806  0.15173422 -28.952
# RELATIONSHIP Not-in-family  -2.34469608  0.05835820 -40.178
# RELATIONSHIP Other-relative -3.02210032  0.26185701 -11.541
# RELATIONSHIP Own-child      -3.72243110  0.16395790 -22.704
# RELATIONSHIP Unmarried      -2.66495726  0.09783246 -27.240
# RELATIONSHIP Wife           -0.00985696  0.07585158  -0.130
# AGE                          0.02285379  0.00171654  13.314
# EDUCATIONNUM                 0.30497481  0.01032637  29.534
# CAPITALGAIN                  0.00031083  0.00001181  26.312
# OCCUPATION_repBlue-Collar   -0.47777138  0.07013154  -6.813
# OCCUPATION_repOther/Unknown -1.33506791  0.13132122 -10.166
# OCCUPATION_repProfessional   0.16489267  0.07982984   2.066
# OCCUPATION_repService       -0.40645290  0.08448125  -4.811
# OCCUPATION_repWhite-Collar   0.24924877  0.06989076   3.566
# Pr(&gt;|z|)    
# (Intercept)                 &lt; 0.0000000000000002 ***
#   RELATIONSHIP Not-in-family  &lt; 0.0000000000000002 ***
#   RELATIONSHIP Other-relative &lt; 0.0000000000000002 ***
#   RELATIONSHIP Own-child      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Unmarried      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Wife                       0.896606    
# AGE                         &lt; 0.0000000000000002 ***
#   EDUCATIONNUM                &lt; 0.0000000000000002 ***
#   CAPITALGAIN                 &lt; 0.0000000000000002 ***
#   OCCUPATION_repBlue-Collar       0.00000000000959 ***
#   OCCUPATION_repOther/Unknown &lt; 0.0000000000000002 ***
#   OCCUPATION_repProfessional              0.038871 *  
#   OCCUPATION_repService           0.00000150055853 ***
#   OCCUPATION_repWhite-Collar              0.000362 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
# Null deviance: 25162  on 22791  degrees of freedom
# Residual deviance: 15542  on 22778  degrees of freedom
# AIC: 15570
# 
# Number of Fisher Scoring iterations: 7</pre><p>並在重新檢視共線性。可以發現去除MARITALSTATUS後，所有變數的VIF值都小於4了。</p><pre class="crayon-plain-tag">library(car)
vif(logitMod)

#                     GVIF Df GVIF^(1/(2*Df))
# RELATIONSHIP   1.197104  5        1.018153
# AGE            1.089616  1        1.043847
# EDUCATIONNUM   1.388249  1        1.178240
# CAPITALGAIN    1.018159  1        1.009038
# OCCUPATION_rep 1.531428  5        1.043541</pre><p>於是我們使用此模型來預測測試資料集。</p><pre class="crayon-plain-tag">prob &lt;- predict(logitMod, testData, type="response")  # predicted scores</pre><p>為了優化模型正確率，我們使用InformationValue套件中的optimalCutoff函數來尋找最適機率切點。發現最適機率值切點跟預設的0.5一樣。</p><pre class="crayon-plain-tag">library(InformationValue)
optCutOff &lt;- optimalCutoff(actuals = testData$ABOVE50K_y,predictedScores = predicted)[1]
optCutOff
# [1] 0.5</pre><p></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>決定了機率切點後，我們來驗證模型使用測試資料集的預測效果如何。我們會分別計算以下指標：</p>
<ul>
<li>Confusion Matrix
<ul>
<li>misClassification Rate</li>
<li>precision</li>
<li>Sensitivity</li>
<li>Specifisity</li>
</ul>
</li>
<li>Concordance</li>
<li>ROC/AUC</li>
</ul>
<p>Confusion Matrix: 左欄為預測值，上方欄則為實際值。</p><pre class="crayon-plain-tag"># 產生Confusion Matrix
confusionMatrix(actuals = testData$ABOVE50K_y,predictedScores = predicted,threshold = optCutOff)

#      0    1
# 0 6935 1074
# 1  481 1279</pre><p>預測錯誤率(misClassification Rate)</p><pre class="crayon-plain-tag">misClassError(actuals = testData$ABOVE50K_y,predictedScores = predicted,threshold = optCutOff)
# [1] 0.1592</pre><p>預測精準度(Precision，及所有預測為1的事件中，真實亦為1的事件比率）。</p><pre class="crayon-plain-tag">precision(actuals = testData$ABOVE50K_y,predictedScores = predicted,threshold = optCutOff)
# [1] 0.7267045</pre><p>Sensitivity</p>
<p>$$Sensitivity=\frac{\sharp\space Actual\space 1&#8217;s\space and\space Predicted\space as\space 1&#8217;s}{\sharp\space of\space Actual\space 1&#8217;s}$$</p>
<p>又稱Recall(捕捉率），即真實為1，且被正確預測為1的比例。（通常與Precision精準度成反比）</p><pre class="crayon-plain-tag">sensitivity(testData$ABOVE50K_y, predicted, threshold = optCutOff)
# [1] 0.5435614</pre><p>Specificity</p>
<p class="">$$Specificity=\frac{\sharp\space Actual\space 0&#8217;s\space and\space Predicted\space as\space 0&#8217;s}{\sharp\space of\space Actual\space 0&#8217;s}$$</p>
<p>即真實為0，且被正確預測為0的比例。</p><pre class="crayon-plain-tag">specificity(testData$ABOVE50K_y, predicted, threshold = optCutOff)
# [1] 0.9351402</pre><p>Concordance。則為所有預測結果(0,1)成對的機率值中，真實為1的事件，1的機率高於0的機率佔所有成對資料的比例。<span style="color: #9f6ad4;">理想中，該比例越高越好</span>，即表示所有預測(0,1)的機率值，若真實為1，則1的預測機率理應都大於0的預測機率。（discordance則為相反結果的比例，tied則為機率無差別的結果比例，三比例相加應為100%）。</p><pre class="crayon-plain-tag">Concordance(testData$ABOVE50K_y, predicted)

# $Concordance
# [1] 0.8866298
# 
# $Discordance
# [1] 0.1133702
# 
# $Tied
# [1] -0.00000000000000004163336
# 
# $Pairs
# [1] 17449848</pre><p>ROC/AUC。可以發現在ROC曲線下面積為88.85%。理想中，會希望ROC曲線前段越陡越好，後段越緩越好。表示模型整體預測能力的好壞。</p><pre class="crayon-plain-tag">plotROC(actuals = testData$ABOVE50K_y,predictedScores = predicted)</pre><p><img loading="lazy" class="alignnone size-large wp-image-972" src="/wp-content/uploads/2018/08/pic19-1024x928.png" alt="logistic regression" width="960" height="870" srcset="/wp-content/uploads/2018/08/pic19-1024x928.png 1024w, /wp-content/uploads/2018/08/pic19-300x272.png 300w, /wp-content/uploads/2018/08/pic19-768x696.png 768w, /wp-content/uploads/2018/08/pic19-1140x1034.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>總結以上指標數據，可以發現模型預測能力還不錯，錯誤率15.92%都還在能接受範圍內。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>7. 模型診斷與調整</h3>
<p>為了近一步看看模型有無優化空間以及合適度，常見做法包括(1)殘差分析 (2)K-fold Cross Validation (3)Bootstrap (4)StepwiseRegression。我們這邊就以逐步回歸為範例，來看看模型是否有優化空間。</p>
<p>我們同時使用逐步向後/向前法，並比較兩法最後的結果(AIC)是否有差。</p><pre class="crayon-plain-tag">為了進一步簡化模型，我們使用逐步向前/向後法來挑選變數
# 依據每一個變數加入/移除模型後AIC資訊的變化，來判斷變數對模型的效果
# full model設定為我們剛剛建模結果
m_full &lt;- logitMod 
# 基礎只有截距的模型
m_null &lt;- glm(formula = ABOVE50K_y ~ 1, family = binomial(link = "logit"),data = trainingData)

# backward selection 逐步向後法
stepModBack &lt;- step(object = m_full,scope = list(lower = m_null, upper = m_full),direction = "backward",trace = F)
summary(stepModBack) #模型結果的AIC跟原始模型相同為15570

# Call:
#   glm(formula = ABOVE50K_y ~ RELATIONSHIP + AGE + EDUCATIONNUM + 
#         CAPITALGAIN + OCCUPATION_rep, family = binomial(link = "logit"), 
#       data = trainingData)
# 
# Deviance Residuals: 
#   Min       1Q   Median       3Q      Max  
# -5.2305  -0.5663  -0.2319  -0.0590   3.5991  
# 
# Coefficients:
#   Estimate  Std. Error z value
# (Intercept)                 -4.39303806  0.15173422 -28.952
# RELATIONSHIP Not-in-family  -2.34469608  0.05835820 -40.178
# RELATIONSHIP Other-relative -3.02210032  0.26185701 -11.541
# RELATIONSHIP Own-child      -3.72243110  0.16395790 -22.704
# RELATIONSHIP Unmarried      -2.66495726  0.09783246 -27.240
# RELATIONSHIP Wife           -0.00985696  0.07585158  -0.130
# AGE                          0.02285379  0.00171654  13.314
# EDUCATIONNUM                 0.30497481  0.01032637  29.534
# CAPITALGAIN                  0.00031083  0.00001181  26.312
# OCCUPATION_repBlue-Collar   -0.47777138  0.07013154  -6.813
# OCCUPATION_repOther/Unknown -1.33506791  0.13132122 -10.166
# OCCUPATION_repProfessional   0.16489267  0.07982984   2.066
# OCCUPATION_repService       -0.40645290  0.08448125  -4.811
# OCCUPATION_repWhite-Collar   0.24924877  0.06989076   3.566
# Pr(&gt;|z|)    
# (Intercept)                 &lt; 0.0000000000000002 ***
#   RELATIONSHIP Not-in-family  &lt; 0.0000000000000002 ***
#   RELATIONSHIP Other-relative &lt; 0.0000000000000002 ***
#   RELATIONSHIP Own-child      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Unmarried      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Wife                       0.896606    
# AGE                         &lt; 0.0000000000000002 ***
#   EDUCATIONNUM                &lt; 0.0000000000000002 ***
#   CAPITALGAIN                 &lt; 0.0000000000000002 ***
#   OCCUPATION_repBlue-Collar       0.00000000000959 ***
#   OCCUPATION_repOther/Unknown &lt; 0.0000000000000002 ***
#   OCCUPATION_repProfessional              0.038871 *  
#   OCCUPATION_repService           0.00000150055853 ***
#   OCCUPATION_repWhite-Collar              0.000362 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
# Null deviance: 25162  on 22791  degrees of freedom
# Residual deviance: 15542  on 22778  degrees of freedom
# AIC: 15570
# 
# Number of Fisher Scoring iterations: 7


# forward selection 逐步向前法
stepModForward &lt;- step(object = m_null,scope = list(lower = m_null, upper = m_full),direction = "forward",trace = F)
summary(stepModForward) 
# 向前法、向後法模型結果AIC值皆與原始模型相同為15570，且模型皆相同
# 因此我們會確定採用原始模型logigMod

# Call:
#   glm(formula = ABOVE50K_y ~ RELATIONSHIP + EDUCATIONNUM + CAPITALGAIN + 
#         OCCUPATION_rep + AGE, family = binomial(link = "logit"), 
#       data = trainingData)
# 
# Deviance Residuals: 
#   Min       1Q   Median       3Q      Max  
# -5.2305  -0.5663  -0.2319  -0.0590   3.5991  
# 
# Coefficients:
#   Estimate  Std. Error z value
# (Intercept)                 -4.39303806  0.15173422 -28.952
# RELATIONSHIP Not-in-family  -2.34469608  0.05835820 -40.178
# RELATIONSHIP Other-relative -3.02210032  0.26185701 -11.541
# RELATIONSHIP Own-child      -3.72243110  0.16395790 -22.704
# RELATIONSHIP Unmarried      -2.66495726  0.09783246 -27.240
# RELATIONSHIP Wife           -0.00985696  0.07585158  -0.130
# EDUCATIONNUM                 0.30497481  0.01032637  29.534
# CAPITALGAIN                  0.00031083  0.00001181  26.312
# OCCUPATION_repBlue-Collar   -0.47777138  0.07013154  -6.813
# OCCUPATION_repOther/Unknown -1.33506791  0.13132122 -10.166
# OCCUPATION_repProfessional   0.16489267  0.07982984   2.066
# OCCUPATION_repService       -0.40645290  0.08448125  -4.811
# OCCUPATION_repWhite-Collar   0.24924877  0.06989076   3.566
# AGE                          0.02285379  0.00171654  13.314
# Pr(&gt;|z|)    
# (Intercept)                 &lt; 0.0000000000000002 ***
#   RELATIONSHIP Not-in-family  &lt; 0.0000000000000002 ***
#   RELATIONSHIP Other-relative &lt; 0.0000000000000002 ***
#   RELATIONSHIP Own-child      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Unmarried      &lt; 0.0000000000000002 ***
#   RELATIONSHIP Wife                       0.896606    
# EDUCATIONNUM                &lt; 0.0000000000000002 ***
#   CAPITALGAIN                 &lt; 0.0000000000000002 ***
#   OCCUPATION_repBlue-Collar       0.00000000000959 ***
#   OCCUPATION_repOther/Unknown &lt; 0.0000000000000002 ***
#   OCCUPATION_repProfessional              0.038871 *  
#   OCCUPATION_repService           0.00000150055853 ***
#   OCCUPATION_repWhite-Collar              0.000362 ***
#   AGE                         &lt; 0.0000000000000002 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# (Dispersion parameter for binomial family taken to be 1)
# 
# Null deviance: 25162  on 22791  degrees of freedom
# Residual deviance: 15542  on 22778  degrees of freedom
# AIC: 15570
# 
# Number of Fisher Scoring iterations: 7</pre><p>我們發現不管是逐步向前或向後法，最後結果都與原始logitMod的結果是一樣的。</p>
<p>而理想中好的模型，偏差平方和(Deviance)應該在+-3之內，我們在此透過繪圖來檢視。</p><pre class="crayon-plain-tag"># 我們建立一個存放模型偏差殘差的資料表
index &lt;- 1:dim(trainingData)[1]
# deviance residuals
dev_resid &lt;- residuals(logitMod)
above50k &lt;- trainingData$ABOVE50K
dff &lt;- data.frame(index,dev_resid,above50k)

ggplot(data = dff, mapping = aes(x = index,y = dev_resid,color = above50k)) +
  geom_point() +
  geom_hline(yintercept = 3,linetype = "dashed", color = "blue") +
  geom_hline(yintercept = -3,linetype = "dashed", color = "blue") +
  labs(title = "Plot of Deviance Residuals") +
  theme(plot.title = element_text(hjust = 0.5))</pre><p>可以發現偏差平方和皆都落在+-3範圍內(藍色虛線)。</p>
<p><img loading="lazy" class="alignnone size-large wp-image-973" src="/wp-content/uploads/2018/08/pic20-1024x928.png" alt="logistic regression" width="960" height="870" srcset="/wp-content/uploads/2018/08/pic20-1024x928.png 1024w, /wp-content/uploads/2018/08/pic20-300x272.png 300w, /wp-content/uploads/2018/08/pic20-768x696.png 768w, /wp-content/uploads/2018/08/pic20-1140x1034.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<h3>8. 模型比較(v.s. Machine Learning Methods)</h3>
<p>我們將傳統羅吉斯回歸預測結果與近期新穎的機器學習法預測結果做比較。</p>
<h4>8-1. 類神經網路Neural Netwoarks(NN)</h4>
<p></p><pre class="crayon-plain-tag">library(nnet)
# A formula of the form class ~ x1 + x2 + ...
# y should be "class"
nn1 &lt;- nnet(as.factor(ABOVE50K_y) ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
            data = trainingData,size = 40 , maxit = 500) 
# maxit: maximum number of iterations. Default 100.
# size: number of units in the hidden layer. Can be zero if there are skip-layer units.

# type只有"raw"=&gt;prob, "class"=&gt;0,1
nn1.predict &lt;- predict(object = nn1,newdata = testData,type = "raw")

misClassError(actuals = testData$ABOVE50K_y,predictedScores = nn1.predict)
# [1] 0.1522 (&lt;0.1592)</pre><p>類神經網路的預測分類錯誤率為15.22%，稍低於羅吉斯回歸的15.92%。</p>
<h4>8-2. 決策樹CART</h4>
<p></p><pre class="crayon-plain-tag">library(rpart)
tree2 &lt;- rpart(formula = as.factor(ABOVE50K_y) ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
               data = trainingData, method = "class",cp = 1e-3)
# 回傳每組預測0,1的機率結果
tree2.pred.prop &lt;- predict(object = tree2, newdata = testData, type = "prob")
# 回傳預測0,1結果
tree2.pred &lt;- predict(object = tree2,newdata = testData, type = "class")

# confusion Matrix
tb2 &lt;- table(tree2.pred, testData$ABOVE50K_y)
tb2
# tree2.pred    0    1
#          0 6965 1022
#          1  451 1331

# missclassification rate
(1022+451) / (6965+1022+451+1331)
# [1] 0.1507831</pre><p>決策樹的預測分類錯誤率為15.07%，低於羅吉斯回歸的15.92%。</p>
<h4>8-3. 隨機森林Random Forest(RF)</h4>
<p></p><pre class="crayon-plain-tag"># 透過產生bootstrapped的決策樹森林來優化模型預測正確率
library(randomForest)
rf3 &lt;- randomForest(as.factor(ABOVE50K_y) ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,
                    data = trainingData, n_tree = 100)
rf3.pred.prob &lt;- predict(object = rf3, newdata = testData, type = "prob")
rf3.pred &lt;- predict(rf3, newdata = testData, type = "class")

# confusion matrix
tb3 &lt;- table(rf3.pred, testData$ABOVE50K_y)
tb3

# rf3.pred    0    1
#        0 6908  968
#        1  508 1385

# missclassification rate
(968+508) / (6908+968+508+1385)
# [1] 0.1510902</pre><p>隨機森林的預測分類錯誤率為15.11%，低於羅吉斯回歸的15.92%。</p>
<h4>8-4. 支援向量機Support Vector Machine(SVM)</h4>
<p></p><pre class="crayon-plain-tag">library(kernlab)
svm4 &lt;- ksvm(as.factor(ABOVE50K_y) ~ RELATIONSHIP + AGE + EDUCATIONNUM + CAPITALGAIN + OCCUPATION_rep,data = trainingData)
svm4.pred.prob &lt;- predict(svm4,testData, type = "decision")
svm4.pred &lt;- predict(svm4,testData, type = "response")

tb4 &lt;- table(svm4.pred, testData$ABOVE50K_y)
tb4

# svm4.pred    0    1
#         0 6976 1060
#         1  440 1293

# missclassification rate
(440+1060) / (6976+1060+440+1293)
# [1] 0.1535469</pre><p>SVM的預測分類錯誤率為15.35%，亦低於羅吉斯回歸的15.92%。</p>
<p>綜合以上結果，我們摘要傳統羅吉斯回歸與機器學習法的預測結果如下表：</p>
<table style="border-collapse: collapse; width: 100%; height: 115px;" border="1">
<tbody>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">演算法</td>
<td style="width: 50%; height: 23px;">預測錯誤率</td>
</tr>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">傳統羅吉斯回歸</td>
<td style="width: 50%; height: 23px;">15.92%</td>
</tr>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">類神經網路</td>
<td style="width: 50%; height: 23px;">15.22%</td>
</tr>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">決策樹(CART)</td>
<td style="width: 50%; height: 23px;">15.11%</td>
</tr>
<tr style="height: 23px;">
<td style="width: 50%; height: 23px;">支持向量機(SVM)</td>
<td style="width: 50%; height: 23px;">15.35%</td>
</tr>
</tbody>
</table>
<p>可以發現機器學習法的對所有測試資料的預測錯誤率沒有比傳統羅吉斯回歸差距太大。</p>
<p>Confusion Matrix看的是模型對於<span style="color: #9f6ad4;">所有data</span>的預測正確度(accuracy)，<span style="color: #9f6ad4;">但因為我們的大部分的資料都是&lt;=50K，所以我們更會比較在不同資料使用水平下，模型在正確度的表現</span>。<span style="color: #9f6ad4;">我們在此會使用<span style="text-decoration: underline;">ROC曲線及曲線下面積AUC</span>來看比較五個模型分別在<span style="text-decoration: underline;">不同資料水平下的模型預測正確率(accuracy)表現</span></span>。</p>
<p>先載入套件ROCR。並替所有模型預測結果產生一組放置(X軸)False Positive Rate和(Y軸)True Positive Rate的data frame，以便後續繪製各模型的ROC曲線。</p>
<p>首先是Logistic Regression的部份如下：</p><pre class="crayon-plain-tag">library(ROCR)

# logistic regression:
# 使用ROC評估分類模型前，都會先創造一個prediction物件，用來將輸入資料標準化
# predictions 為使用測試資料集的模型(logitMod)預測結果(probabilities)，
# 而labels為真實測試集的資料類別(category)
# 兩者個維度必須相同
pr &lt;- prediction(predictions = prob, labels = testData$ABOVE50K_y)
# 產生成效物件。所有的預測評估都會使用這個函數。measure為y軸指標，x.measure為x軸指標。
# 這邊我們會放上ROC曲線的x軸和y軸資訊，分別為1-specificity(x.measure)和sensitivity/recall(measure)
prf &lt;- performance(prediction.obj = pr,measure = "tpr", x.measure = 'fpr')
# 建立一個TP rate和FP rate的data frame
dd &lt;- data.frame(FP = prf@x.values[[1]], TP = prf@y.values[[1]])</pre><p>其他機器學習法如法炮製如下：</p><pre class="crayon-plain-tag"># Neural Networks (NN):
pr1 &lt;- prediction(predictions = nn1.predict,labels = testData$ABOVE50K_y)
prf1 &lt;- performance(prediction.obj = pr1,measure = "tpr",x.measure = "fpr")
dd1 &lt;- data.frame(FP = prf1@x.values[[1]], TP = prf1@y.values[[1]])

# CART:
pr2 &lt;- prediction(predictions = tree2.pred.prop[,2], labels = testData$ABOVE50K_y)
prf2 &lt;- performance(prediction.obj = pr2,measure = "tpr", x.measure = "fpr")
dd2 &lt;- data.frame(FP = prf2@x.values[[1]], TP = prf2@y.values[[1]])

# Random Forest (RF):
pr3 &lt;- prediction(predictions = rf3.pred.prob[,2],labels = testData$ABOVE50K_y)
prf3 &lt;- performance(prediction.obj = pr3, measure = "tpr", x.measure = "fpr")
dd3 &lt;- data.frame(FP = prf3@x.values[[1]], TP = prf3@y.values[[1]])

# SVM:
pr4 &lt;- prediction(predictions = svm4.pred.prob,labels = testData$ABOVE50K_y)
prf4 &lt;- performance(prediction.obj = pr4,measure = "tpr", x.measure = "fpr")
dd4 &lt;- data.frame(FP = prf4@x.values[[1]], TP = prf4@y.values[[1]])</pre><p>準備好各模型預測結果dd, dd1, ~ ,dd4的data frame後，我們將他們轉換成ROC曲線。</p><pre class="crayon-plain-tag"># 將以上幾個模型的ROC曲線畫出：
library(ggplot2)
g &lt;- 
  ggplot() +
    geom_line(data = dd,mapping = aes(x = FP, y = TP, color = 'Logistic Regression')) +
    geom_line(data = dd1,mapping = aes(x = FP, y = TP, color = 'Neural Networks')) +
    geom_line(data = dd2,mapping = aes(x = FP, y = TP, color = 'CART')) +
    geom_line(data = dd3,mapping = aes(x = FP, y = TP, color = 'Random Forest')) +
    geom_line(data = dd4,mapping = aes(x = FP, y = TP, color = 'Support Vector Machine')) +
    geom_segment(mapping = aes(x = 0, xend = 1, y = 0, yend = 1)) +
    ggtitle(label = "ROC Curve") +
    labs(x = "False Positive Rate", y = "True Positive Rate")
g +
  scale_color_manual(name = "classifier",values = c('Logistic Regression'='#E69F00', 
                                                    'Neural Networks'='#56B4E9', 'CART'='#009E73', 
                                                    'Random Forest'='#D55E00', 'Support Vector Machine'='#0072B2'))</pre><p>從圖中可以觀察到，<span style="color: #9f6ad4;">模型中以Neural Networks表現最好，及能使用最少的資料，可以捕捉到最多的目標事件1</span>。(理想中，ROC曲線前段越陡越好，後段則越緩越佳）。</p>
<p><img loading="lazy" class="alignnone size-large wp-image-974" src="/wp-content/uploads/2018/08/pic21-1024x928.png" alt="logistic regression" width="960" height="870" srcset="/wp-content/uploads/2018/08/pic21-1024x928.png 1024w, /wp-content/uploads/2018/08/pic21-300x272.png 300w, /wp-content/uploads/2018/08/pic21-768x696.png 768w, /wp-content/uploads/2018/08/pic21-1140x1034.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>我們進一步將ROC曲線下面積AUC計算出來。</p><pre class="crayon-plain-tag"># AUC
auc &lt;- 
  rbind(performance(pr, measure = 'auc')@y.values[[1]],
        performance(pr1, measure = 'auc')@y.values[[1]],
        performance(pr2, measure = 'auc')@y.values[[1]],
        performance(pr3, measure = 'auc')@y.values[[1]],
        performance(pr4, measure = 'auc')@y.values[[1]])

rownames(auc) &lt;- (c('Logistic Regression', 'Neural Networks', 'CART',
                    'Random Forest', 'Support Vector Machine'))
colnames(auc) &lt;- 'Area Under ROC Curve'
round(auc, 4)

#                        Area Under ROC Curve
# Logistic Regression                  0.8868
# Neural Networks                      0.8970
# CART                                 0.8543
# Random Forest                        0.8646
# Support Vector Machine               0.8651</pre><p>可以發現，Neural Networks曲線下面積最高，Logistic Regression則次之。<span style="color: #9f6ad4;">但因為類神經網路有諸多黑箱邏輯，故為了有更多的解釋基礎，一般來說，還是會選擇使用羅吉斯回歸結果來說明</span>。從此範例也可發現，傳統羅吉斯回歸的對此資料集的預測能力並不亞於諸多新穎的機器學習法。</p>
<h3>總結</h3>
<ul>
<li>羅吉斯回歸與回歸模型相似，必須符合許多基本假設，也因此對資料分配有較多要求，資料前處理的需求亦較多（遺失值、離群值、資料分佈）。</li>
<li>回歸模型基本上並不包含變數篩選，所以事前會使用IV(information value)來做變數初步篩選，事後也會使用VIF檢視共線性。</li>
<li>和線性回歸一樣，羅吉斯會將類別型變數轉換成虛擬變數，因此要避免類別水準值過多的情形。</li>
<li>評估模型適合度，需綜合考量<span style="text-decoration: underline;">整體錯誤率(misclassification rate)</span>與在<span style="text-decoration: underline;"><span style="color: #9f6ad4; text-decoration: underline;">不同資料量水平下的捕捉率(ROC/AUC)</span></span>。</li>
<li>羅吉斯回歸適合用來預<span style="color: #9f6ad4;">測類別變數（二元變數）的<span style="text-decoration: underline;"><strong>機率</strong></span></span>。</li>
<li>模型調整的工約80%是在於資料前處理。</li>
</ul>
<hr />
<p>更多模型建置筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考:</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
			</item>
		<item>
		<title>Logistic Regression 羅吉斯迴歸 &#124; part1 &#8211; 資料探勘與處理 &#124; 統計 R語言</title>
		<link>/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/</link>
					<comments>/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Tue, 28 Aug 2018 06:24:19 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[data pre processing]]></category>
		<category><![CDATA[eda]]></category>
		<category><![CDATA[exploratory data analysis]]></category>
		<category><![CDATA[glm]]></category>
		<category><![CDATA[information value]]></category>
		<category><![CDATA[IV]]></category>
		<category><![CDATA[logistic regression]]></category>
		<category><![CDATA[羅吉斯回歸]]></category>
		<category><![CDATA[資料探索]]></category>
		<category><![CDATA[資料預處理]]></category>
		<guid isPermaLink="false">/?p=846</guid>

					<description><![CDATA[<p>Logistic Regression, 羅吉斯回歸模型，適用於預測二元類別目標變數的發生機率(p)，和線性回歸模型類似，與線性回歸主要不同之處在於：(1) 目 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p class=""></p>
<p>Logistic Regression, 羅吉斯回歸模型，適用於<span style="color: #9f6ad4;">預測<span style="text-decoration: underline;">二元類別目標變數</span>的發生機率(p)</span>，和線性回歸模型類似，與線性回歸主要不同之處在於：(1) 目標變數是目標事件發生機率P經過log函數轉換成log odds值才進行線性預測，且(2)羅吉斯回歸的各項參數是透過最大概似法(MLE)進行估計的。</p>
<h3>Logistic Regression 羅吉斯回歸模型簡介</h3>
<p>線性迴歸模型是用來預測連續型變數，而羅吉斯回歸則是用來預測類別型變數。<br />
羅吉斯回歸和線性回歸模型類似，只不過<span style="color: #9d6ad4;">預測類別目標變數是經過log函數轉換才投入線性模型中的</span>。羅吉斯回歸方程式<span style="color: #9d6ad4;">將類別目標變數轉換為事件的log odds值，也就是\(\log\lgroup\frac{P_{i}}{1-P_{i}}\rgroup\)</span>，來預測Z與預測變數間(X1~Xn)的線性關係。</p>
<p><strong>羅吉斯回歸方程式：</strong></p>
<p><span id="MathJax-Span-15" class="mrow"><span id="MathJax-Span-43" class="mo">$$Z_{i}=\log\lgroup\frac{P_{i}}{1-P_{i}}\rgroup=\beta_{0}+\beta_{1}*x_{1}+&#8230;+\beta_{n}*x_{n}$$</span></span></p>
<p>其中：</p>
<p>(1) \(P_{i}\)為事件發生的機率值。</p>
<p>(2) \(\lgroup\frac{P_{i}}{1-P_{i}}\rgroup\)為勝算比(Odds Ratio)。</p>
<p>在R語言中，上述模型可透過<span style="color: #9d6ad4;">glm()函式</span>，將參數設定為<span style="color: #0000ff;"><span style="color: #9d6ad4;">family=&#8221;binomial&#8221;</span><span style="color: #000000;">來執行</span></span>。</p>
<p>但因為我們真正關心的是模型預測的事件發生機率值Pi。所以，預測結果則透過plogis()函數將事件的log odds轉換為Pi。轉換公式如下：</p>
<p>$$P_{i}=1-\lgroup\frac{1}{1+e_{i}^z}\rgroup$$</p>
<p>羅吉斯回歸透過log函數轉換，產生了一個臨界遞增的S型函數，<span style="color: #9d6ad4;">適用於分析機率模型</span>。<br />
而不同於線性迴歸，羅吉斯回歸分析的各項參數係數，是透過最大概似法(MLE)進行估計。</p>
<h3>分析資料與問題</h3>
<p>Problem: 預測薪資大於50K(ABOVE50K)的影響特徵因素有哪些？</p>
<p>Data: <a href="https://archive.ics.uci.edu/ml/datasets/adult">adult dataset</a> (snapshot)</p>
<p>目標變數(1)：ABOVE50K</p>
<p>預測變數(13)：AGE, WORKCLASS, EDUCATION, EDUCATIONNUM, MARITALSTATUS, OCCUPATION, RELATIONSHIP, RACE, SEX, CAPITALGAIN, CAPITALLOSS, HOURSPERWEEK, NATIVECOUNTRY</p>
<h3>分析步驟</h3>
<p>(part 1 篇會說明步驟1~5的部分)</p>
<ol>
<li><span style="color: #9d6ad4;">資料載入與檢視</span></li>
<li><span style="color: #9d6ad4;">資料探勘</span></li>
<li><span style="color: #9d6ad4;">資料前處理</span></li>
<li><span style="color: #9d6ad4;">產生訓練資料集與測試資料集</span></li>
<li><span style="color: #9d6ad4;">計算特徵變數IV值(Information Value)，篩選變數</span></li>
<li>訓練模型與預測</li>
<li>模型診斷與調整</li>
<li> 模型比較(v.s. Machine Learning Methods)</li>
</ol>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>1. 資料載入與檢視</h3>
<p></p><pre class="crayon-plain-tag"># download from google drive's shareable link:
# https://drive.google.com/file/d/1L71tumU33xmVa2EtxjTyrZ2F6CaXn9iR/view?usp=sharing
id &lt;- "1L71tumU33xmVa2EtxjTyrZ2F6CaXn9iR" # google file ID
inputData &lt;- read.csv(sprintf("https://docs.google.com/uc?id=%s&amp;export=download", id))
# 摘要資料集
summary(inputData)

# AGE                    WORKCLASS         FNLWGT                EDUCATION    
# Min.   :17.00    Private         :22696   Min.   :  12285    HS-grad     :10501  
# 1st Qu.:28.00    Self-emp-not-inc: 2541   1st Qu.: 117827    Some-college: 7291  
# Median :37.00    Local-gov       : 2093   Median : 178356    Bachelors   : 5355  
# Mean   :38.58    ?               : 1836   Mean   : 189778    Masters     : 1723  
# 3rd Qu.:48.00    State-gov       : 1298   3rd Qu.: 237051    Assoc-voc   : 1382  
# Max.   :90.00    Self-emp-inc    : 1116   Max.   :1484705    11th        : 1175  
#                 (Other)          :  981                     (Other)      : 5134  
# EDUCATIONNUM                  MARITALSTATUS              OCCUPATION  
# Min.   : 1.00    Divorced             : 4443    Prof-specialty :4140  
# 1st Qu.: 9.00    Married-AF-spouse    :   23    Craft-repair   :4099  
# Median :10.00    Married-civ-spouse   :14976    Exec-managerial:4066  
# Mean   :10.08    Married-spouse-absent:  418    Adm-clerical   :3770  
# 3rd Qu.:12.00    Never-married        :10683    Sales          :3650  
# Max.   :16.00    Separated            : 1025    Other-service  :3295  
#                  Widowed              :  993   (Other)         :9541  
# RELATIONSHIP                    RACE            SEX       
# Husband       :13193    Amer-Indian-Eskimo:  311    Female:10771  
# Not-in-family : 8305    Asian-Pac-Islander: 1039    Male  :21790  
# Other-relative:  981    Black             : 3124                  
# Own-child     : 5068    Other             :  271                  
# Unmarried     : 3446    White             :27816                  
# Wife          : 1568                                              
# 
# CAPITALGAIN     CAPITALLOSS      HOURSPERWEEK          NATIVECOUNTRY  
# Min.   :    0   Min.   :   0.0   Min.   : 1.00    United-States:29170  
# 1st Qu.:    0   1st Qu.:   0.0   1st Qu.:40.00    Mexico       :  643  
# Median :    0   Median :   0.0   Median :40.00    ?            :  583  
# Mean   : 1078   Mean   :  87.3   Mean   :40.44    Philippines  :  198  
# 3rd Qu.:    0   3rd Qu.:   0.0   3rd Qu.:45.00    Germany      :  137  
# Max.   :99999   Max.   :4356.0   Max.   :99.00    Canada       :  121  
#                                                  (Other)       : 1709  
# ABOVE50K     
# Min.   :0.0000  
# 1st Qu.:0.0000  
# Median :0.0000  
# Mean   :0.2408  
# 3rd Qu.:0.0000  
# Max.   :1.0000</pre><p>由於我們的目標變數目前是數值0,1，為了後續分析，我們決定(1)ABOVE50K轉變成(&lt;=50K, &gt;50K)的factor變數，並(2)另外新增ABOVE50K_y(0,1)的factor變數。</p><pre class="crayon-plain-tag"># 將ABOVE50K原始0,1轉換成&lt;=50K, &gt; 50K的factor資料類型
inputData$ABOVE50K &lt;- factor(x = inputData$ABOVE50K, levels = c(0,1), labels = c('&lt;=50K', '&gt;50K'))
# 產生新factor變數ABOVE50K_y：
library(dplyr)
inputData$ABOVE50K_y &lt;- as.factor(case_when(inputData$ABOVE50K == "&lt;=50K" ~ 0,
                                             inputData$ABOVE50K == "&gt;50K" ~ 1))
# 確認轉換後的變數沒有問題
summary(inputData$ABOVE50K)
# &lt;=50K  &gt;50K 
# 24720  7841 
summary(inputData$ABOVE50K_y)
# 0     1 
# 24720  7841</pre><p>最後再確認一次資料集所有變數型態。</p><pre class="crayon-plain-tag"># 查看資料結構:資料變數、型態
str(inputData)

# 'data.frame':	32561 obs. of  16 variables:
# $ AGE          : int  39 50 38 53 28 37 49 52 31 42 ...
# $ WORKCLASS    : Factor w/ 9 levels " ?"," Federal-gov",..: 8 7 5 5 5 5 5 7 5 5 ...
# $ FNLWGT       : int  77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ...
# $ EDUCATION    : Factor w/ 16 levels " 10th"," 11th",..: 10 10 12 2 10 13 7 12 13 10 ...
# $ EDUCATIONNUM : int  13 13 9 7 13 14 5 9 14 13 ...
# $ MARITALSTATUS: Factor w/ 7 levels " Divorced"," Married-AF-spouse",..: 5 3 1 3 3 3 4 3 5 3 ...
# $ OCCUPATION   : Factor w/ 15 levels " ?"," Adm-clerical",..: 2 5 7 7 11 5 9 5 11 5 ...
# $ RELATIONSHIP : Factor w/ 6 levels " Husband"," Not-in-family",..: 2 1 2 1 6 6 2 1 2 1 ...
# $ RACE         : Factor w/ 5 levels " Amer-Indian-Eskimo",..: 5 5 5 3 3 5 3 5 5 5 ...
# $ SEX          : Factor w/ 2 levels " Female"," Male": 2 2 2 2 1 1 1 2 1 2 ...
# $ CAPITALGAIN  : int  2174 0 0 0 0 0 0 0 14084 5178 ...
# $ CAPITALLOSS  : int  0 0 0 0 0 0 0 0 0 0 ...
# $ HOURSPERWEEK : int  40 13 40 40 40 40 16 45 50 40 ...
# $ NATIVECOUNTRY: Factor w/ 42 levels " ?"," Cambodia",..: 40 40 40 40 6 40 24 40 40 40 ...
# $ ABOVE50K     : Factor w/ 2 levels "&lt;=50K","&gt;50K": 1 1 1 1 1 1 1 2 2 2 ...
# $ ABOVE50K_y   : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 2 2 2 ...</pre><p>幾個光看變數名稱不足以了解的變數說明如下：<br />
(1) fnlwgt : 代表final weight，即該代表個體在母體所佔比例<br />
(2) educatoin_num：代表受教育的時間(年份)<br />
<span style="color: #9d6ad4;">為了簡化分析，我們將忽略fnlwgt每個代表個體在母體中所佔比重。</span></p>
<h3>2. 資料探勘(Exploratory Data Analysis)</h3>
<p></p><pre class="crayon-plain-tag"># 載入繪圖所需套件
library(ggplot2)
library(scales) # needed for labels=percent</pre><p>2-1. Age</p>
<p>查看年齡變數(AGE)各區間次數分佈與每區間薪資水準組成比例。</p><pre class="crayon-plain-tag"># 2-1. age
summary(inputData$AGE)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 17.00   28.00   37.00   38.58   48.00   90.00 

gg &lt;- 
  ggplot(data = inputData, aes(x = as.numeric(AGE))) +
  geom_histogram(color = "black",binwidth = 1, aes(group=ABOVE50K, fill = ABOVE50K)) +
  geom_vline(xintercept = mean(inputData$AGE[inputData$ABOVE50K == 1]), col = "blue", lty = 5, lwd = 0.8) +
  geom_vline(xintercept = mean(inputData$AGE[inputData$ABOVE50K == 0]), col = "black", lty = 5, lwd = 0.8)
gg</pre><p>可以發現薪資水平高於50K的年齡偏高，平均數約落在40-50歲，而薪資水平小於等於50K的族群偏年輕。</p>
<p><img loading="lazy" class="alignnone size-large wp-image-890" src="/wp-content/uploads/2018/08/pic01-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic01-1024x659.png 1024w, /wp-content/uploads/2018/08/pic01-300x193.png 300w, /wp-content/uploads/2018/08/pic01-768x494.png 768w, /wp-content/uploads/2018/08/pic01-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>2-2. Workclass</p><pre class="crayon-plain-tag">summary(inputData$WORKCLASS)
# ?       Federal-gov         Local-gov      Never-worked 
# 1836               960              2093                 7 
# Private      Self-emp-inc  Self-emp-not-inc         State-gov 
# 22696              1116              2541              1298 
# Without-pay 
# 14</pre><p>查看workclass各類別次數分佈。</p><pre class="crayon-plain-tag">inputData$WORKCLASS &lt;- factor(x = inputData$WORKCLASS,levels = names(sort(table(inputData$WORKCLASS),decreasing = TRUE)))
ggplot(data = inputData, aes(x = WORKCLASS, fill = ABOVE50K)) +
  geom_bar() +
  geom_text(stat = "count", aes(label=..count..),size=3.5,position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Paired")</pre><p><img loading="lazy" class="alignnone size-large wp-image-891" src="/wp-content/uploads/2018/08/pic02-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic02-1024x659.png 1024w, /wp-content/uploads/2018/08/pic02-300x193.png 300w, /wp-content/uploads/2018/08/pic02-768x494.png 768w, /wp-content/uploads/2018/08/pic02-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>查看workclass各類別薪資水準組成比例。</p><pre class="crayon-plain-tag"># 首先，先計算類別變數中，不同類別水準值中的薪資水平分佈比例
df2 &lt;- cbind.data.frame(WORKCLASS = inputData$WORKCLASS,ABOVE50K = inputData$ABOVE50K)
df2.summary &lt;- 
  df2 %&gt;% group_by(WORKCLASS,ABOVE50K) %&gt;% 
  summarise(n = n()) %&gt;% 
  mutate(ratio=scales::percent(n/sum(n)))

# 依據ABOVE50K的比例排序，更可以看出哪些類別水準有較高ABOVE50K比例
ggplot(inputData, aes(x = forcats::fct_reorder(f = WORKCLASS, x = as.numeric(ABOVE50K),fun = mean, .desc = TRUE), fill = ABOVE50K)) +
  geom_bar(position = "fill") + 
  geom_text(data=df2.summary, aes(y=n,label=ratio),
            position=position_fill(vjust=0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Paired")</pre><p><img loading="lazy" class="alignnone size-large wp-image-957" src="/wp-content/uploads/2018/08/pic03-1024x928.png" alt="logistic regression" width="960" height="870" srcset="/wp-content/uploads/2018/08/pic03-1024x928.png 1024w, /wp-content/uploads/2018/08/pic03-300x272.png 300w, /wp-content/uploads/2018/08/pic03-768x696.png 768w, /wp-content/uploads/2018/08/pic03-1140x1034.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>2-3. Education</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$EDUCATION)
# 10th          11th          12th       1st-4th       5th-6th       7th-8th 
# 933          1175           433           168           333           646 
# 9th    Assoc-acdm     Assoc-voc     Bachelors     Doctorate       HS-grad 
# 514          1067          1382          5355           413         10501 
# Masters     Preschool   Prof-school  Some-college 
# 1723            51           576          7291</pre><p>次數分配圖</p><pre class="crayon-plain-tag">inputData$EDUCATION &lt;- factor(x = inputData$EDUCATION,levels = names(sort(table(inputData$EDUCATION),decreasing = TRUE)))
ggplot(data = inputData, aes(x = EDUCATION, fill = ABOVE50K)) +
  geom_bar() +
  geom_text(stat = "count", aes(label=..count..),size=3.5,position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Paired") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-893" src="/wp-content/uploads/2018/08/pic04-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic04-1024x659.png 1024w, /wp-content/uploads/2018/08/pic04-300x193.png 300w, /wp-content/uploads/2018/08/pic04-768x494.png 768w, /wp-content/uploads/2018/08/pic04-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>各類別水準值中目標變數分佈圖</p><pre class="crayon-plain-tag"># 首先，先計算類別變數中，不同類別水準值中的薪資水平分佈比例
df2 &lt;- cbind.data.frame(EDUCATION = inputData$EDUCATION,ABOVE50K = inputData$ABOVE50K)
df2.summary &lt;- 
  df2 %&gt;% group_by(EDUCATION) %&gt;% 
  count(ABOVE50K) %&gt;% 
  mutate(ratio=scales::percent(n/sum(n))) %&gt;% 
  ungroup()

# 依據ABOVE50K的比例排序，更可以看出哪些類別水準有較高ABOVE50K比例
ggplot(inputData, aes(x = forcats::fct_reorder(f = EDUCATION, x = as.numeric(ABOVE50K), fun = mean, .desc = TRUE), fill = ABOVE50K)) +
  geom_bar(position = "fill") + 
  geom_text(data=df2.summary, aes(y=n,label=ratio),
            position=position_fill(vjust=0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Paired") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-960" src="/wp-content/uploads/2018/08/pic05-1024x928.png" alt="logistic regression" width="960" height="870" srcset="/wp-content/uploads/2018/08/pic05-1024x928.png 1024w, /wp-content/uploads/2018/08/pic05-300x272.png 300w, /wp-content/uploads/2018/08/pic05-768x696.png 768w, /wp-content/uploads/2018/08/pic05-1140x1034.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>2-4. Educationnum (教育年份)</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$AGE)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 17.00   28.00   37.00   38.58   48.00   90.00</pre><p>次數分配圖</p><pre class="crayon-plain-tag">inputData$AGE_cut &lt;- inputData$AGE 
inputData$AGE_cut &lt;- cut(inputData$AGE_cut, breaks = seq(15,90,5),include.lowest = TRUE)
ggplot(data = inputData, aes(x = AGE_cut, fill = ABOVE50K)) +
  geom_bar() +
  geom_text(stat = "count", aes(label=..count..),size=3.5,position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Paired") +
  labs(x = "AGE")</pre><p><img loading="lazy" class="alignnone size-large wp-image-895" src="/wp-content/uploads/2018/08/pic06-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic06-1024x659.png 1024w, /wp-content/uploads/2018/08/pic06-300x193.png 300w, /wp-content/uploads/2018/08/pic06-768x494.png 768w, /wp-content/uploads/2018/08/pic06-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>各類別水準值中目標變數分佈圖</p><pre class="crayon-plain-tag"># 首先，先計算類別變數中，不同類別水準值中的薪資水平分佈比例
df2 &lt;- cbind.data.frame(AGE_cut = inputData$AGE_cut,ABOVE50K = inputData$ABOVE50K)
df2.summary &lt;- 
  df2 %&gt;% group_by(AGE_cut) %&gt;% 
  count(ABOVE50K) %&gt;% 
  mutate(ratio=scales::percent(n/sum(n))) %&gt;% 
  ungroup()

# 依據ABOVE50K的比例排序，更可以看出哪些類別水準有較高ABOVE50K比例
ggplot(inputData, aes(x = forcats::fct_reorder(f = AGE_cut, x = as.numeric(ABOVE50K), fun = mean, .desc = TRUE), fill = ABOVE50K)) +
  geom_bar(position = "fill") + 
  geom_text(data=df2.summary, aes(y=n,label=ratio),
            position=position_fill(vjust=0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Paired") +
  labs(x = "AGE")</pre><p><img loading="lazy" class="alignnone size-large wp-image-896" src="/wp-content/uploads/2018/08/pic07-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic07-1024x659.png 1024w, /wp-content/uploads/2018/08/pic07-300x193.png 300w, /wp-content/uploads/2018/08/pic07-768x494.png 768w, /wp-content/uploads/2018/08/pic07-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>2-5. MaritalStatus</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$MARITALSTATUS)
# Divorced      Married-AF-spouse     Married-civ-spouse 
# 4443                     23                  14976 
# Married-spouse-absent          Never-married              Separated 
# 418                  10683                   1025 
# Widowed 
# 993</pre><p>次數分配圖</p><pre class="crayon-plain-tag">inputData$MARITALSTATUS &lt;- factor(x = inputData$MARITALSTATUS,levels = names(sort(table(inputData$MARITALSTATUS),decreasing = TRUE)))
ggplot(data = inputData, aes(x = MARITALSTATUS, fill = ABOVE50K)) +
  geom_bar() +
  geom_text(stat = "count", aes(label=..count..),size=3.5,position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Paired") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-897" src="/wp-content/uploads/2018/08/pic08-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic08-1024x659.png 1024w, /wp-content/uploads/2018/08/pic08-300x193.png 300w, /wp-content/uploads/2018/08/pic08-768x494.png 768w, /wp-content/uploads/2018/08/pic08-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>各類別水準值中目標變數分佈圖</p><pre class="crayon-plain-tag"># 首先，先計算類別變數中，不同類別水準值中的薪資水平分佈比例
df2 &lt;- cbind.data.frame(MARITALSTATUS = inputData$MARITALSTATUS,ABOVE50K = inputData$ABOVE50K)
df2.summary &lt;- 
  df2 %&gt;% group_by(MARITALSTATUS) %&gt;% 
  count(ABOVE50K) %&gt;% 
  mutate(ratio=scales::percent(n/sum(n))) %&gt;% 
  ungroup()

# 依據ABOVE50K的比例排序，更可以看出哪些類別水準有較高ABOVE50K比例
ggplot(inputData, aes(x = forcats::fct_reorder(f = MARITALSTATUS, x = as.numeric(ABOVE50K), fun = mean, .desc = TRUE), fill = ABOVE50K)) +
  geom_bar(position = "fill") + 
  geom_text(data=df2.summary, aes(y=n,label=ratio),
            position=position_fill(vjust=0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Paired") +
  labs(x = "MARITALSTATUS", y = "percentage") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-898" src="/wp-content/uploads/2018/08/pic09-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic09-1024x659.png 1024w, /wp-content/uploads/2018/08/pic09-300x193.png 300w, /wp-content/uploads/2018/08/pic09-768x494.png 768w, /wp-content/uploads/2018/08/pic09-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>如果想要同一張圖y軸可以看到frequency，stacked bar可以看到目標類別水平的比例分佈，可以使用下面方法繪製。</p><pre class="crayon-plain-tag"># 如果想要在同一張圖同時看到frequencies &amp; proportion:
# 先新增一欄label位置的欄位
# (依照ABOVE50K的類別水準順序，計算累積數值，並將數值置中於各類別累積高度)
df3.summary &lt;-
  df2.summary %&gt;%
  group_by(MARITALSTATUS) %&gt;%
  #因為繪圖資訊是先畫&gt;50K，再畫&lt;=50K，故要調整累積數值計算的順序
  do( data.frame(with(data=., .[order(desc(ABOVE50K)),] )) ) %&gt;% 
  ungroup() %&gt;% 
  group_by(MARITALSTATUS) %&gt;%
  mutate(pos = (cumsum(n) - (0.5 * n))) %&gt;%
  as.data.frame() %&gt;% 
  ungroup()

ggplot(data = df3.summary,mapping = aes(x = MARITALSTATUS, y = n, fill = ABOVE50K)) +
  geom_bar(stat = "identity") + 
  geom_text(mapping = aes(y = pos, label = ratio), size = 3) +
  scale_fill_brewer(palette="Paired") +
  labs(title = "MARITALSTATUS VS ABOVE50K",
       x = "MARITALSTATUS", y = "Frequencies") + # Add Title and Labels
  theme(plot.title = element_text(hjust = 0.5)) # Change the appearance of the main title</pre><p><img loading="lazy" class="alignnone size-large wp-image-958" src="/wp-content/uploads/2018/08/pic10-1024x928.png" alt="logistic regression" width="960" height="870" srcset="/wp-content/uploads/2018/08/pic10-1024x928.png 1024w, /wp-content/uploads/2018/08/pic10-300x272.png 300w, /wp-content/uploads/2018/08/pic10-768x696.png 768w, /wp-content/uploads/2018/08/pic10-1140x1034.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>2-6. Occupation職業</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$OCCUPATION)
# ?       Adm-clerical       Armed-Forces       Craft-repair 
# 1843               3770                  9               4099 
# Exec-managerial    Farming-fishing  Handlers-cleaners  Machine-op-inspct 
# 4066                994               1370               2002 
# Other-service    Priv-house-serv     Prof-specialty    Protective-serv 
# 3295                149               4140                649 
# Sales       Tech-support   Transport-moving 
# 3650                928               1597</pre><p>可以發現類別水準值滿複雜的。</p>
<p>次數分配圖</p><pre class="crayon-plain-tag">inputData$OCCUPATION &lt;- factor(x = inputData$OCCUPATION,levels = names(sort(table(inputData$OCCUPATION),decreasing = TRUE)))
ggplot(data = inputData, aes(x = OCCUPATION, fill = ABOVE50K)) +
  geom_bar() +
  geom_text(stat = "count", aes(label=..count..),size=3.5,position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Paired") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-900" src="/wp-content/uploads/2018/08/pic11-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic11-1024x659.png 1024w, /wp-content/uploads/2018/08/pic11-300x193.png 300w, /wp-content/uploads/2018/08/pic11-768x494.png 768w, /wp-content/uploads/2018/08/pic11-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>各類別水準值中目標變數分佈圖</p><pre class="crayon-plain-tag"># 首先，先計算類別變數中，不同類別水準值中的薪資水平分佈比例
df2 &lt;- cbind.data.frame(OCCUPATION = inputData$OCCUPATION,ABOVE50K = inputData$ABOVE50K)
df2.summary &lt;- 
  df2 %&gt;% group_by(OCCUPATION) %&gt;% 
  count(ABOVE50K) %&gt;% 
  mutate(ratio=scales::percent(n/sum(n))) %&gt;% 
  as.data.frame() %&gt;% 
  ungroup()

# 依據ABOVE50K的比例排序，更可以看出哪些類別水準有較高ABOVE50K比例
ggplot(inputData, aes(x = forcats::fct_reorder(f = OCCUPATION, x = as.numeric(ABOVE50K), fun = mean, .desc = TRUE), fill = ABOVE50K)) +
  geom_bar(position = "fill") + 
  geom_text(data=df2.summary, aes(y=n,label=ratio),
            position=position_fill(vjust=0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Paired") +
  labs(x = "OCCUPATION", y = "percentage") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-901" src="/wp-content/uploads/2018/08/pic12-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic12-1024x659.png 1024w, /wp-content/uploads/2018/08/pic12-300x193.png 300w, /wp-content/uploads/2018/08/pic12-768x494.png 768w, /wp-content/uploads/2018/08/pic12-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>如果想要同一張圖y軸可以看到frequency，stacked bar可以看到目標類別水平的比例分佈，可以使用下面方法繪製。</p><pre class="crayon-plain-tag">df3.summary &lt;-
  df2.summary %&gt;%
  group_by(OCCUPATION) %&gt;%
  #因為繪圖資訊是先畫&gt;50K，再畫&lt;=50K，故要調整累積數值計算的順序
  do( data.frame(with(data=., .[order(desc(ABOVE50K)),] )) ) %&gt;% 
  ungroup() %&gt;% 
  group_by(OCCUPATION) %&gt;%
  mutate(pos = (cumsum(n) - (0.5 * n))) %&gt;%
  as.data.frame() %&gt;% 
  ungroup()

ggplot(data = df3.summary,mapping = aes(x = OCCUPATION, y = n, fill = ABOVE50K)) +
  geom_bar(stat = "identity") + 
  geom_text(mapping = aes(y = pos, label = ratio), size = 3) +
  scale_fill_brewer(palette="Paired") +
  labs(title = "OCCUPATION VS ABOVE50K",
       x = "OCCUPATION", y = "Frequencies") + # Add Title and Labels
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-902" src="/wp-content/uploads/2018/08/pic13-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic13-1024x659.png 1024w, /wp-content/uploads/2018/08/pic13-300x193.png 300w, /wp-content/uploads/2018/08/pic13-768x494.png 768w, /wp-content/uploads/2018/08/pic13-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>2-7. Relationship</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$RELATIONSHIP)
# Husband   Not-in-family  Other-relative       Own-child       Unmarried 
# 13193            8305             981            5068            3446 
# Wife 
# 1568</pre><p>次數分配圖</p><pre class="crayon-plain-tag">inputData$RELATIONSHIP &lt;- factor(x = inputData$RELATIONSHIP,levels = names(sort(table(inputData$RELATIONSHIP),decreasing = TRUE)))
ggplot(data = inputData, aes(x = RELATIONSHIP, fill = ABOVE50K)) +
  geom_bar() +
  geom_text(stat = "count", aes(label=..count..),size=3.5,position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Paired")</pre><p><img loading="lazy" class="alignnone size-large wp-image-903" src="/wp-content/uploads/2018/08/pic14-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic14-1024x659.png 1024w, /wp-content/uploads/2018/08/pic14-300x193.png 300w, /wp-content/uploads/2018/08/pic14-768x494.png 768w, /wp-content/uploads/2018/08/pic14-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>各類別水準值中目標變數分佈圖</p><pre class="crayon-plain-tag"># 首先，先計算類別變數中，不同類別水準值中的薪資水平分佈比例
df2 &lt;- cbind.data.frame(RELATIONSHIP = inputData$RELATIONSHIP,ABOVE50K = inputData$ABOVE50K)
df2.summary &lt;- 
  df2 %&gt;% group_by(RELATIONSHIP) %&gt;% 
  count(ABOVE50K) %&gt;% 
  mutate(ratio=scales::percent(n/sum(n))) %&gt;% 
  as.data.frame() %&gt;% 
  ungroup()

# 依據ABOVE50K的比例排序，更可以看出哪些類別水準有較高ABOVE50K比例
ggplot(inputData, aes(x = forcats::fct_reorder(f = RELATIONSHIP, x = as.numeric(ABOVE50K), fun = mean, .desc = TRUE), fill = ABOVE50K)) +
  geom_bar(position = "fill") + 
  geom_text(data=df2.summary, aes(y=n,label=ratio),
            position=position_fill(vjust=0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Paired") +
  labs(x = "RELATIONSHIP", y = "percentage") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-904" src="/wp-content/uploads/2018/08/pic15-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic15-1024x659.png 1024w, /wp-content/uploads/2018/08/pic15-300x193.png 300w, /wp-content/uploads/2018/08/pic15-768x494.png 768w, /wp-content/uploads/2018/08/pic15-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>繪製同時一張圖呈現frequency與stacked bar目標類別水平的比例分佈圖。</p><pre class="crayon-plain-tag">df3.summary &lt;-
  df2.summary %&gt;%
  group_by(RELATIONSHIP) %&gt;%
  #因為繪圖資訊是先畫&gt;50K，再畫&lt;=50K，故要調整累積數值計算的順序
  do( data.frame(with(data=., .[order(desc(ABOVE50K)),] )) ) %&gt;% 
  ungroup() %&gt;% 
  group_by(RELATIONSHIP) %&gt;%
  mutate(pos = (cumsum(n) - (0.5 * n))) %&gt;%
  as.data.frame() %&gt;% 
  ungroup()

ggplot(data = df3.summary,mapping = aes(x = RELATIONSHIP, y = n, fill = ABOVE50K)) +
  geom_bar(stat = "identity") + 
  geom_text(mapping = aes(y = pos, label = ratio), size = 3) +
  scale_fill_brewer(palette="Paired") +
  labs(title = "RELATIONSHIP VS ABOVE50K",
       x = "RELATIONSHIP", y = "Frequencies") + # Add Title and Labels
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 30, hjust = 1)) # Change the appearance of the main title</pre><p><img loading="lazy" class="alignnone size-large wp-image-905" src="/wp-content/uploads/2018/08/pic16-1024x659.png" alt="logistic regression" width="960" height="618" srcset="/wp-content/uploads/2018/08/pic16-1024x659.png 1024w, /wp-content/uploads/2018/08/pic16-300x193.png 300w, /wp-content/uploads/2018/08/pic16-768x494.png 768w, /wp-content/uploads/2018/08/pic16-1140x734.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>2-8. Race種族</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$RACE)
# Amer-Indian-Eskimo  Asian-Pac-Islander               Black               Other               White 
# 311                1039                3124                 271               27816</pre><p>次數分配圖</p><pre class="crayon-plain-tag">inputData$RACE &lt;- factor(x = inputData$RACE,levels = names(sort(table(inputData$RACE),decreasing = TRUE)))
ggplot(data = inputData, aes(x = RACE, fill = ABOVE50K)) +
  geom_bar() +
  geom_text(stat = "count", aes(label=..count..),size=3.5,position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Paired") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-906" src="/wp-content/uploads/2018/08/pic17-1024x933.png" alt="logistic regression" width="960" height="875" srcset="/wp-content/uploads/2018/08/pic17-1024x933.png 1024w, /wp-content/uploads/2018/08/pic17-300x273.png 300w, /wp-content/uploads/2018/08/pic17-768x700.png 768w, /wp-content/uploads/2018/08/pic17-1140x1039.png 1140w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>各類別水準值中目標變數分佈圖</p><pre class="crayon-plain-tag"># 首先，先計算類別變數中，不同類別水準值中的薪資水平分佈比例
df2 &lt;- cbind.data.frame(RACE = inputData$RACE,ABOVE50K = inputData$ABOVE50K)
df2.summary &lt;- 
  df2 %&gt;% group_by(RACE) %&gt;% 
  count(ABOVE50K) %&gt;% 
  mutate(ratio=scales::percent(n/sum(n))) %&gt;% 
  as.data.frame() %&gt;% 
  ungroup()

# 依據ABOVE50K的比例排序，更可以看出哪些類別水準有較高ABOVE50K比例
ggplot(inputData, aes(x = forcats::fct_reorder(f = RACE, x = as.numeric(ABOVE50K), fun = mean, .desc = TRUE), fill = ABOVE50K)) +
  geom_bar(position = "fill") + 
  geom_text(data=df2.summary, aes(y=n,label=ratio),
            position=position_fill(vjust=0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Paired") +
  labs(x = "RACE", y = "percentage") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-1581" src="/wp-content/uploads/2018/09/pic18-1-1024x928.png" alt="logistic regression" width="1024" height="928" srcset="/wp-content/uploads/2018/09/pic18-1-1024x928.png 1024w, /wp-content/uploads/2018/09/pic18-1-300x272.png 300w, /wp-content/uploads/2018/09/pic18-1-768x696.png 768w, /wp-content/uploads/2018/09/pic18-1-830x753.png 830w, /wp-content/uploads/2018/09/pic18-1-230x209.png 230w, /wp-content/uploads/2018/09/pic18-1-350x317.png 350w, /wp-content/uploads/2018/09/pic18-1-480x435.png 480w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>繪製同時一張圖呈現frequency與stacked bar目標類別水平的比例分佈圖。</p><pre class="crayon-plain-tag">df3.summary &lt;-
  df2.summary %&gt;%
  group_by(RACE) %&gt;%
  #因為繪圖資訊是先畫&gt;50K，再畫&lt;=50K，故要調整累積數值計算的順序
  do( data.frame(with(data=., .[order(desc(ABOVE50K)),] )) ) %&gt;% 
  ungroup() %&gt;% 
  group_by(RACE) %&gt;%
  mutate(pos = (cumsum(n) - (0.5 * n))) %&gt;%
  as.data.frame() %&gt;% 
  ungroup()

ggplot(data = df3.summary,mapping = aes(x = RACE, y = n, fill = ABOVE50K)) +
  geom_bar(stat = "identity") + 
  geom_text(mapping = aes(y = pos, label = ratio), size = 3) +
  scale_fill_brewer(palette="Paired") +
  labs(title = "RACE VS ABOVE50K",
       x = "RACE", y = "Frequencies") + # Add Title and Labels
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 30, hjust = 1)) # Change the appearance of the main title</pre><p><img loading="lazy" class="alignnone size-large wp-image-1572" src="/wp-content/uploads/2018/09/pic19-1024x602.png" alt="logistic regression" width="1024" height="602" srcset="/wp-content/uploads/2018/09/pic19-1024x602.png 1024w, /wp-content/uploads/2018/09/pic19-300x176.png 300w, /wp-content/uploads/2018/09/pic19-768x451.png 768w, /wp-content/uploads/2018/09/pic19-830x488.png 830w, /wp-content/uploads/2018/09/pic19-230x135.png 230w, /wp-content/uploads/2018/09/pic19-350x206.png 350w, /wp-content/uploads/2018/09/pic19-480x282.png 480w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>2-9. Sex 性別</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$SEX)
# Female    Male 
# 10771   21790</pre><p>次數分配圖</p><pre class="crayon-plain-tag">inputData$SEX &lt;- factor(x = inputData$SEX,levels = names(sort(table(inputData$SEX),decreasing = TRUE)))
ggplot(data = inputData, aes(x = SEX, fill = ABOVE50K)) +
  geom_bar() +
  geom_text(stat = "count", aes(label=..count..),size=3.5,position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Paired")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1574" src="/wp-content/uploads/2018/09/pic20-1024x602.png" alt="logistic regression" width="1024" height="602" srcset="/wp-content/uploads/2018/09/pic20-1024x602.png 1024w, /wp-content/uploads/2018/09/pic20-300x176.png 300w, /wp-content/uploads/2018/09/pic20-768x451.png 768w, /wp-content/uploads/2018/09/pic20-830x488.png 830w, /wp-content/uploads/2018/09/pic20-230x135.png 230w, /wp-content/uploads/2018/09/pic20-350x206.png 350w, /wp-content/uploads/2018/09/pic20-480x282.png 480w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>各類別水準值中目標變數分佈圖</p><pre class="crayon-plain-tag"># 首先，先計算類別變數中，不同類別水準值中的薪資水平分佈比例
df2 &lt;- cbind.data.frame(SEX = inputData$SEX,ABOVE50K = inputData$ABOVE50K)
df2.summary &lt;- 
  df2 %&gt;% group_by(SEX) %&gt;% 
  count(ABOVE50K) %&gt;% 
  mutate(ratio=scales::percent(n/sum(n))) %&gt;% 
  as.data.frame() %&gt;% 
  ungroup()

# 依據ABOVE50K的比例排序，更可以看出哪些類別水準有較高ABOVE50K比例
ggplot(inputData, aes(x = forcats::fct_reorder(f = SEX, x = as.numeric(ABOVE50K), fun = mean, .desc = TRUE), fill = ABOVE50K)) +
  geom_bar(position = "fill") + 
  geom_text(data=df2.summary, aes(y=n,label=ratio),
            position=position_fill(vjust=0.5)) +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette="Paired") +
  labs(x = "SEX", y = "percentage") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-1575" src="/wp-content/uploads/2018/09/pic21-1024x602.png" alt="logistic regression" width="1024" height="602" srcset="/wp-content/uploads/2018/09/pic21-1024x602.png 1024w, /wp-content/uploads/2018/09/pic21-300x176.png 300w, /wp-content/uploads/2018/09/pic21-768x451.png 768w, /wp-content/uploads/2018/09/pic21-830x488.png 830w, /wp-content/uploads/2018/09/pic21-230x135.png 230w, /wp-content/uploads/2018/09/pic21-350x206.png 350w, /wp-content/uploads/2018/09/pic21-480x282.png 480w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>繪製同時一張圖呈現frequency與stacked bar目標類別水平的比例分佈圖。</p><pre class="crayon-plain-tag">df3.summary &lt;-
  df2.summary %&gt;%
  group_by(SEX) %&gt;%
  #因為繪圖資訊是先畫&gt;50K，再畫&lt;=50K，故要調整累積數值計算的順序
  do( data.frame(with(data=., .[order(desc(ABOVE50K)),] )) ) %&gt;% 
  ungroup() %&gt;% 
  group_by(SEX) %&gt;%
  mutate(pos = (cumsum(n) - (0.5 * n))) %&gt;%
  as.data.frame() %&gt;% 
  ungroup()

ggplot(data = df3.summary,mapping = aes(x = SEX, y = n, fill = ABOVE50K)) +
  geom_bar(stat = "identity") + 
  geom_text(mapping = aes(y = pos, label = ratio), size = 3) +
  scale_fill_brewer(palette="Paired") +
  labs(title = "SEX VS ABOVE50K",
       x = "SEX", y = "Frequencies") + # Add Title and Labels
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 30, hjust = 1)) # Change the appearance of the main title</pre><p>&nbsp;</p>
<p><img loading="lazy" class="alignnone size-large wp-image-1576" src="/wp-content/uploads/2018/09/pic22-1024x602.png" alt="logistic regression" width="1024" height="602" srcset="/wp-content/uploads/2018/09/pic22-1024x602.png 1024w, /wp-content/uploads/2018/09/pic22-300x176.png 300w, /wp-content/uploads/2018/09/pic22-768x451.png 768w, /wp-content/uploads/2018/09/pic22-830x488.png 830w, /wp-content/uploads/2018/09/pic22-230x135.png 230w, /wp-content/uploads/2018/09/pic22-350x206.png 350w, /wp-content/uploads/2018/09/pic22-480x282.png 480w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>2-10. CapitalGain 資本獲利</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$CAPITALGAIN)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0       0       0    1078       0   99999</pre><p>可以發現資料很右偏，至少有75％(第三個四分位數)的資料都是0。</p>
<p>次數分配圖</p><pre class="crayon-plain-tag">ggplot(data = inputData,mapping = aes(x = CAPITALGAIN, fill = ABOVE50K, group = ABOVE50K)) +
  geom_histogram(bins = 10,color = 'black',lwd =0.3, binwidth = 10000) +  # default bins = 30
  stat_bin(binwidth = 10000, geom = "text", color = "white", size = 3.5,
           mapping = aes(label = ..count.., group = ABOVE50K),position=position_stack(vjust=0.5)) +
  scale_fill_brewer(palette="Paired") +
  scale_x_continuous(breaks= seq(0,max(inputData$CAPITALGAIN), 10000))</pre><p>我們可以觀察到，視覺化的資料極右偏，幾乎所有資料都是0</p>
<p><img loading="lazy" class="alignnone size-large wp-image-1577" src="/wp-content/uploads/2018/09/pic23-1024x602.png" alt="logistic regression" width="1024" height="602" srcset="/wp-content/uploads/2018/09/pic23-1024x602.png 1024w, /wp-content/uploads/2018/09/pic23-300x176.png 300w, /wp-content/uploads/2018/09/pic23-768x451.png 768w, /wp-content/uploads/2018/09/pic23-830x488.png 830w, /wp-content/uploads/2018/09/pic23-230x135.png 230w, /wp-content/uploads/2018/09/pic23-350x206.png 350w, /wp-content/uploads/2018/09/pic23-480x282.png 480w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>看一下capitalgain資料是0的比例：</p><pre class="crayon-plain-tag">sum(inputData$CAPITALGAIN == 0)/length(inputData$CAPITALGAIN)
# [1] 0.9167102</pre><p>2-11. Capital Loss 資本失利</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$CAPITALLOSS)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.0     0.0     0.0    87.3     0.0  4356.0</pre><p>次數分配圖</p><pre class="crayon-plain-tag">ggplot(data = inputData,mapping = aes(x = CAPITALLOSS, fill = ABOVE50K, group = ABOVE50K)) +
  geom_histogram(color = 'black',lwd =0.3, binwidth = 1000) +  # default bins = 30
  stat_bin(geom = "text", color = "white", size = 3.5, binwidth = 1000,
           mapping = aes(label = ..count.., group = ABOVE50K),position=position_stack(vjust=0.5)) +
  scale_fill_brewer(palette="Paired")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1578" src="/wp-content/uploads/2018/09/pic24-1024x602.png" alt="logistic regression" width="1024" height="602" srcset="/wp-content/uploads/2018/09/pic24-1024x602.png 1024w, /wp-content/uploads/2018/09/pic24-300x176.png 300w, /wp-content/uploads/2018/09/pic24-768x451.png 768w, /wp-content/uploads/2018/09/pic24-830x488.png 830w, /wp-content/uploads/2018/09/pic24-230x135.png 230w, /wp-content/uploads/2018/09/pic24-350x206.png 350w, /wp-content/uploads/2018/09/pic24-480x282.png 480w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>看一下CAPITALLOSS資料是0的比例：</p><pre class="crayon-plain-tag">sum(inputData$CAPITALLOSS == 0)/length(inputData$CAPITALLOSS)
# [1] 0.9533491</pre><p>2-12. HOURSPERWEEK</p>
<p>基礎敘述統計</p><pre class="crayon-plain-tag">summary(inputData$HOURSPERWEEK)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 1.00   40.00   40.00   40.44   45.00   99.00</pre><p>次數分配圖</p><pre class="crayon-plain-tag">ggplot(data = inputData,mapping = aes(x = HOURSPERWEEK, fill = ABOVE50K, group = ABOVE50K)) +
  geom_histogram(bins = 10, color = 'black',lwd =0.3) +  # default bins = 30
  stat_bin(geom = "text", color = "white", size = 3.5, bins = 10,
           mapping = aes(label = ..count.., group = ABOVE50K),position=position_stack(vjust=0.5)) +
  scale_fill_brewer(palette="Paired")</pre><p><img loading="lazy" class="alignnone size-large wp-image-1579" src="/wp-content/uploads/2018/09/pic25-1024x602.png" alt="logistic regression" width="1024" height="602" srcset="/wp-content/uploads/2018/09/pic25-1024x602.png 1024w, /wp-content/uploads/2018/09/pic25-300x176.png 300w, /wp-content/uploads/2018/09/pic25-768x451.png 768w, /wp-content/uploads/2018/09/pic25-830x488.png 830w, /wp-content/uploads/2018/09/pic25-230x135.png 230w, /wp-content/uploads/2018/09/pic25-350x206.png 350w, /wp-content/uploads/2018/09/pic25-480x282.png 480w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p>2-13. NATIVECOUNTRY 國籍</p>
<p>次數分配圖</p><pre class="crayon-plain-tag">inputData$NATIVECOUNTRY &lt;- factor(x = inputData$NATIVECOUNTRY,levels = names(sort(table(inputData$NATIVECOUNTRY),decreasing = TRUE)))
ggplot(data = inputData, aes(x = NATIVECOUNTRY, fill = ABOVE50K)) +
  geom_bar() +
  geom_text(stat = "count", aes(label=..count..),size=3.5,position = position_stack(vjust = 0.5)) +
  scale_fill_brewer(palette="Paired") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))</pre><p><img loading="lazy" class="alignnone size-large wp-image-1580" src="/wp-content/uploads/2018/09/pic26-1024x602.png" alt="logistic regression" width="1024" height="602" srcset="/wp-content/uploads/2018/09/pic26-1024x602.png 1024w, /wp-content/uploads/2018/09/pic26-300x176.png 300w, /wp-content/uploads/2018/09/pic26-768x451.png 768w, /wp-content/uploads/2018/09/pic26-830x488.png 830w, /wp-content/uploads/2018/09/pic26-230x135.png 230w, /wp-content/uploads/2018/09/pic26-350x206.png 350w, /wp-content/uploads/2018/09/pic26-480x282.png 480w" sizes="(max-width: 1024px) 100vw, 1024px" /></p>
<p><span style="color: #9d6ad4;">可以發現資料超偏，都集中在美國，因此不適合投入模型</span>。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>3. 資料前處理</h3>
<p>3- 1. 類別水準值簡化</p>
<p>OCCUPATION的部分，因為水平值偏多，且沒有替代變數欄位，故我們考慮將此變數水平值簡化。我們將之簡化為以下幾群：<br />
Blue-Collar, Professional, Sales, Service, and White-Collar, Other/Unknown。</p><pre class="crayon-plain-tag"># 先將?改命名成Unknown
levels(inputData$OCCUPATION)[1] &lt;- "Unknown"

# 新增一個replacement水準值後的欄位：
inputData$OCCUPATION_rep &lt;- inputData$OCCUPATION

# (1) Craft-repair, Farming-fishing, Handlers-cleaners , Machine-op-inspct, Transport-moving =&gt; 合併成Blue-Collar
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Craft-repair', replacement = 'Blue-Collar', x = inputData$OCCUPATION_rep)
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Farming-fishing', replacement = 'Blue-Collar', x = inputData$OCCUPATION_rep)
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Handlers-cleaners', replacement = 'Blue-Collar', x = inputData$OCCUPATION_rep)
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Machine-op-inspct', replacement = 'Blue-Collar', x = inputData$OCCUPATION_rep)
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Transport-moving', replacement = 'Blue-Collar', x = inputData$OCCUPATION_rep)

# (2) Prof-specialty, =&gt; Professional
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Prof-specialty', replacement = 'Professional', x = inputData$OCCUPATION_rep)

# (3) Sales =&gt; Sales不變

# (4) Other-service, Priv-house-serv, Protective-serv, Tech-support, =&gt; Service
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Other-service', replacement = 'Service', x = inputData$OCCUPATION_rep)
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Priv-house-serv', replacement = 'Service', x = inputData$OCCUPATION_rep)
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Protective-serv', replacement = 'Service', x = inputData$OCCUPATION_rep)
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Tech-support', replacement = 'Service', x = inputData$OCCUPATION_rep)

# (5) Adm-clerical, Exec-managerial =&gt; 合併成White-Collar
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Adm-clerical', replacement = 'White-Collar', x = inputData$OCCUPATION_rep)
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Exec-managerial', replacement = 'White-Collar', x = inputData$OCCUPATION_rep)
# (6) ?,Armed-Forces =&gt; Other/Unknown
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^Unknown', replacement = 'Other/Unknown', x = inputData$OCCUPATION_rep)
inputData$OCCUPATION_rep &lt;- gsub(pattern = '^ Armed-Forces', replacement = 'Other/Unknown', x = inputData$OCCUPATION_rep)

inputData$OCCUPATION_rep &lt;- as.factor(inputData$OCCUPATION_rep)

summary(inputData$OCCUPATION_rep)
# Sales   Blue-Collar Other/Unknown  Professional       Service  White-Collar 
# 3650         10062          1852          4140          5021          7836
levels(inputData$OCCUPATION_rep)
# [1] " Sales"        "Blue-Collar"   "Other/Unknown" "Professional"  "Service"       "White-Collar"</pre><p>3-2. (optional)產生類別變數的WOE值<span style="color: #9d6ad4;">(本分析不會執行此段程式碼）</span></p><pre class="crayon-plain-tag"># 這是一個選擇性的步驟，我們在本分析不會執行
# factor_vars &lt;- c ("WORKCLASS", "EDUCATION", "MARITALSTATUS", "OCCUPATION", "OCCUPATION_rep", "RELATIONSHIP", "RACE", "SEX", "NATIVECOUNTRY")

# not run
# for(factor_var in factor_vars){
#   inputData[[factor_var]] &lt;- WOE(X = inputData[,factor_var], Y = inputData$ABOVE50K_y)
# }</pre><p></p>
<h3>4. 產生訓練資料集與測試資料集</h3>
<p></p><pre class="crayon-plain-tag"># 產生訓練資料集
input_ones &lt;- inputData[which(inputData$ABOVE50K_y == 1),]
input_zeros &lt;- inputData[which(inputData$ABOVE50K_y == 0), ]
set.seed(100)
input_ones_training_row &lt;- sample(1:nrow(input_ones),0.7*nrow(input_ones))
input_zeros_training_row &lt;- sample(1:nrow(input_zeros),0.7*nrow(input_zeros))

training_ones &lt;- input_ones[input_ones_training_row,]
training_zeros &lt;- input_zeros[input_zeros_training_row,]
trainingData &lt;- rbind(training_ones, training_zeros)

# 產生測試資料集
test_ones &lt;- input_ones[-input_ones_training_row,]
test_zeros &lt;- input_zeros[-input_zeros_training_row,]
testData &lt;- rbind(test_ones, test_zeros)</pre><p></p>
<h3>5. 計算IV值，篩選變數</h3>
<p></p><pre class="crayon-plain-tag"># 我們會使用套件中的函數smbinning::smbinning，來將連續變數切割為類別變數
library(smbinning)
# 將連續型變數和類別型變數分開
# 記得加入"OCCUPATION_rep"
factor_vars &lt;- c ("WORKCLASS", "EDUCATION", "MARITALSTATUS", "OCCUPATION","OCCUPATION_rep", "RELATIONSHIP", "RACE", "SEX", "NATIVECOUNTRY")
continuous_vars &lt;- c("AGE", "FNLWGT","EDUCATIONNUM", "HOURSPERWEEK", "CAPITALGAIN", "CAPITALLOSS")

# 建立一個變數IV值表格 : numeric(參數要放變數行數)
iv_df &lt;- data.frame(VARS = c(factor_vars, continuous_vars), IV = numeric(15))

# 計算類別變數的IV值
for(factor_var in factor_vars){
  smb &lt;- smbinning.factor(df = trainingData ,y = "ABOVE50K_y",x = factor_var)
  if(class(smb) != "character"){
    iv_df[iv_df$VARS == factor_var,"IV"] &lt;- smb$iv
  }
}
# 計算連續變數的IV值
for(continuous_var in continuous_vars){
  smb &lt;- smbinning(df = trainingData,y = "ABOVE50K_y",x = continuous_var)
  if(class(smb) != "character"){
    iv_df[iv_df$VARS == continuous_var,"IV"] &lt;- smb$iv
  }
}

# 將變數依據IV值高低排列
iv_df &lt;- iv_df[order(-iv_df$IV),]
iv_df

# VARS     IV
# 6    RELATIONSHIP 1.5435
# 3   MARITALSTATUS 1.3195
# 10            AGE 1.1815
# 12   EDUCATIONNUM 0.7169
# 14    CAPITALGAIN 0.6849
# 13   HOURSPERWEEK 0.4592
# 5  OCCUPATION_rep 0.3594
# 8             SEX 0.3127
# 1       WORKCLASS 0.1673
# 7            RACE 0.0634
# 2       EDUCATION 0.0000
# 4      OCCUPATION 0.0000
# 9   NATIVECOUNTRY 0.0000
# 11         FNLWGT 0.0000
# 15    CAPITALLOSS 0.0000</pre><p>關於IV Table的部分，我們可以得出以下資訊：</p>
<ol>
<li>根據經驗，IV&gt;=0.3即表示該預測變數與目標變數有較強的關係。</li>
<li>類別變數IV=0表示類別水準值過多。
<ul>
<li>EDUCATION: 可以使用IV值較高的EDUCATIONNUM代替。</li>
<li>NATIVECOUNTRY:如我們一開始在資料探勘時所見，絕大多數資料皆為United States，因此不予進一步討論。</li>
</ul>
</li>
<li>連續變數IV=0表示沒有顯著的切點存在。
<ul>
<li>FNLWGT：一開始為簡化分析，即不考慮納入模型分析。</li>
<li>CAPITALLOSS: 因為該特徵資料分佈極偏，故不納入進一步模型分析。</li>
</ul>
</li>
<li>從IV表數值，我們決定初步篩選出變數RELATIONSHIP, MARITALSTATUS, AGE, EDUCATIONNUM, CAPITALGAIN, OCCUPATION_rep進行模型分析。</li>
</ol>
<p>Logistic Regression Modeling <span style="color: #9f6ad4;">羅吉斯回歸建模 part2 </span>的部分請參考: <a href="/logistic_regression_part2/" target="_blank" rel="noopener noreferrer">羅吉斯回歸 &#8211; part2 模型建置、診斷與比較</a>。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<hr />
<p>更多模型建置筆記連結：</p>
<ol>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考:</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
	</channel>
</rss>
