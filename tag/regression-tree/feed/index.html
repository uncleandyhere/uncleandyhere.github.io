<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Regression Tree &#8211; 果醬珍珍•JamJam</title>
	<atom:link href="/tag/regression-tree/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>健忘女孩Jam的學習筆記和生活雜記</description>
	<lastBuildDate>Fri, 03 Jul 2020 02:25:53 +0000</lastBuildDate>
	<language>zh-TW</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Random Forests 隨機森林 &#124; randomForest, ranger, h2o &#124; R語言</title>
		<link>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/</link>
					<comments>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Tue, 19 Mar 2019 14:33:31 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[h2o]]></category>
		<category><![CDATA[random forests]]></category>
		<category><![CDATA[randomForest]]></category>
		<category><![CDATA[ranger]]></category>
		<category><![CDATA[Regression Tree]]></category>
		<category><![CDATA[隨機森林]]></category>
		<guid isPermaLink="false">/?p=2802</guid>

					<description><![CDATA[<p>Bagging法綜合多個樹模型結果，可以降低單一樹模型的高變異性並提升預測正確率。但Bagging法中樹與樹之間的相關性會降低模型整體的表現。隨機森林 Rand [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Bagging法</a>綜合多個樹模型結果，可以降低單一樹模型的高變異性並提升預測正確率。但Bagging法中樹與樹之間的相關性會降低模型整體的表現。隨機森林 Random forests 是Bagging修改後的版本，它是由「去相關性」的樹模型所組成的集成演算法，有很不錯的預測正確率且是一個受歡迎、開箱即用的演算法。</p>
<h3>載入所需套件</h3>
<p></p><pre class="crayon-plain-tag">library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # an extremely fast java-based platform
library(dplyr)
library(magrittr)</pre><p>準備資料。</p>
<ul>
<li>使用AmesHousing套件中的Ames Housing Data。</li>
<li>使用resample package中的initial_split()將資料切分成7:3(將參數設定為prop = 0.7)。</li>
<li>再分別用training()和testing()函數將切分好的資料萃取出。</li>
<li>並使用set.seed()來確保資料切分結果是可再現的。</li>
</ul>
<p></p><pre class="crayon-plain-tag"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split &lt;- initial_split(data = AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</pre><p></p>
<h3>Random Forests 概念介紹</h3>
<p>隨機森林模型的基礎概念和Decision Trees和Bagging一樣的(可以參考<a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">決策樹,Decision Trees</a>和<a href="/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/" target="_blank" rel="noopener noreferrer">Bagging</a>)。Bagging Trees模型在演算法中納入了隨機的元素，有效的降低了單一樹模型的高變異性與提升模型預測正確率。然而在bagging中的trees並非所有都是彼此相互獨立的，因為在每一棵樹切分節點時都是考慮所有原始的預測變數。也因爲上述關係，來自不同bootstrapped samples的樹彼此的結構都會有些類似（尤其是在樹的上半部，用來切割的前幾大變數都會非常類似）。</p>
<p>樹的結構相遇的這個特性就稱作tree correlation，它阻礙了Bagging最適地降低預測目標值的變異(variance)。為了更近一步降低變異，我們需要最小化樹與樹之間的相關性。這可以透過注入更多的隨機性到長樹的過程。 Random Forests是透過以下兩步驟來達成的：</p>
<ol>
<li>Bootstrap (拔靴法) : 跟Bagging很類似，每一顆樹都是建立自不同的bootstrapped sample，讓他們稍稍不一樣並稍稍去相關性。</li>
<li>Split-variable randomization (變數切割的隨機性) : 每一次在執行變數切割時，搜尋切割變數的範圍被限縮為隨機的子集合，即隨機挑選m個隸屬於總p個變數的子集合作為切割搜尋變數的範圍。對回歸樹來說，預設使用\(m=\frac{p}{3}\)，是個可經調教的參數。當\(m = p\)的時候，則跟只進行步驟1的結果一樣。</li>
</ol>
<p>因為每棵樹都是來自不同的隨機bootstrapped sample且每一次切割都是隨機挑選變數的子集合，因此樹與樹之間的關聯性會下降地較Bagging更低。</p>
<h4>OOB error vs. test set error</h4>
<p>和Bagging一樣，bootstrap resample法的一個天然好處，就是隨機森林模型可以透過out-of-bag(OOB)的樣本誤差來作為有效與合理近似實驗誤差(test error)。不需要額外產生或犧牲訓練資料集，OOB sample可供作為一個內建的驗證子集合。OOB sample 的存在讓尋找使模型錯誤率趨於穩定的最適樹模型數量(ntree)更有效率。但是OOB error和test error終究還是預期會不太相同。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2804" src="/wp-content/uploads/2019/03/Rplot.png" alt="random forests-隨機森林" width="800" height="500" srcset="/wp-content/uploads/2019/03/Rplot.png 800w, /wp-content/uploads/2019/03/Rplot-300x188.png 300w, /wp-content/uploads/2019/03/Rplot-768x480.png 768w, /wp-content/uploads/2019/03/Rplot-230x144.png 230w, /wp-content/uploads/2019/03/Rplot-350x219.png 350w, /wp-content/uploads/2019/03/Rplot-480x300.png 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>[上圖：Random forest out-of-bag error versus validation error.]</p>
<p>&nbsp;</p>
<p>此外，有很多package都沒有可以追蹤在某一棵樹模型中，哪些是OOB sample哪些不是的功能。這樣在比須比較多個模型的成效時，想要使用相同的驗證資料集來幫每個模型打分數是不可行的。而且，技術上雖然可以對OOB sample計算特定指標(metrics)如root mean squared logarithmic error (RMSLE)，但並非所有package都有內建這樣的運算功能。所以如果你想要比較多個模型的成效或是使用較不傳統的損失函數指標，你可能還是會選擇cross validation。</p>
<h4>Advantages &amp; Disadvantages of Random Forests</h4>
<h5>優點</h5>
<ol>
<li>隨機森林模型通常具有非常好的成效。</li>
<li>非常好的「開箱即用」的模型-不太需要調整什麼參數。</li>
<li>有內建的驗證資料集validation set &#8211; 不須為了額外驗證而犧牲資料。</li>
<li>不需前處理(pre-processing)。</li>
<li>對於離群值的處理是強大的。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>當運算大型資料時會變的非常慢。</li>
<li>雖然模型預測正確率高，但通常無法跟更先進的boosting演算法相比。</li>
<li>較不易解釋。</li>
</ol>
<h3>基本實作</h3>
<p>在R中有超過20種的Random Forests Packages。以下會使用歷史最悠久且最受歡迎randomForest套件來說明示範基本的Random Forests模型實作。但必須注意，當你的資料集變得很大的時候，randomForest回無法很好的擴展到大型資料（即使使用foreach進行平行運算）。此外，為了探索和比較不同模型參數的效果，我們也可找到更多有效的套件。因此，在模型tuning階段，我們會說明如何使用ranger和h2o package來進行更有效率的Random Forests modeling。</p>
<p>randomForest::randomForest可以使用formula或x,y matrix表示的方式來指定模型資料。我們以下以formula的方式來指定模型設定並使用randomForest模型預設參數。</p><pre class="crayon-plain-tag"># for reproduciblity
set.seed(123)

# default RF model
m1 &lt;- randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train
)

m1</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 661089658
##                     % Var explained: 89.8</pre><p>從m1結果來看：</p>
<ul>
<li>randomForest預設會使用500棵樹。</li>
<li>每一次切割(each split)會隨機篩選出\(\frac{Features}{3}=26\)個預測變數作為base。(原始data扣除目標變數Sale_Price的number of features = 80)</li>
<li>m1$mse(regression only)代表的是由OOB sample所計算出的「平均誤差平方(mean squared erre)」向量，即殘差平方和(sum of squared residuals)除上樹的個數(n)。</li>
<li>綜合500棵樹的mse(OOB error)為m1$mse[500] = 6.6108966 × 10<sup>8</sup>。</li>
</ul>
<p>我們可以進一步將不同棵樹所組成的隨機森林模型(ntree)對應的平均誤差(MSE)給繪出：</p><pre class="crayon-plain-tag">plot(m1)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-4-1-3.png" alt="random forests-隨機森林" /></p>
<p>從上圖可以發現，模型平均誤差大概在100棵樹時開始趨於穩定，且平均誤差值降低的速度開始於300棵樹左右時開始變緩。</p>
<p>我們可以進一步找出使得MSE最小的樹的個數。</p><pre class="crayon-plain-tag">which.min(m1$mse)</pre><p></p><pre class="crayon-plain-tag">## [1] 447</pre><p>最適隨機森林的RMSE則為(平均Sale_Price的誤差值)：</p><pre class="crayon-plain-tag">sqrt(m1$mse[which.min(m1$mse)])</pre><p></p><pre class="crayon-plain-tag">## [1] 25648.78</pre><p>如果我們不想要OOB error，randomForest函數亦提供驗證資料集(validation set)來幫助我們計算模型預測正確率。</p>
<p>要計算驗證誤差，首先我們進一步將訓練資料集依據8:2的比例切成訓練和驗證資料集，並分別使用analysis()和assessment()函數萃取出訓練和驗證資料。</p><pre class="crayon-plain-tag"># create training and validation data 
set.seed(123)
valid_split &lt;- initial_split(ames_train, .8)

# training data
ames_train_v2 &lt;- analysis(valid_split)

# validation datas
ames_valid &lt;- assessment(valid_split)

# 將驗證資料整理成x_test和y_test
x_test &lt;- ames_valid[setdiff(names(ames_valid),"Sale_Price")]
y_test &lt;- ames_valid$Sale_Price

# 在randomForest函數中使用x-test和y-test當作驗證資料集的參數
rf_oob_comp &lt;- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train_v2,
  xtest = x_test,
  ytest = y_test
)

rf_oob_comp</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train_v2,      xtest = x_test, ytest = y_test) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 667486651
##                     % Var explained: 89.17
##                        Test set MSE: 841004412
##                     % Var explained: 89.16</pre><p>可以看到模型結果多了Test set MSE，和原始的OOB MES不太一樣。我們將不同顆樹組成的模型所對應的OOB error和test error萃取出來並畫出誤差隨著樹的數量的變化圖，並比較兩者的差距。</p><pre class="crayon-plain-tag"># extract OOB &amp; validation errors
oob &lt;- sqrt(rf_oob_comp$mse)
validation &lt;- sqrt(rf_oob_comp$test$mse)

data.frame(
  ntrees = 1:rf_oob_comp$ntree,
  OOB.error = oob,
  Test.error = validation
) %&gt;% 
  gather(key = metric, value = RMSE, 2:3) %&gt;% 
  ggplot(aes(x = ntrees, y = RMSE, color = metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-8-1-3.png" alt="random forests-隨機森林" /></p>
<p>Random Forests是其中一個很好的「開箱即用」的演算法之一。基本上不需要調整什麼參數(tuning)，模型的預測能力就可ㄧ有很好的成效。</p>
<p>舉例來說，從上圖中，我們不需要調整參數就可以得到小於$30K的RMSE，比完整tuning後的<a href="/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/" target="_blank" rel="noopener noreferrer">Bagging模</a>RMSE降低超過$6K；比完整tuning的<a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">elastic net模型</a>RMSE降低超過$2K(沒有將目標變數取log的elastic net model版本請參考以下)。而我們還可以進一步透過參數調整tuning將Random Forests模型的預測正確性優化。</p><pre class="crayon-plain-tag"># elastic net
library(caret)
ames_train_x &lt;- model.matrix(object = Sale_Price ~ ., data =  ames_train)[, -1]
ames_train_y &lt;- ames_train$Sale_Price

# ames_test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1]
# ames_test_y &lt;- ames_test$Sale_Price

train_control &lt;- trainControl(method = "cv", number = 10)

caret_mod &lt;- train(
  x = ames_train_x, 
  y = ames_train_y,
  method = "glmnet", 
  prePro = c("center","scale","zv","nzv"),
  trControl = train_control,
  tuneLength = 10
)

min(caret_mod$results$RMSE)</pre><p></p><pre class="crayon-plain-tag">## [1] 32268.14</pre><p></p>
<h4>tuning</h4>
<p>Random Forests在tuning上非常簡單，因為只有幾個tuning parameters。通常tuning模型一開始最主要的考量點就是每一次分割所用來挑選的潛在變數名單，另外就是幾個需要注意的hyperparameters，包括如下：(這些hyperparameters在不同package的命名可能有所不同)</p>
<ul>
<li>ntree : number of trees。我們希望有足夠的樹來穩定模型的誤差，但過多的樹會是沒效率且沒必要的，特別是遇到大型資料集的時候。</li>
<li>mtry : 每次在決定切割變數時，所隨機抽樣的潛在變數清單數量。當mtry = p(即所有特徵變數數量)，Random Forests的結果就會和bagging一樣。而當mtry = 1，會造就每一次split所使用的變數completely random，每個變數都有機會但會造成非常偏差的結果。</li>
<li>sampsize : 訓練每棵樹模型的樣本數大小。預設是使用63.25%訓練資料集的比例，因為這個是獨立觀察值出現在bootstrapped sample的期望機率值。較低的樣本數大小雖然會降低訓練時間，但可能會產生不必要的偏差。增加樣本數大小可以提升模型正確率，但有可能會產生overfitting(因為會增加模型變異性)。所以一般來說，我們校正此樣本大小參數時會使用60-80%的比例。</li>
<li>nodesize : 末梢(葉)節點最小觀察資料個數。用來控制樹模型的複雜度。小的葉節點大小允許更深更複雜的樹模型，大的葉節點大小則會產生叫淺的樹模型。這又是一種「偏差v.s.變異度(bias -variance)」的權衡，當樹長得越深時，模型變異性愈高(有過度配適的風險)，而當樹長得越潛時則會有較多偏差(有沒辦法完整捕捉資料中的模式跟關係)。</li>
<li>maxnode : 內部節點最大個數值。另一種控制模型複雜度的變數，內部節點樹越多則會長出更深的樹，內部節點越少則產生越淺的樹。</li>
</ul>
<h4>Initial tuning with randomForest</h4>
<p>如果一開始只針對mtry進行校正，可以使用randomForest::tuneRF來進行簡易快速的評估。tunRF會從指定的mtry值開始，並每次增加給定的間距，直到模型OOB error降低的幅度開始小於特定幅度為止。</p>
<p>比如說，以下想知道mtry從5開始，每間隔相加1.5，所得到到的OOB error，直到OOB error改善的不度不超過1%為止。</p>
<p>因為tuneRF需要使用x,y形式指定資料，因此首先我們先使用setdiff()函數將變數名稱依據目標變數與預測變數分開。</p>
<p>模型跑完後會自動將每個mtry值所對應的OOB error值繪出。我們可以發現，當mtry &gt; 22後，OOB開始不再下降，最適mtry水準值約與features/3 = 80/3 = 26差不多。</p><pre class="crayon-plain-tag"># 篩選出預測變數名稱
features &lt;- setdiff(x = names(ames_train), y = "Sale_Price")

# 固定不同mtry參數值的模型所使用的隨機OOB sample一樣的
set.seed(123)

m2 &lt;- tuneRF(x = ames_train[features], y = ames_train$Sale_Price, mtryStart = 5, ntreeTry = 500 ,stepFactor = 1.5, improve = 0.01, trace = FALSE)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-10-1-4.png" alt="random forests-隨機森林" /></p><pre class="crayon-plain-tag">plot(m2)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-10-2-4.png" alt="random forests-隨機森林" /></p>
<h4>Full grid search with ranger</h4>
<p>如果想要套用綜合mtry以及其他參數的hyperparameter組合，我們需要建立一個grid並使用loop迴圈的方式，去測試每一個hyperparameter的組合和模型的成效。但因為randomForest()函數無法有效的將運算擴展至大型數據運算，因此我們會使用以c++執行的ranger()函數來解決。</p>
<p>我們可先使用Sys.time()稍稍比較tuneRF和ranger執行一種隨機森林模型所需的時間。</p><pre class="crayon-plain-tag">system.time(
  ames_randomForest &lt;- randomForest(
    formula = Sale_Price ~ ., 
    data = ames_train, 
    ntree = 500, 
    mtry = floor(length(features)/3)
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
## 111.957   0.735 169.989</pre><p></p><pre class="crayon-plain-tag">system.time(
  ames_ranger &lt;- ranger(
    formula   = Sale_Price ~ ., 
    data      = ames_train, 
    num.trees = 500,
    mtry      = floor(length(features) / 3)
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  10.367   0.174   5.665</pre><p>由以上結果我們可以看到，同樣是執行一次隨機森林，ranger()所需的時間僅約6秒，而randomForests則需要169秒。</p>
<p>為了進行grid search，我們首先先建立一個hyperparameters的grid，由許多不同的mtry, minimum node size,和 sample size所組成。總共會有96種組合。</p><pre class="crayon-plain-tag"># hyperparameter grid search
hyper_grid &lt;- expand.grid(
  mtry = seq(20, 30, by = 2),
  node_size = seq(3, 9, by = 2),
  sample_size = c(0.55, 0.632, 0.7, 0.8),
  OOB_RMSE = 0
)

# total number of combinations
nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 96</pre><p>我們使用loop迴圈，一一帶入不同hyperparameters到ranger()函數中，並固定每一次randomForests的的森林樹木數量為500(因為從前面經驗我們知道500棵樹即足夠使OOB error趨於穩定並收斂)。另外每一次randomForests執行時，我們也固定隨機亂數種子，讓同樣sample_size參數值所對應的抽樣樣本可以相同，凸顯其他參數變化所帶來的效果。</p><pre class="crayon-plain-tag">for (i in 1:nrow(hyper_grid)) {
  # train model
  model &lt;- ranger(
    formula = Sale_Price ~ .,
    data = ames_train, 
    num.trees = 500, 
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i], 
    sample.fraction = hyper_grid$sample_size[i],
    seed = 123
  )

  # 並將每一此訓練模型的OOB RMSE萃取儲存
  hyper_grid$OOB_RMSE[i] &lt;- sqrt(model$prediction.error)
}

# 我們將結果依序OOB_RMSE由小至大排列，取模型成效前十名印出
hyper_grid %&gt;% 
  dplyr::arrange(OOB_RMSE) %&gt;% 
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    mtry node_size sample_size OOB_RMSE
## 1    20         5         0.8 25918.20
## 2    20         3         0.8 25963.96
## 3    28         3         0.8 25997.78
## 4    22         5         0.8 26041.05
## 5    22         3         0.8 26050.63
## 6    20         7         0.8 26061.72
## 7    26         3         0.8 26069.40
## 8    28         5         0.8 26069.83
## 9    26         7         0.8 26075.71
## 10   20         9         0.8 26091.08</pre><p>從前十名模型成效結果我們可以發現：</p>
<ul>
<li>OOB_RMSE大致落在26K左右。</li>
<li>最適mtry的值落在所有20~30範圍區間。表示mtry在此區間對於OOB_RMSE沒有太大影響。</li>
<li>最適最小節點觀察值數量大約落在3~5。</li>
<li>最適抽樣比例約為0.8。</li>
<li>表示抽樣比例高(~80%)和深度較長(葉節點觀測個數大小3~5)的隨機森林成效較好(OOB RMSE)。</li>
</ul>
<h5>調整變數型態</h5>
<p>雖然我們已知random forests對於原始類別型變數處理效果是不錯的，我們還是進一步來試試將類別變數重新編碼為dummy variables是否能提升random forests的預測表現。</p>
<p>以下，我們使用dummyVars()函數將類別變數重新編碼為虛擬變數。</p><pre class="crayon-plain-tag">to_dum &lt;- dummyVars(formula = ~., data = ames_train, fullRank = FALSE)</pre><p>原始類別預測變數(80)被轉換完後變成353個欄位。</p><pre class="crayon-plain-tag">ames_to_dum &lt;- predict(to_dum, newdata = ames_train) %&gt;% as.data.frame()</pre><p>將資料名稱變成ranger相容的名稱。</p><pre class="crayon-plain-tag">names(ames_to_dum) &lt;- make.names(names = names(ames_to_dum), allow_ = FALSE)</pre><p>建立hyperparameter grid。並將mtry的區間調整為更大範圍。並執行grid search。</p><pre class="crayon-plain-tag">hyper_grid_2 &lt;- expand.grid(
  mtry = seq(50, 200, by = 25),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE  = 0
)

for(i in 1:nrow(hyper_grid_2)){
  model &lt;- ranger(
    formula = Sale.Price ~.,
    data = ames_to_dum, 
    num.trees = 500, 
    mtry = hyper_grid_2$mtry[i],
    min.node.size = hyper_grid_2$node_size[i], 
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed = 123
  )

  hyper_grid_2$OOB_RMSE[i] &lt;- sqrt(model$prediction.error)
}</pre><p></p><pre class="crayon-plain-tag">hyper_grid_2 %&gt;% 
  dplyr::arrange(OOB_RMSE) %&gt;% 
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    mtry node_size sampe_size OOB_RMSE
## 1    50         3        0.8 26981.17
## 2    75         3        0.8 27000.85
## 3    75         5        0.8 27040.55
## 4    75         7        0.8 27086.80
## 5    50         5        0.8 27113.23
## 6   125         3        0.8 27128.26
## 7   100         3        0.8 27131.08
## 8   125         5        0.8 27136.93
## 9   125         3        0.7 27155.03
## 10  200         3        0.8 27171.37</pre><p>由結果可分發線</p>
<ul>
<li>OOB RMSE 落在27K左右，並沒有比類別變數重新編碼前的26K來得好。</li>
<li>將類別變數重新編碼成dummy variables是無法提升模型成效的。</li>
</ul>
<p>所以到目前為至，最適的random forests模型參數分別為mtry = 20, node_size = 5, sample_size = 0.8。我們重複執行100次這個參數設定的模型，來計算此模型的error rate的期望值大小。</p><pre class="crayon-plain-tag">OOB_RMSE &lt;- vector(mode = "numeric", length = 100)

for(i in 1:length(OOB_RMSE)){
  optimal_ranger &lt;- ranger(
    formula         = Sale_Price ~ ., 
    data            = ames_train, 
    num.trees       = 500,
    mtry            = 20,
    min.node.size   = 5,
    sample.fraction = .8,
    importance      = 'impurity'
  )

  OOB_RMSE[i] &lt;- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-19-1-3.png" alt="random forests-隨機森林" /></p>
<p>執行100次random forests的結果後，我們可以觀察到OOB RMSE的期望值約落在26000~26200區間。</p>
<p>另外，我們在在模型參數importance = &#8216;impurity&#8217;。這表示我們是依據節點不純度(node impurity)的改善幅度來衡量每個變數的重要性。變數的重要性是計算每一次使用不同變數切割結點後，總能使MSE下降的程度(跨多棵樹模型)，而那些無法透過該變數切割所降低的模型錯誤率，則被稱作node impurity。而能夠降低越多MSE的變數則被重要性越高。</p>
<p>因此，在每一次節點分割時，我們都會計算每個變數所造成的MSE下降程度，而累積下降MSE幅度最高者，則被認為是較為重要的變數。</p>
<p>我們將變數重要性結果繪出：</p><pre class="crayon-plain-tag">options(scipen = -1)
optimal_ranger$variable.importance %&gt;% 
  as.matrix() %&gt;% 
  as.data.frame() %&gt;% 
  add_rownames() %&gt;% 
  `colnames&lt;-`(c("varname","imp")) %&gt;%
  arrange(desc(imp)) %&gt;% 
  top_n(25,wt = imp) %&gt;% 
  ggplot(mapping = aes(x = reorder(varname, imp), y = imp)) +
  geom_col() +
  coord_flip() +
  ggtitle(label = "Top 25 important variables") +
  theme(
    axis.title = element_blank()
  )</pre><p></p><pre class="crayon-plain-tag">## Warning: Deprecated, use tibble::rownames_to_column() instead.</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-20-1-3.png" alt="random forests-隨機森林" /></p>
<p>由上圖我們可以看到前三名重要變數依序為：Overall_Qual, Gr_Liv_Area, Garage_Cars。</p>
<h4>Full grid search with H2O</h4>
<p>我們已經知道剛剛在執行ranger進行hyperparameter grid計算時，還花滿長一段時間的。雖然ranger在計算上是有效率的，<br />
但是當遇到大型的grid時，我們手寫的loop迴圈會變得非常沒有效率。</p>
<p>而這時候，h2o套件則是一個強大有效率的java-based介面，可以提供平行分布式運算方法。此外，h20還可以提供不同的&#8221;search path&#8221;。有別於一一執行每一種hyperparameter grid組合，h2o可允許不同的最適搜尋路徑來執行，直到模型成效改善達一定程度等search path。使得在tuning模型上更具效率。以下便來介紹如何使用h2o套件執行random forests。</p><pre class="crayon-plain-tag"># start up h2o
h2o.no_progress() # turn off progress bars when creating reports/tutorials)
h2o.init(max_mem_size = "4g")</pre><p></p><pre class="crayon-plain-tag">##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 days 3 hours 
##     H2O cluster timezone:       Asia/Taipei 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.22.1.1 
##     H2O cluster version age:    2 months and 19 days  
##     H2O cluster name:           H2O_started_from_R_peihsuan_qcx617 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   0.04 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        FALSE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20)</pre><p>首先我們先來使用完整grid search path (又叫做full cartesian)，即表示會逐一檢視所有我們所指派的參數組合(hyper_grid.h2o)。<br />
根據hyper_grid.h2o的參數組合，共會有4*3*2 = 24種組合。也因為這邊是採用cartisian法，所以也不會比上面的方法快多少。</p><pre class="crayon-plain-tag"># create feature names
y &lt;- "Sale_Price"
x &lt;- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o &lt;- as.h2o(ames_train)</pre><p></p><pre class="crayon-plain-tag"># 指派參數組合 hyperparameter grid
hyper_grid.h2o &lt;- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 25, by = 2),
  sample_rate = c(.70, .80)
)

# build grid search 
# 以「cartesian」法逐一執行每一個參數組合的隨機森林模型

# 測試24種組合所需的時間
system.time(
grid &lt;- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid",
  x = x, 
  y = y, 
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = list(strategy = "Cartesian")
  )
)</pre><p></p><pre class="crayon-plain-tag">##   user  system elapsed 
##  9.628   3.104 874.954</pre><p></p><pre class="crayon-plain-tag"># 蒐集結果並依照每一種參數組合模型的MSE誤差來排名
# collect the results and sort by our model performance metric of choice
grid_perf &lt;- h2o.getGrid(
  grid_id = "rf_grid", 
  sort_by = "mse", 
  decreasing = FALSE
  )</pre><p></p><pre class="crayon-plain-tag">print(grid_perf)</pre><p></p><pre class="crayon-plain-tag">## H2O Grid Details
## ================
## 
## Grid ID: rf_grid 
## Used hyper parameters: 
##   -  mtries 
##   -  ntrees 
##   -  sample_rate 
## Number of models: 91 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   mtries ntrees sample_rate        model_ids                 mse
## 1     20    400         0.8 rf_grid_model_85 6.073124636667156E8
## 2     26    300         0.8 rf_grid_model_82 6.119220802095695E8
## 3     20    300         0.8 rf_grid_model_79 6.148210280053709E8
## 4     26    500         0.7 rf_grid_model_70 6.154919117655025E8
## 5     26    400         0.8 rf_grid_model_88 6.155747553830479E8
## 
## ---
##    mtries ntrees sample_rate        model_ids                 mse
## 86     24    200        0.55  rf_grid_model_3 6.629919566473577E8
## 87     28    200         0.7 rf_grid_model_53 6.643770708728626E8
## 88     28    300        0.55 rf_grid_model_11 6.644256979260525E8
## 89     30    200        0.55  rf_grid_model_6 6.658494098992392E8
## 90     22    500        0.55 rf_grid_model_20 6.755417850891622E8
## 91     28    200        0.55  rf_grid_model_5 6.786043491214715E8</pre><p>然而我們從上述結果可以注意到，最好的模型的OOB RMSE只有2.464371 × 10<sup>4</sup> (&lt;25K)，比我們先前所校正的模型效果都還更好。<br />
這是由於h2o套件在參數包括「最小節點大小(minimum node size)」、「樹的深度(tree depth)」等都更「慷慨」，比如說h2o預設最小節點大小為1，而ranger和randomForest該參數則都預設為5。</p>
<h4>Random Discrete grid search with H2O</h4>
<p>當遇到參數變很多的情況下，額外增加一個參數將會巨幅拉大grid search執行時間。為了因應這樣的不變，h2o提供了一種叫做「RandomDiscrete」的grid search搜尋路徑，有別於「Cartisian」逐一執行所有組合，「RandomDiscrete」會隨機挑選參數組合，直到達到一定程度的改善幅度、或是超過一定的時間、或是已執行一定數目的模型數（或是以上三種情況的交叉組合）。雖然使用「RandomDiscrete」搜尋可能會錯過最佳的參數組合效果，但基本上他已經能夠調教出不錯的模型了。</p>
<p>比如說，以下範例的參數組同樣為24種，我們設計一個「RandomDiscrete」grid search，停止條件為達成以下任一條件：近10組模型的MSE相較於最佳模性的改善幅度未超過0.5%、執行時間超過600秒(30 min)。</p><pre class="crayon-plain-tag"># hyperparameter grid
hyper_grid.h2o &lt;- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 25, by = 2),
  sample_rate = c(.70, .80)
)

# random grid search criteria
search_criteria &lt;- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 30*60
  )


# build grid search 
system.time(
random_grid &lt;- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = x,
  y = y,
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )
)</pre><p></p><pre class="crayon-plain-tag">##  user  system elapsed 
##  6.721   1.877 713.918</pre><p></p><pre class="crayon-plain-tag"># collect the results and sort by our model performance metric of choice
grid_perf2 &lt;- h2o.getGrid(
  grid_id = "rf_grid2",
  sort_by = "mse",
  decreasing = FALSE
  )</pre><p></p><pre class="crayon-plain-tag">print(grid_perf2)</pre><p></p><pre class="crayon-plain-tag">H2O Grid Details
================

Grid ID: rf_grid2 
Used hyper parameters: 
  -  mtries 
  -  ntrees 
  -  sample_rate 
Number of models: 24 
Number of failed models: 0 

Hyper-Parameter Search Summary: ordered by increasing mse
  mtries ntrees sample_rate         model_ids                 mse
1     20    500         0.8 rf_grid2_model_19 5.996100879211967E8
2     22    500         0.8 rf_grid2_model_24  6.09073686599265E8
3     22    500         0.7  rf_grid2_model_4 6.096855932933546E8
4     24    400         0.8  rf_grid2_model_8 6.132880206896532E8
5     22    400         0.8 rf_grid2_model_13 6.151492899918578E8

---
   mtries ntrees sample_rate         model_ids                 mse
19     20    500         0.7 rf_grid2_model_12 6.347692770119294E8
20     22    300         0.7 rf_grid2_model_21 6.361713674445038E8
21     24    400         0.7  rf_grid2_model_6 6.431105576136292E8
22     20    400         0.7  rf_grid2_model_9   6.4353236575248E8
23     22    200         0.7  rf_grid2_model_5 6.477837564818393E8
24     24    200         0.8  rf_grid2_model_1  6.50234231493715E8</pre><p>檢視「RandomDiscrete」grid search結果我們發現，經「隨機」檢視24組參數組合後，最佳的模型MSE只有2.448694 × 10<sup>4</sup>  (v.s. Cartisian grid search找到的2.464371 × 10<sup>4</sup>已非常接近)。且random discrete法只花了約11分鐘(=713/60)的時間(v.s. complete search的15分鐘(=874 sec / 60)。</p>
<p>一旦找到了最佳模型，我們就可以將模型套用在hold-out test set測試資料上來計算最後的「驗證誤差 test error」。結果顯示驗證RMSE誤差為23K，比elastic nets(32K)和bagging(36K)法低了$10K左右。</p><pre class="crayon-plain-tag"># 根據Cartesian法中，選出MSE最低的model，
# Grab the model_id for the top model, chosen by validation error
best_model_id &lt;- grid_perf2@model_ids[[1]]
best_model &lt;- h2o.getModel(best_model_id)</pre><p></p><pre class="crayon-plain-tag"># Now let’s evaluate the model performance on a test set
ames_test.h2o &lt;- as.h2o(ames_test)</pre><p></p><pre class="crayon-plain-tag">best_model_perf &lt;- h2o.performance(model = best_model, newdata = ames_test.h2o)</pre><p></p><pre class="crayon-plain-tag"># RMSE of best model
h2o.mse(best_model_perf) %&gt;% sqrt()</pre><p></p><pre class="crayon-plain-tag">## [1] 23303.05</pre><p></p>
<h3>預測</h3>
<p>一但我們挑出了我們偏好的模型，就像之前一樣，可以使用predict()函數將模型套用在新的資料集上做預測。我們可以來比較所有模型類別的預測效果(randomForest, ranger, h2o)(隨然呈現的結果有稍稍的不一樣)。另外要注意的就是h2o模型使用資料的格式不太依樣。</p>
<p>randomForest</p><pre class="crayon-plain-tag"># randomForest
pred_randomForest &lt;- predict(ames_randomForest, ames_test)
head(pred_randomForest)</pre><p></p><pre class="crayon-plain-tag">##        1        2        3        4        5        6 
## 128454.9 155459.6 264329.4 382519.7 211966.4 214173.0</pre><p>ranger</p><pre class="crayon-plain-tag"># ranger
pred_ranger &lt;- predict(ames_ranger, ames_test)
head(pred_ranger$predictions)</pre><p></p><pre class="crayon-plain-tag">## [1] 128893.3 154095.1 270183.4 389106.2 222629.6 210352.8</pre><p>h2o</p><pre class="crayon-plain-tag"># h2o
pred_h2o &lt;- predict(best_model, ames_test.h2o)</pre><p></p><pre class="crayon-plain-tag">head(pred_h2o)</pre><p></p><pre class="crayon-plain-tag">##    predict
## 1 119430.6
## 2 152900.0
## 3 278208.3
## 4 283966.7
## 5 225166.7
## 6 200583.3</pre><p></p>
<h3>小結</h3>
<ol>
<li>random forest是一個非常強大的「開箱即用」的演算法。不用太多的參數調教就會有相當不錯的預測能力。</li>
<li>此外，random forest也是模型中對資料前處理要求最少的模型，不太需要做資料轉換，是許多解決預測問題時最快能應用的方法。</li>
<li>以bagging為基礎，並透過(1)每棵樹進行bootstrapped sample 與 (2)節點分割時隨機挑選變數清單來「去除樹模型間的相關性」，能有效提升模型的高變異性與提升預測精準度。</li>
</ol>
<hr />
<p>參考文章連結：</p>
<p><a href="http://uc-r.github.io/random_forests" target="_blank" rel="noopener noreferrer">隨機森林 random forest</a></p>
<p>更多Decision Tree相關的統計學習筆記：</p>
<p><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></p>
<p><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a></p>
<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></p>
<p><a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a></p>
<p>更多Regression相關統計學習筆記：</p>
<p><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></p>
<p><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 – 資料探勘與處理 | 統計 R語言</a></p>
<p><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 – 模型建置、診斷與比較 | R語言</a></p>
<p><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 – Ridge, Lasso, Elastic Net | R語言</a></p>
<p>更多Clustering集群分析統計學習筆記：</p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></p>
<p><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a></p>
<p>其他統計學習筆記：</p>
<p><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></p>
<p>這篇文章 <a rel="nofollow" href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>Regression Tree &#124; 迴歸樹, Bagging, Bootstrap Aggregation &#124; R語言</title>
		<link>/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/</link>
					<comments>/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Wed, 02 Jan 2019 14:06:06 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[Bagging]]></category>
		<category><![CDATA[Bootstrap Aggregation]]></category>
		<category><![CDATA[ensemble learning]]></category>
		<category><![CDATA[Regression Tree]]></category>
		<category><![CDATA[集成學習]]></category>
		<guid isPermaLink="false">/?p=2322</guid>

					<description><![CDATA[<p>有別於「分類」樹(classification tree)是用來找尋「最能區分標籤資料類別」的一系列變數，「迴歸」樹(regression tree)則是用來找 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>有別於「分類」樹(classification tree)是用來找尋「最能區分標籤資料類別」的一系列變數，「迴歸」樹(regression tree)則是用來找尋「最能區分目標連續變數相近度」的一系列變數。迴歸樹投入變數可以式任何資料型態(與分類樹一樣)，唯一差別是迴歸樹的目標變數是連續型變數。</p>
<p>不管是哪種型態的決策樹，單一決策樹模型的結果不穩定度高（high variance），預測能力也較弱，也因此我們多半會搭配使用bootstrap aggregating(or Bagging)(一種集成學習法ensemble learning)，綜合多顆決策樹的預測結果，降低單一決策樹的變異度或不穩定性，避免overfitting過度配適的問題。而諸多更複雜的決策數模型也是由此衍伸，如隨機森林(Random Forest)和(Gradient Boosting Machine)。</p>
<p>而此篇學習筆記將會實作Regression Tree以及bagging。</p>
<h3>Regression Tree</h3>
<h3>載入實作所需的套件</h3>
<p>其中決策樹模型會使用到的套件就是rpart(分類決策樹也是使用該套件)。</p><pre class="crayon-plain-tag">library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging</pre><p>而範例資料則是使用AmesHousing package中的Ames Housing數據。</p>
<p>將資料分成70%訓練集，30%測試集：</p><pre class="crayon-plain-tag">set.seed(123)
ames_split &lt;- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</pre><p></p>
<h3>決定切點 (Deciding on splits)</h3>
<ol>
<li>決策樹的分枝演算法會由上而下進行，貪心的切割出最完整的樹。而每一個分裂點的選取，都會去檢視每一個投入變數的值切分的結果，找出使得切分後兩群(R1, R2)的組內誤差平方和(SSE, sums of squares error)最小的變數和切點。目標函示如下：<br />
\[minimize\space \bigg\{SSE = \sum_{i \in R1}(y_{i} &#8211; c_{1})^2 + \sum_{i \in R2}(y_{i} &#8211; c_{2})^2 \bigg\}\]</li>
<li>分枝演算法會反覆在各個切分後的子群聚中執行，直到達到停止切割的條件。</li>
<li>而這一棵經過貪心分枝結果的決策樹，會長的非常大，複雜度高。即便可能可以很好的預測訓練資料集，但很可能已過度配適(overfit)，當將預測規則套用在新資料集，預測能力不穩定度高。</li>
</ol>
<h3>Cost complexity criterion</h3>
<p>為了優化決策樹在新資料上的預測能力穩定度，我們會需要在訓練結果的複雜度、深度與預測穩定度取得平衡。</p>
<p>要找到這樣的平衡，我們的方法就是先長出一棵完整和最複雜的樹，並加入樹的複雜度作為目標函式的懲罰因子(cost complexity parameter,\(\alpha\))，並以此為目標來修剪出最適的決策樹模型。加入懲罰因子限制是的目標函數如下：</p>
<p>\[minimize \bigg \{ SSE + \alpha |T| \bigg \}\]</p>
<p>其中，T代表的是決策樹的終端節點(Terminal nodes)數量，\(\alpha\)則為懲罰參數，cost complexity parameter。</p>
<p>在不同給定的懲罰參數\(\alpha\)值下，我們可以找出修剪後最小的一棵樹，使得使得上述加入懲罰限制式的懲罰誤差(penalized error)(\(SSE + \alpha |T|\))最小(the lowest penalized error)。(以上概念與regularized regression的懲罰限制式很像。)</p>
<p>懲罰參數越小，會傾向得到較大較複雜的修剪後決策樹，而懲罰參數越大，修剪後決策樹則會較小較精簡。</p>
<p>因此，上述加入懲罰的誤差目標函示可以確保說，樹繼續變複雜的前提就是SSE的下降幅度要高於複雜度的懲罰成本(cost complexity penalty)。</p>
<p>而要怎麼決定最適的懲罰參數(\(\alpha\))，我們會評估多組對應不同水準\(\alpha\)值的模型組合，並使用交叉驗證法來計算每一個\(\alpha\)值的交叉驗證誤差(cross-validated error, X-val error)，來尋找使得交叉驗證誤差最小的的最適\(\alpha\)值(optimal \(\alpha\))和最適的決策樹子集合(optimal subtree)。</p>
<p>「交叉驗證誤差(X-Val error)」：</p>
<ul>
<li>此處的「誤差」就是加入懲罰因子的「懲罰誤差」\(SSE + \alpha |T|\)，只是是透過k-fold交叉驗證所計算出來的，所以亦稱做「交叉驗證誤差(X-val error)」。</li>
<li>也因為此誤差是計算在<strong>測試資料集</strong>與真實值的誤差，故亦可稱作<strong>Predicted Residual sum of squares(PRESS)</strong>，跟<strong>SSE</strong>不太一樣，SSE是使用<strong>訓練資料</strong>本身來計算與真實資料的誤差，因此SSE通常會比PRESS來的小。</li>
</ul>
<h3>優缺點</h3>
<p>迴歸樹的優點包括：</p>
<ol>
<li>結果很好解釋。</li>
<li>可以快速產生預測規則（每一次分割只需找出最適的變數與切點，沒有太複雜的計算）。</li>
<li>可以從樹的長相與順序得知變數的重要性，越先被拿來切割的變數，該變數所能降低的SSE幅度越大。</li>
<li>如果模型預測途中遇到遺失值，雖然不能從上而下將該筆資料列分類，但我們可以在遇遺失值的節點處，由下而上取每個節點的平均值回溯估計該遺失值。</li>
<li>決策樹模型提供一個非線性的&#8221;jagged&#8221;(參差不齊的) response平面，所以他可以配適真實不是很平滑(smooth)的回歸平面。而如果真實回歸平面是平滑的，則片段的常數(piece-wise constant)可以任意逼近它。</li>
<li>存在許多快速、可靠的演算法來學習這些樹模型。</li>
</ol>
<p>而迴歸樹的幾個缺點包括：</p>
<ol>
<li>單一迴歸樹的變異數大，預測能力不穩定（使用訓練資料的子集合即能大大改變樹的末梢節點長相。）</li>
<li>也因為單一迴歸樹的變異數大，預測精準度也不太佳。</li>
</ol>
<h3>使用R來執行基本的迴歸樹演算法</h3>
<p>我們可以使用rpart()來執行迴歸樹演算法，並使用rpart.plot()來繪製模型結果。方式大致與分類樹相同，唯一差別在於method參數要設定為&#8221;anova&#8221;。</p><pre class="crayon-plain-tag">m1 &lt;- rpart(
  formula = Sale_Price ~ .,
  data    = ames_train,
  method  = "anova"
  )</pre><p>我們可以執行m1來看一下模型訓練結果，會詳細說明每一次切割所使用的變數和切點，以及切割前後的資料筆數變化。</p><pre class="crayon-plain-tag">m1</pre><p></p><pre class="crayon-plain-tag">## n= 2051 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 2051 13299200000000 181620.20  
##    2) Overall_Qual=Very_Poor,Poor,Fair,Below_Average,Average,Above_Average,Good 1699  4001092000000 156147.10  
##      4) Neighborhood=North_Ames,Old_Town,Edwards,Sawyer,Mitchell,Brookside,Iowa_DOT_and_Rail_Road,South_and_West_of_Iowa_State_University,Meadow_Village,Briardale,Northpark_Villa,Blueste 1000  1298629000000 131787.90  
##        8) Overall_Qual=Very_Poor,Poor,Fair,Below_Average 195   173369900000  98238.33 *
##        9) Overall_Qual=Average,Above_Average,Good 805   852605100000 139914.80  
##         18) First_Flr_SF&amp;lt; 1150.5 553   302338400000 129936.80 *
##         19) First_Flr_SF&amp;gt;=1150.5 252   374390700000 161810.90 *
##      5) Neighborhood=College_Creek,Somerset,Northridge_Heights,Gilbert,Northwest_Ames,Sawyer_West,Crawford,Timberland,Northridge,Stone_Brook,Clear_Creek,Bloomington_Heights,Veenker,Green_Hills 699  1260199000000 190995.90  
##       10) Gr_Liv_Area&amp;lt; 1477.5 300   247261100000 164045.20 *
##       11) Gr_Liv_Area&amp;gt;=1477.5 399   631199000000 211259.60  
##         22) Total_Bsmt_SF&amp;lt; 1004.5 232   164042700000 192946.30 *
##         23) Total_Bsmt_SF&amp;gt;=1004.5 167   281257000000 236700.80 *
##    3) Overall_Qual=Very_Good,Excellent,Very_Excellent 352  2874510000000 304571.10  
##      6) Overall_Qual=Very_Good 254   885511300000 273369.50  
##       12) Gr_Liv_Area&amp;lt; 1959.5 155   325667700000 247662.30 *
##       13) Gr_Liv_Area&amp;gt;=1959.5 99   297033800000 313618.30 *
##      7) Overall_Qual=Excellent,Very_Excellent 98  1100817000000 385440.30  
##       14) Gr_Liv_Area&amp;lt; 1990 42    78801640000 325358.30 *
##       15) Gr_Liv_Area&amp;gt;=1990 56   756691700000 430501.80  
##         30) Neighborhood=College_Creek,Edwards,Timberland,Veenker 8   115305100000 281887.50 *
##         31) Neighborhood=Old_Town,Somerset,Northridge_Heights,Northridge,Stone_Brook 48   435248600000 455270.80  
##           62) Total_Bsmt_SF&amp;lt; 1433 12    31430660000 360094.20 *
##           63) Total_Bsmt_SF&amp;gt;=1433 36   258880600000 486996.40 *</pre><p>我們可以將上述模型結果用rpart.plot()函數來進行視覺化呈現。該函數參數可以進階調整圖形視覺畫呈現方式，可參考<a href="http://www.milbo.org/rpart-plot/prp.pdf" target="_blank" rel="noopener noreferrer">這篇文件</a>。</p><pre class="crayon-plain-tag">rpart.plot(m1)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-76-1.png" alt="plot of chunk unnamed-chunk-76" /></p>
<p>從上圖可以觀察到：</p>
<ul>
<li>每一個節點的觀測值個數和目標變數平均值，顏色越深表示目標變數平均值越大。</li>
<li>該完整的決策樹共12個葉節點(leaf nodes or terminal nodes)以及11個內部節點(internal nodes)。</li>
</ul>
<p>此外，我們也可以使用plotcp()函數，看一下在rpart()修樹過程中，套用不同懲罰參數(\(\alpha\))值，使用預設10-fold cross validation (k=10)，計算hold-out資料與真實資料的誤差(懲罰誤差or交叉驗證誤差)變化圖。</p><pre class="crayon-plain-tag">plotcp(m1)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-77-1.png" alt="plot of chunk unnamed-chunk-77" /></p>
<p>由上圖可以發現：</p>
<ul>
<li>下x軸為cp or cost complexity parameter (\(\alpha\))，上x軸為末梢節點數(number of terminal nodes, |T|)，y軸為交叉驗證誤差(X-val relative error)。</li>
<li>隨著懲罰係數減少，X-val error降低幅度呈遞減趨勢，直到cp達到預設的0.01則停止。</li>
<li>而根據專家建議，通常可以接受使用<strong>與最小X-val error相距一個標準差以內</strong>所對應的Tree Size(|T|)來作為修樹的最佳大小<strong>（1-SE rule）</strong>。比如說上圖中，水平虛線通過minimum X-val error &#8211; 1 SE的誤差值約對應在cp = 0.015和T = 9，因此我們亦可以選擇T = 9。</li>
</ul>
<p>為了進一步說明為何要選擇T=12（或T=9 如果你採用 1 -SE rule)。我們可以將參數最小cp值設定為0，即沒有任何懲罰條件存在的狀況下，讓決策樹長到最大最完整。</p><pre class="crayon-plain-tag">m2 &lt;- rpart(
    formula = Sale_Price ~ .,
    data    = ames_train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

{plotcp(m2)
  abline(v = 12,lty = "dashed", col = "red")}</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-78-1.png" alt="plot of chunk unnamed-chunk-78" /></p>
<p>由上圖我們可以發現，當T&gt;12後，即使樹繼續長大，但交叉驗證誤差下降幅度遞減，因此我們可以果斷將樹修剪到最精簡T=12且誤差也是最小的程度。</p>
<p>我們也可以看一下詳細的cp值和對應的X-Val error。rpart()預設的懲罰參數為cp=0.1，並執行一連串由大到小的\(\alpha\)值來計算誤差，直到cp=0.1所對應最小樹的大小為12(or 11 splits)，最小交叉驗證誤差為0.262(xerror)。</p><pre class="crayon-plain-tag">m1$cptable</pre><p></p><pre class="crayon-plain-tag">##            CP nsplit rel error    xerror       xstd
## 1  0.48300624      0 1.0000000 1.0017486 0.05769371
## 2  0.10844747      1 0.5169938 0.5189120 0.02898242
## 3  0.06678458      2 0.4085463 0.4126655 0.02832854
## 4  0.02870391      3 0.3417617 0.3608270 0.02123062
## 5  0.02050153      4 0.3130578 0.3325157 0.02091087
## 6  0.01995037      5 0.2925563 0.3228913 0.02127370
## 7  0.01976132      6 0.2726059 0.3175645 0.02115401
## 8  0.01550003      7 0.2528446 0.3096765 0.02117779
## 9  0.01397824      8 0.2373446 0.2857729 0.01902451
## 10 0.01322455      9 0.2233663 0.2833382 0.01936841
## 11 0.01089820     10 0.2101418 0.2687777 0.01917474
## 12 0.01000000     11 0.1992436 0.2621273 0.01957837</pre><p></p>
<h3>Tuning 修樹</h3>
<p>除了使用懲罰參數cost complexity parameter (\(\alpha\))來限制樹的大小，我們也常透過以下參數來修樹：</p>
<ul>
<li>minsplit : 分裂前至少/最小所需的資料筆數。預設為20筆。如果將此數值調小，可讓末梢節點(terminal nodes)即使僅有少數一些資料，也可以產生對應的預測值。</li>
<li>maxdepth : 從根節點(root nodes)到葉節點(terminal nodes)間的最大內部節點數量上限。預設為30，可以長出還滿大一棵樹。</li>
</ul>
<p>rpart()函數中，是使用control這個參數來設定一系列的參數水準值(hyperparameter setting)。<br />
比如說，我們想要建立一顆minsplit = 10和maxdepth = 12的決策樹模型。可以透過以下方式：</p><pre class="crayon-plain-tag">m3 &lt;-
  rpart(formula = Sale_Price ~ .,
        data = ames_train,
        method = "anova",
        control = list(minsplit = 10, maxdepth = 12, xval = 10))

m3$cptable</pre><p></p><pre class="crayon-plain-tag">##            CP nsplit rel error    xerror       xstd
## 1  0.48300624      0 1.0000000 1.0007911 0.05768347
## 2  0.10844747      1 0.5169938 0.5192042 0.02900726
## 3  0.06678458      2 0.4085463 0.4140423 0.02835387
## 4  0.02870391      3 0.3417617 0.3556013 0.02106960
## 5  0.02050153      4 0.3130578 0.3251197 0.02071312
## 6  0.01995037      5 0.2925563 0.3151983 0.02095032
## 7  0.01976132      6 0.2726059 0.3106164 0.02101621
## 8  0.01550003      7 0.2528446 0.2913458 0.01983930
## 9  0.01397824      8 0.2373446 0.2750055 0.01725564
## 10 0.01322455      9 0.2233663 0.2677136 0.01714828
## 11 0.01089820     10 0.2101418 0.2506827 0.01561141
## 12 0.01000000     11 0.1992436 0.2480154 0.01583340</pre><p>雖然這個參數設定的方法挺管用，但如果想要評估不同參數水準組合的模型效果，手動調整就不具效率，此時，可以使用grid search的方法，來自動執行與比較不同參數水準值組合的效果並依據此來選擇最適合的模型參數設定。</p>
<p>執行grid search之前，我們先建立一個hyperparameter grid。舉例來說，想測試minsplit落在5-20區間和maxdepth落在8~15之間的參數組合(因為原始模型顯示最是模型節點數目為12)。這樣一來總共就會有16*8=128種參數組合的模型。</p><pre class="crayon-plain-tag"># 建立grid
hyper_grid &lt;- expand.grid(
  minsplit = seq(5,20,1),
  maxdepth = seq(8,15,1)
)
head(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">##   minsplit maxdepth
## 1        5        8
## 2        6        8
## 3        7        8
## 4        8        8
## 5        9        8
## 6       10        8</pre><p>來看grid中的總共組合數。</p><pre class="crayon-plain-tag">nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 128</pre><p>接著使用loop迴圈來一一建立這128種不同參數組合的模型。</p><pre class="crayon-plain-tag">models &lt;- list()

for(i in 1:nrow(hyper_grid)){
  minsplit &lt;- hyper_grid$minsplit[i]
  maxdepth &lt;- hyper_grid$maxdepth[i]

  models[[i]] &lt;- rpart(
    formula = Sale_Price ~., 
    data = ames_train, 
    method = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
  )
}</pre><p>接著，我們在撰寫一個函式來將每一組模型的最小交叉驗證誤差（x-val error)和對應的\(\alpha\)值取出，加到grid的資料集中。</p><pre class="crayon-plain-tag"># create the function to extract minimum error and associated alpha
# get CP
get_cp &lt;- function(x){
  min &lt;- which.min(x$cptable[,"xerror"]) # 回傳發生最小xerror的列index
  cp &lt;- x$cptable[min, "CP"]
}

get_min_error &lt;- function(x){
  min &lt;- which.min(x$cptable[,"xerror"])
  xerror &lt;- x$cptable[min, "xerror"]
}

# 新增每組cp產生模型的cp和minXerror兩個欄位，並依照交叉驗證誤差由小到大排列，取前五個組合
hyper_grid %&gt;% 
  mutate(
    cp = purrr::map_dbl(models,get_cp),
    error = purrr::map_dbl(models,get_min_error)
  ) %&gt;% 
  arrange(error) %&gt;% 
  top_n(-5, wt = error)</pre><p></p><pre class="crayon-plain-tag">##   minsplit maxdepth        cp     error
## 1        5       13 0.0108982 0.2421256
## 2        6        8 0.0100000 0.2453631
## 3       12       10 0.0100000 0.2454067
## 4        8       13 0.0100000 0.2459588
## 5       19        9 0.0100000 0.2460173</pre><p>可發現hyperparameter參數組選出的最適模型的交叉驗證誤差較原始模型改善了一些(m1 的最小交叉驗證誤差 0.262 v.s. 最適模型的 0.242)。</p>
<p>我們將這組得到最小交叉誤差的模型套用在test data進行預測。</p><pre class="crayon-plain-tag">optimal_tree &lt;- rpart(
  formula = Sale_Price ~ .,
  data = ames_train,
  method = "anova",
  control = list(minsplit = 5, maxdepth = 13, cp = 0.01)
)

pred &lt;- predict(optimal_tree, newdata = ames_test)
RMSE(pred = pred, obs = ames_test$Sale_Price)</pre><p></p><pre class="crayon-plain-tag">## [1] 39145.39</pre><p>我們預測的Root-Mean-Squared-Error 平均誤差平方和開根號(均方根誤差)為39145。</p>
<h3>Bagging</h3>
<h4>Bagging概念介紹</h4>
<p>就像之前提到的，單一棵決策樹有變異度高(high variance)的問題。雖然修剪樹規則可以降低變異度，但其實有更有效的方法來大副降低變異度並提升決策樹的預測效果，其中一個方法就是Boostrap Aggregation (或稱Bagging)(概念來自於 <a href="https://link.springer.com/article/10.1023%2FA%3A1018054314350" target="_blank" rel="noopener noreferrer">Breiman, 1996</a>)。</p>
<p>Bagging整合和平均多組模型的預測結果。透過平均每組模型的預測結果，<br />
可以有效降低來自單一模型的變異度，並且避免過度配適overfitting。基本上Bagging可以分為三個基本步驟：</p>
<ol>
<li>從訓練資料集(training data)中產生m組 bootstrap samples。Bootstrapped sample可以讓我們創造出有些差異的資料集，且這些資料集都是與原始訓練資料擁有相同分佈的。</li>
<li>對每一個bootstrapped sample訓練一組單一且未修剪的決策樹。</li>
<li>將每一組模型的結果平均，產生一個整體平均的預測值。</li>
</ol>
<p><img loading="lazy" class="alignnone size-full wp-image-2333" src="/wp-content/uploads/2019/01/Untitled-presentation.jpg" alt="Regression Tree, Bagging" width="960" height="720" srcset="/wp-content/uploads/2019/01/Untitled-presentation.jpg 960w, /wp-content/uploads/2019/01/Untitled-presentation-300x225.jpg 300w, /wp-content/uploads/2019/01/Untitled-presentation-768x576.jpg 768w, /wp-content/uploads/2019/01/Untitled-presentation-830x623.jpg 830w, /wp-content/uploads/2019/01/Untitled-presentation-230x173.jpg 230w, /wp-content/uploads/2019/01/Untitled-presentation-350x263.jpg 350w, /wp-content/uploads/2019/01/Untitled-presentation-480x360.jpg 480w" sizes="(max-width: 960px) 100vw, 960px" /></p>
<p>Bagging的概念可以套用在任何回歸和分類模型上；然而Bagging主要還是對於具有高變異度的模型較有效果。比如說，較為穩定的有母數模型(parametric model)如線性回歸和multi-adaptive regression splines模型，使用Bagging所能改善的預測能力幅度都較小。</p>
<p>Bagging的一個好處為，平均來說，一個Bootstrap sample會包含63%(2/3)的訓練資料集，剩餘約33%(1/3)的訓練資料不再bootstrapped sample內。我們稱這一包不在bootstrapped sample內的資料為out-of-bag (OOB) sample。我們可以透過這個OOB sample來衡量模型的準確度，產生一個自然而然的交叉驗證過程。</p>
<h4>Bagging with ipred</h4>
<p>配飾bagged tree model是簡單的。我們改使用ipred::bagging來建立模型。並將參數coob設定為TRUE來表示我們將使用OOB sample來估計模型錯誤率。</p><pre class="crayon-plain-tag"># make bootstrapping reproducible
set.seed(123)

bagged_m1 &lt;- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  coob = TRUE
)

bagged_m1</pre><p></p><pre class="crayon-plain-tag">## 
## Bagging regression trees with 25 bootstrap replications 
## 
## Call: bagging.data.frame(formula = Sale_Price ~ ., data = ames_train, 
##     coob = TRUE)
## 
## Out-of-bag estimate of root mean squared error:  36543.37</pre><p>我們可以發現，經由bagged後的決策數模型錯誤率將近下降了快2602(from 39145 to 36543)。</p>
<p>需要注意的一點就是，一般來說，越多樹模型效果越好。當我們加入更多樹模型，就可以平均更多高變異度的單一樹模型。隨著模型的數量增加，我們可以得到大幅降低的變異度（以及錯誤率），並且直到某一數量水準值為止，變異度下降的幅度開始趨緩收斂，即可得到建立穩定模型所需的最適的樹數量。從經驗上來說，通常不太會用到超過50棵樹來穩定模型的誤差。</p>
<p>bagging預設會產生25組bootstrap sample和樹模型，但有時候我們可能需要更多顆樹模型。我們可以觀察樹的多寡和誤差的變化。我們觀察10~50棵樹在穩定模型誤差的效果變化。</p><pre class="crayon-plain-tag"># assess 10-50 bagged trees
ntree &lt;- 10:50

# create empty vector to store OOB RMSE values
rmse &lt;- vector(mode = "numeric",length = length(ntree))

for(i in seq_along(ntree)){
  # reproducibility: 固定bootstrap亂數結果
  set.seed(123)

  # perform bagged model
  model &lt;- bagging(
    formula = Sale_Price ~.,
    data = ames_train,
    coob = TRUE,
    nbagg = ntree[i]
  )

  # get OOB error
  rmse[i] &lt;- model$err
}

# 將不同樹數量與誤差的圖繪出
{plot(x = ntree, y = rmse,type = "l",lwd = 2)
abline(v = 25, col = "red", lty = "dashed")}</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-87-1.png" alt="plot of chunk unnamed-chunk-87" /></p>
<p>我們可以發現差不多在樹數量為25時誤差水準趨於穩定。所以如果單純僅加入更多樹不太能在優化模型的錯誤率。</p>
<h4>Bagging with caret</h4>
<p>使用ipred::bagging來進行bagging是簡單的，但使用caret來進行bagging會有更多好處如下：</p>
<ol>
<li>他可以更簡單的進行cross validation。雖然我們可以使用OOB error，但使用cross validation可以提供強大的真實test error的誤差期望值。</li>
<li>我們可以跨多個bagged trees來衡量變數的重要性。</li>
</ol>
<p>我們來建立一個10-fold cross validation的模型。</p><pre class="crayon-plain-tag"># Specify 10-fold cross validation
ctrl &lt;- trainControl(method = "cv", number = 10)

# CV bagged model
bagged_cv &lt;- train(
  Sale_Price ~.,
  data = ames_train,
  method = "treebag", 
  trControl = ctrl,
  importance = TRUE
)


# assess result
bagged_cv</pre><p></p><pre class="crayon-plain-tag">## Bagged CART 
## 
## 2051 samples
##   80 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 1846, 1845, 1847, 1845, 1846, 1847, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   36477.25  0.8001783  24059.85</pre><p>我們看到，交叉驗證的RMSE為36477。</p>
<p>我們亦可以看前20名重要變數分別為哪些。</p><pre class="crayon-plain-tag"># plot most important variables
plot(varImp(bagged_cv),20)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-89-1.png" alt="plot of chunk unnamed-chunk-89" /></p>
<p>在回歸模型中，衡量變數的重要性為根據該變數切割(split)所能讓總SSE減少的量，並綜合平均m顆樹的效果。有最大影響SSE平均下降幅度的變數會被認為是最重要的變數。而上圖中重要性的值(importance value)則是每個變數平均使SSE下降程度相較於最重要變數的相對百分比(值的區間為0-100)。</p>
<p>同時，我們亦來比較將此模型套用在測試資料集的錯誤率(v.s. 交叉驗證錯誤率)。</p><pre class="crayon-plain-tag">pred &lt;- predict(object = bagged_cv,newdata = ames_test)
RMSE(pred = pred,obs = ames_test$Sale_Price)</pre><p></p><pre class="crayon-plain-tag">## [1] 35262.59</pre><p>可以發現cross validation(36477)和套用在test set(out of sample)的估計錯誤率(estimated error)(35263)是非常相近的。</p>
<p>而在之後的學習筆記中，會看從bagging概念所延伸的模型(如隨機森林Random Forest和GBMs)可以如何更好的改善模型錯誤率。</p>
<h3>小結</h3>
<ol>
<li>決策樹是一個非常直覺的模型演算法，有許多好處，但卻有高變異度(high variance)的問題。雖然可以透過更改修剪樹的規則(如cp,minsplit,maxdepth)來改善誤差和過度配飾(overfitting)，但程度有限。</li>
<li>能有效解決高變異度的問題可以透過Bagging(Bootstrap Aggregation)法，綜合和平均多棵樹模型的預測值，降低單一棵樹模型所產生的高變異度，並提升模型預側能力（降低錯誤率）。隨著樹的數量的增加，Bagging模型錯誤率會在某一最適值下趨於收斂與穩定。</li>
<li>評估Bagging模型錯誤率常用的方法可以是簡單的OOB error或是更強大的cross-validation error，兩者分別可以透過inpred::bagging和caret::train來得到。而caret::train 另外的好處就是可以計算出跨bagged tree所計算出來的變數重要性。</li>
<li>Bagging的概念可以套用在任何回歸或分類演算法上，但對於本身就非常穩定的有母數模型如linear regression等效果就沒有這麼大。</li>
<li>Bagging概念也是更複雜模強大模型如隨機森林(Random Forest)和Gradient Boosting Machines (GBMs)的延伸。</li>
</ol>
<hr />
<p>更多統計模型學習筆記：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考連結：</p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
<li><a href="http://uc-r.github.io/regression_trees" target="_blank" rel="noopener noreferrer">Regression Trees</a></li>
<li><a href="https://rpubs.com/skydome20/R-Note16-Ensemble_Learning" target="_blank" rel="noopener noreferrer">R筆記 – (16) Ensemble Learning(集成學習)</a></li>
<li><a href="http://www.milbo.org/rpart-plot/prp.pdf" target="_blank" rel="noopener noreferrer">Plotting rpart trees with the rpart.plot package</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
	</channel>
</rss>
