<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>regression &#8211; 果醬珍珍•JamJam</title>
	<atom:link href="/tag/regression/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>健忘女孩Jam的學習筆記和生活雜記</description>
	<lastBuildDate>Fri, 03 Jul 2020 02:25:08 +0000</lastBuildDate>
	<language>zh-TW</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Regularized Regression &#124; 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net &#124; R語言</title>
		<link>/regularized-regression-ridge-lasso-elastic/</link>
					<comments>/regularized-regression-ridge-lasso-elastic/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Fri, 04 Jan 2019 05:38:32 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[elastic net]]></category>
		<category><![CDATA[general linear regression]]></category>
		<category><![CDATA[lasso]]></category>
		<category><![CDATA[multicollinearity]]></category>
		<category><![CDATA[overfitting]]></category>
		<category><![CDATA[regression]]></category>
		<category><![CDATA[regularization]]></category>
		<category><![CDATA[regularized regression]]></category>
		<category><![CDATA[ridge]]></category>
		<guid isPermaLink="false">/?p=2359</guid>

					<description><![CDATA[<p>在線性回歸模型中，為了最佳化目標函式(最小化誤差平方和)，資料需符合許多假設，才能得到不偏回歸係數，使得模型變異量最低。可現實中數據非常可能有多個特徵變數，使得 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/regularized-regression-ridge-lasso-elastic/">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>在線性回歸模型中，為了最佳化目標函式(最小化誤差平方和)，資料需符合許多假設，才能得到不偏回歸係數，使得模型變異量最低。可現實中數據非常可能有多個特徵變數，使得模型假設不成立而產生過度配適問題，這時則需透過正規化法(regularized regression)來控制回歸係數，藉此降低模型變異以及樣本外誤差。</p>
<h3>Regularized Regression</h3>
<h3>載入實作所需的套件</h3>
<p></p><pre class="crayon-plain-tag">library(rsample)  # data splitting 
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting</pre><p>資料準備：Data 使用 AmesHousing package中的 Ames Housing data</p><pre class="crayon-plain-tag"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split &lt;- initial_split(data = AmesHousing::make_ames(),prop = 0.7, strata = "Sale_Price")
ames_train &lt;- training(ames_split)
ames_test &lt;- testing(ames_split)</pre><p></p>
<h3>為何需要資料正規化(Regularization)?</h3>
<p>我們知道，OLS(Ordinary Least Squares)最小平方和線性回歸的最佳化目標函式就是尋找一個平面，使得預測與實際值的誤差平方和(Sum of Squared Error, SSE)最小化（如下圖，紫色點代表實際觀測值，粉紅色為預測平面，黑色實線則表示實際與預測值得殘差）。</p>
<p><img src="/wp-content/uploads/2019/01/unnamed-chunk-170-1.png" alt="Regularized Regression" /></p>
<p>線性回歸的目標函式：</p>
<p>\[minimize\bigg \{ SSE = \sum_{i=1}^n (y_{i} &#8211; \hat{y}_{i})^2 \bigg \}\]</p>
<p>而要最佳化目標函式，資料必須符合以下幾個基本假設：</p>
<ul>
<li>線性關係</li>
<li>變數成常態分配</li>
<li>變數間無自相關</li>
<li>殘差變異同質性</li>
<li>觀測值個數(n)需大於特徵個數(p)(n&gt;p)</li>
<li>模型不能有共線性問題(否則估計回歸係數會有問題)</li>
</ul>
<p>但現實中，數據往往存在許多特徵變數(p很大)，隨著特徵變數量增加，許多基本假設不再成立，以至於我們必須用替代方法來解決線性預測問題。具體來說，當特徵變數量增加(p增加)，我們常會遇到的三個主要問題包括：</p>
<h4>1. Multicollinearity 多元共線性</h4>
<p>當特徵變數個數增加(p增加)，我們就有越高的機會捕捉到存在共線性的變數。而當模型存在共線性時，回歸係數項就會變得非常不穩定(high variance, 高變異)。<br />
舉例來說，我們先從多達81個特徵變數中:</p>
<ol>
<li>找出相關係數絕對值高於0.6的變數組合。</li>
<li>找出哪些變數與自變數(Sale_Price)具有高度相關性(相關係數絕對值大於 0.6)。</li>
</ol>
<p>首先我們先計算出每對變數間的相關係數和P-value的data frame</p><pre class="crayon-plain-tag">library(Hmisc) # rcorr()函數
data &lt;- as.data.frame(AmesHousing::make_ames())
# 使用rcorr()產生Matrix of correlations and P-values
res2 &lt;- rcorr(as.matrix(data[,sapply(data, is.numeric)]))

# 將相關係數與p-value矩陣轉換成data frame的函數
flattenCorrMatrix &lt;- function(cormat, pmat) {
  ut &lt;- upper.tri(cormat) # Lower and Upper Triangular Part of a Matrix
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

cor_table &lt;- flattenCorrMatrix(res2$r, res2$P)
head(cor_table)</pre><p></p><pre class="crayon-plain-tag">##            row         column        cor                     p
## 1 Lot_Frontage       Lot_Area 0.13686214 0.0000000000001005862
## 2 Lot_Frontage     Year_Built 0.02613050 0.1573419061953282849
## 3     Lot_Area     Year_Built 0.02325850 0.2081737048985332628
## 4 Lot_Frontage Year_Remod_Add 0.06950923 0.0001662509781792387
## 5     Lot_Area Year_Remod_Add 0.02168222 0.2406818444157394765
## 6   Year_Built Year_Remod_Add 0.61209525 0.0000000000000000000</pre><p>列出相關係數絕對值高於0.6的變數組合</p><pre class="crayon-plain-tag">cor_table %&gt;% filter(abs(cor) &gt; 0.6) %&gt;% arrange(desc(cor))</pre><p></p><pre class="crayon-plain-tag">##              row         column       cor p
## 1    Garage_Cars    Garage_Area 0.8898660 0
## 2    Gr_Liv_Area  TotRms_AbvGrd 0.8077721 0
## 3  Total_Bsmt_SF   First_Flr_SF 0.8004287 0
## 4    Gr_Liv_Area     Sale_Price 0.7067799 0
## 5  Bedroom_AbvGr  TotRms_AbvGrd 0.6726472 0
## 6  Second_Flr_SF    Gr_Liv_Area 0.6552512 0
## 7    Garage_Cars     Sale_Price 0.6475616 0
## 8    Garage_Area     Sale_Price 0.6401383 0
## 9  Total_Bsmt_SF     Sale_Price 0.6325288 0
## 10   Gr_Liv_Area      Full_Bath 0.6303208 0
## 11  First_Flr_SF     Sale_Price 0.6216761 0
## 12    Year_Built Year_Remod_Add 0.6120953 0
## 13 Second_Flr_SF      Half_Bath 0.6116337 0</pre><p>列出與目標變數Sale_Price有高度相關(相關係數絕對值高於0.5)的變數組合</p><pre class="crayon-plain-tag">cor_table %&gt;% filter(row == "Sale_Price" | column == "Sale_Price") %&gt;% filter(abs(round(cor,digits = 2)) &gt;= 0.5) %&gt;% arrange(desc(cor))</pre><p></p><pre class="crayon-plain-tag">##               row     column       cor p
## 1     Gr_Liv_Area Sale_Price 0.7067799 0
## 2     Garage_Cars Sale_Price 0.6475616 0
## 3     Garage_Area Sale_Price 0.6401383 0
## 4   Total_Bsmt_SF Sale_Price 0.6325288 0
## 5    First_Flr_SF Sale_Price 0.6216761 0
## 6      Year_Built Sale_Price 0.5584261 0
## 7       Full_Bath Sale_Price 0.5456039 0
## 8  Year_Remod_Add Sale_Price 0.5329738 0
## 9    Mas_Vnr_Area Sale_Price 0.5021960 0
## 10  TotRms_AbvGrd Sale_Price 0.4954744 0</pre><p>我們取具有高相關性的兩變數Gr_Liv_Area、TotRms_AbvGrd來說明。兩變數相關係數高達0.81。其中，Gr_Liv_Area和TotRms_AbvGrd皆分別與目標變數具有高度相關(相關係數分別為：cor = 0.71 and cor = 0.50)。</p>
<p>我們將兩變數投入線性模型進行配適，並比較各自投入線性回歸的係數。</p>
<p>模型1: 投入兩高相關性變數</p><pre class="crayon-plain-tag">lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)    Gr_Liv_Area  TotRms_AbvGrd  
##       49953.6          137.3       -11788.2</pre><p>模型2: 單獨使用Gr_Liv_Area進行回歸的結果。</p><pre class="crayon-plain-tag">lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)
## 
## Coefficients:
## (Intercept)  Gr_Liv_Area  
##       17797          108</pre><p>模型3: 單獨使用TotRms_AbvGrd進行回歸的結果。</p><pre class="crayon-plain-tag">lm(Sale_Price ~ TotRms_AbvGrd, data = ames_train)</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
## lm(formula = Sale_Price ~ TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)  TotRms_AbvGrd  
##         26820          23731</pre><p>可以發現：</p>
<ul>
<li>兩高相關變數同時進行回歸配適時，會得到Gr_Liv_Area與目標變數為正相關而TotRms_AbvGrd與目標變數為負相關的係數結果。</li>
<li>單獨投入回歸配適時，Gr_Liv_Area和TotRms_AbvGrd都變成正向係數。</li>
</ul>
<p>以上係數正負號有問題的結果，就是模型存在共線性問題時常遇到的結果。具高度相關的變數係數會過度膨脹(over-inflated)且變得很不穩定(fluctuate significantly)。係數項大幅波動的結果，就是過度配適(overfitting)，也就是說，在權衡“bias-variance”(即誤差v.s模型變異度)階段中具模型有很高的變異度。雖然我們可以在建模後(事後)，使用VIF(variance inflation factors)變異數膨脹因子來檢查哪些變數有共線性並移除，但仍不太清楚要移除哪個變數好，或者是擔心移除變數會讓模型失去有價值的訊號。</p>
<h4>2. Insufficient Solution 解決方案不充分</h4>
<p>當特徵個數(p)超過觀測個數(n)(p&gt;n)時，OLS(最小平方法)回歸解矩陣是不可逆的(solution matrix is not invertible)。這代表(1)最小平方估計參數解不是唯一。會存在無限的可用的解，但這些解大多都過度配適資料。(2)在大多數的情況下，這些結果在計算上是不可行的(computationally infeasible)。</p>
<p>因此，只能透過移除特徵變數直到(n&gt;p)再將資料投入最小平方回歸模型進行配適。雖然可以透過人工的方式事前處理特徵變數過多的問題，但可能很麻煩且容易出錯。</p>
<h4>3. Interpretability 可解釋性</h4>
<p>當我們的特徵變數個數量很大時，我們會希望識別出具有最強解釋效果的較小子集合(Subsetting)。通常我們會偏好透過變數選取(feature selection)的方法來解決。其中一個變數選取法叫做「hard thresholding feature selection(硬閾值特徵選取)」，可以透過線性模型選取(linear model selection)來進行（best subsets &amp; stepwise regression)，但這個方法通常計算上效率較低也不好擴展，而且是直接透過增加或減少特徵變數的方式來進行模型比較。另一個方法叫做「soft thresholding feature selection(軟閾值特徵選取)」，此法將慢慢的將特徵效果推向0。</p>
<h3>Regularized Regression 正規化回歸</h3>
<p>當遇到以上問題，一個替代OLS回歸的方法就是透過「正規化回歸 regularized regression」(又稱作penalized models或shrinkage method)來對回歸係數做管控。正規化回歸模型會對回歸係數大小做出約束，並逐漸的將回歸係數壓縮到零。而對回歸係數的限制將有助於降低係數的幅度和波動，並降低模型的變異。</p>
<p>正規化回歸的目標函式與OLS回歸類似，但多了一個懲罰參數(penalty parameter, P)：</p>
<p>\[minimize\bigg \{ SSE + P \bigg \}\]</p>
<p>而常見的懲罰係數有兩種(分別對應到ridge回歸模型 &amp; lasso回歸模型)，效果是類似的。懲罰係數將會限制回歸係數的大小，除非該變數可以使誤差平方和(SSE)降低對應水準，該特徵係數才會上升。以下就來進一步介紹兩種最常見的正規化回歸法。</p>
<h3>Ridge Regression</h3>
<p>Ridge Regression透過將懲罰參數\(\lambda \sum_{j=1}^p \beta_{j}^2\)加入目標函式中。也因為該參數為對係數做出二階懲罰，故又稱為L2 Penalty懲罰參數。</p>
<p>\[minimize\bigg \{ SSE +\lambda \sum_{j=1}^p \beta_{j}^2 \bigg \}\]</p>
<p>而L2懲罰參數的值可以透過「tuninig parameter, \(\lambda\)」來控制。當\(\lambda \rightarrow 0\)，L2懲罰參數就跟OLS回歸一樣，目標函式只有最小化SSE；而當\(\lambda \rightarrow \infty\)時，懲罰效果最大，迫使所有係數都趨近於0。(如下圖範例所示，係數隨著\(\lambda\)由0變化到821(log(821) = 6.7)，係數逐漸被ridge法正規化的過程。)</p>
<p><img src="/wp-content/uploads/2019/01/unnamed-chunk-177-1.png" alt="Regularized Regression, Ridge" /></p>
<p>由上圖我們可以觀察到：</p>
<ul>
<li>部分特徵係數會波動，直到\(\log(\lambda) \simeq 0\)才逐漸穩定開始收斂至0。這表示存在多重共線性，唯有透過\(\log(\lambda) &gt; 0\)校正參數來限制係數，以降低模型變異和誤差。</li>
</ul>
<p>但如何決定最適的shrinkage的程度(校正參數\(\lambda\))來最小化模型誤差呢？我們會透過以下的R程式語言實作來說明。</p>
<h4>實作ridge regression using R</h4>
<p>這邊主要會使用glmnet套件來執行。因為glmnet函數沒使用formula參數，故我們需要將資料分成x和y來帶入glmnet函數參數值，且x參數須為matrix型態。而我們將會使用model.matrix()函數來將質性變數轉換成dummy variables虛擬變量（如果資料維度很大的時候，則可參考更有效率的處理函數 Matrix::sparse.model.matrix）(並將第一行的截距項忽略)。另外，我們也將目標變數進行log轉換(因為目標變數分佈有偏)。</p>
<p>step 1: 將資料分成預測變數矩陣與目標變數，並替目標變數進行log轉換</p><pre class="crayon-plain-tag">ames_train_x &lt;- model.matrix(object = Sale_Price ~ ., data =  ames_train)[, -1]
ames_train_y &lt;- log(ames_train$Sale_Price)

ames_test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1]
ames_test_y &lt;- log(ames_test$Sale_Price)</pre><p>查看質化變數經轉換成虛擬變量後，我們特徵變數維度從原始81個維度，增加為307個維度（不含截距項）。</p><pre class="crayon-plain-tag"># What is the dimension of of your feature matrix?
dim(ames_train_x)</pre><p></p><pre class="crayon-plain-tag">## [1] 2054  307</pre><p>step 2: 執行ridge model</p>
<p>我們使用glmnet::glmnet()來建立Ridge模型，並使用參數alpha來指定說要使用哪種懲罰參數，alpha = 0為Ridge，alpha = 1為lasso，或是 0 \(\leq\) alpha \(\leq\) 1為elastic net。</p>
<p>glmnet會在執行時，主要會做兩件事：</p>
<ol>
<li>預測變數在投入正規化回歸模型(regularized regression)前是需要被標準化(standardized)。而glmnet函數則會幫你處理標準化這件事。如果你已在使用glmnet前標準化預測變數，則可將參數設定為standaradize = FALSE。</li>
<li>glmnet會跨非常大的\(\lambda\)區間來執行ridge model。如下圖所示：</li>
</ol>
<p></p><pre class="crayon-plain-tag"># Apply Ridge regression to ames data
ames_ridge &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

plot(ames_ridge, xvar = "lambda")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-181-1.png" alt="Regularized Regression, Ridge" /></p>
<p>我們可以用以下方式看\(\lambda\)的區間。glmnet預設為使用100組\(\lambda\)，當然你也可以自行指定\(\lambda\)區間，但絕大多數時候是不太需要去調整的。</p><pre class="crayon-plain-tag">ames_ridge$lambda</pre><p></p><pre class="crayon-plain-tag">##   [1] 279.10348741 254.30870285 231.71661862 211.13155287 192.37520764
##   [6] 175.28512441 159.71327708 145.52478975 132.59676852 120.81723707
##  [11] 110.08416672 100.30459276  91.39380920  83.27463509  75.87674603
##  [16]  69.13606504  62.99420758  57.39797580  52.29889783  47.65280789
##  [21]  43.41946378  39.56219829  36.04760164  32.84523206  29.92735217
##  [26]  27.26868869  24.84621355  22.63894441  20.62776299  18.79524938
##  [31]  17.12553123  15.60414624  14.21791689  12.95483634  11.80396439
##  [36]  10.75533273   9.79985861   8.92926618   8.13601479   7.41323366
##  [41]   6.75466241   6.15459682   5.60783940   5.10965440   4.65572679
##  [46]   4.24212485   3.86526617   3.52188658   3.20901188   2.92393211
##  [51]   2.66417804   2.42749981   2.21184742   2.01535299   1.83631458
##  [56]   1.67318146   1.52454063   1.38910464   1.26570041   1.15325908
##  [61]   1.05080672   0.95745595   0.87239820   0.79489675   0.72428031
##  [66]   0.65993724   0.60131024   0.54789149   0.49921832   0.45486914
##  [71]   0.41445982   0.37764035   0.34409183   0.31352366   0.28567108
##  [76]   0.26029285   0.23716915   0.21609970   0.19690199   0.17940976
##  [81]   0.16347149   0.14894914   0.13571691   0.12366019   0.11267456
##  [86]   0.10266486   0.09354440   0.08523417   0.07766220   0.07076291
##  [91]   0.06447653   0.05874861   0.05352954   0.04877413   0.04444117
##  [96]   0.04049314   0.03689584   0.03361811   0.03063157   0.02791035</pre><p>我們亦可透過coef()函數去查看每個\(\lambda\)懲罰係數模型產生的係數。glmnet會儲存每個模型的所有係數，並按照\(\lambda\)由大到小排列。<br />
但由於預測變數實在太多，我們只挑Gr_Liv_Area和TotRms_AbvGrd兩個變數分別在\(lamda\)為最大(279.1034874)和最小值(0.0279103)的係數值。</p>
<p>對應到最小\(\lambda\)值的係數。</p><pre class="crayon-plain-tag"># coefficients for the smallest lambda parameters
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 100]</pre><p></p><pre class="crayon-plain-tag">##   Gr_Liv_Area TotRms_AbvGrd 
##  0.0001004011  0.0096383231</pre><p>對應到最大\(\lambda\)值的係數。可以看到當\(\lambda\)懲罰係數很大時，係數幾乎都被壓縮逼近到0。</p><pre class="crayon-plain-tag"># coefficients for the largest lambda parameters
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 1]</pre><p></p><pre class="crayon-plain-tag">##                                      Gr_Liv_Area 
## 0.0000000000000000000000000000000000000005551202 
##                                    TotRms_AbvGrd 
## 0.0000000000000000000000000000000000001236183840</pre><p>雖然看到了懲罰係數\(\lambda\)對係數限制的效果，但到目前為止我們還不了解懲罰限制式對模型的改善程度。</p>
<h4>Tuning</h4>
<p>\(\lambda\)是一個用來校正參數，用來避免模型對訓練資料集產生過度配適的情況。然而，為了找出最適的\(\lambda\)，我們會需要利用cross-validation來協助。我們可以透過cv.glmnet()函數來執行k-fold cross validation，預設k=10。</p><pre class="crayon-plain-tag"># Apply CV Ridge regression to ames data
ames_ridge &lt;- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

# plot result
plot(ames_ridge)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-185-1.png" alt="Regularized Regression, Ridge" /></p>
<p>上圖為執行10-fold cross validation，不同\(\lambda\)值所對應的MSE(mean squared error)的結果。我們可以就MSE變化的區間發現，其實改善幅度有限；但是我們發現，當我們對係數使用\(\log(\lambda) \geq 0\)的懲罰係數，則MSE就會大幅上升。圖上方的數字(299)表示模型中的變數數目。因為Ridge Model不會直接強制讓變數係數變成0，所以模型中的變數數目維持不變。(但在lasso和elastic模型中就不大一樣)。</p>
<p>圖中的第一和第二條垂直虛線則分別代表：</p>
<ul>
<li>對應最小MSE的\(\lambda\)</li>
<li>對應最小MSE一個標準差內的最大\(\lambda\)</li>
</ul>
<p>我們可以透過以下指令查看兩者的值：</p>
<p>最小MSE值</p><pre class="crayon-plain-tag">min(ames_ridge$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02207459</pre><p>對應最小MSE的\(\lambda\)</p><pre class="crayon-plain-tag">ames_ridge$lambda.min</pre><p></p><pre class="crayon-plain-tag">## [1] 0.1489491</pre><p>對應最小MSE的\(\log(\lambda)\)</p><pre class="crayon-plain-tag">log(ames_ridge$lambda.min)</pre><p></p><pre class="crayon-plain-tag">## [1] -1.90415</pre><p>距離最小MSE一個標準差內的MSE值</p><pre class="crayon-plain-tag">ames_ridge$cvm[ames_ridge$lambda == ames_ridge$lambda.1se]</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02474565</pre><p>對應距離最小MSE一個標準差內的\(\lambda\)</p><pre class="crayon-plain-tag">ames_ridge$lambda.1se</pre><p></p><pre class="crayon-plain-tag">## [1] 0.6013102</pre><p>對應距離最小MSE一個標準差內的\(\log(\lambda)\)</p><pre class="crayon-plain-tag">log(ames_ridge$lambda.1se)</pre><p></p><pre class="crayon-plain-tag">## [1] -0.5086443</pre><p>在lasso和elastic模型中，使用對應距離最小MSE一個標準差內的\(\lambda\)是很明顯的結果。但在ridge模型中，我們可先用視覺化圖表來衡量。</p>
<p>我們將所有對應不同\(\lambda\)的係數值繪出，並以紅色垂直虛線表示對應最小MSE一個標準差內\(\lambda\)。</p><pre class="crayon-plain-tag">ames_ridge_min &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

{
  plot(ames_ridge_min, xvar = "lambda")
  abline(v = log(ames_ridge$lambda.1se), col = "red", lty = "dashed")
  }</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-192-1.png" alt="Regularized Regression, Ridge" /></p>
<p>由上圖我們可以看到，在極大化預測精準度的同時，我們可以對係數做出多少限制。</p>
<h4>Advantages &amp; Disadvantages</h4>
<h5>優點</h5>
<ol>
<li>實質上，Ridge模型會將具有相關性的變數推向彼此，並避免使得其中一個有極大正係數另一個有極大負係數的情況。</li>
<li>此外，許多不相干的變數係數會被逼近為0(不會等於0)。表示我們可以降低我們資料集中的雜訊，幫助我們更清楚的識別出模型中真正的訊號(signals)。</li>
</ol>
<p>比如說以下我們來看對應\(\lambda\)值為最小MSE一個標準差的模型中，Top 25個具有影響力的變數分別有哪些。</p><pre class="crayon-plain-tag">coef(ames_ridge, s = "lambda.1se") %&gt;% # 308 x 1 sparse Matrix of class "dgCMatrix"
  as.matrix() %&gt;% 
  as.data.frame() %&gt;% 
  add_rownames(var = "var") %&gt;% 
  `colnames&lt;-`(c("var","coef")) %&gt;%
  filter(var != "(Intercept)") %&gt;%  #剔除截距項
  top_n(25, wt = coef) %&gt;% 
  ggplot(aes(coef, reorder(var, coef))) +
  geom_point() +
  ggtitle("Top 25 influential variables") +
  xlab("Coefficient") +
  ylab(NULL)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-193-1.png" alt="Regularized Regression, Ridge" /></p>
<h5>缺點</h5>
<ol>
<li>然而，Ridge模型會保留所有變數。假如你覺得需要保留所有變數並將較無影響力的變數雜訊給減弱並最小化共線性，則模型是好的。</li>
<li>Ridge 模型是不具有變數挑選(feature selection)功能的。假如妳需要更近一步減少資料中的訊號(signals)並尋找subset來解釋，則lasso模型會更適合。</li>
</ol>
<h3>Lasso Regression</h3>
<p>Lasso的全名為Least absolute shrinkage and selection operator(<a href="https://www.jstor.org/stable/2346178?seq=1#page_scan_tab_contents" target="_blank" rel="noopener noreferrer"> Tibshirani, 1996</a>)。是Ridge模型外的另一個選擇，並在目標函式的限制式有所調整。有別於二階懲罰(L2 Penalty)，Lasso模型在目標函式中所使用的是一階懲罰式(L1 Penalty)\(\lambda \sum_{j=1}^p |\beta_{j}|\)。</p>
<p>\[ minimize \bigg \{ SSE + \lambda \sum_{j=1}^p |\beta_{j}| \bigg\} \]</p>
<p>不像Ridge模型只會將係數逼近到接近零（但不會真的是0），Lasso模型則真的會將係數推進成0(如下圖)。因此，Lasso模型不僅能使用正規化(regulariztion)來優化模型，亦可以自動執行變數篩選(Feature selection)。</p>
<p><img src="/wp-content/uploads/2019/01/unnamed-chunk-194-1.png" alt="Regularized Regression, Lasso" /></p>
<p>從上圖我們可以看到，在\(\log(\lambda) = -6\)時，所有8個變數（圖表上方數字）都包還在模型內，而當在\(\log(\lambda) = -3\)時只剩下6個變數，最後當在\(\log(\lambda) = -1\)時，只剩2個變數被保留在模型內。因此，當遇到資料變數非常多時，Lasso模型是可以幫你識別並挑選出有最強（也最一致）訊號的變數。</p>
<h4>使用R實作Lasso模型</h4>
<p>建置Lasso模型的方法跟Ridge模型作法一樣，只需將glmnet()函數中alpha參數改設定為1。</p><pre class="crayon-plain-tag">ames_lasso &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso, xvar = "lambda")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-195-1.png" alt="Regularized Regression, Lasso" /></p>
<p>從上圖中可以看到大約是在\(\log(\lambda) \rightarrow -6\)的時候，被保留在模型中的變數數目大量下降。而且，當\(\log(\lambda) = -10 \rightarrow \lambda = 0\)，在沒有懲罰限制式(跟OLS模型一樣)時，數個變數的係數值都非常大，表示可能變數與變數間存在高度相關而導致他們的係數變得極大。當我們對模型做出限制，這些資料中為雜訊的變數係數就會被推進成0。</p>
<p>同樣的，如何選擇最適的懲罰係數\(\lambda\)亦和Ridge模型一樣透過執行cross validation來看不同\(\lambda\)對應的MSE值，並找出使MSE發生最小值(或在一個標準差之內)的\(\lambda\)來決定。</p>
<h4>Tuning</h4>
<p>為了找出最適的\(\lambda\)，我們跟Ridge模型一樣，使用cv.glmnet()函數來看不同\(\lambda\)對應的MSE值，並將alpha參數設定為1。</p><pre class="crayon-plain-tag">ames_lasso &lt;- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso)</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-196-1.png" alt="Regularized Regression, Lasso" /></p>
<p>從上圖可以發現，我們可以透過採用大約\( -6 \leq \log(\lambda) \leq -4\)的懲罰係數區間來最小化模型的MSE。而且在該懲罰係數區間，不僅最小化模型的MSE，同時也壓縮模型內被保留下來的變數數目至大約\(131 \geq p \geq 63\)的區間。</p>
<p>同樣的，我們可以透過以下方法來取得最小MSE和一個標準差內的MSE以及對應的\(\lambda\)。</p>
<p>minimum MSE</p><pre class="crayon-plain-tag">min(ames_lasso$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02309004</pre><p>對應最小MSE的\(\log(\lambda)\)</p><pre class="crayon-plain-tag">log(ames_lasso$lambda.min)</pre><p></p><pre class="crayon-plain-tag">## [1] -5.555725</pre><p>within 1 S.E. of minimum MSE</p><pre class="crayon-plain-tag">ames_lasso$cvm[ames_lasso$lambda == ames_lasso$lambda.1se]</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02626365</pre><p>對應到最小MSE一個標準差內的最大\(\log(\lambda)\)</p><pre class="crayon-plain-tag">ames_lasso$lambda.1se</pre><p></p><pre class="crayon-plain-tag">## [1] 0.01295484</pre><p>此時在Lasso模型的例子中，使用對應到最小MSE一個標準差內的最大\(\lambda\)當作懲罰係數的優勢是更明顯的。假設我們使用對應最小MSE的\(\lambda\)當作懲罰係數，那麼我們可以將原始變數集個數從307降到小於150左右。但考量到MSE也會有些變化性(variability)，因此我們可以合理假設，可以使用稍微更高的懲罰係數來達成類似的MSE程度，其\(\lambda\)所對應的變數數目約為小於70的水準左右。如果你分析的目的志在說明與詮釋預測變數，這樣懲罰係數的選擇應會有很大的幫助。</p><pre class="crayon-plain-tag">ames_lasso_min &lt;- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

{
  plot(ames_lasso_min, xvar = "lambda")
  abline(v = log(ames_lasso$lambda.min), col = "red", lty = "dashed")
  abline(v = log(ames_lasso$lambda.1se), col = "red", lty = "dashed")
}</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-201-1.png" alt="Regularized Regression, Lasso" /></p>
<h4>Advantages and Disadvantages</h4>
<h5>優點</h5>
<ol>
<li>與Ridge模型一樣，Lasso模型亦會將具有相關性的變數推向彼此，並避免使得其中一個有極大正係數另一個有極大負係數的情況。</li>
<li>與Ridge模型最大的差別，就是Lasso會將不具影響力的變數係數變成0，自動進行變數篩選(Feature selection)。這樣的處理方式簡化並自動化識別出那些對模型預測正確性有高度影響力的變數。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>然而，時常在我們移除變數的同時也會犧牲掉模型的正確性。所以為了得到Lasso產生的更清楚與簡潔的模型結果，我們也會降低模型的正確性。</li>
</ol>
<p>一般來說，Ridge和Lasso模型所產生的最小MSE不會有太大差別(如下結果所示)。所以除非你單純只看最小化MSE的結果，實質上他們兩的差異並不顯著。</p><pre class="crayon-plain-tag"># minimum Ridge MSE
min(ames_ridge$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02207459</pre><p></p><pre class="crayon-plain-tag"># minimum Ridge MSE
min(ames_lasso$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02309004</pre><p></p>
<h3>Elastic Net</h3>
<p>涵蓋Ridge和Lasso兩個模型的就是Elastic Net模型(<a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2005.00503.x" target="_blank" rel="noopener noreferrer">Zou and Hasie,005</a>)，該模型綜合了兩個懲罰限制式。</p>
<p>\[ minimize \bigg \{ SSE + \lambda_{1} \sum_{j=1}^p \beta_{j}^2 + \lambda_{2} \sum_{j=1}^p |\beta_{j}| \bigg \} \]</p>
<p>雖然Lasso模型會執行變數挑選，但一個源自於懲罰參數的結果就是，通常當兩個高度相關的變數的係數在被逼近成為0的過程中，可能一個會完全變成0但另為一個仍保留在模型中。此外，這種一個在內、一個在外的處理方法不是很有系統。相對的，Ridge模型的懲罰參數就稍具效率一點，可以有系統的<br />
將高相關性變數的係數一起降低。於是，Elastic Net模型的優勢就在於，它綜合了Ridge Penalty達到有效正規化優勢以及Lasso Penalty能夠進行變數挑選優勢。</p>
<h4>使用R實作 Elastic Net</h4>
<p>跟Ridge和Lasso一樣是使用glmnet()，並調整介於0~1之間的alpha參數。當alpha = 0.5時，Ridge和Lasso的組合是平均的，而當alpha\(\rightarrow\)0時，會有較多的Ridge Penalty權重，而當alpha\(\rightarrow\)1時，則會有較多的Lasso Penalty權重。</p><pre class="crayon-plain-tag">lasso    &lt;- glmnet(ames_train_x, ames_train_y, alpha = 1.0) 
elastic1 &lt;- glmnet(ames_train_x, ames_train_y, alpha = 0.25) 
elastic2 &lt;- glmnet(ames_train_x, ames_train_y, alpha = 0.75) 
ridge    &lt;- glmnet(ames_train_x, ames_train_y, alpha = 0.0)

par(mfrow = c(2,2), mar = c(4,2,4,2), + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1) \n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = 0.75) \n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = 0.25) \n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0) \n\n")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-204-1.png" alt="Regularized Regression, Elastic Net" /></p>
<h4>tuning</h4>
<p>在Ridge和Lasso模型中，\(\lambda\)是我們主要調整的參數，然而在Elastic Net模型中，我們會需要調教\(\lambda\)和alpha兩個參數。</p>
<p>首先，我們先建立一個fold_id，用來固定每一次建模每筆資料所在的fold id(1 ~ nfold)，即每次建模使用的CV folds都是相同的(如果單純只使用nfolds = 10，雖然每次建模都會進行10-fold的CV，但每一次資料所在的fold id卻是不一樣的)。(*在前的Ridge和Lasso的例子中，因為只會run一次CV.glmnet，來看不同\(\lambda\)值交叉驗證的MSE值，因此只需指定nfold數即可；但在Elastic Net模型中，會根據不同alpha(i.e., Ridge和Lasso權重佔比)來跑不同的cv.glmnet，為了確保模型結果不受亂數fold if結果影響，故固定每一次資料所在的fold id）</p>
<p>接著我們再建立一個tuning grid用來搜羅從0-1的區間的alpha值以及對應的空欄位值（包括最小MSE和一個標準差內的MSE以及兩者分別所對應的\(\lambda\)值）後續將用來儲存不同alpha參數跑出的CV模型結果。</p><pre class="crayon-plain-tag"># maintain the same folds across all models
fold_id &lt;- sample(x = 1:10, size = length(ames_train_y), replace = TRUE)

# search across a range of alphas
tuning_grid &lt;- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)</pre><p>接著，我們便可以開始迭代不同的alpha值(不同的Ridge &amp; Lasso權重組成)，套用CV Elastic Net，萃取出每一個alpha值對應的模型結果，包括最小MSE以及一個標準差內的MSE、以及兩者分別所對應的\(\lambda\)值。</p><pre class="crayon-plain-tag">for(i in seq_along(tuning_grid$alpha)){
  # fit CV model for each alpha value
  fit &lt;- cv.glmnet(x = ames_train_x, y = ames_train_y, alpha = tuning_grid$alpha[i],foldid = fold_id)

  # extract MSE and lambda values
  tuning_grid$mse_min[i]    &lt;- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    &lt;- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] &lt;- fit$lambda.min
  tuning_grid$lambda_1se[i] &lt;- fit$lambda.1se
}

tuning_grid</pre><p></p><pre class="crayon-plain-tag">## # A tibble: 11 x 5
##    alpha mse_min mse_1se lambda_min lambda_1se
##    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1   0    0.0217  0.0247    0.124       0.601 
##  2   0.1  0.0217  0.0251    0.0292      0.108 
##  3   0.2  0.0219  0.0257    0.0146      0.0590
##  4   0.3  0.0221  0.0257    0.0107      0.0393
##  5   0.4  0.0222  0.0258    0.00802     0.0295
##  6   0.5  0.0223  0.0258    0.00642     0.0236
##  7   0.6  0.0223  0.0264    0.00535     0.0216
##  8   0.7  0.0223  0.0265    0.00458     0.0185
##  9   0.8  0.0224  0.0265    0.00401     0.0162
## 10   0.9  0.0224  0.0265    0.00357     0.0144
## 11   1    0.0226  0.0262    0.00292     0.0118</pre><p>接著我們每一個alpha值跑出的模型中，最適\(\lambda\)值所對應的「最小MSE\(\pm\)1個標準差內的MSE區間」繪出，由下圖我們可以發現，<br />
正負一個標準差內的MSE都落在相近的正確率區間(即不同alpha水準值間，模型正確率沒有太大差別)。因此，我們可以選取alpha = 1的Lasso模型來重重倚賴他的變數挑選功能，並合理假設模型正確率不會有損失(因為alpha從1變化到0正確率都沒有什麼變化)。</p><pre class="crayon-plain-tag">tuning_grid %&gt;%
  mutate(se = mse_1se - mse_min) %&gt;% # 計算1個SE的距離
  ggplot(aes(alpha, mse_min)) + # 繪製不同alpha參數下，cv所得的最小MSE值
  geom_line(size = 2) +
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha = .25) +
  ggtitle("MSE ± one standard error")</pre><p><img src="/wp-content/uploads/2019/01/unnamed-chunk-207-1.png" alt="Regularized Regression, Elastic Net" /></p>
<h4>優缺點</h4>
<h5>優點</h5>
<ol>
<li>Elastic Model的優勢就是它綜合了Ridge Penalty的有效的正規化過程以及Lasso Penalty的變數篩選功能。讓我們能夠控制變數共線性的問題，能夠在p&gt;n時執行回歸，並降低資料中過多的雜訊，以幫助我們將具有影響力的變數獨立出來且維持住模型正確率。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>然而，Elastic Net，以及一般的regularization models，依舊有假設預測變數和目標變數需具有線性關係。雖然我們可以結合non-additive models(一種無母數回歸模型，non-parametric regression)交互作用，但當資料變數很多的時候，會是非常繁瑣與困難的。因此，當非線性關係存在時，可能考慮使用非線性迴歸的方法。</li>
</ol>
<h3>Predicting</h3>
<p>一旦決定好最適的模型後，我們可以使用predict()函數，來將模型套用在新的資料集上做預測。唯一要注意的是，你需要提供predict()s參數，來指定你要的\(\lambda\)值。</p>
<p>比如說以下我們想要建立一個Lasso模型，並使用對應最小MSE的\(\lambda\)值來做預測。</p><pre class="crayon-plain-tag"># create a lasso model
cv_lasso &lt;- cv.glmnet(x = ames_train_x, y = ames_train_y, alpha = 1)
min(cv_lasso$cvm)</pre><p></p><pre class="crayon-plain-tag">## [1] 0.02243528</pre><p></p><pre class="crayon-plain-tag">pred &lt;- predict(cv_lasso,newx = ames_test_x, s = cv_lasso$lambda.min)
mean((ames_test_y - pred)^2) #計算MSE</pre><p></p><pre class="crayon-plain-tag">## [1] 0.01483042</pre><p>我們預測結果的平均誤差平方為0.01483，比cv MSE的(0.02244)還來的更低些。</p>
<h3>使用其他Package來實作: caret &amp; h20</h3>
<p>glmnet不是唯一能夠處理Regularized Regression的套件。常用的幾種套件包括caret和h20。以下僅簡單介紹caret套件的執行方法。</p>
<p>caret</p><pre class="crayon-plain-tag"># load caret package
library(caret)

train_control &lt;- trainControl(method = "cv", number = 10)

caret_mod &lt;- train(
  x = ames_train_x, 
  y = ames_train_y,
  method = "glmnet", 
  prePro = c("center","scale","zv","nzv"),
  trControl = train_control,
  tuneLength = 10
)

head(caret_mod$results)</pre><p></p><pre class="crayon-plain-tag">##   alpha       lambda      RMSE  Rsquared       MAE     RMSESD RsquaredSD
## 1   0.1 0.0001289530 0.1621572 0.8442073 0.1056494 0.02614671 0.04570000
## 2   0.1 0.0002978982 0.1621441 0.8442298 0.1056370 0.02616059 0.04573050
## 3   0.1 0.0006881835 0.1619713 0.8445118 0.1053968 0.02646506 0.04638065
## 4   0.1 0.0015897932 0.1617801 0.8448180 0.1050524 0.02695754 0.04747639
## 5   0.1 0.0036726286 0.1615155 0.8453152 0.1045659 0.02754710 0.04877829
## 6   0.1 0.0084842486 0.1610666 0.8462117 0.1039097 0.02810395 0.04988037
##         MAESD
## 1 0.005227902
## 2 0.005231369
## 3 0.005283405
## 4 0.005294270
## 5 0.005145444
## 6 0.005222816</pre><p></p>
<h3>小結</h3>
<ol>
<li>鑑於傳統一般線性回歸模型沒有挑選變數之功能，且又遇到變數特徵數量非常大的時候，會容易使得模型假設不成立並發生模型過度配適的問題（變異度高），為降低此變異和樣本外誤差，可以使用Regularized Regression。</li>
<li>常見的Regularized Regression方法包括，Ridge(alpha = 0)、Lasso(alpha=1)和Elastic Net(\(0\leq alpha \leq 1\))。他們分別透過二階的Ridge Penalty和一階的Lasso Penalty或綜合兩種Penalty(並以alpha設定Ridge &amp; Lasso Penalty的權重)來對傳統OLS的目標函示的加上係數懲罰限制式，唯有該係數能夠降低的SSE幅度夠大才能增加其的係數大小。</li>
<li>Ridge會有系統的將干擾變數係數逼近0(但不會等於0)，而Lasso則會將較無影響力的變數係數變成0。前者正規化過程較有效(即同為高度相關的變數，最終不會產生一個係數為零一個係數不為零的情況)，但從頭到尾都會保留模型中所有變數；後者則具有變數挑選的功能，但正規化過程較不具系統性。而Elastic Net則綜合上述兩者的優勢，同時兼顧有效的正規化過程以及變數挑選功能。</li>
<li>但不管是Ridge, Lasso還是Elastic Net，這些一般性線性回歸模型都收限於「預測變數需與目標變數成線性關係」之假設，若假設不成立，仍得考慮非線性的回歸模型。</li>
<li>此學習筆記僅介紹Regularized Regrssion的基本概念。Regularized Regression方法亦延伸出其他parametric generalized linear models（包括logistic regression, multinomial, poisson, support vector machines）。此外亦存在許多其他可替代方法如Least Angle Regression和The Bayesian Lasso。</li>
</ol>
<hr />
<p><strong>更多統計學習筆記：</strong></p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p><strong>參考文章連結：</strong></p>
<ol>
<li><a href="https://tinyurl.com/y796qqca">歐萊禮  R資料科學</a></li>
<li><a href="http://uc-r.github.io/regularized_regression" target="_blank" rel="noopener noreferrer">正規化回歸 Regularized Regression</a></li>
<li><a href="http://rpubs.com/skydome20/R-Note18-Subsets_Shrinkage_Methods" target="_blank" rel="noopener noreferrer">R筆記 – (18) Subsets &amp; Shrinkage Regression (Stepwise &amp; Lasso)</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/regularized-regression-ridge-lasso-elastic/">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/regularized-regression-ridge-lasso-elastic/feed/</wfw:commentRss>
			<slash:comments>5</slash:comments>
		
		
			</item>
		<item>
		<title>Linear Regression &#124; 線性迴歸模型 &#124; using AirQuality Dataset</title>
		<link>/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/</link>
					<comments>/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Mon, 06 Aug 2018 12:45:23 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[AIC]]></category>
		<category><![CDATA[airquality]]></category>
		<category><![CDATA[assumption of linear regression]]></category>
		<category><![CDATA[BIC]]></category>
		<category><![CDATA[eda]]></category>
		<category><![CDATA[exploratory data analysis]]></category>
		<category><![CDATA[ggplot2]]></category>
		<category><![CDATA[kfold cross validation]]></category>
		<category><![CDATA[Linear Regression]]></category>
		<category><![CDATA[linearregression]]></category>
		<category><![CDATA[lm]]></category>
		<category><![CDATA[Missing Values Treatment]]></category>
		<category><![CDATA[modeling]]></category>
		<category><![CDATA[predict]]></category>
		<category><![CDATA[regression]]></category>
		<category><![CDATA[R語言]]></category>
		<category><![CDATA[test]]></category>
		<category><![CDATA[train]]></category>
		<category><![CDATA[統計]]></category>
		<category><![CDATA[線性回歸]]></category>
		<category><![CDATA[線性回歸假設]]></category>
		<category><![CDATA[資料視覺化]]></category>
		<category><![CDATA[遺失值填補]]></category>
		<guid isPermaLink="false">https://jamleecute.wordpress.com/?p=518</guid>

					<description><![CDATA[<p>Linear Regression 線性迴歸模型是用來預測連續型目標變數與預測變數間的線性關係，並存在許多資料符合常態分佈與線性關係等基本假設。預測變數可以是數 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Linear Regression 線性迴歸模型是用來預測<span style="color: #9f6ad4;">連續型目標變數</span>與預測變數間的線性關係，並存在許多資料符合常態分佈與線性關係等基本假設。預測變數可以是數值或類別型態，投入模型進行時類別變數都會被轉換成虛擬變數。對於有遺失值的觀測值則會直接忽略，且易受極端值影響，因此在使用模型前必須做好資料前處理。</p>
<h3>Linear Regression Introduction</h3>
<p>Linear Regression 線性迴歸模型主要是使用最適線性迴歸線來建立依變數Y和一個或多個自變數X間的關係，並可以此線性迴歸方程式，使用給定自變數X的值來預測依變數Y值（適用於目標變數為連續變數時，若目標變數為二元變數，則適用<a href="/logistic_regression_part1/" target="_blank" rel="noopener noreferrer">羅吉斯回歸</a>）。（＊線性迴歸方程式的建立則是使用最小平方法Least Sqaures Method來找尋X和Y之間的關係與趨勢）</p>
<p>Linear Regression 線性迴歸方程式：<span style="font-weight: 400;">\(Y=\beta_{1}+\beta_{2}*X+\varepsilon\)</span></p>
<p><img loading="lazy" class="wp-image-717 aligncenter" src="/wp-content/uploads/2018/08/統計概念圖_Linear-Regression.jpg" alt="linear regression" width="539" height="404" srcset="/wp-content/uploads/2018/08/統計概念圖_Linear-Regression.jpg 960w, /wp-content/uploads/2018/08/統計概念圖_Linear-Regression-300x225.jpg 300w, /wp-content/uploads/2018/08/統計概念圖_Linear-Regression-768x576.jpg 768w" sizes="(max-width: 539px) 100vw, 539px" /></p>
<h3>Example Problem</h3>
<p>使用資料集為R dataset套件中的<a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airquality.html">airquality</a>。此資料集主要搜集自1973年5月到9月，每日空氣品質相關的衡量指標，包括臭氧濃度、太陽輻射、平均風速、最高溫度，以及資料月份與資料日。</p>
<p><span style="color: #9f6ad4;"><strong>分析目標：找出影響空氣中臭氧(Ozone)濃度的重要影響因子與關係。</strong></span></p>
<p>目標變數（依變數）：連續變數(臭氧濃度，Ozone)<br />
預測變數（自變數）：連續/類別變數(太陽輻射Solar.R、平均風速Wind、最高溫度Temp、資料月份Month、資料日Day)(*此資料集剛好都是連續變數)</p>
<p>為了達成分析目標，主要的資料處理與分析架構會包括以下步驟：</p>
<ol>
<li>資料載入與檢視</li>
<li>資料探索(Exploratory Data Analysis, EDA)：視覺化分析
<ol>
<li>視覺化預測變數與目標變數間的關係：相關係數矩陣、散佈圖scatter plot</li>
<li>偵測離群值：盒須圖box plot</li>
<li>檢查預測變數分佈是否為常態分佈(鐘型)而沒有左右偏移：機率密度圖density plot</li>
</ol>
</li>
<li>資料前處理
<ol>
<li>離群值處理</li>
<li>標準化-使變數間關係衡量不受單位規模數值影響</li>
<li>變數轉換-使變數接近常態分佈，以符合模型假設</li>
<li>遺失值填補-使用MICE()套件來預測遺失值</li>
</ol>
</li>
<li>建立預測線性模型
<ol>
<li>Pooling &#8211; 綜合多組遺失值填補後的datasets建立回歸模型，並摘要模型配適結果</li>
<li>CompleteData &#8211; 挑選其中一組遺失填補後的dataset建立回歸模型，並摘要模型配適結果</li>
</ol>
</li>
<li>線性回歸模型適合度診斷
<ol>
<li>資料是否符合線性迴歸模型假設</li>
<li>使用gvlma套件檢測</li>
<li>使用shapiro test 來看變數是否成常態分佈</li>
</ol>
</li>
<li>模型驗證
<ol>
<li>檢視(比較)模型配適能力常用指標</li>
<li>80% 訓練 20%驗證
<ul>
<li> 真實值與預測值相關性(Correlation)</li>
<li>正確率(min_max_accuracy)</li>
<li>錯誤率(MAPE ,Mean absolute percentage error)</li>
</ul>
</li>
<li>加強版模型效果驗證：K-fold Cross Validation
<ol>
<li>mean squared error</li>
</ol>
</li>
</ol>
</li>
<li>結論</li>
</ol>
<h3>1. 資料載入與檢視</h3>
<p>載入資料</p><pre class="crayon-plain-tag"># 1. Load and View Dataset library(datasets) 
# 載入今天分析主要會用到的資料集：AirQuality data("airquality") 
inputData &lt;- airquality 

# 查看資料結構:資料變數、型態
str(inputData)

# 'data.frame':	153 obs. of  6 variables:
# $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
# $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
# $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
# $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
# $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
# $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...</pre><p>檢視資料</p><pre class="crayon-plain-tag"># 資料欄位詳細說明可在console輸入： ?datasets::airquality 
# 查看資料頭幾列 head(inputData)
head(inputData)

#   Ozone Solar.R Wind Temp Month Day
# 1    41     190  7.4   67     5   1
# 2    36     118  8.0   72     5   2
# 3    12     149 12.6   74     5   3
# 4    18     313 11.5   62     5   4
# 5    NA      NA 14.3   56     5   5
# 6    28      NA 14.9   66     5   6</pre><p></p>
<h3>2. 資料探索(Exploratory Data Analysis, EDA)</h3>
<h4>基本敘述統計</h4>
<p>使用summary()函數查看基本敘述統計。可以看到此資料集共有153列，以及6個特徵變數的最大最小值、第一與第三分位數以及遺失值數量。</p><pre class="crayon-plain-tag"># mean,median,25th and 75th quartiles,min,max, NA數量
summary(inputData)

#         Ozone          Solar.R         Wind            Temp          Month           Day      
# Min.   :  1.0   Min.   :  7   Min.   : 1.70   Min.   :56.0   Min.   :5.00   Min.   : 1.0  
# 1st Qu.: 18.0   1st Qu.:116   1st Qu.: 7.40   1st Qu.:72.0   1st Qu.:6.00   1st Qu.: 8.0  
# Median : 31.5   Median :205   Median : 9.70   Median :79.0   Median :7.00   Median :16.0  
# Mean   : 42.1   Mean   :186   Mean   : 9.96   Mean   :77.9   Mean   :6.99   Mean   :15.8  
# 3rd Qu.: 63.2   3rd Qu.:259   3rd Qu.:11.50   3rd Qu.:85.0   3rd Qu.:8.00   3rd Qu.:23.0  
# Max.   :168.0   Max.   :334   Max.   :20.70   Max.   :97.0   Max.   :9.00   Max.   :31.0  
# NA's   :37      NA's   :7</pre><p></p>
<h4>視覺化分析</h4>
<h4>視覺化預測變數與目標變數間的關係</h4>
<p>使用GGally套件中ggcorr()來查看各變數間的相關係數強弱。</p><pre class="crayon-plain-tag">library(&quot;GGally&quot;)
# ggcorr(): Plot a correlation matrix
# The function ggcorr() draws a correlation matrix plot using ggplot2.
ggcorr(data = inputData, palette = &quot;RdYlGn&quot;,
label = TRUE, label_color = &quot;black&quot;)</pre><p><img loading="lazy" class="alignnone size-full wp-image-682" src="/wp-content/uploads/2018/08/Rplot01.jpeg" alt="linear regression" width="600" height="569" srcset="/wp-content/uploads/2018/08/Rplot01.jpeg 600w, /wp-content/uploads/2018/08/Rplot01-300x285.jpeg 300w" sizes="(max-width: 600px) 100vw, 600px" /></p>
<p>由上圖可以觀察到</p>
<p>(1)具有較強正相關性(相關係數&gt;=0.4)的變數關係有：</p>
<ul>
<li>氣溫(Temp)&amp;月份(Month)</li>
<li>臭氧濃度(Ozone)與氣溫(Temp)</li>
</ul>
<p>(2)具有較強負相關性(相關係數&lt;= -0.4)的變數關係有：</p>
<ul>
<li>風速(Wind)與氣溫(Temp)</li>
<li>臭氧濃度(Ozone)與風速(Wind)</li>
</ul>
<p>但僅參考相關係數，並無法知道兩者是否符合線性關係。</p>
<p>散佈圖scatter plot ＆ 機率密度圖</p><pre class="crayon-plain-tag"># 使用 ggpairs() 函數產生多個變數間關係的視覺化矩陣散佈圖
ggpairs(data = inputData)</pre><p><img loading="lazy" class="alignnone size-full wp-image-684" src="/wp-content/uploads/2018/08/Rplot02.jpeg" alt="linear regression" width="800" height="759" srcset="/wp-content/uploads/2018/08/Rplot02.jpeg 800w, /wp-content/uploads/2018/08/Rplot02-300x285.jpeg 300w, /wp-content/uploads/2018/08/Rplot02-768x729.jpeg 768w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>由上圖可以觀察到三種數據型態，分別為：</p>
<ol>
<li>對角線上各個變數的機率密度分佈（用來檢視變數分佈是否為常態）
<ul>
<li>除了Temp稍微看似常態，其餘變數Ozone(右偏嚴重)、Solar.R、Wind都不太滿足常態分佈。</li>
</ul>
</li>
<li>左上方三角區塊變數間的相關係數數值大小
<ul>
<li>可以與前面ggcorr()畫的圖做比對</li>
</ul>
</li>
<li>左下方三角區塊變數間的散佈圖
<ul>
<li><span style="color: #9f6ad4;">可以發現此散佈圖在繪製時，已自動使用各變數的最大最小值將數值標準化，所以變數間的散佈圖將不受尺度大小所影響，都剛好在方形的繪圖範圍中</span>。</li>
</ul>
</li>
</ol>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>雖然ggpar可以一次畫出全部，但如果要<span style="color: #9f6ad4;">進一步加上趨勢輔助線，看變數x和y是否為線性關係，可以使用下列語法檢視</span>。</p>
<p>我們將剛剛得知有較強相關性的變數關係畫成散佈圖如下。(Ozone, Temp, Wind, Month)</p><pre class="crayon-plain-tag">scatter.smooth(x = inputData$Month, y = inputData$Temp,main = &quot;Temp ~ Month&quot;)
scatter.smooth(x = inputData$Temp, y = inputData$Ozone,main = &quot;Ozone ~ Temp&quot;)
scatter.smooth(x = inputData$Wind, y = inputData$Ozone,main = &quot;Ozone ~ Wind&quot;)
scatter.smooth(x = inputData$Wind, y = inputData$Temp,main = &quot;Temp ~ Wind&quot;)</pre><p><a href='/wp-content/uploads/2018/08/Rplot03.jpeg'><img width="600" height="569" src="/wp-content/uploads/2018/08/Rplot03.jpeg" class="attachment-full size-full" alt="Rplot03" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot03.jpeg 600w, /wp-content/uploads/2018/08/Rplot03-300x285.jpeg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot04.jpeg'><img width="600" height="569" src="/wp-content/uploads/2018/08/Rplot04.jpeg" class="attachment-full size-full" alt="Rplot04" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot04.jpeg 600w, /wp-content/uploads/2018/08/Rplot04-300x285.jpeg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot05.jpeg'><img width="600" height="569" src="/wp-content/uploads/2018/08/Rplot05.jpeg" class="attachment-full size-full" alt="Rplot05" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot05.jpeg 600w, /wp-content/uploads/2018/08/Rplot05-300x285.jpeg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot06.jpeg'><img width="600" height="569" src="/wp-content/uploads/2018/08/Rplot06.jpeg" class="attachment-full size-full" alt="Rplot06" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot06.jpeg 600w, /wp-content/uploads/2018/08/Rplot06-300x285.jpeg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a>
</p>
<p>由上面四張散佈圖，可大略看到線性關係如下：</p>
<ol>
<li>雖然氣溫和部分月份數值成正向關，但根據常識，這並不是我們希望的正向關。</li>
<li>臭氧濃度會隨著氣溫上升而上升。</li>
<li>臭氧濃度會隨著風速上升而下降。</li>
<li>氣溫會隨著風速上升而下降。</li>
</ol>
<h4>離群值偵測</h4>
<p>經過上述變數關係探索後，<span style="color: #0000ff;"><span style="color: #9f6ad4;">我們將僅針對聚焦後的變數(Ozone,Temp,Wind)，進行偵測離群值</span>。</span></p>
<p>我們將使用合鬚圖(box-plot)法來偵測離群值。</p><pre class="crayon-plain-tag">boxplot(inputData$Ozone, main="Ozone", sub=paste("Outlier rows: ", paste(boxplot.stats(inputData$Ozone)$out, collapse = ","))) #有離群值
boxplot(inputData$Wind, main="Wind", sub=paste("Outlier rows: ", paste(boxplot.stats(inputData$Wind)$out, collapse = ","))) #有三個離群值
boxplot(inputData$Temp, main="Temp", sub=paste("Outlier rows: ", paste(boxplot.stats(inputData$Temp)$out, collapse = ","))) #無離群值</pre><p>&nbsp;</p>

<a href='/wp-content/uploads/2018/08/Rplot07.jpeg'><img width="600" height="569" src="/wp-content/uploads/2018/08/Rplot07.jpeg" class="attachment-full size-full" alt="Rplot07" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot07.jpeg 600w, /wp-content/uploads/2018/08/Rplot07-300x285.jpeg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot08.jpeg'><img width="600" height="569" src="/wp-content/uploads/2018/08/Rplot08.jpeg" class="attachment-full size-full" alt="Rplot08" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot08.jpeg 600w, /wp-content/uploads/2018/08/Rplot08-300x285.jpeg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot09.jpeg'><img width="600" height="569" src="/wp-content/uploads/2018/08/Rplot09.jpeg" class="attachment-full size-full" alt="Rplot09" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot09.jpeg 600w, /wp-content/uploads/2018/08/Rplot09-300x285.jpeg 300w" sizes="(max-width: 600px) 100vw, 600px" /></a>

<p>由盒鬚圖可知，臭氧濃度(Ozone)跟風速(Wind)都有一些離群值。</p>
<p>由上述發現可知，<span style="color: #9f6ad4;">針對聚焦後的變數(Ozone, Temp, Wind)</span>，接下來需要處理：</p>
<ol>
<li>離群值處理：Ozone(2), Wind(3)。</li>
<li>變數標準化: 使不衡量尺度（單位）間的變數關係不受數值大小所影響。</li>
<li>變數轉換：盡可能讓變數分佈符合常態，以符合線性迴歸基本假設。</li>
<li>空值填補：Ozone(37)。</li>
</ol>
<h3>3. 資料前處理</h3>
<h4>3-1. 離群值處理</h4>
<p>針對聚焦的三個變數(Ozone,Temp,Wind)，<span style="color: #0000ff;"><span style="color: #9f6ad4;">本範例會將離群值填補為NA，再之後跟其他空值一併使用MICE方法預測</span>。</span></p>
<p>偵測離群值(<span style="color: #9f6ad4;">回傳的是離群數值，而非資料列index</span>)。</p><pre class="crayon-plain-tag">boxplot.stats(inputData$Ozone)$out
# [1] 135 168
boxplot.stats(inputData$Wind)$out
# [1] 20.1 18.4 20.7
boxplot.stats(inputData$Temp)$out
# integer(0)</pre><p>將離群值填補為NA。</p><pre class="crayon-plain-tag"># 將遺失值用NA填補
tempData &lt;- inputData
outlier &lt;- function(x) {
x[x &gt; quantile(x,probs = 0.75,na.rm = T) + 1.5 * IQR(x, na.rm = T) |
x &lt; quantile(x,probs = 0.25, na.rm = T) - 1.5 * IQR(x, na.rm = T)] &lt;- NA
x
}
# apply()的第一個參數只能接受「矩陣」物件，意味著所有元素必須是同一類
# 若用在其他結構的物件，如data.frame，他就會先被轉換成matrix
tempData &lt;- apply(X = tempData,MARGIN = 2,FUN = outlier)
# tempData
tempData&lt;- as.data.frame(tempData)</pre><p>檢視離群值處理後的結果。</p><pre class="crayon-plain-tag">summary(tempData) #check Ozone NA數為37+2, Wind NA數為3

#         Ozone          Solar.R         Wind            Temp          Month           Day      
# Min.   :  1.0   Min.   :  7   Min.   : 1.70   Min.   :56.0   Min.   :5.00   Min.   : 1.0  
# 1st Qu.: 18.0   1st Qu.:116   1st Qu.: 7.40   1st Qu.:72.0   1st Qu.:6.00   1st Qu.: 8.0  
# Median : 30.5   Median :205   Median : 9.70   Median :79.0   Median :7.00   Median :16.0  
# Mean   : 40.2   Mean   :186   Mean   : 9.76   Mean   :77.9   Mean   :6.99   Mean   :15.8  
# 3rd Qu.: 60.5   3rd Qu.:259   3rd Qu.:11.50   3rd Qu.:85.0   3rd Qu.:8.00   3rd Qu.:23.0  
# Max.   :122.0   Max.   :334   Max.   :16.60   Max.   :97.0   Max.   :9.00   Max.   :31.0  
# NA's   :39      NA's   :7     NA's   :3</pre><p>檢視離群值處理後變數分佈的變化。</p><pre class="crayon-plain-tag"># 經過極端值處理後：
# Ozone超級偏(skewness = 1.21 -&amp;gt; 0.94)
plot(density(tempData$Ozone, na.rm = TRUE), main=&quot;Density Plot: Ozone&quot;, ylab=&quot;Frequency&quot;, sub=paste(&quot;Skewness:&quot;, round(e1071::skewness(tempData$Ozone, na.rm = TRUE), 2)))
# Wind 平均值右偏(skewness = 0.34 -&amp;gt; 0.06)
plot(density(tempData$Wind, na.rm = TRUE), main=&quot;Density Plot: Wind&quot;, ylab=&quot;Frequency&quot;, sub=paste(&quot;Skewness:&quot;, round(e1071::skewness(tempData$Wind, na.rm = TRUE), 2)))
# Temp 平均值左篇(skewness = -0.37 -&amp;gt; =0.37)沒影響
plot(density(tempData$Temp, na.rm = TRUE), main=&quot;Density Plot: Temp&quot;, ylab=&quot;Frequency&quot;, sub=paste(&quot;Skewness:&quot;, round(e1071::skewness(tempData$Temp, na.rm = TRUE), 2)))</pre><p>可以看到：</p>
<ol>
<li>Ozone skewness有減少(skewness = 1.21 -&gt; 0.94)，但視覺上還是沒有非常接近常態分佈。</li>
<li>Wind的skewness有從0.34減少到0.06，視覺上已接近常態。</li>
<li>Temp的部份因為透過box-plot沒有偵測到明顯離群值，故在此沒有變化。</li>
</ol>

<a href='/wp-content/uploads/2018/08/Rplot12.jpeg'><img width="800" height="787" src="/wp-content/uploads/2018/08/Rplot12.jpeg" class="attachment-full size-full" alt="Rplot12" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot12.jpeg 800w, /wp-content/uploads/2018/08/Rplot12-300x295.jpeg 300w, /wp-content/uploads/2018/08/Rplot12-768x756.jpeg 768w, /wp-content/uploads/2018/08/Rplot12-75x75.jpeg 75w" sizes="(max-width: 800px) 100vw, 800px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot13.jpeg'><img width="800" height="787" src="/wp-content/uploads/2018/08/Rplot13.jpeg" class="attachment-full size-full" alt="Rplot13" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot13.jpeg 800w, /wp-content/uploads/2018/08/Rplot13-300x295.jpeg 300w, /wp-content/uploads/2018/08/Rplot13-768x756.jpeg 768w, /wp-content/uploads/2018/08/Rplot13-75x75.jpeg 75w" sizes="(max-width: 800px) 100vw, 800px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot14.jpeg'><img width="800" height="787" src="/wp-content/uploads/2018/08/Rplot14.jpeg" class="attachment-full size-full" alt="Rplot14" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot14.jpeg 800w, /wp-content/uploads/2018/08/Rplot14-300x295.jpeg 300w, /wp-content/uploads/2018/08/Rplot14-768x756.jpeg 768w, /wp-content/uploads/2018/08/Rplot14-75x75.jpeg 75w" sizes="(max-width: 800px) 100vw, 800px" /></a>

<p>用新的資料偵測離群值，可發現使用box-plot法，已無離群值。</p><pre class="crayon-plain-tag"># (2)outlier detection 離群值偵測 =&amp;gt;都沒有了
boxplot(tempData$Ozone, main=&quot;Ozone&quot;, sub=paste(&quot;Outlier rows: &quot;, paste(boxplot.stats(tempData$Ozone)$out, collapse = &quot;,&quot;))) #有離群值
boxplot(tempData$Wind, main=&quot;Wind&quot;, sub=paste(&quot;Outlier rows: &quot;, paste(boxplot.stats(tempData$Wind)$out, collapse = &quot;,&quot;))) #有三個離群值
boxplot(tempData$Temp, main=&quot;Temp&quot;, sub=paste(&quot;Outlier rows: &quot;, paste(boxplot.stats(tempData$Temp)$out, collapse = &quot;,&quot;))) #無黎群值</pre><p><a href='/wp-content/uploads/2018/08/Rplot03-1.jpeg'><img width="700" height="586" src="/wp-content/uploads/2018/08/Rplot03-1.jpeg" class="attachment-full size-full" alt="Rplot03" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot03-1.jpeg 700w, /wp-content/uploads/2018/08/Rplot03-1-300x251.jpeg 300w, /wp-content/uploads/2018/08/Rplot03-1-230x193.jpeg 230w, /wp-content/uploads/2018/08/Rplot03-1-350x293.jpeg 350w, /wp-content/uploads/2018/08/Rplot03-1-480x402.jpeg 480w" sizes="(max-width: 700px) 100vw, 700px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot04-1.jpeg'><img width="700" height="586" src="/wp-content/uploads/2018/08/Rplot04-1.jpeg" class="attachment-full size-full" alt="Rplot04" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot04-1.jpeg 700w, /wp-content/uploads/2018/08/Rplot04-1-300x251.jpeg 300w, /wp-content/uploads/2018/08/Rplot04-1-230x193.jpeg 230w, /wp-content/uploads/2018/08/Rplot04-1-350x293.jpeg 350w, /wp-content/uploads/2018/08/Rplot04-1-480x402.jpeg 480w" sizes="(max-width: 700px) 100vw, 700px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot05-1.jpeg'><img width="700" height="586" src="/wp-content/uploads/2018/08/Rplot05-1.jpeg" class="attachment-full size-full" alt="Rplot05" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot05-1.jpeg 700w, /wp-content/uploads/2018/08/Rplot05-1-300x251.jpeg 300w, /wp-content/uploads/2018/08/Rplot05-1-230x193.jpeg 230w, /wp-content/uploads/2018/08/Rplot05-1-350x293.jpeg 350w, /wp-content/uploads/2018/08/Rplot05-1-480x402.jpeg 480w" sizes="(max-width: 700px) 100vw, 700px" /></a>
</p>
<h4>3-2. 標準化</h4>
<p>將所有變數標準化(0 to 1)。</p><pre class="crayon-plain-tag">tempData2 &lt;- apply(tempData, 2, function(x){(x-min(x, na.rm = T))/(max(x, na.rm = T)-min(x, na.rm = T))})
tempData2 &lt;- as.data.frame(tempData2)</pre><p></p>
<h4>3-3. 變數轉換(\(X_{transform} = \log_{10}X\))</h4>
<p>透過變數log10轉換，來使變數更近似常態分配(僅針對Ozone)。</p><pre class="crayon-plain-tag">subTempData &lt;- tempData2[,c(1)]
subTempData2 &lt;- sapply(subTempData, function(x){x &lt;- log10(x+1)})
tempData2$Ozone_log10 &lt;- as.vector(subTempData2)</pre><p>檢視變數轉換後Ozone的機率密度分佈圖。</p><pre class="crayon-plain-tag"># Ozone_log10(skewness = 1.21 -&gt; 0.94 -&gt;0.69)
plot(density(tempData2$Ozone_log10, na.rm = TRUE), main="Density Plot: Ozone", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(tempData2$Ozone_log10, na.rm = TRUE), 2)))</pre><p>可以發現經log10轉換後的Ozone_log10的skewness有降低，但視覺上分佈還是不是那麼常態。(skewness = 1.21 -&gt; 0.94 -&gt;0.69)</p>
<p><img loading="lazy" class="alignnone size-full wp-image-727" src="/wp-content/uploads/2018/08/Rplot15.jpeg" alt="linear regression" width="800" height="787" srcset="/wp-content/uploads/2018/08/Rplot15.jpeg 800w, /wp-content/uploads/2018/08/Rplot15-300x295.jpeg 300w, /wp-content/uploads/2018/08/Rplot15-768x756.jpeg 768w, /wp-content/uploads/2018/08/Rplot15-75x75.jpeg 75w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h4>3-4. 空值填補(MICE, Multivariate Imputation by Chained Equations)</h4>
<p>當一個特徵變數空值資料占比超過5%就有必要做空值填補之處理。我們建立一個函數用來檢查每個特徵變數遺失資料列佔比。可以發現臭氧Ozone的遺失比例高達25%(原始資料遺失＋離群值NA)。</p><pre class="crayon-plain-tag">pMiss &lt;- function(x){sum(is.na(x))/length(x)*100}
apply(tempData2[,-1],2,pMiss)

#  Solar.R        Wind        Temp       Month         Day Ozone_log10 
# 4.575163    1.960784    0.000000    0.000000    0.000000   25.490196</pre><p>在此我們選擇使用<span style="text-decoration: underline;"><span style="color: #9f6ad4; text-decoration: underline;">MICE Package</span></span>來處理Ozone的遺失值。（更多有關遺失值處理請參考：「<a href="/%e3%80%8c%e7%b5%b1%e8%a8%88%e2%80%a2r%e8%aa%9e%e8%a8%80%e2%80%a2%e9%81%ba%e5%a4%b1%e5%80%bc%e8%99%95%e7%90%86%e3%80%8dmissing-value-treatment/" target="_blank" rel="noopener noreferrer">遺失值處理方法</a>」）</p>
<p><strong>MICE代表： Multivariate Imputations by Chained Equations (MICE)，具有以下幾個特點：</strong></p>
<ul>
<li>主要可用來產生<span style="text-decoration: underline;">多元變數(multivariate)</span>的<span style="text-decoration: underline;">多組空值填補值(multiple imputations)</span>（可透過mice()函式中的<span style="color: #9f6ad4;"><span style="text-decoration: underline;">參數m，number of mutiple imputations</span>，</span>來指定要產生幾組填補值解，預設為5組）。</li>
<li>透過<a href="https://zh.wikipedia.org/wiki/%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7">Gibbs sampling</a>法來對多元變數產稱多組空值填補值。</li>
<li>MICE填補方法是採用Fully Conditional Specification，即<span style="color: #9f6ad4;">每一個不完整的變數都是分別用獨立的模型來預測空值的</span>。</li>
<li>MICE可處理連續型變數(continuous)、二元變數(binary)、無順序類別型變數(unordered categorical)、有序類別變數(ordered categorical)。</li>
<li>MICE在預測目標欄位時(target column)，<span style="text-decoration: underline;">預設會使用其他非目標欄位來作為預測變數(predictors)</span>。如遇預測變數本身也不完整的情況，最新產生的一組填補值會被用來完整預測變數，再進行目標變數的填補預測。</li>
<li>我們可以替不同欄位指定各自的單變數填補模型(univariate imputation model)，使用方法範例：mice(data, meth=c(&#8216;sample&#8217;,&#8217;pmm&#8217;,&#8217;logreg&#8217;,&#8217;norm&#8217;)))。</li>
<li>單變數填補模型可以使用<span style="text-decoration: underline;">內建的方法</span>或是<span style="text-decoration: underline; color: #9f6ad4;">自訂方法(mice.impute.myfunc)</span>(mice(method = c(&#8220;xxx&#8221;, &#8220;<span style="color: #9f6ad4;">myfunc</span>&#8220;))。</li>
<li>常見的幾種內建填補模型包括：
<ul>
<li>pmm (Predictive mean matching) : 任何變數型態</li>
<li>cart (Classification and regression trees) : 任何變數型態</li>
<li>rf (Random forest imputations) : 任何變數型態</li>
<li> logreg (Logistic regression) : 二元類別型態 (binary)</li>
</ul>
</li>
</ul>
<p><strong>MICE package套件中的主要函式說明：</strong></p>
<ul>
<li>md.pattern() : inspect the missing data pattern</li>
<li>mice(): impute the missing values *m* times</li>
<li>with(): analyzed completed data sets =&gt; Performs a computation of each of imputed datasets in data. with (data, expr<span style="color: #9f6ad4;"> = formula</span>, &#8230;)。將產生的m組填補值套用到計算公式。</li>
<li>pool(): combine parameter estimates =&gt; Pool the results of the repeated analyses。綜合m組填補數據所產生的模型的估計係數(estimates)、標準差(std.error)、和p-value。</li>
<li><a href="https://www.rdocumentation.org/packages/mice/versions/3.3.0/topics/pool.compare">pool.compare()</a> : pool.compare(fit1, fit0, method = c(&#8220;wald&#8221;, &#8220;likelihood&#8221;), data = NULL)。使用方法&#8221;wald&#8221;或&#8221;likelihood&#8221;來比較<span style="text-decoration: underline;">兩種模型公式</span>套用在所有填補數據的綜合結果。</li>
<li>complete(): export imputed data 儲存和輸出其中一組完整填補數據</li>
<li>ampute(): generate missing data =&gt; Generate simulated incomplete data 產生模擬的不完整數據</li>
</ul>
<p>首先，因為MICE處理遺失值的假設為，<span style="color: #9f6ad4;">資料為隨機遺失(Missing Completely At Random, MCAR)</span>，所以在使用套件前，<span style="color: #9f6ad4;">我們先使用mice套件中的md.pattern()函數來檢視資料遺失分佈的狀況</span>。我們可以看到有106列資料是完整的，37列資料缺Ozone，5列資料缺Solar.R，2列資料同時缺Ozone&amp;Solar.R。3列資料缺Wind。Ozone總遺失數為39，Solar.R總遺失數為7，Wind總遺失數為3。</p><pre class="crayon-plain-tag">library(mice)
md.pattern(tempData2[,-1])

#     Temp Month Day Wind Solar.R Ozone_log10   
# 106    1     1   1    1       1           1  0
# 37     1     1   1    1       1           0  1
# 5      1     1   1    1       0           1  1
# 2      1     1   1    1       0           0  2
# 3      1     1   1    0       1           1  1
#        0     0   0    3       7          39 49</pre><p>我們使用VIM(Visualization and Imputation of Missing Values)套件中的aggr()函數將上述結果用更有效的視覺化圖表來檢視。</p><pre class="crayon-plain-tag">library(VIM)
aggr_plot &lt;- aggr(tempData2[,-1], col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))</pre><p></p>
<ul>
<li>左圖：各特徵變數的遺失資料比例</li>
<li>右圖：遺失狀況分佈pattern。由圖可知，有近7成資料是完整的，單單遺失Ozone的資料列高達近25%。</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-1415" src="/wp-content/uploads/2018/08/Rplot01-2.jpeg" alt="linear regression" width="700" height="586" srcset="/wp-content/uploads/2018/08/Rplot01-2.jpeg 700w, /wp-content/uploads/2018/08/Rplot01-2-300x251.jpeg 300w, /wp-content/uploads/2018/08/Rplot01-2-230x193.jpeg 230w, /wp-content/uploads/2018/08/Rplot01-2-350x293.jpeg 350w, /wp-content/uploads/2018/08/Rplot01-2-480x402.jpeg 480w" sizes="(max-width: 700px) 100vw, 700px" /></p>
<p>&nbsp;</p>
<p>而如何檢視資料的<span style="color: #9f6ad4;">遺失分佈是屬於隨機(Missing Completely At Random, MCAR)</span>，就是使用<span style="color: #9f6ad4;">VIM套件的marginplot()</span>函數。關於marginplot()的一些特性如下：</p>
<ul>
<li>一次只能畫出兩維度(變量A&amp;B)遺失分佈關係。在此，因為 Ozone和Solar.R的遺失比例最高，我們將檢視此二變數遺失分佈是否隨機</li>
<li>圖的兩軸最外側盒鬚圖：變量A中，對應變量B完整（藍色）/遺失（紅色）的分佈。</li>
<li>圖的兩軸內側散佈圖：單一變量A在的變量B發生遺失值的散佈圖。</li>
<li>圖的最左下角為變量A與變量B同時遺失的列數。其上方與右方數字則分別表單變量遺失的列數。</li>
<li>圖中央則為兩變數A&amp;B的散佈圖。</li>
<li>如果IQR區間重疊，且盒鬚圖分佈相似，則變量A與變量B的遺失值分佈為隨機，符合MCAR假設。</li>
</ul>
<p></p><pre class="crayon-plain-tag"># Solar.R &amp;amp; Ozone_log10
marginplot(tempData2[c(2,7)])</pre><p>我們從下圖可觀察到：</p>
<ul>
<li>圖左側紅色盒鬚圖為Ozone中Solar.R遺失資料點的分佈，藍色則為其餘Ozone資料點分佈。</li>
<li>圖下側紅色盒鬚圖為Solar.R中Ozone遺失資料點的分佈，藍色則為其餘Solar.R資料點分佈。</li>
<li>從圖兩側我們可以發現，IQR區間是重疊的，且紅色藍色盒鬚圖分佈相似，因此Var1(Solar.R)與Var2(Ozone)的遺失分佈為隨機的，符合MCAR假設。</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-1420" src="/wp-content/uploads/2018/08/Rplot02-3.jpeg" alt="linear regression" width="700" height="586" srcset="/wp-content/uploads/2018/08/Rplot02-3.jpeg 700w, /wp-content/uploads/2018/08/Rplot02-3-300x251.jpeg 300w, /wp-content/uploads/2018/08/Rplot02-3-230x193.jpeg 230w, /wp-content/uploads/2018/08/Rplot02-3-350x293.jpeg 350w, /wp-content/uploads/2018/08/Rplot02-3-480x402.jpeg 480w" sizes="(max-width: 700px) 100vw, 700px" /></p>
<p>&nbsp;</p>
<p>使用mice()函數進行空值填補，幾個重要參數如下：</p>
<ul>
<li>參數m表示要產生幾組填補數據集。預設為產生5組填補數據集。我們將它設為50組。</li>
<li>參數method (or meth)表示填補方法。在本範例中，因為變數皆為數值，我將使用預設的pmm(predictive mean matching)法。其他方法包括：
<ul>
<li>PMM (Predictive Mean Matching) – For any type variables</li>
<li>logreg(Logistic Regression) – For binary variables( with 2 levels)</li>
<li>polyreg(Polytomous logistic regression) – For unordered categorical/factor variables (&gt;= 2 levels)</li>
<li>polr (Proportional odds model) –  For ordered categorical variables ( &gt;= 2 levels)</li>
</ul>
</li>
<li>參數maxit表示用於計算遺失值的<span style="color: #9f6ad4;">迭代次數，預設為5</span>。本範例將迭代次數設為50。</li>
</ul>
<p></p><pre class="crayon-plain-tag"># 先選取後續會分析的變數Wind,Temp,Ozone_log10
tempData2 &lt;- tempData2[,c(3,4,7)]
tempData3 &lt;- mice(tempData2,m=50,maxit=50,meth='pmm',seed=500)</pre><p>使用summary()來看一下填補的結果。可以看到各特徵變數所使用的補值法分別為何，以及填補各目標變數(y)所投入的預測變數(x)。</p><pre class="crayon-plain-tag">summary(tempData3)

# Class: mids
# Number of multiple imputations:  50 
# Imputation methods:
#        Wind        Temp Ozone_log10 
#       "pmm"          ""       "pmm" 
# PredictorMatrix:
#             Wind Temp Ozone_log10
# Wind           0    1           1
# Temp           1    0           1
# Ozone_log10    1    1           0</pre><p>查看特定特徵值變數填補後的數值可用$imp$var。（列數為該變數有遺失值的列數，行數為各組填補數據集數，本範例為50組）。以Wind為例，遺失的三筆資料，填補後的50組數據集如下：</p><pre class="crayon-plain-tag">#&gt; tempData3$imp$Wind
#&gt; 1     2     3     4     5     6     7     8     9    10    11    12    13    14    15    16    17    18    19
#&gt; 9  0.617 0.846 0.537 0.846 0.577 0.423 0.537 0.349 0.349 0.537 0.349 0.463 0.349 0.349 0.617 0.349 0.812 0.577 0.846
#&gt; 18 0.846 0.577 0.577 0.846 0.537 0.423 0.537 0.577 0.577 0.537 0.886 0.537 0.577 0.537 0.537 0.349 0.846 0.537 0.577
#&gt; 48 0.617 0.617 0.423 0.577 0.886 0.349 0.423 0.926 0.617 0.423 0.537 0.617 0.383 0.423 0.658 0.577 0.383 0.349 0.658
#&gt; 20    21    22    23    24    25    26    27    28    29    30    31    32    33    34    35    36    37    38
#&gt; 9  0.537 0.577 0.537 0.537 1.000 0.846 0.772 0.617 0.658 0.658 0.577 0.846 0.537 0.846 0.423 0.537 0.537 0.423 0.846
#&gt; 18 0.537 0.617 0.537 0.537 0.732 0.423 0.658 0.503 0.423 0.658 0.846 0.577 0.846 0.846 0.537 0.617 0.577 0.423 0.463
#&gt; 48 0.423 0.463 0.309 0.886 0.812 0.195 0.309 0.383 0.383 0.423 0.537 0.658 0.577 0.349 0.309 0.537 0.383 0.537 0.886
#&gt; 39    40    41    42    43    44    45    46    47    48    49    50
#&gt; 9  1.000 1.000 0.846 0.577 0.617 0.537 0.537 0.349 0.503 0.812 0.617 0.537
#&gt; 18 0.577 0.537 0.886 0.732 0.537 0.537 0.537 0.537 0.846 0.846 0.846 0.537
#&gt; 48 0.349 0.309 0.537 0.383 0.658 0.349 0.812 0.886 0.349 0.503 0.383 0.423</pre><p></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>4. 建立線性模型 linear regression model</h3>
<p>最後我們來使用處理好的數據來建立回歸模型。</p>
<p>4-1. Pooling: 使用剛剛填補的50組的資料建模，並摘要綜合50組模型的效果。建模結果顯示預測變數皆為顯著。(p-value &lt; 0.05)</p><pre class="crayon-plain-tag"># 使用不同的datasets(m=50)分別建模，並綜合摘要結果
modelFit1 &lt;- with(tempData3,lm(Ozone_log10 ~ Temp + Wind))
summary(pool(modelFit1))
# 根據p-value，預測變數Temp &amp; Wind 都有顯著

#                estimate  std.error statistic       df      p.value
# (Intercept)  0.06810694 0.02245463  3.033091 67.91444 3.255054e-03
# Temp         0.19479181 0.02223289  8.761425 80.70210 2.420286e-13
# Wind        -0.10332966 0.02533499 -4.078536 64.42491 1.056795e-04</pre><p>4-2. 使用complete()隨機挑選一組填補後的dataset來建模(預設是第一組，但也可以透過參數調整)。發現模型配適結果也是顯著的(p-value &lt; 0.05)。</p><pre class="crayon-plain-tag">completedData &lt;- complete(tempData3,1) #預設是挑第一組，也可以更改第二個參數
modelFit2 &lt;- lm(Ozone_log10 ~ Temp + Wind, completedData)
summary(modelFit2)

# Call:
# lm(formula = Ozone_log10 ~ Temp + Wind, data = completedData)
# 
# Residuals:
#       Min        1Q    Median        3Q       Max 
# -0.108651 -0.030307 -0.004117  0.034658  0.142650 
# 
# Coefficients:
#               Estimate Std. Error t value Pr(&gt;|t|)    
#   (Intercept)  0.06882    0.01713   4.017 9.29e-05 ***
#   Temp         0.18569    0.01784  10.409  &lt; 2e-16 ***
#   Wind        -0.10191    0.01883  -5.412 2.42e-07 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Residual standard error: 0.04581 on 150 degrees of freedom
# Multiple R-squared:  0.6041,	Adjusted R-squared:  0.5988 
# F-statistic: 114.4 on 2 and 150 DF,  p-value: &lt; 2.2e-16</pre><p><span style="text-decoration: underline;">評估<strong>係數(Coefficients)</strong>指標說明：</span></p>
<ol>
<li>Estimate: 表示各變數的估計係數。</li>
<li>Std. Error (SE) of Estimate: 估計標準誤，用來衡量估計回歸式的精準度。表示觀測值與回歸線間的分散或離散程度。<br />
\(\beta_{0}, \beta_{1}\)計算方式分別如下：<br />
$$SE(\beta_{0})^2 = \sigma^2[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n (x_{i} &#8211; \bar{x})^2}],\space \space SE(\beta_{1})^2 = \frac{\sigma^2}{\sum_{i=1}^n (x_{i} &#8211; \bar{x})^2}$$<br />
其中，\(\sigma^2 = Var(\epsilon)\)表示隨機誤差的變異數。<br />
我們可以透過標準誤SE來計算95%信心水準係數區間如下：<br />
<pre class="crayon-plain-tag">confint(object = modelFit2,level = 0.95)
#                   2.5 %      97.5 %
# (Intercept)  0.03497014  0.10267304
# Temp         0.15044504  0.22094442
# Wind        -0.13912079 -0.06470414</pre><br />
從以上數據來說，在95%信心水準下\(\beta{1}(Temp)\)的係數區間[0.15,0.22]並沒有包括0，也因此我們可以推論說，當溫度升高100度時，log(臭氧濃度)會增加15-22個單位。</li>
<li>t-value: 衡量\(\beta{1}\)距離0有多少個標準差。t-value越大，p-value越小，表示該變數與目標變數關係顯著。t-value計算公式如下：<br />
$$t = \frac{\beta_{1}-0}{SE(\beta_{1})}$$</li>
<li>Pr(&gt;|t|): 兩個預測變數p-value都小於0.05(95%信心水準)，表示與目標變數有顯著關係。</li>
</ol>
<p><span style="text-decoration: underline;">評估<strong>模型正確性(Model Accuracy)</strong>指標說明：</span></p>
<p>我們可以透過以下幾個數值指標來了解模型配適資料的適合度(goodness-of-fit)。</p>
<ol>
<li><strong>Residual standard error (RSE)</strong>：估計誤差標準差(estimate the standard deviation of error \(\epsilon\))。此模型誤差標準差為0.0458 (越小越好)，表示平均來說，實際觀測到的臭氧濃度會偏離回歸預測值約0.0458個單位(這是取log轉換後的數值）。<br />
誤差標準差衡量的是平均觀測值偏離回歸的程度。計算公式如下：<br />
$$RSE = \sqrt{\frac{1}{n-2} \sum_{i=1}^n (y_{i} &#8211; \hat{y}_{i})^2}$$<br />
RSE除了顯示在sumary(modelFit2)的底端，亦可透過以下sigma()函數取得。<br />
<pre class="crayon-plain-tag">sigma(modelFit2)
# [1] 0.04580728</pre><br />
但畢竟RSE評估模型配適資料合適度是用<span style="text-decoration: underline;">實際觀測值偏離回歸預測值多少個Y單位</span>，無從得知構成模型配適</li>
<li><strong>R-squared (判定係數)</strong>: 衡量的是預測變數共解釋了多少變異。\(R^2\)是一個介於0到1的數值，越大越好，且數值不受Y單位所影響。\(R^2\)計算公式如下：<br />
$$R^2 = 1 &#8211; \frac{SSR}{SST} = 1 &#8211; \frac{\sum_{i=1}^n (y_{i} &#8211; \hat{y}_{i})^2}{\sum_{i=1}^n (y_{i} &#8211; \bar{y}_{i})^2}$$<br />
除了可以從sumary(modelFit2)最後幾行得知\(R^2\)的資訊，我們亦可透過以下<span style="text-decoration: underline;">modelr套件中的rsqaure()函數</span>取得。<br />
<pre class="crayon-plain-tag">rsquare(modelFit2, data = completedData)
# [1] 0.604058</pre><br />
此模型的預測變數(Temp &amp; Wind)共解釋了60%目標變數(Ozone)的變異。</li>
<li><strong>Adjusted R-squared</strong>: 調整後的\(R^2\)有綜合考量變數總數(模型複雜度)的懲罰效果。此模型adj.\(R^2\)為 0.599(會比\(R^2\)來的小)，近似60%。</li>
<li><strong>F-statistic</strong>: <span style="text-decoration: underline;"><span style="color: #9f6ad4; text-decoration: underline;">F-statistic test是用來檢測是否模型中至少有一預測變數係數不為0(當模型為複回歸時，此檢測結果特別重要)</span></span>。F統計量的計算公式如下：<br />
$$F = \frac{TSS &#8211; RSS/p}{RSS/n-p-1}$$<br />
F統計量越大，p-value越顯著(p&lt;0.05)。<br />
<span style="text-decoration: underline;">此模型的F統計量為114</span><span style="color: #9f6ad4;"><span style="text-decoration: underline;">(越大越好)，對應的p-value為 p &lt; 2.2e-16</span>。</span></li>
</ol>
<h3>5. linear regression 線性回歸模型適合度診斷</h3>
<h4>5-1. 基本假設檢定</h4>
<p>檢視模型配適度一個重要的步驟，就是透過<span style="text-decoration: underline;">視覺化<span style="color: #9f6ad4; text-decoration: underline;">殘差分佈</span></span>，來檢視模型基本假設是否皆符合。</p><pre class="crayon-plain-tag"># Reset par to the default values at startup
dev.off()
# 5.1 是否符合線性迴歸之基本假設：殘差變異數均一性
plot(modelFit2, which = 1) # 殘差變異分佈不在水平線上，且不接近零
# 5.2 殘差是否為常態分佈
plot(modelFit2, which = 2) # 殘差值分佈不在45度線上
# 5.3 標準化殘差分佈亦不成水平線分佈
plot(modelFit2, which = 3)
# 5.4 沒有對模型影響力過強的資料點（如果有的話，會出現在虛線cook's distance之外）
plot(modelFit2, which = 5)</pre><p>由下圖可以發現：</p>
<ul>
<li>左上：殘差變異數不符合均一性。(理想上，希望紅色線條要接近水平線並接近0)</li>
<li>右上：殘差分佈不符合常態。（理想上，希望殘差值分佈不在45度線上）</li>
<li>左下：標準化殘差變異數不符合均一性。(理想上，希望紅色線條要接近水平線並接近0)</li>
<li>右下：檢視是否有對模型影響力過高的資料點。（若是有，資料點將會超過cook&#8217;s distance虛線外）</li>
</ul>
<p>綜合以上，會發現線性模型似乎不適合用來配適此組資料。</p>

<a href='/wp-content/uploads/2018/08/Rplot16.jpeg'><img width="800" height="787" src="/wp-content/uploads/2018/08/Rplot16.jpeg" class="attachment-full size-full" alt="Rplot16" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot16.jpeg 800w, /wp-content/uploads/2018/08/Rplot16-300x295.jpeg 300w, /wp-content/uploads/2018/08/Rplot16-768x756.jpeg 768w, /wp-content/uploads/2018/08/Rplot16-75x75.jpeg 75w" sizes="(max-width: 800px) 100vw, 800px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot17.jpeg'><img width="800" height="787" src="/wp-content/uploads/2018/08/Rplot17.jpeg" class="attachment-full size-full" alt="Rplot17" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot17.jpeg 800w, /wp-content/uploads/2018/08/Rplot17-300x295.jpeg 300w, /wp-content/uploads/2018/08/Rplot17-768x756.jpeg 768w, /wp-content/uploads/2018/08/Rplot17-75x75.jpeg 75w" sizes="(max-width: 800px) 100vw, 800px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot18.jpeg'><img width="800" height="787" src="/wp-content/uploads/2018/08/Rplot18.jpeg" class="attachment-full size-full" alt="Rplot18" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot18.jpeg 800w, /wp-content/uploads/2018/08/Rplot18-300x295.jpeg 300w, /wp-content/uploads/2018/08/Rplot18-768x756.jpeg 768w, /wp-content/uploads/2018/08/Rplot18-75x75.jpeg 75w" sizes="(max-width: 800px) 100vw, 800px" /></a>
<a href='/wp-content/uploads/2018/08/Rplot19.jpeg'><img width="800" height="787" src="/wp-content/uploads/2018/08/Rplot19.jpeg" class="attachment-full size-full" alt="Rplot19" loading="lazy" srcset="/wp-content/uploads/2018/08/Rplot19.jpeg 800w, /wp-content/uploads/2018/08/Rplot19-300x295.jpeg 300w, /wp-content/uploads/2018/08/Rplot19-768x756.jpeg 768w, /wp-content/uploads/2018/08/Rplot19-75x75.jpeg 75w" sizes="(max-width: 800px) 100vw, 800px" /></a>

<h4>5-2. gvlma package</h4>
<p>另外，也可以使用gvlma套件來檢定模型是否符合假設。可以發現有部分線性迴歸模型的基礎假設是不成立的。(不過要注意的是，gvlma套件結果相關說明的資料不是很充足。)</p><pre class="crayon-plain-tag"># Check Assumptions Automatically
library(gvlma)
# mod &lt;- lm(Ozone ~ Solar.R, data=airquality)
gvlma(modelFit2)
summary(gvlma(modelFit2))
# 但網路上相關資源目前並沒有太多有關gvlma套件產生結果的說明

# Call:
# lm(formula = Ozone_log10 ~ Temp + Wind, data = completedData)
# 
# Residuals:
#       Min        1Q    Median        3Q       Max 
# -0.108651 -0.030307 -0.004117  0.034658  0.142650 
# 
# Coefficients:
#               Estimate Std. Error t value Pr(&gt;|t|)    
#   (Intercept)  0.06882    0.01713   4.017 9.29e-05 ***
#   Temp         0.18569    0.01784  10.409  &lt; 2e-16 ***
#   Wind        -0.10191    0.01883  -5.412 2.42e-07 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Residual standard error: 0.04581 on 150 degrees of freedom
# Multiple R-squared:  0.6041,	Adjusted R-squared:  0.5988 
# F-statistic: 114.4 on 2 and 150 DF,  p-value: &lt; 2.2e-16
# 
# 
# ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
# USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
#   Level of Significance =  0.05 
# 
# Call:
#   gvlma(x = modelFit2) 
# 
#                          Value   p-value                   Decision
#   Global Stat        31.296325 2.664e-06 Assumptions NOT satisfied!
#   Skewness            1.321357 2.503e-01    Assumptions acceptable.
#   Kurtosis            0.004069 9.491e-01    Assumptions acceptable.
#   Link Function      28.858171 7.788e-08 Assumptions NOT satisfied!
#   Heteroscedasticity  1.112728 2.915e-01    Assumptions acceptable.</pre><p></p>
<h4>5-3. shapiro test常態分佈檢定</h4>
<p>另外，我們亦使用shapiro test 來看鎖定變數是否成常態分佈(H0: 分佈等於常態分佈)。可以發現除了Wind符合常態分配，另外兩個變數都離常態分佈有些距離。<span style="color: #9f6ad4;">因此除非進一步將變數有效轉換以符合常態，否則不適合用線性迴規模性來進行資料配適</span>。</p><pre class="crayon-plain-tag">options(scipen = 999)
normality_test &lt;- apply(X = completedData,MARGIN = 2,FUN = function(x) round(shapiro.test(x)$p.value, digits = 4))
normality_test
#   Wind        Temp Ozone_log10 
# 0.0928      0.0093      0.0000</pre><p></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>6. 模型驗證Validation</h3>
<h4>6-1. 常見模型效果評估指標</h4>
<table style="width: 100%; border-collapse: collapse; height: 323px;" border="1">
<tbody>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" bgcolor="#ecd5ed"><strong>STATISTIC</strong></td>
<td style="width: 52.232%; height: 23px;" bgcolor="#ecd5ed"><strong>CRITERION</strong></td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;R-Squared&quot;}">R-Squared</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;R-Squared&quot;}">Higher the better (&gt; 0.70)。表示解釋變異量佔總變異量的百分比。</td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;Adj R-Squared&quot;}">Adj R-Squared</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;Adj R-Squared&quot;}">Higher the better。表示調整後(對模型複雜度加入懲罰)解釋變異量佔總變異量的百分比。</td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;F-Statistic&quot;}">F-Statistic</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;F-Statistic&quot;}">Higher the better。</td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;Std. Error&quot;}">Std. Error (標準誤)</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;Std. Error&quot;}">Closer to zero the better。 (表抽樣結果與母體的偏差)</td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;t-statistic&quot;}">t-statistic</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;t-statistic&quot;}">Should be greater 1.96 for p-value to be less than 0.05</td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;AIC&quot;}">AIC</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;AIC&quot;}">Lower the better</td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;BIC&quot;}">BIC</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;BIC&quot;}">Lower the better</td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;Mallows cp&quot;}">Mallows cp</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;Mallows cp&quot;}">Should be close to the number of predictors in model</td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;MAPE (Mean absolute percentage error)&quot;}">MAPE (Mean absolute percentage error)</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;MAPE (Mean absolute percentage error)&quot;}">Lower the better</td>
</tr>
<tr style="height: 23px;">
<td style="width: 46.903%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;MSE (Mean squared error)&quot;}">MSE (Mean squared error)</td>
<td style="width: 52.232%; height: 23px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;MSE (Mean squared error)&quot;}">Lower the better</td>
</tr>
<tr style="height: 70px;">
<td style="width: 46.903%; height: 70px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;Min_Max Accuracy =&gt; \nmean(min(actual, predicted)/\nmax(actual, predicted))&quot;}">Min_Max Accuracy =&gt;<br />
mean(min(actual, predicted)/<br />
max(actual, predicted))</td>
<td style="width: 52.232%; height: 70px;" data-sheets-value="{&quot;1&quot;:2,&quot;2&quot;:&quot;Min_Max Accuracy =&gt; \nmean(min(actual, predicted)/\nmax(actual, predicted))&quot;}">Higher the better</td>
</tr>
</tbody>
</table>
<h4>6-2. 模型驗證(80%Training20%Testing)</h4>
<p>將資料隨機區分為訓練資料集(80%)與測試資料集(20%)</p><pre class="crayon-plain-tag"># 將資料切分為訓練集與測試集
set.seed(1234)
trainingRowIndex &lt;- sample(1:nrow(completedData),0.7*nrow(completedData))
trainingData &lt;- completedData[trainingRowIndex,]
testData &lt;- completedData[-trainingRowIndex,]</pre><p>使用訓練及資料建置模型 &amp; 使用測試集資料預測結果。並摘要訓練的模型效果summary(mod)。</p><pre class="crayon-plain-tag"># 使用訓練及資料建置模型 &amp; 使用測試集資料預測結果
mod &lt;- lm(Ozone_log10 ~ ., data=trainingData, na.action = na.omit)
predictY &lt;- predict(object = mod, newdata = testData)

summary(mod)

# Call:
# lm(formula = Ozone_log10 ~ ., data = trainingData, na.action = na.omit)
# 
# Residuals:
#       Min        1Q    Median        3Q       Max 
# -0.099618 -0.031069 -0.003893  0.032104  0.138297 
# 
# Coefficients:
#               Estimate Std. Error t value        Pr(&gt;|t|)    
#   (Intercept)  0.08839    0.02154   4.103 0.0000812697310 ***
#   Wind        -0.11489    0.02344  -4.901 0.0000035281189 ***
#   Temp         0.16479    0.02187   7.534 0.0000000000188 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Residual standard error: 0.04639 on 104 degrees of freedom
# Multiple R-squared:  0.5734,	Adjusted R-squared:  0.5652 
# F-statistic: 69.89 on 2 and 104 DF,  p-value: &lt; 0.00000000000000022</pre><p>檢視模型預測的效果。</p><pre class="crayon-plain-tag">actuals_predicts &lt;- data.frame(cbind(actuals = testData$Ozone_log10, predicteds = predictY)])</pre><p>計算真實數據與預測數據的相關係數：越高越好。</p><pre class="crayon-plain-tag">cor(actuals_predicts,use="complete.obs") # 0.8136252
cor(actuals_predicts,use="pairwise.complete.obs") # 0.8136252</pre><p>計算AIC, BIC =&gt; 越低越好。</p><pre class="crayon-plain-tag">AIC(mod)
# [1] -348.5126
BIC(mod)
# [1] -337.8213</pre><p>計算模型預測正確率min_max_accuracy： 70.4% = &gt; 越高越好。</p><pre class="crayon-plain-tag">min_max_accuracy &lt;- mean(apply(actuals_predicts, 1, min) / apply(actuals_predicts, 1, max))
min_max_accuracy
# [1] 0.7036471</pre><p>計算模型預測錯誤率 MAPE (Mean absolute percentage error)： 56% =&gt; 越低越好。</p><pre class="crayon-plain-tag">mape &lt;- mean(abs((actuals_predicts$predicteds - actuals_predicts$actuals))/actuals_predicts$actuals)
mape
# [1] 0.5598302</pre><p></p>
<h4>6-3. 加強版模型驗證: K-fold Cross Validation</h4>
<p>使用k-fold交叉驗證。</p><pre class="crayon-plain-tag">library(DAAG)
cvResults &lt;- suppressWarnings(CVlm(data=completedData, form.lm=Ozone_log10 ~ ., m=5, dots=FALSE, seed=29, legend.pos="topleft", main="Small symbols are predicted values while bigger ones are actuals.")) # performs the CV</pre><p>查看交叉驗證的平均平方誤差(MSE):越低越好。</p><pre class="crayon-plain-tag"># MSE (Mean squared error): Lower the better
attr(cvResults, 'ms')  
# [1] 0.0105</pre><p>交叉5次模型配適結果圖如下。小圖示為模型預測值，大圖示則為真實資料值。<span style="color: #9f6ad4;">可以發現第五次模型擬和的結果與其他四次有較大差異</span>。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-738" src="/wp-content/uploads/2018/08/Rplot20.jpeg" alt="linear regression" width="800" height="787" srcset="/wp-content/uploads/2018/08/Rplot20.jpeg 800w, /wp-content/uploads/2018/08/Rplot20-300x295.jpeg 300w, /wp-content/uploads/2018/08/Rplot20-768x756.jpeg 768w, /wp-content/uploads/2018/08/Rplot20-75x75.jpeg 75w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<h3>7. 結論</h3>
<p>我們可以發現，由於airquality<span style="color: #9f6ad4;">資料型態不接近理想的常態分佈，故使用Linear Regression 線性迴歸模型做資料配適不是很合適<span style="color: #333333;">(根據「回歸模型適合度診斷」)</span></span>。如果遇到資料分配非常態時，建議可以考慮其他模型如<a href="/%e3%80%8c%e7%b5%b1%e8%a8%88%e2%80%a2r%e8%aa%9e%e8%a8%80%e2%80%a2%e6%b1%ba%e7%ad%96%e6%a8%b9%e3%80%8ddecision-tree-classification-and-regression-tree-cart/" target="_blank" rel="noopener noreferrer">決策樹</a>（不受限母體分配）。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<hr />
<p>更多線性回歸模型應用：</p>
<ol>
<li><a href="/app-mobile-ad-revenue-admob-%e8%a1%8c%e5%8b%95%e5%bb%a3%e5%91%8a%e6%94%b6%e5%85%a5/" target="_blank" rel="noopener noreferrer">開發一個免費App能賺多少錢? 靠 AdMod 廣告月收3萬實例分享</a></li>
</ol>
<p><strong>更多統計模型學習筆記連結：</strong></p>
<ol>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, Random Forest</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>參考文章連結：</p>
<ol>
<li><a href="http://r-statistics.co/Linear-Regression.html">Linear Regression</a></li>
<li><a href="http://rpubs.com/Nitika/linearRegression_Airquality">Linear Regression using AirQuality Dataset</a></li>
<li><a href="https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/">Imputing missing data with R; MICE package</a></li>
<li><a href="http://r-statistics.co/Assumptions-of-Linear-Regression.html">Assumptions of Linear Regression</a></li>
<li><a href="https://ademos.people.uic.edu/Chapter12.html">Chapter 12: Regression: Basics, Assumptions, &amp; Diagnostics</a></li>
<li><a href="https://view.officeapps.live.com/op/view.aspx?src=http://cyber.mcu.edu.tw/%E7%AE%A1%E7%90%86%E5%AD%B8%E9%99%A2/%E7%B5%B1%E8%A8%88%E5%AD%B8%E9%A1%9E/%E8%A4%87%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90%E4%B8%80/%E8%A4%87%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90%E4%B8%80.ppt">複回歸分析</a></li>
<li><a href="http://uc-r.github.io/linear_regression">Linear Regression (2)</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/feed/</wfw:commentRss>
			<slash:comments>3</slash:comments>
		
		
			</item>
	</channel>
</rss>
