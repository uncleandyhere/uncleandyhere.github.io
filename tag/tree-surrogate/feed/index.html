<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>tree surrogate &#8211; 果醬珍珍•JamJam</title>
	<atom:link href="/tag/tree-surrogate/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>健忘女孩Jam的學習筆記和生活雜記</description>
	<lastBuildDate>Sun, 14 Apr 2019 15:01:36 +0000</lastBuildDate>
	<language>zh-TW</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Tree Surrogate &#124; Tree Surrogate Variables in CART &#124; R 統計</title>
		<link>/decision-tree-surrogate-in-cart/</link>
					<comments>/decision-tree-surrogate-in-cart/#respond</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Sun, 02 Sep 2018 14:33:42 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[tree surrogate]]></category>
		<category><![CDATA[樹替代]]></category>
		<category><![CDATA[決策樹空值填補]]></category>
		<category><![CDATA[空值填補]]></category>
		<category><![CDATA[空值預測]]></category>
		<guid isPermaLink="false">/?p=1039</guid>

					<description><![CDATA[<p>Tree Surrogate 樹替代是決策樹CART演算法裡面內建的處理遺失值的一個很棒的演算法。只要資料列有目標變數搭配只少一個未遺失的特徵值，即可進行遺失值 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-surrogate-in-cart/">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Tree Surrogate 樹替代是決策樹CART演算法裡面內建的處理遺失值的一個很棒的演算法。只要資料列有目標變數搭配只少一個未遺失的特徵值，即可進行遺失值的預測，計算預測遺失值的surrogate variable list來作為替代值順位。</p>
<h3>CART決策樹模型遺失值預測法 Tree Surrogate 簡介</h3>
<ul>
<li>在使用rpart()(Recursive Partitioning Tree)建模時，我們不用特別針對遺失值進行處理。</li>
<li>只要觀測值有y值，和至少一個未遺失的x值，則可投入模型。</li>
<li>在遇到觀測資料有遺失值的情況，會使用Surrogate Variable List中的代理變數順位最高者來預測遺失變數值</li>
</ul>
<h3>Tree Surrogate Variables</h3>
<table>
<colgroup>
<col />
<col />
<col />
<col />
<col /></colgroup>
<tbody>
<tr bgcolor="#ddd">
<td class="">
<div>ID</div>
</td>
<td class="">
<div>A</div>
</td>
<td class="">
<div>B</div>
</td>
<td class="">
<div><span style="color: #0000ff;">C</span></div>
</td>
<td class="">
<div>T</div>
</td>
</tr>
<tr>
<td class="">
<div>1</div>
</td>
<td class="">
<div>T</div>
</td>
<td class="">
<div>?</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>Y</div>
</td>
</tr>
<tr>
<td class="">
<div>2</div>
</td>
<td class="">
<div>?</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>?</div>
</td>
<td class="">
<div>N</div>
</td>
</tr>
<tr>
<td class="">
<div>3</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>Y</div>
</td>
</tr>
<tr>
<td class="">
<div>4</div>
</td>
<td class="">
<div>F</div>
</td>
<td class="">
<div>T</div>
</td>
<td class="">
<div>T</div>
</td>
<td class="">
<div>N</div>
</td>
</tr>
</tbody>
</table>
<p>其中：</p>
<ul>
<li>T欄為目標變數</li>
<li>A,B,C欄為預測變數</li>
<li>而根據Gini Index，C會被選中為決策樹變數</li>
</ul>
<p>對於C欄位有遺失值的觀測資料列，CART決策樹會使用計算出來的代理變數(Surrogate Variables)來預測C值。</p>
<p>計算C欄位Surrogate Variables的方法如下：依據計算除了C以外的預測變數作為Surrogate Variables的錯誤率並比較排序。</p>
<p>Step 1: C is missing but B has value</p>
<div>首先計算B作為Surrogate Variable的錯誤率：</div>
<div></div>
<table>
<colgroup>
<col />
<col /></colgroup>
<tbody>
<tr bgcolor="#ddd">
<td class="">
<div>B</div>
</td>
<td class="">
<div>C</div>
</td>
</tr>
<tr>
<td class="">
<div>F</div>
</td>
<td class="">
<div>F</div>
</td>
</tr>
<tr>
<td class="">
<div>T</div>
</td>
<td class="">
<div>T</div>
</td>
</tr>
</tbody>
</table>
<div></div>
<div>使用決策樹模型產生用B預測C的邏輯：</div>
<ul>
<li>
<div>B=T -&gt; C=T</div>
</li>
<li>
<div>B=F -&gt; C=F</div>
</li>
<li>
<div>Error rate = 0%</div>
</li>
</ul>
<div>
<div>Step 2: C is missing but A has value</div>
</div>
<div>計算Ａ作為Surrogate Variable預測C值得錯誤率：</div>
<div></div>
<div>
<table>
<colgroup>
<col />
<col /></colgroup>
<tbody>
<tr bgcolor="#ddd">
<td class="">A</td>
<td class="">C</td>
</tr>
<tr>
<td class="">
<div>T</div>
</td>
<td class="">
<div>F</div>
</td>
</tr>
<tr>
<td class="">
<div>F</div>
</td>
<td class="">
<div>F</div>
</td>
</tr>
<tr>
<td class="">
<div>F</div>
</td>
<td class="">
<div>T</div>
</td>
</tr>
</tbody>
</table>
<p>使用決策樹模型產生用A預測C的邏輯：</p>
<ul>
<li>
<div>A=T -&gt; C=F</div>
</li>
<li>
<div>A=F -&gt; C=T</div>
</li>
<li>
<div>Erro rate = 33%</div>
</li>
</ul>
<div>
<div>總結以上，預測C欄位各Surrogate Variables的預測錯誤率分別為：</div>
</div>
<ul>
<li>
<div>A 有33% 的預測錯誤率</div>
</li>
<li>
<div>B 有0％的預測錯誤率</div>
</li>
<li>
<div>Naive隨機則有50%的預測錯誤率 (blind rule)</div>
</li>
<li>若錯誤率比Naive隨機預測結果還高者，會從Surrogate Variables list給移除</li>
</ul>
<div>因此，Surrogate Variables的順位為：B,A,Naive</div>
<ul>
<li>
<div>以ID2為例，C值會被指派為（根據B）F</div>
</li>
</ul>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<p>更多rpart()函數對surrogate variable說明可參考：<a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf" target="_blank" rel="noopener noreferrer">https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf</a></p>
<hr />
<p>更多統計模型筆記連結：</p>
<p><a href="/linear_regression_using_airquality_dataset/" target="_blank" rel="noopener noreferrer">線性回歸模型</a></p>
<p><a href="/logistic_regression_part1/" target="_blank" rel="noopener noreferrer">羅吉斯回歸模型part1</a></p>
<p><a href="/logistic_regression_part2/" target="_blank" rel="noopener noreferrer">羅吉斯回歸模型part2</a></p>
<p style="text-align: left;"><a href="/decision_tree_cart/" target="_blank" rel="noopener noreferrer">決策樹/隨機森林</a></p>
<p><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">random forests</a></p>
<p><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines (GBMs)</a></p>
<p><a href="/hierarchical-clustering/" target="_blank" rel="noopener noreferrer">階層式分群法</a></p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">分割式分群法</a></p>
</div>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-surrogate-in-cart/">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/decision-tree-surrogate-in-cart/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Decision Tree 決策樹 &#124; CART, Conditional Inference Tree, RandomForest</title>
		<link>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/</link>
					<comments>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Fri, 31 Aug 2018 06:08:33 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[cart]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[prune]]></category>
		<category><![CDATA[random forest]]></category>
		<category><![CDATA[rpart]]></category>
		<category><![CDATA[tree surrogate]]></category>
		<category><![CDATA[決策樹]]></category>
		<category><![CDATA[隨機森林]]></category>
		<guid isPermaLink="false">/?p=1026</guid>

					<description><![CDATA[<p>Decision Tree 決策樹模型是一個不受資料分配限制的模型，模型結果以樹狀呈現，簡單易懂，解釋性極高，且模型同時兼具變數挑選與遺失值填補的機制，並能處理 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Decision Tree 決策樹模型是一個不受資料分配限制的模型，模型結果以樹狀呈現，簡單易懂，解釋性極高，且模型同時兼具變數挑選與遺失值填補的機制，並能處理分類與回歸問題，是一個廣泛被使用的模型。另外，以決策樹為基礎集成學習而成的隨機森林，更能有效降低模型的錯誤率與並解決過度配適等問題的著名機器學習法之一。</p>
<h3>Decision Tree 決策樹簡介</h3>
<ul>
<li><span style="color: #9f6ad4;">決策樹</span>是一個多功能的機器學習演算法，不僅可以進行<span style="color: #9f6ad4;">分類</span>亦可進行<span style="color: #9f6ad4;">回歸</span>任務。</li>
<li>可以配適複雜的資料集，是個強大的演算法。</li>
<li>屬於<span style="color: #9f6ad4;">無母數回歸</span>方法(<span style="color: #9f6ad4;">non-parametric</span>)：對資料長相的要求不像回歸模型（有母數法，parametric）嚴格，不需要假設資料的線性關係與常態分佈。<br />
(無母數介紹請參考)</li>
<li>決策樹演算法也是隨機森林演算法的基礎(隨機森林也是至今具潛力的演算法之一)。</li>
<li>有諸多演算法，常見的包括CART, CHAID。</li>
<li>決策樹可以用來建立非線性模型，通常被用在迴歸，也可以用在對於遞迴預測變數最二元分類。</li>
</ul>
<h4>補充-無母數統計：</h4>
<ol>
<li>適用於母體分佈情況未知、小樣本、母體分佈不為常態或不易轉換為常態，對資料長相的要求小。</li>
<li>無母數統計推論時所使用的<span style="color: #9f6ad4;">樣本統計量</span>分配通常與母體分配無關，不需要使用樣本統計量去推論母體中位數、適合度、獨立性、隨機性。</li>
<li>無母數統計又稱作「不受分配限制統計法」(distribution-free)。</li>
</ol>
<h4>常見的決策樹演算法比較</h4>
<table style="height: 116px; width: 100%; border-collapse: collapse; background-color: #ffffff;" border="1">
<tbody>
<tr style="height: 23px;" bgcolor="#ddd">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">演算法</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">資料屬性</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">分割規則</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">修剪樹規則</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">ID3</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;">
<div><span style="color: #000000;">Entropy,</span></div>
<div><span style="color: #000000;">Gain Ratio</span></div>
</td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Predicted Error Rate</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">C4.5</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Gain Ratio</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Predicted Error Rate</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">CHAID</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;">
<div><span style="color: #000000;">Chi-Square Test</span></div>
</td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">No Pruning</span></td>
</tr>
<tr style="height: 24px;">
<td style="width: 25%; height: 24px;"><span style="color: #000000;">CART</span></td>
<td style="width: 25%; height: 24px;"><span style="color: #000000;">離散與連續型</span></td>
<td style="width: 25%; height: 24px;"><span style="color: #000000;">Gini Index</span></td>
<td class="" style="width: 25%; height: 24px;">
<div><span style="color: #000000;">Entire Error Rate</span></div>
<div><span style="color: #000000;">(Training and Predicted)</span></div>
</td>
</tr>
</tbody>
</table>
<h4>決策樹挑選變數常用的測量值</h4>
<p>常見的資訊量（衡量資料<span style="color: #9f6ad4;">純度</span>）：</p>
<ul>
<li><strong>Entropy (熵)</strong>:<br />
$$I_{H}(t)=-\sum_{i=1}^c p(i|t) \log_{2} p(i|t)$$<br />
其中，H代表Homogeneity(同質性)。<br />
<span style="color: #9f6ad4;">當Entropy=0表示completely homogeneous(pure)，而當Entropy=1則表示資料為50%-50%之組成，是不純的</span><span style="color: #9f6ad4;">(impurity)</span>。</li>
<li><strong>Gini Impurity (Gini不純度)</strong>:<br />
$$I_{G}(t) =\sum_{i=1}^c p(i|t)(1-p(i|t)) = 1-\sum_{i=1}^c p(i|t)^2$$<br />
其中，G則代表Gini Impurity。</li>
</ul>
<p>決定切割點的測量值：</p>
<ul>
<li><strong>Information Gain (資訊增益)</strong>: 則衡量節點切割前後資料純度的變化。<span style="color: #9f6ad4;">節點的選擇，當選IG值越大的變數為佳</span>。<br />
$$IG = Info(D) &#8211; Info_{A}(D)$$<br />
其中，\(Info(D)\)為原始資料純度，而\(Info_{\space A}(D)\)則表示使用A規則切割後的資料純度。<br />
$$Info_{\space A}(D)=\sum_{j=1}^m \frac{N_{j}}{N_{p}} Info(D_{j})$$<br />
當m=2，即為二元分類時，<br />
$$IG = Info(D) &#8211; \frac{N_{left}}{N_{p}} Info(D_{left}) &#8211; \frac{N_{right}}{N_{p}} Info(D_{right})$$</li>
</ul>
<h4>資料與分析問題</h4>
<ul>
<li>Data: 鐵達尼資料集包含13個變數與1309筆觀測值。</li>
<li>Problem: 我們想分析與預測具有什麼樣特徵的乘客，比較有機會在冰山撞船後可以存活下來。</li>
<li>Method: 使用CART(Classification and Regression Tree)決策樹模型來找出重要解釋變數。</li>
</ul>
<h4>訓練與視覺化決策樹，我們將進行以下步驟：</h4>
<ol>
<li>載入資料</li>
<li>資料探勘</li>
<li>資料前處理</li>
<li>產生訓練與測試資料集</li>
<li>建置模型</li>
<li>進行預測</li>
<li>衡量模型表現</li>
<li>修剪樹(Post-pruning)</li>
<li>K-Fold Cross Validation</li>
<li>模型比較(1)：條件推論樹(Conditional Inference Tree)</li>
<li>模型比較(2)：隨機森林(Random Forest)</li>
</ol>
<h3>Step1: 載入資料</h3>
<p></p><pre class="crayon-plain-tag"># 從google drive shareable link 讀入csv檔案
# https://drive.google.com/file/d/1S7S-siBGkMR3YUVAbaTkfS1CxOji_Ngd/view?usp=sharing
id &lt;- "1S7S-siBGkMR3YUVAbaTkfS1CxOji_Ngd" # google file ID
inputData &lt;- read.csv(sprintf("https://docs.google.com/uc?id=%s&amp;export=download", id))
head(inputData)

#   pclass survived                                            name    sex     age sibsp parch ticket     fare
# 1      1        1                   Allen, Miss. Elisabeth Walton female 29.0000     0     0  24160 211.3375
# 2      1        1                  Allison, Master. Hudson Trevor   male  0.9167     1     2 113781 151.5500
# 3      1        0                    Allison, Miss. Helen Loraine female  2.0000     1     2 113781 151.5500
# 4      1        0            Allison, Mr. Hudson Joshua Creighton   male 30.0000     1     2 113781 151.5500
# 5      1        0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000     1     2 113781 151.5500
# 6      1        1                             Anderson, Mr. Harry   male 48.0000     0     0  19952  26.5500
#     cabin embarked                       home.dest
# 1      B5        S                    St Louis, MO
# 2 C22 C26        S Montreal, PQ / Chesterville, ON
# 3 C22 C26        S Montreal, PQ / Chesterville, ON
# 4 C22 C26        S Montreal, PQ / Chesterville, ON
# 5 C22 C26        S Montreal, PQ / Chesterville, ON
# 6     E12        S                    New York, NY

tail(inputData)
#      pclass survived                      name    sex  age sibsp parch ticket    fare cabin embarked home.dest
# 1304      3        0     Yousseff, Mr. Gerious   male   NA     0     0   2627 14.4583              C          
# 1305      3        0      Zabour, Miss. Hileni female 14.5     1     0   2665 14.4542              C          
# 1306      3        0     Zabour, Miss. Thamine female   NA     1     0   2665 14.4542              C          
# 1307      3        0 Zakarian, Mr. Mapriededer   male 26.5     0     0   2656  7.2250              C          
# 1308      3        0       Zakarian, Mr. Ortin   male 27.0     0     0   2670  7.2250              C          
# 1309      3        0        Zimmerman, Mr. Leo   male 29.0     0     0 315082  7.8750              S</pre><p>我們可以發現數據是經過排列過的，因為這樣會嚴重影響到我們後續隨機產生訓練與測試資料集，所以我們必須將資料重新隨機排列。</p>
<p>使用sample()隨機產生一組數列index。</p><pre class="crayon-plain-tag">shuffle_index &lt;- sample(x = 1:nrow(inputData))
head(shuffle_index)</pre><p>並將隨機數列index指派給titanic資料集。即可觀察到資料已無排序。</p><pre class="crayon-plain-tag">inputData &lt;- inputData[shuffle_index,]
head(inputData)

#      pclass survived                                   name    sex age sibsp parch     ticket   fare cabin
# 632       3        0            Andersson, Mr. Johan Samuel   male  26     0     0     347075  7.775      
# 526       2        0                       Pain, Dr. Alfred   male  23     0     0     244278 10.500      
# 822       3        0              Goldsmith, Mr. Frank John   male  33     1     1     363291 20.525      
# 485       2        1           Lemore, Mrs. (Amelia Milley) female  34     0     0 C.A. 34260 10.500   F33
# 627       3        0 Andersson, Miss. Ida Augusta Margareta female  38     4     2     347091  7.775      
# 1183      3        1       Salkjelsvik, Miss. Anna Kristine female  21     0     0     343120  7.650      
#      embarked                         home.dest
# 632         S                      Hartford, CT
# 526         S                      Hamilton, ON
# 822         S Strood, Kent, England Detroit, MI
# 485         S                       Chicago, IL
# 627         S      Vadsbro, Sweden Ministee, MI
# 1183        S</pre><p></p>
<h3>Step2: 資料探勘</h3>
<p>使用summary()摘要基礎統計。</p><pre class="crayon-plain-tag">summary(inputData)

#        pclass         survived                                   name          sex           age         
# Min.   :1.000   Min.   :0.000   Connolly, Miss. Kate            :   2   female:466   Min.   : 0.1667  
# 1st Qu.:2.000   1st Qu.:0.000   Kelly, Mr. James                :   2   male  :843   1st Qu.:21.0000  
# Median :3.000   Median :0.000   Abbing, Mr. Anthony             :   1                Median :28.0000  
# Mean   :2.295   Mean   :0.382   Abbott, Master. Eugene Joseph   :   1                Mean   :29.8811  
# 3rd Qu.:3.000   3rd Qu.:1.000   Abbott, Mr. Rossmore Edward     :   1                3rd Qu.:39.0000  
# Max.   :3.000   Max.   :1.000   Abbott, Mrs. Stanton (Rosa Hunt):   1                Max.   :80.0000  
#                                 (Other)                         :1301                NA's   :263      
#          sibsp            parch            ticket          fare                     cabin      embarked
# Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000                  :1014    :  2   
# 1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896   C23 C25 C27    :   6   C:270   
# Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454   B57 B59 B63 B66:   5   Q:123   
# Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295   G6             :   5   S:914   
# 3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275   B96 B98        :   4           
# Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329   C22 C26        :   4           
#                                  (Other) :1261   NA's   :1         (Other)        : 271           
#                home.dest  
# :564  
# New York, NY        : 64  
# London              : 14  
# Montreal, PQ        : 10  
# Cornwall / Akron, OH:  9  
# Paris, France       :  9  
# (Other)             :639</pre><p>使用str()查看資料結構。</p><pre class="crayon-plain-tag">str(inputData)
# 'data.frame':	1309 obs. of  12 variables:
# $ pclass   : int  3 2 3 2 3 3 1 3 3 2 ...
# $ survived : int  0 0 0 1 0 1 1 0 1 1 ...
# $ name     : Factor w/ 1307 levels "Abbing, Mr. Anthony",..: 41 920 459 703 36 1068 864 949 1092 516 ...
# $ sex      : Factor w/ 2 levels "female","male": 2 2 2 1 1 1 1 2 1 1 ...
# $ age      : num  26 23 33 34 38 21 23 NA NA 7 ...
# $ sibsp    : int  0 0 1 0 4 0 1 0 0 0 ...
# $ parch    : int  0 0 1 0 2 0 0 0 0 2 ...
# $ ticket   : Factor w/ 929 levels "110152","110413",..: 453 194 584 767 468 410 574 415 388 783 ...
# $ fare     : num  7.78 10.5 20.52 10.5 7.78 ...
# $ cabin    : Factor w/ 187 levels "","A10","A11",..: 1 1 1 183 1 1 134 1 1 1 ...
# $ embarked : Factor w/ 4 levels "","C","Q","S": 4 4 4 4 4 4 2 4 3 4 ...
# $ home.dest: Factor w/ 370 levels "","?Havana, Cuba",..: 154 150 317 64 345 1 191 1 1 165 ...</pre><p>我們可初步發現：</p>
<ol>
<li>pclass(座艙等級)和survuved(生存與否)應由int轉換成factor變數</li>
<li><span style="color: #9f6ad4;">類別水準數過多</span>的變數：<span style="color: #9f6ad4;">name</span>(1307 levels),<span style="color: #9f6ad4;">ticket</span>(929 levels), <span style="color: #9f6ad4;">cabin</span>(187 levels), <span style="color: #9f6ad4;">home.dest</span>(370 levels)<span style="color: #9f6ad4;">應予以排除</span>。</li>
<li>排除以上變數後，存在許多遺失值(NA value)的變數有：age(263), fare(1)。<span style="color: #9f6ad4;">但由於CART決策樹rpart()演算法中，預設會刪除y遺失的資料列，並保留至少有一個預測變數未遺失的觀察資料列，並使用Surrogate Variables來預測遺失特徵值。因此我們不會特別處理遺失值的部分</span>。(*更多決策樹遺失值預測請參考<a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate in CART</a>)</li>
</ol>
<h3>Step3: 資料前處理</h3>
<p>根據資料探勘結果，要處理的項目如下：</p>
<ol>
<li>移除變數<span style="color: #9f6ad4;">name</span>(1307 levels),<span style="color: #9f6ad4;">ticket</span>(929 levels), <span style="color: #9f6ad4;">cabin</span>(187 levels), <span style="color: #9f6ad4;">home.dest</span></li>
<li>將變數pclass(座艙等級)和survuved(生存與否)轉換為factor變數。</li>
</ol>
<p></p><pre class="crayon-plain-tag">library(dplyr)

clean_inputData &lt;- 
  inputData %&gt;% 
  # Drop variables
  select(-c(home.dest, cabin, name, ticket)) %&gt;% 
  #Convert to factor level
  mutate(pclass = factor(pclass, levels = c(1, 2, 3), labels = c('Upper', 'Middle', 'Lower')),
         survived = factor(survived, levels = c(0, 1), labels = c('No', 'Yes')))

glimpse(clean_inputData)

# Observations: 1,309
# Variables: 8
# $ pclass   &lt;fct&gt; Lower, Middle, Lower, Middle, Lower, Lower, Upper, Lower, Lower, Middle, Lower, Upper, Upper, Middle, Lower, Lower, Lower, Upper, Lower, Lower,...
# $ survived &lt;fct&gt; No, No, No, Yes, No, Yes, Yes, No, Yes, Yes, No, Yes, No, Yes, No, No, No, Yes, No, Yes, No, Yes, Yes, Yes, No, Yes, No, No, No, Yes, No, Yes, ...
# $ sex      &lt;fct&gt; male, male, male, female, female, female, female, male, female, female, male, female, male, female, male, male, female, female, male, female, m...
# $ age      &lt;dbl&gt; 26.0, 23.0, 33.0, 34.0, 38.0, 21.0, 23.0, NA, NA, 7.0, 1.0, 16.0, 58.0, 24.0, 33.0, NA, NA, 36.0, 36.0, 19.0, 19.0, 25.0, 51.0, 4.0, 40.0, 18.0...
# $ sibsp    &lt;int&gt; 0, 0, 1, 0, 4, 0, 1, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 8, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ parch    &lt;int&gt; 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 3, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ fare     &lt;dbl&gt; 7.7750, 10.5000, 20.5250, 10.5000, 7.7750, 7.6500, 113.2750, 7.7750, 7.7792, 26.2500, 46.9000, 57.9792, 113.2750, 27.0000, 7.7750, 8.0500, 7.75...
# $ embarked &lt;fct&gt; S, S, S, S, S, S, C, S, Q, S, S, C, C, S, S, S, Q, C, S, S, S, C, S, S, S, C, S, S, S, S, S, C, C, S, S, S, S, S, Q, S, S, S, S, Q, Q, S, C, S,...</pre><p></p>
<h3>Step4: 產生訓練與測試資料集</h3>
<p>為了確保兩組資料集中生還比例不要差異太大，我們會先將資料依據目標變數(survived)分成兩組(No, Yes)，再進行隨機切割成80%訓練組跟20%測試組。</p><pre class="crayon-plain-tag">input_ones &lt;- clean_inputData[which(clean_inputData$survived == 'Yes'),]
input_zeros &lt;- clean_inputData[which(clean_inputData$survived == 'No'), ]
set.seed(100)
input_ones_training_row &lt;- sample(1:nrow(input_ones),0.8*nrow(input_ones))
input_zeros_training_row &lt;- sample(1:nrow(input_zeros),0.8*nrow(input_zeros))

training_ones &lt;- input_ones[input_ones_training_row,]
training_zeros &lt;- input_zeros[input_zeros_training_row,]
trainingData &lt;- rbind(training_ones, training_zeros)

# 產生測試資料集
test_ones &lt;- input_ones[-input_ones_training_row,]
test_zeros &lt;- input_zeros[-input_zeros_training_row,]
testData &lt;- rbind(test_ones, test_zeros)</pre><p>檢查切割完的資料集大小與目標變數的分佈比例：</p>
<ul>
<li>原始資料列1309被隨機切割為80%訓練資料集(1047筆)與20%測試資料集(262筆)。</li>
<li>發現訓練及測試資料集的目標變數survived比例都是38%。差異在1%以內。</li>
</ul>
<p></p><pre class="crayon-plain-tag">dim(trainingData)
# [1] 1047    8
dim(testData)
# [1] 262   8

# 確認兩資料是隨機的
prop.table(table(trainingData$survived))
#        No       Yes 
# 0.6179561 0.3820439 

prop.table(table(testData$survived))
#        No       Yes 
# 0.6183206 0.3816794</pre><p></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step5: 建置模型</h3>
<p>我們使用CART(Classification and Regression Tree)決策樹演算法-rpart()。</p>
<ul>
<li>rpart為遞迴分割法(<span style="color: #9f6ad4;">R</span>ecursive <span style="color: #9f6ad4;">Part</span>itioning Tree)的縮寫。</li>
<li>對所有參數和分割點進行評估。</li>
<li>最佳選擇是使分割後的組內資料更為「一致(pure)」。
<ul>
<li>「一致(pure)」是指組內資料的應變數取直變異較小。</li>
</ul>
</li>
<li><span style="color: #9f6ad4;">使用Gini值測量資料的「一致(pure)」性(Homogeneity)</span>。</li>
<li>建模過程分為兩階段(2 stages)：
<ul>
<li>先長出最複雜的樹(grow the complex/full tree)。(直到Leaf size樹葉內的觀測個數少於5個或是模型沒有優化的空間為止）</li>
<li>再使用交叉驗證(Cross Validation)來修剪樹(Prune)。並尋找<span style="color: #9f6ad4;">使估計風險值(estimate of risk)參數(complexity parameter)</span>最小值的決策樹。</li>
</ul>
</li>
</ul>
<p>rpart()參數設定：</p>
<ul>
<li>method分成 “anova”、”poisson”、”class”和”exp”。當目標變數為factor時，我們將其設定為&#8221;class&#8221;。</li>
<li>control: 通常會使用rpart.control()另外作設定(事前修樹，pre-prune)。</li>
<li>na.action: <a href="https://www.rdocumentation.org/packages/rpart/versions/4.1-13/topics/rpart">預設為<span style="color: #9f6ad4;">na.rpart</span></a>，即使用CART演算法中的<a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">surrogate variables</a>做預測。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(rpart)
library(rpart.plot)

fit &lt;- rpart(formula = survived ~ ., data = trainingData, method = 'class')
# arguments:
# method: 
# - "class" for a classification tree (y is a factor) 			
# - "anova" for a regression tree</pre><p>使用rpart.plot()檢視決策樹規則。</p><pre class="crayon-plain-tag">rpart.plot(fit, extra= 106)</pre><p><span style="color: #9f6ad4;">節點顏色越綠越深，代表該節點(條件下)的survived機率越高（目標事件發生機率越高）</span>。</p>
<p>每個Node節點上的數值分別代表:</p>
<ul>
<li>預測類別(0,1)</li>
<li>預測目標類別的機率(1的機率)</li>
<li>節點中觀測資料個數佔比</li>
</ul>
<p><img loading="lazy" class="alignnone wp-image-1032" src="/wp-content/uploads/2018/08/Rplot01-1.jpeg" alt="Decision Tree " width="648" height="609" /></p>
<p>將決策樹規則使用rpart.rules()印出。</p><pre class="crayon-plain-tag">rpart.rules(x = fit,cover = TRUE)
# survived                                                                                         cover
#     0.06 when sex is   male                             &amp; age &lt;  9.5              &amp; sibsp &gt;= 3      2%
#     0.07 when sex is female &amp; pclass is           Lower              &amp; fare &gt;= 23                   3%
#     0.17 when sex is   male                             &amp; age &gt;= 9.5                               61%
#     0.58 when sex is female &amp; pclass is           Lower              &amp; fare &lt;  23                  14%
#     0.90 when sex is   male                             &amp; age &lt;  9.5              &amp; sibsp &lt;  3      2%
#     0.93 when sex is female &amp; pclass is Upper or Middle                                            19%</pre><p>可發現規則依照survived比例（目標事件發生機率）由低到高排序。cover則代表該節點觀測資料個數占比。</p>
<p>檢視交叉驗證(cross-validation)的不同cp值(complexity parameter)下的錯誤率。</p>
<p><span style="color: #9f6ad4;">cp值代表的是每一個規則（切割）所能改善模型適合度的程度(cross validation relative error, or X-val relative error)。可以發現每一個新的規則的cp呈遞減趨勢。且rpart()預設cp=0.01，即代表如果該規則（切割）沒有達到至少0.01的模型適合度改善，則停止。</span>(*<a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf" target="_blank" rel="noopener noreferrer">rpart函數對complexity parameter的說明</a>)</p><pre class="crayon-plain-tag">printcp(x = fit) 

# Classification tree:
#   rpart(formula = survived ~ ., data = trainingData, method = "class")
# 
# Variables actually used in tree construction:
#   [1] age    fare   pclass sex    sibsp 
# 
# Root node error: 400/1047 = 0.38204
# 
# n= 1047 
# 
#      CP nsplit rel error xerror     xstd
# 1 0.425      0     1.000  1.000 0.039305
# 2 0.030      1     0.575  0.575 0.033492
# 3 0.020      3     0.515  0.530 0.032507
# 4 0.010      5     0.475  0.510 0.032040</pre><p>將模型的cp table畫出。<span style="color: #9f6ad4;">可以觀察到，隨著模型的複雜度（成本）增加，所能改善的模型適合度的空間降低(X-val relative error)</span>。</p><pre class="crayon-plain-tag">plotcp(x = fit)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1034" src="/wp-content/uploads/2018/08/Rplot02-1.jpeg" alt="Decision Tree " width="816" height="771" srcset="/wp-content/uploads/2018/08/Rplot02-1.jpeg 816w, /wp-content/uploads/2018/08/Rplot02-1-300x283.jpeg 300w, /wp-content/uploads/2018/08/Rplot02-1-768x726.jpeg 768w, /wp-content/uploads/2018/08/Rplot02-1-230x217.jpeg 230w, /wp-content/uploads/2018/08/Rplot02-1-350x331.jpeg 350w, /wp-content/uploads/2018/08/Rplot02-1-480x454.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<h3>Step6: 進行預測</h3>
<p>使用predict()將訓練好的模型套用在測試資料集上。</p><pre class="crayon-plain-tag">predicted &lt;- predict(object = fit,newdata = testData,type = 'class')
# 參數說明：
# type: Type of prediction			
# - 'class': for classification			
# - 'prob': to compute the probability of each class			
# - 'vector': Predict the mean response at the node level</pre><p></p>
<h3>Step7: 衡量模型表現</h3>
<p>由於預測結果為類別型(0,1)，故我們以Confusion Matrix為基礎，來計算以下幾個常用指標：</p>
<ul>
<li>Accuracy/Misclassification Rate</li>
<li>Precision</li>
<li>Sensitivity(or Recall)</li>
<li>Specificity</li>
</ul>
<p>計算Confusion Matrix（數據左欄位預測值，上方列為真實值）</p><pre class="crayon-plain-tag">tbl &lt;- table(predicted,testData$survived)
tbl
# predicted  No Yes
#       No  140  28
#       Yes  22  72</pre><p>計算模型的正確率Accuracy</p><pre class="crayon-plain-tag"># Accuracy
accuracy &lt;- sum(diag(tbl)) / sum(tbl)
accuracy
# [1] 0.8091603</pre><p>可以發現<span style="color: #9f6ad4;">未修剪的模型</span>對測試資料的<span style="color: #9f6ad4;">預測正確率高達近81％</span>。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step8: 修剪樹(Post-Pruning)</h3>
<p>一般來說，修剪樹可以分為事前與事後。</p>
<ul>
<li><strong>事前</strong>：透過<span style="color: #9f6ad4;">rpart.control()</span>來調整重要參數，包括：
<ul>
<li><strong>minsplit</strong>：每一個node最少要幾個觀測值，預設為20。</li>
<li><strong>minbucket</strong>：在末端的node上(Leaf,樹葉)最少要幾個觀測值，預設為round(minsplit/3)。</li>
<li><strong>cp</strong>：complexity parameter。決定當新規則加入，改善模型相對誤差(x-val relative value)的程度如沒有大於cp值，則不加入該規則。<span style="color: #9f6ad4;">預設為0.01</span>。</li>
<li><strong>maxdepth</strong>：決策樹的深度，建議不超過6層。</li>
</ul>
</li>
<li><strong>事後</strong>：則是透過prune(x = , cp = )來設定。</li>
</ul>
<p>我們這邊採用post-pruning法。並選擇讓交叉驗證中相對誤差改變量最小的cp值。</p><pre class="crayon-plain-tag"># prune tree ：
fit.prune &lt;- prune(fit, cp = fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])</pre><p>將依據cp門檻值修剪後的樹規則繪出。</p><pre class="crayon-plain-tag"># plot the pruned tree 
rpart.plot(fit.prune, extra= 106, tweak = 1.1, shadow.col = "gray", branch.lty = 3, roundint = TRUE)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1035" src="/wp-content/uploads/2018/08/Rplot03_prune.jpeg" alt="Decision Tree " width="816" height="771" srcset="/wp-content/uploads/2018/08/Rplot03_prune.jpeg 816w, /wp-content/uploads/2018/08/Rplot03_prune-300x283.jpeg 300w, /wp-content/uploads/2018/08/Rplot03_prune-768x726.jpeg 768w, /wp-content/uploads/2018/08/Rplot03_prune-230x217.jpeg 230w, /wp-content/uploads/2018/08/Rplot03_prune-350x331.jpeg 350w, /wp-content/uploads/2018/08/Rplot03_prune-480x454.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>查看prune tree預測正確率。</p><pre class="crayon-plain-tag">tbl_prune &lt;- table(predicted = predicted.prune, actuals = testData$survived)

tbl_prune   
#          actuals
# predicted  No Yes
#       No  140  28
#       Yes  22  72

# Accuracy
accuracy &lt;- sum(diag(tbl_prune)) / sum(tbl_prune)
accuracy

# [1] 0.8091603</pre><p>可以發現pruned tree 和full tree兩者長得一樣，Accuracy也相同。<span style="color: #9f6ad4;">原因在於，因為在建立full tree時，預設cp=0.01，跟prune()使用的cp值是相同的</span>。</p>
<h3>Step 9: K-Fold Cross Validation</h3>
<p>為了確保模型無過度配適(overfitting)和預測準度的穩定性，我們使用k-fold cross validation(k=10)重新抽樣樣本進行模型驗證。理想中，交叉驗證後的平均正確率應與prune tree相近。</p>
<p>其中必須注意的是，因為資料中有遺失值觀測值，且train()函數中<span style="color: #9f6ad4;">參數na.action預設值為na.fail(即遇到有遺失值程序會失敗）</span>，故必須<span style="color: #0000ff;"><span style="color: #9f6ad4;">將設定調整為na.pass(不採取任何動作)或na.omit(忽略有遺失值的觀測值)</span><span style="color: #333333;">，方能正常執行函數指令。</span></span></p><pre class="crayon-plain-tag">library(caret)
library(e1071)
# 選則resampling的方法
train_control &lt;- trainControl(method = "cv",number = 10) # k = 10
# specify the model 
train_control.model &lt;- train(survived ~ ., data = trainingData, method = 'rpart',na.action = na.pass, trControl = train_control)
train_control.model

# CART 
# 
# 1047 samples
#    7 predictor
#    2 classes: 'No', 'Yes' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 943, 942, 942, 943, 942, 943, ... 
# Resampling results across tuning parameters:
#   
#   cp     Accuracy   Kappa    
#   0.020  0.8041850  0.5738715
#   0.030  0.7851282  0.5357307
#   0.425  0.6676099  0.1752418
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.02.</pre><p>進行10次交叉驗證的平均正確率為80.4%，與修剪後的樹模型正確率80.91%沒有太大差異（差異百分比在1%以內）。表示模型沒有overfitting的問題。</p>
<p>如果將參數na.action調整為na.rpart（使用CART中的代理變數surrogate variables來預測)。</p><pre class="crayon-plain-tag">train_control.model.2 &lt;- train(survived ~ ., data = trainingData, method = 'rpart',na.action = na.rpart, trControl = train_control)
train_control.model.2
# CART 
# 
# 1047 samples
# 7 predictor
# 2 classes: 'No', 'Yes' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 942, 943, 942, 943, 942, 942, ... 
# Resampling results across tuning parameters:
#   
#   cp     Accuracy   Kappa    
# 0.020  0.8042308  0.5735154
# 0.030  0.7880037  0.5452657
# 0.425  0.6685714  0.1862793
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.02.</pre><p>進行10次交叉驗證的平均正確率亦約為80.4%。</p>
<h3>Step 10: 模型比較(1)-條件推論樹(Conditional Inference Tree)</h3>
<ul>
<li>R的party套件提供<span style="color: #9f6ad4;">無母數回歸(non-parametric regression)樹模型</span>，可處理名目(nominal)、尺度(ordinal)、數值(numeric)、設限(censored)、多變量(multivariate)資料型態。</li>
<li>你可以使用ctree(formula, data = )函數來產生分類或回歸樹模型，樹模型類型會根據目標變數型態而有所不同。</li>
<li>ctree()透過統計檢驗來決定預測變數與分割點之選擇。
<ul>
<li>先假設所有預測變數與目標變數獨立(Null Hypothesis)。</li>
<li>進行<span style="color: #9f6ad4;">卡方獨立檢定(Chi-Square Test)</span>。</li>
<li>檢驗<span style="color: #9f6ad4;">p-value</span>小於threshold(ex: 0.05)則拒絕虛無假設，表示預測變數與目標變數具有顯著相關性，加入模型。</li>
<li>將相關性最強的變數選做第一次分割的變數。</li>
<li>繼續在各自子資料集進行分割變數計算與選擇。</li>
</ul>
</li>
<li>因為樹是<span style="color: #9f6ad4;">根據統計量顯著與否</span>來判斷規則之必要性，<span style="color: #9f6ad4;">因此與rpart()不同，ctree()是不需要剪枝的(prune)</span>。</li>
<li>另參數na.action預設為na.pass (即不採取任何動作)。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(party)
fit_ctree &lt;- ctree(survived ~ ., data = trainingData)
plot(fit_ctree)
predicted.ctree &lt;- predict(object = fit_ctree, newdata = testData)

tbl_ctree &lt;-table(predicted = predicted.ctree, actuals = testData$survived)
tbl_ctree

#          actuals
# predicted  No Yes
#       No  140  29
#       Yes  22  71

# Accuracy
accuracy &lt;- sum(diag(tbl_ctree)) / sum(tbl_ctree)
accuracy
# [1] 0.8053435</pre><p>可以發現模型準確率為80.5%，跟rpart演算法的k-fold交叉驗證的平均正確率80.4%沒有差太多。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step 11: 模型比較(2)-隨機森林(Random Forest)</h3>
<ul>
<li>隨機森林是一個<span style="color: #9f6ad4;">集成學習法(ensemble learning)</span>，意思是將幾個建立好的模型結果整合在一起，以提升預測準確率。</li>
<li>由集成學習法建立的模型<span style="color: #9f6ad4;">較能不容易發生過度配適的問題</span>，雖然提供較好的預測，但在推論和解釋度方面就會有所限制。</li>
<li>隨機森林由好幾個決策樹所組成，而不同決策樹是由不同隨機抽取的預測變數形成的。</li>
<li>而且特別的是，<span style="text-decoration: underline;">隨機森林不止對列(Row)進行抽樣，亦對行(Column)進行抽樣</span>，因此所產生的子集資料，其實是行與列同時抽樣後的結果。</li>
<li>對<span style="color: #9f6ad4;">列抽樣</span>，可以部分解決因<span style="color: #9f6ad4;">類別不平衡(Class Imbalance)</span>對預測帶來的問題；而對<span style="color: #9f6ad4;">行抽樣</span>，則可解決部分因<span style="color: #9f6ad4;">共線性(collinearity)</span>對預測造成的問題。<br />
(若是探討對「變數解釋性」的影響，則需要用 Lasso和Stepwise來解決)。</li>
<li>我們可用R裡面randomForest套件中的<span style="color: #9f6ad4;">randomForest()函數</span>來建立隨機森林。</li>
<li>參數na.action預設為na.fail (即遇到遺失值則停止執行)。因為資料集中有遺失觀測值，故必須將之調整為na.omit。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(randomForest)
set.seed(101)
fit.rf &lt;- randomForest(survived ~ ., data = trainingData, na.action = na.omit)</pre><p>檢視模型訓練結果。</p>
<ul>
<li>Number of trees: 隨機森林由500棵隨機生成的決策樹所組成。</li>
<li><span class="bash">利用<span style="color: #9f6ad4;">OOB(Out Of Bag)</span>運算出來的</span><span style="color: #9f6ad4;">錯誤率</span>為<span style="color: #0000ff;"><span style="color: #9f6ad4;">18.82%</span><span style="color: #333333;">。</span></span></li>
</ul>
<p></p><pre class="crayon-plain-tag"># Call:
#   randomForest(formula = survived ~ ., data = trainingData, na.action = na.omit) 
# Type of random forest: classification
# Number of trees: 500
# No. of variables tried at each split: 2
# 
# OOB estimate of  error rate: 18.82%
# Confusion matrix:
#      No Yes class.error
# No  463  44  0.08678501
# Yes 116 227  0.33819242</pre><p>自己驗證與計算較精確的OOB estimate正確率Accuracy為81.17%。</p><pre class="crayon-plain-tag">tbl.rf &lt;- fit.rf$confusion[,c(1,2)]
accuracy &lt;- sum(diag(tbl.rf)) / sum(tbl.rf)
accuracy
# [1] 0.8117647</pre><p>另外，我們將「<span style="color: #9f6ad4;">增加每一顆決策樹，整體誤差的改變量</span>」繪出，以輔助決策「需要多少顆決策樹，整體誤差才會趨於穩定」。</p>
<ul>
<li>當為分類樹時(classification tree)
<ul>
<li>誤差為OOB(out-of-bag) Erro Rates。</li>
<li><span style="color: #9f6ad4;">黑色實線表示整體的OOB error rate，而其他顏色虛線表示各類別的OOB Error Rate</span>。</li>
</ul>
</li>
<li>當為回歸樹時(regression tree)
<ul>
<li>誤差為OOB(out-of-bag) MSE。</li>
<li><span style="color: #9f6ad4;">只會有一條黑色實線代表整體的OOB MSE</span>。</li>
</ul>
</li>
</ul>
<p></p><pre class="crayon-plain-tag">plot(fit.rf)</pre><p>從圖中幾條線可以觀察到：</p>
<ul>
<li>整體錯誤率（黑色實線）隨著決策樹數量上升，下降到約18%並趨於穩定。</li>
<li>實際類別為Yes的錯誤率（綠色虛線）隨著決策樹數量的上升，下降到約33.8%並趨於穩定。</li>
<li>實際類別為No的錯誤率（紅色虛線）隨著決策樹數量的上升，下降到約8.6%並趨於穩定。</li>
<li>而「<span style="color: #9f6ad4;">最佳決策樹數目(ntree)</span><span style="color: #0000ff;">」</span>，約100多棵樹即足夠使誤差趨於穩定（不需要到500棵樹）。</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-1103" src="/wp-content/uploads/2018/08/Rplot04_rf_plot.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot04_rf_plot.jpeg 816w, /wp-content/uploads/2018/08/Rplot04_rf_plot-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot04_rf_plot-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot04_rf_plot-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot04_rf_plot-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot04_rf_plot-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>另外一個隨機森林中一個重要參數：mtry，表示每一個樹節點(node)在進行切割時(split)隨機抽樣的變數數量。可使用tuneRF()來調整mtry的值。</p><pre class="crayon-plain-tag">trainingData_naomit &lt;- na.omit(trainingData)
tuneRF(x = trainingData_naomit[,-2], y = trainingData_naomit[,2])


# mtry = 2  OOB error = 19.53% 
# Searching left ...
# mtry = 1 	OOB error = 20.71% 
# -0.06024096 0.05 
# Searching right ...
# mtry = 4 	OOB error = 21.53% 
# -0.1024096 0.05 
#       mtry  OOBError
# 1.OOB    1 0.2070588
# 2.OOB    2 0.1952941
# 4.OOB    4 0.2152941</pre><p>可以發現在mtry=2時，誤差最小。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1111" src="/wp-content/uploads/2018/08/Rplot05_tuneRF.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot05_tuneRF.jpeg 816w, /wp-content/uploads/2018/08/Rplot05_tuneRF-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot05_tuneRF-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot05_tuneRF-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot05_tuneRF-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot05_tuneRF-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>randomForest中類別樹預設的mtry=sqrt(p)，其中p代表x變數的數目。因為原始隨機森林模型預設值跟tuneRF建議的值相同，故我們另外不調整。</p><pre class="crayon-plain-tag">fit.rf$mtry
# [1] 2</pre><p>看每個x變數的重要性(<span style="color: #9f6ad4;">the mean decrease in Gini index</span>)，即看哪個變數對<span style="color: #9f6ad4;">損失函數Loss Function</span>最有貢獻。(*randomForest參數importance預設值為False，僅會產生the mean decrease in Gini index，如果要產生其他指標如mean decrease in accuracy，要將其改為TRUE。)</p><pre class="crayon-plain-tag">round(importance(fit.rf),2) # importance of each predictor
# or
# round(fit.rf$importance, 2)

#          MeanDecreaseGini
# pclass              27.55
# sex                100.67
# age                 55.09
# sibsp               13.89
# parch               13.40
# fare                59.79
# embarked            11.55</pre><p>將變數重要性（貢獻度）繪出。</p><pre class="crayon-plain-tag">varImpPlot(fit.rf)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1115" src="/wp-content/uploads/2018/08/Rplot06_varImpPlot.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot06_varImpPlot.jpeg 816w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>最後將調整好的模型應用在testData並評估正確性。</p><pre class="crayon-plain-tag">predicted.rf &lt;- predict(object = fit.rf, newdata = testData)
tbl.rf &lt;- table(predicted = predicted.rf, actuals = testData$survived)
accuracy &lt;- sum(diag(tbl.rf)) / sum(tbl.rf)
accuracy
# [1] 0.8307692</pre><p>隨機森林的預測準確率為83%，較原先的決策樹(accuracy = 80%)改善約4%。</p>
<h3>總結</h3>
<ul>
<li>Decision Tree 決策樹模型具有簡單易懂的樹狀邏輯圖，可解釋度高。</li>
<li>Decision Tree 決策樹對於資料的要求低，沒有常態分配與線性關係的假設，不受資料分配限制。</li>
<li>Decision Tree 決策樹的<span style="color: #9f6ad4;">有變數篩選機制</span>。
<ul>
<li>rpart演算法進行Gini Index檢定，並計算complexity parameter來進行變數篩選。</li>
<li>ctree演算法進行chi-square檢定，檢驗各投入變數是否與目標變數相關並計算p-value來看相關性的顯著效果。</li>
</ul>
</li>
<li>Decision Tree 決策樹亦<span style="color: #9f6ad4;">有空值填補機制</span> &#8211; <a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate</a>。
<ul>
<li>rpart演算法na.action預設為na.rpart。(更多有關決策樹遺失值預測請參考 <a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate in CART</a>)</li>
<li><a href="https://www.rdocumentation.org/packages/partykit/versions/1.2-2/topics/ctree">ctree演算法na.action預設為na.pass</a>，可以將其改為na.rpart。</li>
</ul>
</li>
<li>Decision Tree 演算法<span style="color: #9f6ad4;">rpart和ctree皆能處理連續型(continuous)與類別型(categorical)變數之切割</span>。</li>
<li>由Decision Tree 決策樹衍伸出的集成學習法「<span style="color: #9f6ad4;">隨機森林 random forest</span>」可以有效降低模型的錯誤率、解決過度配適、透過反覆抽樣解決類別不平衡與共線性問題。</li>
</ul>
<hr />
<p>更多<span style="color: #9f6ad4;">統計模型</span>筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>學習筆記參考連結：</p>
<ol>
<li><a href="https://www.guru99.com/r-decision-trees.html" target="_blank" rel="noopener noreferrer">Decision Tree in R with Example </a></li>
<li><a href="https://www.statmethods.net/advstats/cart.html">Tree-Based Models</a></li>
<li><a href="https://rstudio-pubs-static.s3.amazonaws.com/275285_90aaf9a2a64d43a5846a86dbcde8eba9.html">R_programming &#8211; (8)決策樹(Decision Tree)</a></li>
<li><a href="https://dzone.com/articles/decision-trees-and-pruning-in-r">Decision Trees and Pruning in R</a></li>
<li><a href="https://statinfer.com/203-3-10-pruning-a-decision-tree-in-r/">Pruning a Decision Tree in R</a></li>
<li><a href="https://www.edureka.co/blog/decision-trees/">How To Create A Perfect Decision Tree</a></li>
<li><a href="https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda">[資料分析&amp;機器學習] 第3.5講 : 決策樹(Decision Tree)以及隨機森林(Random Forest)介紹</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
	</channel>
</rss>
