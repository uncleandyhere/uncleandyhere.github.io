<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>隨機森林 &#8211; 果醬珍珍•JamJam</title>
	<atom:link href="/tag/%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>健忘女孩Jam的學習筆記和生活雜記</description>
	<lastBuildDate>Sun, 14 Apr 2019 15:01:36 +0000</lastBuildDate>
	<language>zh-TW</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.7.2</generator>
	<item>
		<title>Random Forests 隨機森林 &#124; randomForest, ranger, h2o &#124; R語言</title>
		<link>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/</link>
					<comments>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Tue, 19 Mar 2019 14:33:31 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[h2o]]></category>
		<category><![CDATA[random forests]]></category>
		<category><![CDATA[randomForest]]></category>
		<category><![CDATA[ranger]]></category>
		<category><![CDATA[Regression Tree]]></category>
		<category><![CDATA[隨機森林]]></category>
		<guid isPermaLink="false">/?p=2802</guid>

					<description><![CDATA[<p>Bagging法綜合多個樹模型結果，可以降低單一樹模型的高變異性並提升預測正確率。但Bagging法中樹與樹之間的相關性會降低模型整體的表現。隨機森林 Rand [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Bagging法</a>綜合多個樹模型結果，可以降低單一樹模型的高變異性並提升預測正確率。但Bagging法中樹與樹之間的相關性會降低模型整體的表現。隨機森林 Random forests 是Bagging修改後的版本，它是由「去相關性」的樹模型所組成的集成演算法，有很不錯的預測正確率且是一個受歡迎、開箱即用的演算法。</p>
<h3>載入所需套件</h3>
<p></p><pre class="crayon-plain-tag">library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # an extremely fast java-based platform
library(dplyr)
library(magrittr)</pre><p>準備資料。</p>
<ul>
<li>使用AmesHousing套件中的Ames Housing Data。</li>
<li>使用resample package中的initial_split()將資料切分成7:3(將參數設定為prop = 0.7)。</li>
<li>再分別用training()和testing()函數將切分好的資料萃取出。</li>
<li>並使用set.seed()來確保資料切分結果是可再現的。</li>
</ul>
<p></p><pre class="crayon-plain-tag"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)
ames_split &lt;- initial_split(data = AmesHousing::make_ames(), prop = .7)
ames_train &lt;- training(ames_split)
ames_test  &lt;- testing(ames_split)</pre><p></p>
<h3>Random Forests 概念介紹</h3>
<p>隨機森林模型的基礎概念和Decision Trees和Bagging一樣的(可以參考<a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">決策樹,Decision Trees</a>和<a href="/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/" target="_blank" rel="noopener noreferrer">Bagging</a>)。Bagging Trees模型在演算法中納入了隨機的元素，有效的降低了單一樹模型的高變異性與提升模型預測正確率。然而在bagging中的trees並非所有都是彼此相互獨立的，因為在每一棵樹切分節點時都是考慮所有原始的預測變數。也因爲上述關係，來自不同bootstrapped samples的樹彼此的結構都會有些類似（尤其是在樹的上半部，用來切割的前幾大變數都會非常類似）。</p>
<p>樹的結構相遇的這個特性就稱作tree correlation，它阻礙了Bagging最適地降低預測目標值的變異(variance)。為了更近一步降低變異，我們需要最小化樹與樹之間的相關性。這可以透過注入更多的隨機性到長樹的過程。 Random Forests是透過以下兩步驟來達成的：</p>
<ol>
<li>Bootstrap (拔靴法) : 跟Bagging很類似，每一顆樹都是建立自不同的bootstrapped sample，讓他們稍稍不一樣並稍稍去相關性。</li>
<li>Split-variable randomization (變數切割的隨機性) : 每一次在執行變數切割時，搜尋切割變數的範圍被限縮為隨機的子集合，即隨機挑選m個隸屬於總p個變數的子集合作為切割搜尋變數的範圍。對回歸樹來說，預設使用\(m=\frac{p}{3}\)，是個可經調教的參數。當\(m = p\)的時候，則跟只進行步驟1的結果一樣。</li>
</ol>
<p>因為每棵樹都是來自不同的隨機bootstrapped sample且每一次切割都是隨機挑選變數的子集合，因此樹與樹之間的關聯性會下降地較Bagging更低。</p>
<h4>OOB error vs. test set error</h4>
<p>和Bagging一樣，bootstrap resample法的一個天然好處，就是隨機森林模型可以透過out-of-bag(OOB)的樣本誤差來作為有效與合理近似實驗誤差(test error)。不需要額外產生或犧牲訓練資料集，OOB sample可供作為一個內建的驗證子集合。OOB sample 的存在讓尋找使模型錯誤率趨於穩定的最適樹模型數量(ntree)更有效率。但是OOB error和test error終究還是預期會不太相同。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-2804" src="/wp-content/uploads/2019/03/Rplot.png" alt="random forests-隨機森林" width="800" height="500" srcset="/wp-content/uploads/2019/03/Rplot.png 800w, /wp-content/uploads/2019/03/Rplot-300x188.png 300w, /wp-content/uploads/2019/03/Rplot-768x480.png 768w, /wp-content/uploads/2019/03/Rplot-230x144.png 230w, /wp-content/uploads/2019/03/Rplot-350x219.png 350w, /wp-content/uploads/2019/03/Rplot-480x300.png 480w" sizes="(max-width: 800px) 100vw, 800px" /></p>
<p>[上圖：Random forest out-of-bag error versus validation error.]</p>
<p>&nbsp;</p>
<p>此外，有很多package都沒有可以追蹤在某一棵樹模型中，哪些是OOB sample哪些不是的功能。這樣在比須比較多個模型的成效時，想要使用相同的驗證資料集來幫每個模型打分數是不可行的。而且，技術上雖然可以對OOB sample計算特定指標(metrics)如root mean squared logarithmic error (RMSLE)，但並非所有package都有內建這樣的運算功能。所以如果你想要比較多個模型的成效或是使用較不傳統的損失函數指標，你可能還是會選擇cross validation。</p>
<h4>Advantages &amp; Disadvantages of Random Forests</h4>
<h5>優點</h5>
<ol>
<li>隨機森林模型通常具有非常好的成效。</li>
<li>非常好的「開箱即用」的模型-不太需要調整什麼參數。</li>
<li>有內建的驗證資料集validation set &#8211; 不須為了額外驗證而犧牲資料。</li>
<li>不需前處理(pre-processing)。</li>
<li>對於離群值的處理是強大的。</li>
</ol>
<h5>缺點</h5>
<ol>
<li>當運算大型資料時會變的非常慢。</li>
<li>雖然模型預測正確率高，但通常無法跟更先進的boosting演算法相比。</li>
<li>較不易解釋。</li>
</ol>
<h3>基本實作</h3>
<p>在R中有超過20種的Random Forests Packages。以下會使用歷史最悠久且最受歡迎randomForest套件來說明示範基本的Random Forests模型實作。但必須注意，當你的資料集變得很大的時候，randomForest回無法很好的擴展到大型資料（即使使用foreach進行平行運算）。此外，為了探索和比較不同模型參數的效果，我們也可找到更多有效的套件。因此，在模型tuning階段，我們會說明如何使用ranger和h2o package來進行更有效率的Random Forests modeling。</p>
<p>randomForest::randomForest可以使用formula或x,y matrix表示的方式來指定模型資料。我們以下以formula的方式來指定模型設定並使用randomForest模型預設參數。</p><pre class="crayon-plain-tag"># for reproduciblity
set.seed(123)

# default RF model
m1 &lt;- randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train
)

m1</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 661089658
##                     % Var explained: 89.8</pre><p>從m1結果來看：</p>
<ul>
<li>randomForest預設會使用500棵樹。</li>
<li>每一次切割(each split)會隨機篩選出\(\frac{Features}{3}=26\)個預測變數作為base。(原始data扣除目標變數Sale_Price的number of features = 80)</li>
<li>m1$mse(regression only)代表的是由OOB sample所計算出的「平均誤差平方(mean squared erre)」向量，即殘差平方和(sum of squared residuals)除上樹的個數(n)。</li>
<li>綜合500棵樹的mse(OOB error)為m1$mse[500] = 6.6108966 × 10<sup>8</sup>。</li>
</ul>
<p>我們可以進一步將不同棵樹所組成的隨機森林模型(ntree)對應的平均誤差(MSE)給繪出：</p><pre class="crayon-plain-tag">plot(m1)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-4-1-3.png" alt="random forests-隨機森林" /></p>
<p>從上圖可以發現，模型平均誤差大概在100棵樹時開始趨於穩定，且平均誤差值降低的速度開始於300棵樹左右時開始變緩。</p>
<p>我們可以進一步找出使得MSE最小的樹的個數。</p><pre class="crayon-plain-tag">which.min(m1$mse)</pre><p></p><pre class="crayon-plain-tag">## [1] 447</pre><p>最適隨機森林的RMSE則為(平均Sale_Price的誤差值)：</p><pre class="crayon-plain-tag">sqrt(m1$mse[which.min(m1$mse)])</pre><p></p><pre class="crayon-plain-tag">## [1] 25648.78</pre><p>如果我們不想要OOB error，randomForest函數亦提供驗證資料集(validation set)來幫助我們計算模型預測正確率。</p>
<p>要計算驗證誤差，首先我們進一步將訓練資料集依據8:2的比例切成訓練和驗證資料集，並分別使用analysis()和assessment()函數萃取出訓練和驗證資料。</p><pre class="crayon-plain-tag"># create training and validation data 
set.seed(123)
valid_split &lt;- initial_split(ames_train, .8)

# training data
ames_train_v2 &lt;- analysis(valid_split)

# validation datas
ames_valid &lt;- assessment(valid_split)

# 將驗證資料整理成x_test和y_test
x_test &lt;- ames_valid[setdiff(names(ames_valid),"Sale_Price")]
y_test &lt;- ames_valid$Sale_Price

# 在randomForest函數中使用x-test和y-test當作驗證資料集的參數
rf_oob_comp &lt;- randomForest(
  formula = Sale_Price ~ .,
  data = ames_train_v2,
  xtest = x_test,
  ytest = y_test
)

rf_oob_comp</pre><p></p><pre class="crayon-plain-tag">## 
## Call:
##  randomForest(formula = Sale_Price ~ ., data = ames_train_v2,      xtest = x_test, ytest = y_test) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 26
## 
##           Mean of squared residuals: 667486651
##                     % Var explained: 89.17
##                        Test set MSE: 841004412
##                     % Var explained: 89.16</pre><p>可以看到模型結果多了Test set MSE，和原始的OOB MES不太一樣。我們將不同顆樹組成的模型所對應的OOB error和test error萃取出來並畫出誤差隨著樹的數量的變化圖，並比較兩者的差距。</p><pre class="crayon-plain-tag"># extract OOB &amp; validation errors
oob &lt;- sqrt(rf_oob_comp$mse)
validation &lt;- sqrt(rf_oob_comp$test$mse)

data.frame(
  ntrees = 1:rf_oob_comp$ntree,
  OOB.error = oob,
  Test.error = validation
) %&gt;% 
  gather(key = metric, value = RMSE, 2:3) %&gt;% 
  ggplot(aes(x = ntrees, y = RMSE, color = metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-8-1-3.png" alt="random forests-隨機森林" /></p>
<p>Random Forests是其中一個很好的「開箱即用」的演算法之一。基本上不需要調整什麼參數(tuning)，模型的預測能力就可ㄧ有很好的成效。</p>
<p>舉例來說，從上圖中，我們不需要調整參數就可以得到小於$30K的RMSE，比完整tuning後的<a href="/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/" target="_blank" rel="noopener noreferrer">Bagging模</a>RMSE降低超過$6K；比完整tuning的<a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">elastic net模型</a>RMSE降低超過$2K(沒有將目標變數取log的elastic net model版本請參考以下)。而我們還可以進一步透過參數調整tuning將Random Forests模型的預測正確性優化。</p><pre class="crayon-plain-tag"># elastic net
library(caret)
ames_train_x &lt;- model.matrix(object = Sale_Price ~ ., data =  ames_train)[, -1]
ames_train_y &lt;- ames_train$Sale_Price

# ames_test_x &lt;- model.matrix(Sale_Price ~ ., ames_test)[, -1]
# ames_test_y &lt;- ames_test$Sale_Price

train_control &lt;- trainControl(method = "cv", number = 10)

caret_mod &lt;- train(
  x = ames_train_x, 
  y = ames_train_y,
  method = "glmnet", 
  prePro = c("center","scale","zv","nzv"),
  trControl = train_control,
  tuneLength = 10
)

min(caret_mod$results$RMSE)</pre><p></p><pre class="crayon-plain-tag">## [1] 32268.14</pre><p></p>
<h4>tuning</h4>
<p>Random Forests在tuning上非常簡單，因為只有幾個tuning parameters。通常tuning模型一開始最主要的考量點就是每一次分割所用來挑選的潛在變數名單，另外就是幾個需要注意的hyperparameters，包括如下：(這些hyperparameters在不同package的命名可能有所不同)</p>
<ul>
<li>ntree : number of trees。我們希望有足夠的樹來穩定模型的誤差，但過多的樹會是沒效率且沒必要的，特別是遇到大型資料集的時候。</li>
<li>mtry : 每次在決定切割變數時，所隨機抽樣的潛在變數清單數量。當mtry = p(即所有特徵變數數量)，Random Forests的結果就會和bagging一樣。而當mtry = 1，會造就每一次split所使用的變數completely random，每個變數都有機會但會造成非常偏差的結果。</li>
<li>sampsize : 訓練每棵樹模型的樣本數大小。預設是使用63.25%訓練資料集的比例，因為這個是獨立觀察值出現在bootstrapped sample的期望機率值。較低的樣本數大小雖然會降低訓練時間，但可能會產生不必要的偏差。增加樣本數大小可以提升模型正確率，但有可能會產生overfitting(因為會增加模型變異性)。所以一般來說，我們校正此樣本大小參數時會使用60-80%的比例。</li>
<li>nodesize : 末梢(葉)節點最小觀察資料個數。用來控制樹模型的複雜度。小的葉節點大小允許更深更複雜的樹模型，大的葉節點大小則會產生叫淺的樹模型。這又是一種「偏差v.s.變異度(bias -variance)」的權衡，當樹長得越深時，模型變異性愈高(有過度配適的風險)，而當樹長得越潛時則會有較多偏差(有沒辦法完整捕捉資料中的模式跟關係)。</li>
<li>maxnode : 內部節點最大個數值。另一種控制模型複雜度的變數，內部節點樹越多則會長出更深的樹，內部節點越少則產生越淺的樹。</li>
</ul>
<h4>Initial tuning with randomForest</h4>
<p>如果一開始只針對mtry進行校正，可以使用randomForest::tuneRF來進行簡易快速的評估。tunRF會從指定的mtry值開始，並每次增加給定的間距，直到模型OOB error降低的幅度開始小於特定幅度為止。</p>
<p>比如說，以下想知道mtry從5開始，每間隔相加1.5，所得到到的OOB error，直到OOB error改善的不度不超過1%為止。</p>
<p>因為tuneRF需要使用x,y形式指定資料，因此首先我們先使用setdiff()函數將變數名稱依據目標變數與預測變數分開。</p>
<p>模型跑完後會自動將每個mtry值所對應的OOB error值繪出。我們可以發現，當mtry &gt; 22後，OOB開始不再下降，最適mtry水準值約與features/3 = 80/3 = 26差不多。</p><pre class="crayon-plain-tag"># 篩選出預測變數名稱
features &lt;- setdiff(x = names(ames_train), y = "Sale_Price")

# 固定不同mtry參數值的模型所使用的隨機OOB sample一樣的
set.seed(123)

m2 &lt;- tuneRF(x = ames_train[features], y = ames_train$Sale_Price, mtryStart = 5, ntreeTry = 500 ,stepFactor = 1.5, improve = 0.01, trace = FALSE)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-10-1-4.png" alt="random forests-隨機森林" /></p><pre class="crayon-plain-tag">plot(m2)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-10-2-4.png" alt="random forests-隨機森林" /></p>
<h4>Full grid search with ranger</h4>
<p>如果想要套用綜合mtry以及其他參數的hyperparameter組合，我們需要建立一個grid並使用loop迴圈的方式，去測試每一個hyperparameter的組合和模型的成效。但因為randomForest()函數無法有效的將運算擴展至大型數據運算，因此我們會使用以c++執行的ranger()函數來解決。</p>
<p>我們可先使用Sys.time()稍稍比較tuneRF和ranger執行一種隨機森林模型所需的時間。</p><pre class="crayon-plain-tag">system.time(
  ames_randomForest &lt;- randomForest(
    formula = Sale_Price ~ ., 
    data = ames_train, 
    ntree = 500, 
    mtry = floor(length(features)/3)
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
## 111.957   0.735 169.989</pre><p></p><pre class="crayon-plain-tag">system.time(
  ames_ranger &lt;- ranger(
    formula   = Sale_Price ~ ., 
    data      = ames_train, 
    num.trees = 500,
    mtry      = floor(length(features) / 3)
  )
)</pre><p></p><pre class="crayon-plain-tag">##    user  system elapsed 
##  10.367   0.174   5.665</pre><p>由以上結果我們可以看到，同樣是執行一次隨機森林，ranger()所需的時間僅約6秒，而randomForests則需要169秒。</p>
<p>為了進行grid search，我們首先先建立一個hyperparameters的grid，由許多不同的mtry, minimum node size,和 sample size所組成。總共會有96種組合。</p><pre class="crayon-plain-tag"># hyperparameter grid search
hyper_grid &lt;- expand.grid(
  mtry = seq(20, 30, by = 2),
  node_size = seq(3, 9, by = 2),
  sample_size = c(0.55, 0.632, 0.7, 0.8),
  OOB_RMSE = 0
)

# total number of combinations
nrow(hyper_grid)</pre><p></p><pre class="crayon-plain-tag">## [1] 96</pre><p>我們使用loop迴圈，一一帶入不同hyperparameters到ranger()函數中，並固定每一次randomForests的的森林樹木數量為500(因為從前面經驗我們知道500棵樹即足夠使OOB error趨於穩定並收斂)。另外每一次randomForests執行時，我們也固定隨機亂數種子，讓同樣sample_size參數值所對應的抽樣樣本可以相同，凸顯其他參數變化所帶來的效果。</p><pre class="crayon-plain-tag">for (i in 1:nrow(hyper_grid)) {
  # train model
  model &lt;- ranger(
    formula = Sale_Price ~ .,
    data = ames_train, 
    num.trees = 500, 
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i], 
    sample.fraction = hyper_grid$sample_size[i],
    seed = 123
  )

  # 並將每一此訓練模型的OOB RMSE萃取儲存
  hyper_grid$OOB_RMSE[i] &lt;- sqrt(model$prediction.error)
}

# 我們將結果依序OOB_RMSE由小至大排列，取模型成效前十名印出
hyper_grid %&gt;% 
  dplyr::arrange(OOB_RMSE) %&gt;% 
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    mtry node_size sample_size OOB_RMSE
## 1    20         5         0.8 25918.20
## 2    20         3         0.8 25963.96
## 3    28         3         0.8 25997.78
## 4    22         5         0.8 26041.05
## 5    22         3         0.8 26050.63
## 6    20         7         0.8 26061.72
## 7    26         3         0.8 26069.40
## 8    28         5         0.8 26069.83
## 9    26         7         0.8 26075.71
## 10   20         9         0.8 26091.08</pre><p>從前十名模型成效結果我們可以發現：</p>
<ul>
<li>OOB_RMSE大致落在26K左右。</li>
<li>最適mtry的值落在所有20~30範圍區間。表示mtry在此區間對於OOB_RMSE沒有太大影響。</li>
<li>最適最小節點觀察值數量大約落在3~5。</li>
<li>最適抽樣比例約為0.8。</li>
<li>表示抽樣比例高(~80%)和深度較長(葉節點觀測個數大小3~5)的隨機森林成效較好(OOB RMSE)。</li>
</ul>
<h5>調整變數型態</h5>
<p>雖然我們已知random forests對於原始類別型變數處理效果是不錯的，我們還是進一步來試試將類別變數重新編碼為dummy variables是否能提升random forests的預測表現。</p>
<p>以下，我們使用dummyVars()函數將類別變數重新編碼為虛擬變數。</p><pre class="crayon-plain-tag">to_dum &lt;- dummyVars(formula = ~., data = ames_train, fullRank = FALSE)</pre><p>原始類別預測變數(80)被轉換完後變成353個欄位。</p><pre class="crayon-plain-tag">ames_to_dum &lt;- predict(to_dum, newdata = ames_train) %&gt;% as.data.frame()</pre><p>將資料名稱變成ranger相容的名稱。</p><pre class="crayon-plain-tag">names(ames_to_dum) &lt;- make.names(names = names(ames_to_dum), allow_ = FALSE)</pre><p>建立hyperparameter grid。並將mtry的區間調整為更大範圍。並執行grid search。</p><pre class="crayon-plain-tag">hyper_grid_2 &lt;- expand.grid(
  mtry = seq(50, 200, by = 25),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE  = 0
)

for(i in 1:nrow(hyper_grid_2)){
  model &lt;- ranger(
    formula = Sale.Price ~.,
    data = ames_to_dum, 
    num.trees = 500, 
    mtry = hyper_grid_2$mtry[i],
    min.node.size = hyper_grid_2$node_size[i], 
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed = 123
  )

  hyper_grid_2$OOB_RMSE[i] &lt;- sqrt(model$prediction.error)
}</pre><p></p><pre class="crayon-plain-tag">hyper_grid_2 %&gt;% 
  dplyr::arrange(OOB_RMSE) %&gt;% 
  head(10)</pre><p></p><pre class="crayon-plain-tag">##    mtry node_size sampe_size OOB_RMSE
## 1    50         3        0.8 26981.17
## 2    75         3        0.8 27000.85
## 3    75         5        0.8 27040.55
## 4    75         7        0.8 27086.80
## 5    50         5        0.8 27113.23
## 6   125         3        0.8 27128.26
## 7   100         3        0.8 27131.08
## 8   125         5        0.8 27136.93
## 9   125         3        0.7 27155.03
## 10  200         3        0.8 27171.37</pre><p>由結果可分發線</p>
<ul>
<li>OOB RMSE 落在27K左右，並沒有比類別變數重新編碼前的26K來得好。</li>
<li>將類別變數重新編碼成dummy variables是無法提升模型成效的。</li>
</ul>
<p>所以到目前為至，最適的random forests模型參數分別為mtry = 20, node_size = 5, sample_size = 0.8。我們重複執行100次這個參數設定的模型，來計算此模型的error rate的期望值大小。</p><pre class="crayon-plain-tag">OOB_RMSE &lt;- vector(mode = "numeric", length = 100)

for(i in 1:length(OOB_RMSE)){
  optimal_ranger &lt;- ranger(
    formula         = Sale_Price ~ ., 
    data            = ames_train, 
    num.trees       = 500,
    mtry            = 20,
    min.node.size   = 5,
    sample.fraction = .8,
    importance      = 'impurity'
  )

  OOB_RMSE[i] &lt;- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-19-1-3.png" alt="random forests-隨機森林" /></p>
<p>執行100次random forests的結果後，我們可以觀察到OOB RMSE的期望值約落在26000~26200區間。</p>
<p>另外，我們在在模型參數importance = &#8216;impurity&#8217;。這表示我們是依據節點不純度(node impurity)的改善幅度來衡量每個變數的重要性。變數的重要性是計算每一次使用不同變數切割結點後，總能使MSE下降的程度(跨多棵樹模型)，而那些無法透過該變數切割所降低的模型錯誤率，則被稱作node impurity。而能夠降低越多MSE的變數則被重要性越高。</p>
<p>因此，在每一次節點分割時，我們都會計算每個變數所造成的MSE下降程度，而累積下降MSE幅度最高者，則被認為是較為重要的變數。</p>
<p>我們將變數重要性結果繪出：</p><pre class="crayon-plain-tag">options(scipen = -1)
optimal_ranger$variable.importance %&gt;% 
  as.matrix() %&gt;% 
  as.data.frame() %&gt;% 
  add_rownames() %&gt;% 
  `colnames&lt;-`(c("varname","imp")) %&gt;%
  arrange(desc(imp)) %&gt;% 
  top_n(25,wt = imp) %&gt;% 
  ggplot(mapping = aes(x = reorder(varname, imp), y = imp)) +
  geom_col() +
  coord_flip() +
  ggtitle(label = "Top 25 important variables") +
  theme(
    axis.title = element_blank()
  )</pre><p></p><pre class="crayon-plain-tag">## Warning: Deprecated, use tibble::rownames_to_column() instead.</pre><p><img src="/wp-content/uploads/2019/03/unnamed-chunk-20-1-3.png" alt="random forests-隨機森林" /></p>
<p>由上圖我們可以看到前三名重要變數依序為：Overall_Qual, Gr_Liv_Area, Garage_Cars。</p>
<h4>Full grid search with H2O</h4>
<p>我們已經知道剛剛在執行ranger進行hyperparameter grid計算時，還花滿長一段時間的。雖然ranger在計算上是有效率的，<br />
但是當遇到大型的grid時，我們手寫的loop迴圈會變得非常沒有效率。</p>
<p>而這時候，h2o套件則是一個強大有效率的java-based介面，可以提供平行分布式運算方法。此外，h20還可以提供不同的&#8221;search path&#8221;。有別於一一執行每一種hyperparameter grid組合，h2o可允許不同的最適搜尋路徑來執行，直到模型成效改善達一定程度等search path。使得在tuning模型上更具效率。以下便來介紹如何使用h2o套件執行random forests。</p><pre class="crayon-plain-tag"># start up h2o
h2o.no_progress() # turn off progress bars when creating reports/tutorials)
h2o.init(max_mem_size = "4g")</pre><p></p><pre class="crayon-plain-tag">##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 days 3 hours 
##     H2O cluster timezone:       Asia/Taipei 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.22.1.1 
##     H2O cluster version age:    2 months and 19 days  
##     H2O cluster name:           H2O_started_from_R_peihsuan_qcx617 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   0.04 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        FALSE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.2 (2018-12-20)</pre><p>首先我們先來使用完整grid search path (又叫做full cartesian)，即表示會逐一檢視所有我們所指派的參數組合(hyper_grid.h2o)。<br />
根據hyper_grid.h2o的參數組合，共會有4*3*2 = 24種組合。也因為這邊是採用cartisian法，所以也不會比上面的方法快多少。</p><pre class="crayon-plain-tag"># create feature names
y &lt;- "Sale_Price"
x &lt;- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o &lt;- as.h2o(ames_train)</pre><p></p><pre class="crayon-plain-tag"># 指派參數組合 hyperparameter grid
hyper_grid.h2o &lt;- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 25, by = 2),
  sample_rate = c(.70, .80)
)

# build grid search 
# 以「cartesian」法逐一執行每一個參數組合的隨機森林模型

# 測試24種組合所需的時間
system.time(
grid &lt;- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid",
  x = x, 
  y = y, 
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = list(strategy = "Cartesian")
  )
)</pre><p></p><pre class="crayon-plain-tag">##   user  system elapsed 
##  9.628   3.104 874.954</pre><p></p><pre class="crayon-plain-tag"># 蒐集結果並依照每一種參數組合模型的MSE誤差來排名
# collect the results and sort by our model performance metric of choice
grid_perf &lt;- h2o.getGrid(
  grid_id = "rf_grid", 
  sort_by = "mse", 
  decreasing = FALSE
  )</pre><p></p><pre class="crayon-plain-tag">print(grid_perf)</pre><p></p><pre class="crayon-plain-tag">## H2O Grid Details
## ================
## 
## Grid ID: rf_grid 
## Used hyper parameters: 
##   -  mtries 
##   -  ntrees 
##   -  sample_rate 
## Number of models: 91 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   mtries ntrees sample_rate        model_ids                 mse
## 1     20    400         0.8 rf_grid_model_85 6.073124636667156E8
## 2     26    300         0.8 rf_grid_model_82 6.119220802095695E8
## 3     20    300         0.8 rf_grid_model_79 6.148210280053709E8
## 4     26    500         0.7 rf_grid_model_70 6.154919117655025E8
## 5     26    400         0.8 rf_grid_model_88 6.155747553830479E8
## 
## ---
##    mtries ntrees sample_rate        model_ids                 mse
## 86     24    200        0.55  rf_grid_model_3 6.629919566473577E8
## 87     28    200         0.7 rf_grid_model_53 6.643770708728626E8
## 88     28    300        0.55 rf_grid_model_11 6.644256979260525E8
## 89     30    200        0.55  rf_grid_model_6 6.658494098992392E8
## 90     22    500        0.55 rf_grid_model_20 6.755417850891622E8
## 91     28    200        0.55  rf_grid_model_5 6.786043491214715E8</pre><p>然而我們從上述結果可以注意到，最好的模型的OOB RMSE只有2.464371 × 10<sup>4</sup> (&lt;25K)，比我們先前所校正的模型效果都還更好。<br />
這是由於h2o套件在參數包括「最小節點大小(minimum node size)」、「樹的深度(tree depth)」等都更「慷慨」，比如說h2o預設最小節點大小為1，而ranger和randomForest該參數則都預設為5。</p>
<h4>Random Discrete grid search with H2O</h4>
<p>當遇到參數變很多的情況下，額外增加一個參數將會巨幅拉大grid search執行時間。為了因應這樣的不變，h2o提供了一種叫做「RandomDiscrete」的grid search搜尋路徑，有別於「Cartisian」逐一執行所有組合，「RandomDiscrete」會隨機挑選參數組合，直到達到一定程度的改善幅度、或是超過一定的時間、或是已執行一定數目的模型數（或是以上三種情況的交叉組合）。雖然使用「RandomDiscrete」搜尋可能會錯過最佳的參數組合效果，但基本上他已經能夠調教出不錯的模型了。</p>
<p>比如說，以下範例的參數組同樣為24種，我們設計一個「RandomDiscrete」grid search，停止條件為達成以下任一條件：近10組模型的MSE相較於最佳模性的改善幅度未超過0.5%、執行時間超過600秒(30 min)。</p><pre class="crayon-plain-tag"># hyperparameter grid
hyper_grid.h2o &lt;- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 25, by = 2),
  sample_rate = c(.70, .80)
)

# random grid search criteria
search_criteria &lt;- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 30*60
  )


# build grid search 
system.time(
random_grid &lt;- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = x,
  y = y,
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )
)</pre><p></p><pre class="crayon-plain-tag">##  user  system elapsed 
##  6.721   1.877 713.918</pre><p></p><pre class="crayon-plain-tag"># collect the results and sort by our model performance metric of choice
grid_perf2 &lt;- h2o.getGrid(
  grid_id = "rf_grid2",
  sort_by = "mse",
  decreasing = FALSE
  )</pre><p></p><pre class="crayon-plain-tag">print(grid_perf2)</pre><p></p><pre class="crayon-plain-tag">H2O Grid Details
================

Grid ID: rf_grid2 
Used hyper parameters: 
  -  mtries 
  -  ntrees 
  -  sample_rate 
Number of models: 24 
Number of failed models: 0 

Hyper-Parameter Search Summary: ordered by increasing mse
  mtries ntrees sample_rate         model_ids                 mse
1     20    500         0.8 rf_grid2_model_19 5.996100879211967E8
2     22    500         0.8 rf_grid2_model_24  6.09073686599265E8
3     22    500         0.7  rf_grid2_model_4 6.096855932933546E8
4     24    400         0.8  rf_grid2_model_8 6.132880206896532E8
5     22    400         0.8 rf_grid2_model_13 6.151492899918578E8

---
   mtries ntrees sample_rate         model_ids                 mse
19     20    500         0.7 rf_grid2_model_12 6.347692770119294E8
20     22    300         0.7 rf_grid2_model_21 6.361713674445038E8
21     24    400         0.7  rf_grid2_model_6 6.431105576136292E8
22     20    400         0.7  rf_grid2_model_9   6.4353236575248E8
23     22    200         0.7  rf_grid2_model_5 6.477837564818393E8
24     24    200         0.8  rf_grid2_model_1  6.50234231493715E8</pre><p>檢視「RandomDiscrete」grid search結果我們發現，經「隨機」檢視24組參數組合後，最佳的模型MSE只有2.448694 × 10<sup>4</sup>  (v.s. Cartisian grid search找到的2.464371 × 10<sup>4</sup>已非常接近)。且random discrete法只花了約11分鐘(=713/60)的時間(v.s. complete search的15分鐘(=874 sec / 60)。</p>
<p>一旦找到了最佳模型，我們就可以將模型套用在hold-out test set測試資料上來計算最後的「驗證誤差 test error」。結果顯示驗證RMSE誤差為23K，比elastic nets(32K)和bagging(36K)法低了$10K左右。</p><pre class="crayon-plain-tag"># 根據Cartesian法中，選出MSE最低的model，
# Grab the model_id for the top model, chosen by validation error
best_model_id &lt;- grid_perf2@model_ids[[1]]
best_model &lt;- h2o.getModel(best_model_id)</pre><p></p><pre class="crayon-plain-tag"># Now let’s evaluate the model performance on a test set
ames_test.h2o &lt;- as.h2o(ames_test)</pre><p></p><pre class="crayon-plain-tag">best_model_perf &lt;- h2o.performance(model = best_model, newdata = ames_test.h2o)</pre><p></p><pre class="crayon-plain-tag"># RMSE of best model
h2o.mse(best_model_perf) %&gt;% sqrt()</pre><p></p><pre class="crayon-plain-tag">## [1] 23303.05</pre><p></p>
<h3>預測</h3>
<p>一但我們挑出了我們偏好的模型，就像之前一樣，可以使用predict()函數將模型套用在新的資料集上做預測。我們可以來比較所有模型類別的預測效果(randomForest, ranger, h2o)(隨然呈現的結果有稍稍的不一樣)。另外要注意的就是h2o模型使用資料的格式不太依樣。</p>
<p>randomForest</p><pre class="crayon-plain-tag"># randomForest
pred_randomForest &lt;- predict(ames_randomForest, ames_test)
head(pred_randomForest)</pre><p></p><pre class="crayon-plain-tag">##        1        2        3        4        5        6 
## 128454.9 155459.6 264329.4 382519.7 211966.4 214173.0</pre><p>ranger</p><pre class="crayon-plain-tag"># ranger
pred_ranger &lt;- predict(ames_ranger, ames_test)
head(pred_ranger$predictions)</pre><p></p><pre class="crayon-plain-tag">## [1] 128893.3 154095.1 270183.4 389106.2 222629.6 210352.8</pre><p>h2o</p><pre class="crayon-plain-tag"># h2o
pred_h2o &lt;- predict(best_model, ames_test.h2o)</pre><p></p><pre class="crayon-plain-tag">head(pred_h2o)</pre><p></p><pre class="crayon-plain-tag">##    predict
## 1 119430.6
## 2 152900.0
## 3 278208.3
## 4 283966.7
## 5 225166.7
## 6 200583.3</pre><p></p>
<h3>小結</h3>
<ol>
<li>random forest是一個非常強大的「開箱即用」的演算法。不用太多的參數調教就會有相當不錯的預測能力。</li>
<li>此外，random forest也是模型中對資料前處理要求最少的模型，不太需要做資料轉換，是許多解決預測問題時最快能應用的方法。</li>
<li>以bagging為基礎，並透過(1)每棵樹進行bootstrapped sample 與 (2)節點分割時隨機挑選變數清單來「去除樹模型間的相關性」，能有效提升模型的高變異性與提升預測精準度。</li>
</ol>
<hr />
<p>參考文章連結：</p>
<p><a href="http://uc-r.github.io/random_forests" target="_blank" rel="noopener noreferrer">隨機森林 random forest</a></p>
<p>更多Decision Tree相關的統計學習筆記：</p>
<p><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></p>
<p><a href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/" target="_blank" rel="noopener noreferrer">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a></p>
<p><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></p>
<p><a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">Tree Surrogate | Tree Surrogate Variables in CART | R 統計</a></p>
<p>更多Regression相關統計學習筆記：</p>
<p><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></p>
<p><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 – 資料探勘與處理 | 統計 R語言</a></p>
<p><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 – 模型建置、診斷與比較 | R語言</a></p>
<p><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 – Ridge, Lasso, Elastic Net | R語言</a></p>
<p>更多Clustering集群分析統計學習筆記：</p>
<p><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></p>
<p><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R 統計</a></p>
<p>其他統計學習筆記：</p>
<p><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></p>
<p>這篇文章 <a rel="nofollow" href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>Decision Tree 決策樹 &#124; CART, Conditional Inference Tree, RandomForest</title>
		<link>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/</link>
					<comments>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/#comments</comments>
		
		<dc:creator><![CDATA[jamleecute]]></dc:creator>
		<pubDate>Fri, 31 Aug 2018 06:08:33 +0000</pubDate>
				<category><![CDATA[ 程式與統計]]></category>
		<category><![CDATA[統計模型]]></category>
		<category><![CDATA[cart]]></category>
		<category><![CDATA[decision tree]]></category>
		<category><![CDATA[prune]]></category>
		<category><![CDATA[random forest]]></category>
		<category><![CDATA[rpart]]></category>
		<category><![CDATA[tree surrogate]]></category>
		<category><![CDATA[決策樹]]></category>
		<category><![CDATA[隨機森林]]></category>
		<guid isPermaLink="false">/?p=1026</guid>

					<description><![CDATA[<p>Decision Tree 決策樹模型是一個不受資料分配限制的模型，模型結果以樹狀呈現，簡單易懂，解釋性極高，且模型同時兼具變數挑選與遺失值填補的機制，並能處理 [&#8230;]</p>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></description>
										<content:encoded><![CDATA[<p>Decision Tree 決策樹模型是一個不受資料分配限制的模型，模型結果以樹狀呈現，簡單易懂，解釋性極高，且模型同時兼具變數挑選與遺失值填補的機制，並能處理分類與回歸問題，是一個廣泛被使用的模型。另外，以決策樹為基礎集成學習而成的隨機森林，更能有效降低模型的錯誤率與並解決過度配適等問題的著名機器學習法之一。</p>
<h3>Decision Tree 決策樹簡介</h3>
<ul>
<li><span style="color: #9f6ad4;">決策樹</span>是一個多功能的機器學習演算法，不僅可以進行<span style="color: #9f6ad4;">分類</span>亦可進行<span style="color: #9f6ad4;">回歸</span>任務。</li>
<li>可以配適複雜的資料集，是個強大的演算法。</li>
<li>屬於<span style="color: #9f6ad4;">無母數回歸</span>方法(<span style="color: #9f6ad4;">non-parametric</span>)：對資料長相的要求不像回歸模型（有母數法，parametric）嚴格，不需要假設資料的線性關係與常態分佈。<br />
(無母數介紹請參考)</li>
<li>決策樹演算法也是隨機森林演算法的基礎(隨機森林也是至今具潛力的演算法之一)。</li>
<li>有諸多演算法，常見的包括CART, CHAID。</li>
<li>決策樹可以用來建立非線性模型，通常被用在迴歸，也可以用在對於遞迴預測變數最二元分類。</li>
</ul>
<h4>補充-無母數統計：</h4>
<ol>
<li>適用於母體分佈情況未知、小樣本、母體分佈不為常態或不易轉換為常態，對資料長相的要求小。</li>
<li>無母數統計推論時所使用的<span style="color: #9f6ad4;">樣本統計量</span>分配通常與母體分配無關，不需要使用樣本統計量去推論母體中位數、適合度、獨立性、隨機性。</li>
<li>無母數統計又稱作「不受分配限制統計法」(distribution-free)。</li>
</ol>
<h4>常見的決策樹演算法比較</h4>
<table style="height: 116px; width: 100%; border-collapse: collapse; background-color: #ffffff;" border="1">
<tbody>
<tr style="height: 23px;" bgcolor="#ddd">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">演算法</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">資料屬性</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">分割規則</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">修剪樹規則</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">ID3</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;">
<div><span style="color: #000000;">Entropy,</span></div>
<div><span style="color: #000000;">Gain Ratio</span></div>
</td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Predicted Error Rate</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">C4.5</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Gain Ratio</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">Predicted Error Rate</span></td>
</tr>
<tr style="height: 23px;">
<td style="width: 25%; height: 23px;"><span style="color: #000000;">CHAID</span></td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">離散型</span></td>
<td style="width: 25%; height: 23px;">
<div><span style="color: #000000;">Chi-Square Test</span></div>
</td>
<td style="width: 25%; height: 23px;"><span style="color: #000000;">No Pruning</span></td>
</tr>
<tr style="height: 24px;">
<td style="width: 25%; height: 24px;"><span style="color: #000000;">CART</span></td>
<td style="width: 25%; height: 24px;"><span style="color: #000000;">離散與連續型</span></td>
<td style="width: 25%; height: 24px;"><span style="color: #000000;">Gini Index</span></td>
<td class="" style="width: 25%; height: 24px;">
<div><span style="color: #000000;">Entire Error Rate</span></div>
<div><span style="color: #000000;">(Training and Predicted)</span></div>
</td>
</tr>
</tbody>
</table>
<h4>決策樹挑選變數常用的測量值</h4>
<p>常見的資訊量（衡量資料<span style="color: #9f6ad4;">純度</span>）：</p>
<ul>
<li><strong>Entropy (熵)</strong>:<br />
$$I_{H}(t)=-\sum_{i=1}^c p(i|t) \log_{2} p(i|t)$$<br />
其中，H代表Homogeneity(同質性)。<br />
<span style="color: #9f6ad4;">當Entropy=0表示completely homogeneous(pure)，而當Entropy=1則表示資料為50%-50%之組成，是不純的</span><span style="color: #9f6ad4;">(impurity)</span>。</li>
<li><strong>Gini Impurity (Gini不純度)</strong>:<br />
$$I_{G}(t) =\sum_{i=1}^c p(i|t)(1-p(i|t)) = 1-\sum_{i=1}^c p(i|t)^2$$<br />
其中，G則代表Gini Impurity。</li>
</ul>
<p>決定切割點的測量值：</p>
<ul>
<li><strong>Information Gain (資訊增益)</strong>: 則衡量節點切割前後資料純度的變化。<span style="color: #9f6ad4;">節點的選擇，當選IG值越大的變數為佳</span>。<br />
$$IG = Info(D) &#8211; Info_{A}(D)$$<br />
其中，\(Info(D)\)為原始資料純度，而\(Info_{\space A}(D)\)則表示使用A規則切割後的資料純度。<br />
$$Info_{\space A}(D)=\sum_{j=1}^m \frac{N_{j}}{N_{p}} Info(D_{j})$$<br />
當m=2，即為二元分類時，<br />
$$IG = Info(D) &#8211; \frac{N_{left}}{N_{p}} Info(D_{left}) &#8211; \frac{N_{right}}{N_{p}} Info(D_{right})$$</li>
</ul>
<h4>資料與分析問題</h4>
<ul>
<li>Data: 鐵達尼資料集包含13個變數與1309筆觀測值。</li>
<li>Problem: 我們想分析與預測具有什麼樣特徵的乘客，比較有機會在冰山撞船後可以存活下來。</li>
<li>Method: 使用CART(Classification and Regression Tree)決策樹模型來找出重要解釋變數。</li>
</ul>
<h4>訓練與視覺化決策樹，我們將進行以下步驟：</h4>
<ol>
<li>載入資料</li>
<li>資料探勘</li>
<li>資料前處理</li>
<li>產生訓練與測試資料集</li>
<li>建置模型</li>
<li>進行預測</li>
<li>衡量模型表現</li>
<li>修剪樹(Post-pruning)</li>
<li>K-Fold Cross Validation</li>
<li>模型比較(1)：條件推論樹(Conditional Inference Tree)</li>
<li>模型比較(2)：隨機森林(Random Forest)</li>
</ol>
<h3>Step1: 載入資料</h3>
<p></p><pre class="crayon-plain-tag"># 從google drive shareable link 讀入csv檔案
# https://drive.google.com/file/d/1S7S-siBGkMR3YUVAbaTkfS1CxOji_Ngd/view?usp=sharing
id &lt;- "1S7S-siBGkMR3YUVAbaTkfS1CxOji_Ngd" # google file ID
inputData &lt;- read.csv(sprintf("https://docs.google.com/uc?id=%s&amp;export=download", id))
head(inputData)

#   pclass survived                                            name    sex     age sibsp parch ticket     fare
# 1      1        1                   Allen, Miss. Elisabeth Walton female 29.0000     0     0  24160 211.3375
# 2      1        1                  Allison, Master. Hudson Trevor   male  0.9167     1     2 113781 151.5500
# 3      1        0                    Allison, Miss. Helen Loraine female  2.0000     1     2 113781 151.5500
# 4      1        0            Allison, Mr. Hudson Joshua Creighton   male 30.0000     1     2 113781 151.5500
# 5      1        0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000     1     2 113781 151.5500
# 6      1        1                             Anderson, Mr. Harry   male 48.0000     0     0  19952  26.5500
#     cabin embarked                       home.dest
# 1      B5        S                    St Louis, MO
# 2 C22 C26        S Montreal, PQ / Chesterville, ON
# 3 C22 C26        S Montreal, PQ / Chesterville, ON
# 4 C22 C26        S Montreal, PQ / Chesterville, ON
# 5 C22 C26        S Montreal, PQ / Chesterville, ON
# 6     E12        S                    New York, NY

tail(inputData)
#      pclass survived                      name    sex  age sibsp parch ticket    fare cabin embarked home.dest
# 1304      3        0     Yousseff, Mr. Gerious   male   NA     0     0   2627 14.4583              C          
# 1305      3        0      Zabour, Miss. Hileni female 14.5     1     0   2665 14.4542              C          
# 1306      3        0     Zabour, Miss. Thamine female   NA     1     0   2665 14.4542              C          
# 1307      3        0 Zakarian, Mr. Mapriededer   male 26.5     0     0   2656  7.2250              C          
# 1308      3        0       Zakarian, Mr. Ortin   male 27.0     0     0   2670  7.2250              C          
# 1309      3        0        Zimmerman, Mr. Leo   male 29.0     0     0 315082  7.8750              S</pre><p>我們可以發現數據是經過排列過的，因為這樣會嚴重影響到我們後續隨機產生訓練與測試資料集，所以我們必須將資料重新隨機排列。</p>
<p>使用sample()隨機產生一組數列index。</p><pre class="crayon-plain-tag">shuffle_index &lt;- sample(x = 1:nrow(inputData))
head(shuffle_index)</pre><p>並將隨機數列index指派給titanic資料集。即可觀察到資料已無排序。</p><pre class="crayon-plain-tag">inputData &lt;- inputData[shuffle_index,]
head(inputData)

#      pclass survived                                   name    sex age sibsp parch     ticket   fare cabin
# 632       3        0            Andersson, Mr. Johan Samuel   male  26     0     0     347075  7.775      
# 526       2        0                       Pain, Dr. Alfred   male  23     0     0     244278 10.500      
# 822       3        0              Goldsmith, Mr. Frank John   male  33     1     1     363291 20.525      
# 485       2        1           Lemore, Mrs. (Amelia Milley) female  34     0     0 C.A. 34260 10.500   F33
# 627       3        0 Andersson, Miss. Ida Augusta Margareta female  38     4     2     347091  7.775      
# 1183      3        1       Salkjelsvik, Miss. Anna Kristine female  21     0     0     343120  7.650      
#      embarked                         home.dest
# 632         S                      Hartford, CT
# 526         S                      Hamilton, ON
# 822         S Strood, Kent, England Detroit, MI
# 485         S                       Chicago, IL
# 627         S      Vadsbro, Sweden Ministee, MI
# 1183        S</pre><p></p>
<h3>Step2: 資料探勘</h3>
<p>使用summary()摘要基礎統計。</p><pre class="crayon-plain-tag">summary(inputData)

#        pclass         survived                                   name          sex           age         
# Min.   :1.000   Min.   :0.000   Connolly, Miss. Kate            :   2   female:466   Min.   : 0.1667  
# 1st Qu.:2.000   1st Qu.:0.000   Kelly, Mr. James                :   2   male  :843   1st Qu.:21.0000  
# Median :3.000   Median :0.000   Abbing, Mr. Anthony             :   1                Median :28.0000  
# Mean   :2.295   Mean   :0.382   Abbott, Master. Eugene Joseph   :   1                Mean   :29.8811  
# 3rd Qu.:3.000   3rd Qu.:1.000   Abbott, Mr. Rossmore Edward     :   1                3rd Qu.:39.0000  
# Max.   :3.000   Max.   :1.000   Abbott, Mrs. Stanton (Rosa Hunt):   1                Max.   :80.0000  
#                                 (Other)                         :1301                NA's   :263      
#          sibsp            parch            ticket          fare                     cabin      embarked
# Min.   :0.0000   Min.   :0.000   CA. 2343:  11   Min.   :  0.000                  :1014    :  2   
# 1st Qu.:0.0000   1st Qu.:0.000   1601    :   8   1st Qu.:  7.896   C23 C25 C27    :   6   C:270   
# Median :0.0000   Median :0.000   CA 2144 :   8   Median : 14.454   B57 B59 B63 B66:   5   Q:123   
# Mean   :0.4989   Mean   :0.385   3101295 :   7   Mean   : 33.295   G6             :   5   S:914   
# 3rd Qu.:1.0000   3rd Qu.:0.000   347077  :   7   3rd Qu.: 31.275   B96 B98        :   4           
# Max.   :8.0000   Max.   :9.000   347082  :   7   Max.   :512.329   C22 C26        :   4           
#                                  (Other) :1261   NA's   :1         (Other)        : 271           
#                home.dest  
# :564  
# New York, NY        : 64  
# London              : 14  
# Montreal, PQ        : 10  
# Cornwall / Akron, OH:  9  
# Paris, France       :  9  
# (Other)             :639</pre><p>使用str()查看資料結構。</p><pre class="crayon-plain-tag">str(inputData)
# 'data.frame':	1309 obs. of  12 variables:
# $ pclass   : int  3 2 3 2 3 3 1 3 3 2 ...
# $ survived : int  0 0 0 1 0 1 1 0 1 1 ...
# $ name     : Factor w/ 1307 levels "Abbing, Mr. Anthony",..: 41 920 459 703 36 1068 864 949 1092 516 ...
# $ sex      : Factor w/ 2 levels "female","male": 2 2 2 1 1 1 1 2 1 1 ...
# $ age      : num  26 23 33 34 38 21 23 NA NA 7 ...
# $ sibsp    : int  0 0 1 0 4 0 1 0 0 0 ...
# $ parch    : int  0 0 1 0 2 0 0 0 0 2 ...
# $ ticket   : Factor w/ 929 levels "110152","110413",..: 453 194 584 767 468 410 574 415 388 783 ...
# $ fare     : num  7.78 10.5 20.52 10.5 7.78 ...
# $ cabin    : Factor w/ 187 levels "","A10","A11",..: 1 1 1 183 1 1 134 1 1 1 ...
# $ embarked : Factor w/ 4 levels "","C","Q","S": 4 4 4 4 4 4 2 4 3 4 ...
# $ home.dest: Factor w/ 370 levels "","?Havana, Cuba",..: 154 150 317 64 345 1 191 1 1 165 ...</pre><p>我們可初步發現：</p>
<ol>
<li>pclass(座艙等級)和survuved(生存與否)應由int轉換成factor變數</li>
<li><span style="color: #9f6ad4;">類別水準數過多</span>的變數：<span style="color: #9f6ad4;">name</span>(1307 levels),<span style="color: #9f6ad4;">ticket</span>(929 levels), <span style="color: #9f6ad4;">cabin</span>(187 levels), <span style="color: #9f6ad4;">home.dest</span>(370 levels)<span style="color: #9f6ad4;">應予以排除</span>。</li>
<li>排除以上變數後，存在許多遺失值(NA value)的變數有：age(263), fare(1)。<span style="color: #9f6ad4;">但由於CART決策樹rpart()演算法中，預設會刪除y遺失的資料列，並保留至少有一個預測變數未遺失的觀察資料列，並使用Surrogate Variables來預測遺失特徵值。因此我們不會特別處理遺失值的部分</span>。(*更多決策樹遺失值預測請參考<a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate in CART</a>)</li>
</ol>
<h3>Step3: 資料前處理</h3>
<p>根據資料探勘結果，要處理的項目如下：</p>
<ol>
<li>移除變數<span style="color: #9f6ad4;">name</span>(1307 levels),<span style="color: #9f6ad4;">ticket</span>(929 levels), <span style="color: #9f6ad4;">cabin</span>(187 levels), <span style="color: #9f6ad4;">home.dest</span></li>
<li>將變數pclass(座艙等級)和survuved(生存與否)轉換為factor變數。</li>
</ol>
<p></p><pre class="crayon-plain-tag">library(dplyr)

clean_inputData &lt;- 
  inputData %&gt;% 
  # Drop variables
  select(-c(home.dest, cabin, name, ticket)) %&gt;% 
  #Convert to factor level
  mutate(pclass = factor(pclass, levels = c(1, 2, 3), labels = c('Upper', 'Middle', 'Lower')),
         survived = factor(survived, levels = c(0, 1), labels = c('No', 'Yes')))

glimpse(clean_inputData)

# Observations: 1,309
# Variables: 8
# $ pclass   &lt;fct&gt; Lower, Middle, Lower, Middle, Lower, Lower, Upper, Lower, Lower, Middle, Lower, Upper, Upper, Middle, Lower, Lower, Lower, Upper, Lower, Lower,...
# $ survived &lt;fct&gt; No, No, No, Yes, No, Yes, Yes, No, Yes, Yes, No, Yes, No, Yes, No, No, No, Yes, No, Yes, No, Yes, Yes, Yes, No, Yes, No, No, No, Yes, No, Yes, ...
# $ sex      &lt;fct&gt; male, male, male, female, female, female, female, male, female, female, male, female, male, female, male, male, female, female, male, female, m...
# $ age      &lt;dbl&gt; 26.0, 23.0, 33.0, 34.0, 38.0, 21.0, 23.0, NA, NA, 7.0, 1.0, 16.0, 58.0, 24.0, 33.0, NA, NA, 36.0, 36.0, 19.0, 19.0, 25.0, 51.0, 4.0, 40.0, 18.0...
# $ sibsp    &lt;int&gt; 0, 0, 1, 0, 4, 0, 1, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 8, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ parch    &lt;int&gt; 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 3, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...
# $ fare     &lt;dbl&gt; 7.7750, 10.5000, 20.5250, 10.5000, 7.7750, 7.6500, 113.2750, 7.7750, 7.7792, 26.2500, 46.9000, 57.9792, 113.2750, 27.0000, 7.7750, 8.0500, 7.75...
# $ embarked &lt;fct&gt; S, S, S, S, S, S, C, S, Q, S, S, C, C, S, S, S, Q, C, S, S, S, C, S, S, S, C, S, S, S, S, S, C, C, S, S, S, S, S, Q, S, S, S, S, Q, Q, S, C, S,...</pre><p></p>
<h3>Step4: 產生訓練與測試資料集</h3>
<p>為了確保兩組資料集中生還比例不要差異太大，我們會先將資料依據目標變數(survived)分成兩組(No, Yes)，再進行隨機切割成80%訓練組跟20%測試組。</p><pre class="crayon-plain-tag">input_ones &lt;- clean_inputData[which(clean_inputData$survived == 'Yes'),]
input_zeros &lt;- clean_inputData[which(clean_inputData$survived == 'No'), ]
set.seed(100)
input_ones_training_row &lt;- sample(1:nrow(input_ones),0.8*nrow(input_ones))
input_zeros_training_row &lt;- sample(1:nrow(input_zeros),0.8*nrow(input_zeros))

training_ones &lt;- input_ones[input_ones_training_row,]
training_zeros &lt;- input_zeros[input_zeros_training_row,]
trainingData &lt;- rbind(training_ones, training_zeros)

# 產生測試資料集
test_ones &lt;- input_ones[-input_ones_training_row,]
test_zeros &lt;- input_zeros[-input_zeros_training_row,]
testData &lt;- rbind(test_ones, test_zeros)</pre><p>檢查切割完的資料集大小與目標變數的分佈比例：</p>
<ul>
<li>原始資料列1309被隨機切割為80%訓練資料集(1047筆)與20%測試資料集(262筆)。</li>
<li>發現訓練及測試資料集的目標變數survived比例都是38%。差異在1%以內。</li>
</ul>
<p></p><pre class="crayon-plain-tag">dim(trainingData)
# [1] 1047    8
dim(testData)
# [1] 262   8

# 確認兩資料是隨機的
prop.table(table(trainingData$survived))
#        No       Yes 
# 0.6179561 0.3820439 

prop.table(table(testData$survived))
#        No       Yes 
# 0.6183206 0.3816794</pre><p></p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step5: 建置模型</h3>
<p>我們使用CART(Classification and Regression Tree)決策樹演算法-rpart()。</p>
<ul>
<li>rpart為遞迴分割法(<span style="color: #9f6ad4;">R</span>ecursive <span style="color: #9f6ad4;">Part</span>itioning Tree)的縮寫。</li>
<li>對所有參數和分割點進行評估。</li>
<li>最佳選擇是使分割後的組內資料更為「一致(pure)」。
<ul>
<li>「一致(pure)」是指組內資料的應變數取直變異較小。</li>
</ul>
</li>
<li><span style="color: #9f6ad4;">使用Gini值測量資料的「一致(pure)」性(Homogeneity)</span>。</li>
<li>建模過程分為兩階段(2 stages)：
<ul>
<li>先長出最複雜的樹(grow the complex/full tree)。(直到Leaf size樹葉內的觀測個數少於5個或是模型沒有優化的空間為止）</li>
<li>再使用交叉驗證(Cross Validation)來修剪樹(Prune)。並尋找<span style="color: #9f6ad4;">使估計風險值(estimate of risk)參數(complexity parameter)</span>最小值的決策樹。</li>
</ul>
</li>
</ul>
<p>rpart()參數設定：</p>
<ul>
<li>method分成 “anova”、”poisson”、”class”和”exp”。當目標變數為factor時，我們將其設定為&#8221;class&#8221;。</li>
<li>control: 通常會使用rpart.control()另外作設定(事前修樹，pre-prune)。</li>
<li>na.action: <a href="https://www.rdocumentation.org/packages/rpart/versions/4.1-13/topics/rpart">預設為<span style="color: #9f6ad4;">na.rpart</span></a>，即使用CART演算法中的<a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">surrogate variables</a>做預測。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(rpart)
library(rpart.plot)

fit &lt;- rpart(formula = survived ~ ., data = trainingData, method = 'class')
# arguments:
# method: 
# - "class" for a classification tree (y is a factor) 			
# - "anova" for a regression tree</pre><p>使用rpart.plot()檢視決策樹規則。</p><pre class="crayon-plain-tag">rpart.plot(fit, extra= 106)</pre><p><span style="color: #9f6ad4;">節點顏色越綠越深，代表該節點(條件下)的survived機率越高（目標事件發生機率越高）</span>。</p>
<p>每個Node節點上的數值分別代表:</p>
<ul>
<li>預測類別(0,1)</li>
<li>預測目標類別的機率(1的機率)</li>
<li>節點中觀測資料個數佔比</li>
</ul>
<p><img loading="lazy" class="alignnone wp-image-1032" src="/wp-content/uploads/2018/08/Rplot01-1.jpeg" alt="Decision Tree " width="648" height="609" /></p>
<p>將決策樹規則使用rpart.rules()印出。</p><pre class="crayon-plain-tag">rpart.rules(x = fit,cover = TRUE)
# survived                                                                                         cover
#     0.06 when sex is   male                             &amp; age &lt;  9.5              &amp; sibsp &gt;= 3      2%
#     0.07 when sex is female &amp; pclass is           Lower              &amp; fare &gt;= 23                   3%
#     0.17 when sex is   male                             &amp; age &gt;= 9.5                               61%
#     0.58 when sex is female &amp; pclass is           Lower              &amp; fare &lt;  23                  14%
#     0.90 when sex is   male                             &amp; age &lt;  9.5              &amp; sibsp &lt;  3      2%
#     0.93 when sex is female &amp; pclass is Upper or Middle                                            19%</pre><p>可發現規則依照survived比例（目標事件發生機率）由低到高排序。cover則代表該節點觀測資料個數占比。</p>
<p>檢視交叉驗證(cross-validation)的不同cp值(complexity parameter)下的錯誤率。</p>
<p><span style="color: #9f6ad4;">cp值代表的是每一個規則（切割）所能改善模型適合度的程度(cross validation relative error, or X-val relative error)。可以發現每一個新的規則的cp呈遞減趨勢。且rpart()預設cp=0.01，即代表如果該規則（切割）沒有達到至少0.01的模型適合度改善，則停止。</span>(*<a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf" target="_blank" rel="noopener noreferrer">rpart函數對complexity parameter的說明</a>)</p><pre class="crayon-plain-tag">printcp(x = fit) 

# Classification tree:
#   rpart(formula = survived ~ ., data = trainingData, method = "class")
# 
# Variables actually used in tree construction:
#   [1] age    fare   pclass sex    sibsp 
# 
# Root node error: 400/1047 = 0.38204
# 
# n= 1047 
# 
#      CP nsplit rel error xerror     xstd
# 1 0.425      0     1.000  1.000 0.039305
# 2 0.030      1     0.575  0.575 0.033492
# 3 0.020      3     0.515  0.530 0.032507
# 4 0.010      5     0.475  0.510 0.032040</pre><p>將模型的cp table畫出。<span style="color: #9f6ad4;">可以觀察到，隨著模型的複雜度（成本）增加，所能改善的模型適合度的空間降低(X-val relative error)</span>。</p><pre class="crayon-plain-tag">plotcp(x = fit)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1034" src="/wp-content/uploads/2018/08/Rplot02-1.jpeg" alt="Decision Tree " width="816" height="771" srcset="/wp-content/uploads/2018/08/Rplot02-1.jpeg 816w, /wp-content/uploads/2018/08/Rplot02-1-300x283.jpeg 300w, /wp-content/uploads/2018/08/Rplot02-1-768x726.jpeg 768w, /wp-content/uploads/2018/08/Rplot02-1-230x217.jpeg 230w, /wp-content/uploads/2018/08/Rplot02-1-350x331.jpeg 350w, /wp-content/uploads/2018/08/Rplot02-1-480x454.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<h3>Step6: 進行預測</h3>
<p>使用predict()將訓練好的模型套用在測試資料集上。</p><pre class="crayon-plain-tag">predicted &lt;- predict(object = fit,newdata = testData,type = 'class')
# 參數說明：
# type: Type of prediction			
# - 'class': for classification			
# - 'prob': to compute the probability of each class			
# - 'vector': Predict the mean response at the node level</pre><p></p>
<h3>Step7: 衡量模型表現</h3>
<p>由於預測結果為類別型(0,1)，故我們以Confusion Matrix為基礎，來計算以下幾個常用指標：</p>
<ul>
<li>Accuracy/Misclassification Rate</li>
<li>Precision</li>
<li>Sensitivity(or Recall)</li>
<li>Specificity</li>
</ul>
<p>計算Confusion Matrix（數據左欄位預測值，上方列為真實值）</p><pre class="crayon-plain-tag">tbl &lt;- table(predicted,testData$survived)
tbl
# predicted  No Yes
#       No  140  28
#       Yes  22  72</pre><p>計算模型的正確率Accuracy</p><pre class="crayon-plain-tag"># Accuracy
accuracy &lt;- sum(diag(tbl)) / sum(tbl)
accuracy
# [1] 0.8091603</pre><p>可以發現<span style="color: #9f6ad4;">未修剪的模型</span>對測試資料的<span style="color: #9f6ad4;">預測正確率高達近81％</span>。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step8: 修剪樹(Post-Pruning)</h3>
<p>一般來說，修剪樹可以分為事前與事後。</p>
<ul>
<li><strong>事前</strong>：透過<span style="color: #9f6ad4;">rpart.control()</span>來調整重要參數，包括：
<ul>
<li><strong>minsplit</strong>：每一個node最少要幾個觀測值，預設為20。</li>
<li><strong>minbucket</strong>：在末端的node上(Leaf,樹葉)最少要幾個觀測值，預設為round(minsplit/3)。</li>
<li><strong>cp</strong>：complexity parameter。決定當新規則加入，改善模型相對誤差(x-val relative value)的程度如沒有大於cp值，則不加入該規則。<span style="color: #9f6ad4;">預設為0.01</span>。</li>
<li><strong>maxdepth</strong>：決策樹的深度，建議不超過6層。</li>
</ul>
</li>
<li><strong>事後</strong>：則是透過prune(x = , cp = )來設定。</li>
</ul>
<p>我們這邊採用post-pruning法。並選擇讓交叉驗證中相對誤差改變量最小的cp值。</p><pre class="crayon-plain-tag"># prune tree ：
fit.prune &lt;- prune(fit, cp = fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])</pre><p>將依據cp門檻值修剪後的樹規則繪出。</p><pre class="crayon-plain-tag"># plot the pruned tree 
rpart.plot(fit.prune, extra= 106, tweak = 1.1, shadow.col = "gray", branch.lty = 3, roundint = TRUE)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1035" src="/wp-content/uploads/2018/08/Rplot03_prune.jpeg" alt="Decision Tree " width="816" height="771" srcset="/wp-content/uploads/2018/08/Rplot03_prune.jpeg 816w, /wp-content/uploads/2018/08/Rplot03_prune-300x283.jpeg 300w, /wp-content/uploads/2018/08/Rplot03_prune-768x726.jpeg 768w, /wp-content/uploads/2018/08/Rplot03_prune-230x217.jpeg 230w, /wp-content/uploads/2018/08/Rplot03_prune-350x331.jpeg 350w, /wp-content/uploads/2018/08/Rplot03_prune-480x454.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>查看prune tree預測正確率。</p><pre class="crayon-plain-tag">tbl_prune &lt;- table(predicted = predicted.prune, actuals = testData$survived)

tbl_prune   
#          actuals
# predicted  No Yes
#       No  140  28
#       Yes  22  72

# Accuracy
accuracy &lt;- sum(diag(tbl_prune)) / sum(tbl_prune)
accuracy

# [1] 0.8091603</pre><p>可以發現pruned tree 和full tree兩者長得一樣，Accuracy也相同。<span style="color: #9f6ad4;">原因在於，因為在建立full tree時，預設cp=0.01，跟prune()使用的cp值是相同的</span>。</p>
<h3>Step 9: K-Fold Cross Validation</h3>
<p>為了確保模型無過度配適(overfitting)和預測準度的穩定性，我們使用k-fold cross validation(k=10)重新抽樣樣本進行模型驗證。理想中，交叉驗證後的平均正確率應與prune tree相近。</p>
<p>其中必須注意的是，因為資料中有遺失值觀測值，且train()函數中<span style="color: #9f6ad4;">參數na.action預設值為na.fail(即遇到有遺失值程序會失敗）</span>，故必須<span style="color: #0000ff;"><span style="color: #9f6ad4;">將設定調整為na.pass(不採取任何動作)或na.omit(忽略有遺失值的觀測值)</span><span style="color: #333333;">，方能正常執行函數指令。</span></span></p><pre class="crayon-plain-tag">library(caret)
library(e1071)
# 選則resampling的方法
train_control &lt;- trainControl(method = "cv",number = 10) # k = 10
# specify the model 
train_control.model &lt;- train(survived ~ ., data = trainingData, method = 'rpart',na.action = na.pass, trControl = train_control)
train_control.model

# CART 
# 
# 1047 samples
#    7 predictor
#    2 classes: 'No', 'Yes' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 943, 942, 942, 943, 942, 943, ... 
# Resampling results across tuning parameters:
#   
#   cp     Accuracy   Kappa    
#   0.020  0.8041850  0.5738715
#   0.030  0.7851282  0.5357307
#   0.425  0.6676099  0.1752418
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.02.</pre><p>進行10次交叉驗證的平均正確率為80.4%，與修剪後的樹模型正確率80.91%沒有太大差異（差異百分比在1%以內）。表示模型沒有overfitting的問題。</p>
<p>如果將參數na.action調整為na.rpart（使用CART中的代理變數surrogate variables來預測)。</p><pre class="crayon-plain-tag">train_control.model.2 &lt;- train(survived ~ ., data = trainingData, method = 'rpart',na.action = na.rpart, trControl = train_control)
train_control.model.2
# CART 
# 
# 1047 samples
# 7 predictor
# 2 classes: 'No', 'Yes' 
# 
# No pre-processing
# Resampling: Cross-Validated (10 fold) 
# Summary of sample sizes: 942, 943, 942, 943, 942, 942, ... 
# Resampling results across tuning parameters:
#   
#   cp     Accuracy   Kappa    
# 0.020  0.8042308  0.5735154
# 0.030  0.7880037  0.5452657
# 0.425  0.6685714  0.1862793
# 
# Accuracy was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.02.</pre><p>進行10次交叉驗證的平均正確率亦約為80.4%。</p>
<h3>Step 10: 模型比較(1)-條件推論樹(Conditional Inference Tree)</h3>
<ul>
<li>R的party套件提供<span style="color: #9f6ad4;">無母數回歸(non-parametric regression)樹模型</span>，可處理名目(nominal)、尺度(ordinal)、數值(numeric)、設限(censored)、多變量(multivariate)資料型態。</li>
<li>你可以使用ctree(formula, data = )函數來產生分類或回歸樹模型，樹模型類型會根據目標變數型態而有所不同。</li>
<li>ctree()透過統計檢驗來決定預測變數與分割點之選擇。
<ul>
<li>先假設所有預測變數與目標變數獨立(Null Hypothesis)。</li>
<li>進行<span style="color: #9f6ad4;">卡方獨立檢定(Chi-Square Test)</span>。</li>
<li>檢驗<span style="color: #9f6ad4;">p-value</span>小於threshold(ex: 0.05)則拒絕虛無假設，表示預測變數與目標變數具有顯著相關性，加入模型。</li>
<li>將相關性最強的變數選做第一次分割的變數。</li>
<li>繼續在各自子資料集進行分割變數計算與選擇。</li>
</ul>
</li>
<li>因為樹是<span style="color: #9f6ad4;">根據統計量顯著與否</span>來判斷規則之必要性，<span style="color: #9f6ad4;">因此與rpart()不同，ctree()是不需要剪枝的(prune)</span>。</li>
<li>另參數na.action預設為na.pass (即不採取任何動作)。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(party)
fit_ctree &lt;- ctree(survived ~ ., data = trainingData)
plot(fit_ctree)
predicted.ctree &lt;- predict(object = fit_ctree, newdata = testData)

tbl_ctree &lt;-table(predicted = predicted.ctree, actuals = testData$survived)
tbl_ctree

#          actuals
# predicted  No Yes
#       No  140  29
#       Yes  22  71

# Accuracy
accuracy &lt;- sum(diag(tbl_ctree)) / sum(tbl_ctree)
accuracy
# [1] 0.8053435</pre><p>可以發現模型準確率為80.5%，跟rpart演算法的k-fold交叉驗證的平均正確率80.4%沒有差太多。</p>
<div align="center"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><br />
<!-- text & display ads 1 --><br />
<ins class="adsbygoogle" style="display: block;" data-ad-client="ca-pub-7946632597933771" data-ad-slot="8154450369" data-ad-format="auto" data-full-width-responsive="true"></ins><br />
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script></div>
<h3>Step 11: 模型比較(2)-隨機森林(Random Forest)</h3>
<ul>
<li>隨機森林是一個<span style="color: #9f6ad4;">集成學習法(ensemble learning)</span>，意思是將幾個建立好的模型結果整合在一起，以提升預測準確率。</li>
<li>由集成學習法建立的模型<span style="color: #9f6ad4;">較能不容易發生過度配適的問題</span>，雖然提供較好的預測，但在推論和解釋度方面就會有所限制。</li>
<li>隨機森林由好幾個決策樹所組成，而不同決策樹是由不同隨機抽取的預測變數形成的。</li>
<li>而且特別的是，<span style="text-decoration: underline;">隨機森林不止對列(Row)進行抽樣，亦對行(Column)進行抽樣</span>，因此所產生的子集資料，其實是行與列同時抽樣後的結果。</li>
<li>對<span style="color: #9f6ad4;">列抽樣</span>，可以部分解決因<span style="color: #9f6ad4;">類別不平衡(Class Imbalance)</span>對預測帶來的問題；而對<span style="color: #9f6ad4;">行抽樣</span>，則可解決部分因<span style="color: #9f6ad4;">共線性(collinearity)</span>對預測造成的問題。<br />
(若是探討對「變數解釋性」的影響，則需要用 Lasso和Stepwise來解決)。</li>
<li>我們可用R裡面randomForest套件中的<span style="color: #9f6ad4;">randomForest()函數</span>來建立隨機森林。</li>
<li>參數na.action預設為na.fail (即遇到遺失值則停止執行)。因為資料集中有遺失觀測值，故必須將之調整為na.omit。</li>
</ul>
<p></p><pre class="crayon-plain-tag">library(randomForest)
set.seed(101)
fit.rf &lt;- randomForest(survived ~ ., data = trainingData, na.action = na.omit)</pre><p>檢視模型訓練結果。</p>
<ul>
<li>Number of trees: 隨機森林由500棵隨機生成的決策樹所組成。</li>
<li><span class="bash">利用<span style="color: #9f6ad4;">OOB(Out Of Bag)</span>運算出來的</span><span style="color: #9f6ad4;">錯誤率</span>為<span style="color: #0000ff;"><span style="color: #9f6ad4;">18.82%</span><span style="color: #333333;">。</span></span></li>
</ul>
<p></p><pre class="crayon-plain-tag"># Call:
#   randomForest(formula = survived ~ ., data = trainingData, na.action = na.omit) 
# Type of random forest: classification
# Number of trees: 500
# No. of variables tried at each split: 2
# 
# OOB estimate of  error rate: 18.82%
# Confusion matrix:
#      No Yes class.error
# No  463  44  0.08678501
# Yes 116 227  0.33819242</pre><p>自己驗證與計算較精確的OOB estimate正確率Accuracy為81.17%。</p><pre class="crayon-plain-tag">tbl.rf &lt;- fit.rf$confusion[,c(1,2)]
accuracy &lt;- sum(diag(tbl.rf)) / sum(tbl.rf)
accuracy
# [1] 0.8117647</pre><p>另外，我們將「<span style="color: #9f6ad4;">增加每一顆決策樹，整體誤差的改變量</span>」繪出，以輔助決策「需要多少顆決策樹，整體誤差才會趨於穩定」。</p>
<ul>
<li>當為分類樹時(classification tree)
<ul>
<li>誤差為OOB(out-of-bag) Erro Rates。</li>
<li><span style="color: #9f6ad4;">黑色實線表示整體的OOB error rate，而其他顏色虛線表示各類別的OOB Error Rate</span>。</li>
</ul>
</li>
<li>當為回歸樹時(regression tree)
<ul>
<li>誤差為OOB(out-of-bag) MSE。</li>
<li><span style="color: #9f6ad4;">只會有一條黑色實線代表整體的OOB MSE</span>。</li>
</ul>
</li>
</ul>
<p></p><pre class="crayon-plain-tag">plot(fit.rf)</pre><p>從圖中幾條線可以觀察到：</p>
<ul>
<li>整體錯誤率（黑色實線）隨著決策樹數量上升，下降到約18%並趨於穩定。</li>
<li>實際類別為Yes的錯誤率（綠色虛線）隨著決策樹數量的上升，下降到約33.8%並趨於穩定。</li>
<li>實際類別為No的錯誤率（紅色虛線）隨著決策樹數量的上升，下降到約8.6%並趨於穩定。</li>
<li>而「<span style="color: #9f6ad4;">最佳決策樹數目(ntree)</span><span style="color: #0000ff;">」</span>，約100多棵樹即足夠使誤差趨於穩定（不需要到500棵樹）。</li>
</ul>
<p><img loading="lazy" class="alignnone size-full wp-image-1103" src="/wp-content/uploads/2018/08/Rplot04_rf_plot.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot04_rf_plot.jpeg 816w, /wp-content/uploads/2018/08/Rplot04_rf_plot-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot04_rf_plot-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot04_rf_plot-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot04_rf_plot-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot04_rf_plot-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>另外一個隨機森林中一個重要參數：mtry，表示每一個樹節點(node)在進行切割時(split)隨機抽樣的變數數量。可使用tuneRF()來調整mtry的值。</p><pre class="crayon-plain-tag">trainingData_naomit &lt;- na.omit(trainingData)
tuneRF(x = trainingData_naomit[,-2], y = trainingData_naomit[,2])


# mtry = 2  OOB error = 19.53% 
# Searching left ...
# mtry = 1 	OOB error = 20.71% 
# -0.06024096 0.05 
# Searching right ...
# mtry = 4 	OOB error = 21.53% 
# -0.1024096 0.05 
#       mtry  OOBError
# 1.OOB    1 0.2070588
# 2.OOB    2 0.1952941
# 4.OOB    4 0.2152941</pre><p>可以發現在mtry=2時，誤差最小。</p>
<p><img loading="lazy" class="alignnone size-full wp-image-1111" src="/wp-content/uploads/2018/08/Rplot05_tuneRF.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot05_tuneRF.jpeg 816w, /wp-content/uploads/2018/08/Rplot05_tuneRF-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot05_tuneRF-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot05_tuneRF-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot05_tuneRF-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot05_tuneRF-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>randomForest中類別樹預設的mtry=sqrt(p)，其中p代表x變數的數目。因為原始隨機森林模型預設值跟tuneRF建議的值相同，故我們另外不調整。</p><pre class="crayon-plain-tag">fit.rf$mtry
# [1] 2</pre><p>看每個x變數的重要性(<span style="color: #9f6ad4;">the mean decrease in Gini index</span>)，即看哪個變數對<span style="color: #9f6ad4;">損失函數Loss Function</span>最有貢獻。(*randomForest參數importance預設值為False，僅會產生the mean decrease in Gini index，如果要產生其他指標如mean decrease in accuracy，要將其改為TRUE。)</p><pre class="crayon-plain-tag">round(importance(fit.rf),2) # importance of each predictor
# or
# round(fit.rf$importance, 2)

#          MeanDecreaseGini
# pclass              27.55
# sex                100.67
# age                 55.09
# sibsp               13.89
# parch               13.40
# fare                59.79
# embarked            11.55</pre><p>將變數重要性（貢獻度）繪出。</p><pre class="crayon-plain-tag">varImpPlot(fit.rf)</pre><p><img loading="lazy" class="alignnone size-full wp-image-1115" src="/wp-content/uploads/2018/08/Rplot06_varImpPlot.jpeg" alt="Decision Tree " width="816" height="900" srcset="/wp-content/uploads/2018/08/Rplot06_varImpPlot.jpeg 816w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-272x300.jpeg 272w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-768x847.jpeg 768w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-230x254.jpeg 230w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-350x386.jpeg 350w, /wp-content/uploads/2018/08/Rplot06_varImpPlot-480x529.jpeg 480w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>最後將調整好的模型應用在testData並評估正確性。</p><pre class="crayon-plain-tag">predicted.rf &lt;- predict(object = fit.rf, newdata = testData)
tbl.rf &lt;- table(predicted = predicted.rf, actuals = testData$survived)
accuracy &lt;- sum(diag(tbl.rf)) / sum(tbl.rf)
accuracy
# [1] 0.8307692</pre><p>隨機森林的預測準確率為83%，較原先的決策樹(accuracy = 80%)改善約4%。</p>
<h3>總結</h3>
<ul>
<li>Decision Tree 決策樹模型具有簡單易懂的樹狀邏輯圖，可解釋度高。</li>
<li>Decision Tree 決策樹對於資料的要求低，沒有常態分配與線性關係的假設，不受資料分配限制。</li>
<li>Decision Tree 決策樹的<span style="color: #9f6ad4;">有變數篩選機制</span>。
<ul>
<li>rpart演算法進行Gini Index檢定，並計算complexity parameter來進行變數篩選。</li>
<li>ctree演算法進行chi-square檢定，檢驗各投入變數是否與目標變數相關並計算p-value來看相關性的顯著效果。</li>
</ul>
</li>
<li>Decision Tree 決策樹亦<span style="color: #9f6ad4;">有空值填補機制</span> &#8211; <a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate</a>。
<ul>
<li>rpart演算法na.action預設為na.rpart。(更多有關決策樹遺失值預測請參考 <a href="/decision-tree-surrogate-in-cart/" target="_blank" rel="noopener noreferrer">tree surrogate in CART</a>)</li>
<li><a href="https://www.rdocumentation.org/packages/partykit/versions/1.2-2/topics/ctree">ctree演算法na.action預設為na.pass</a>，可以將其改為na.rpart。</li>
</ul>
</li>
<li>Decision Tree 演算法<span style="color: #9f6ad4;">rpart和ctree皆能處理連續型(continuous)與類別型(categorical)變數之切割</span>。</li>
<li>由Decision Tree 決策樹衍伸出的集成學習法「<span style="color: #9f6ad4;">隨機森林 random forest</span>」可以有效降低模型的錯誤率、解決過度配適、透過反覆抽樣解決類別不平衡與共線性問題。</li>
</ul>
<hr />
<p>更多<span style="color: #9f6ad4;">統計模型</span>筆記連結：</p>
<ol>
<li><a href="/linear-regression-%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8%e6%a8%a1%e5%9e%8b/" target="_blank" rel="noopener noreferrer">Linear Regression | 線性迴歸模型 | using AirQuality Dataset</a></li>
<li><a href="/regularized-regression-ridge-lasso-elastic/" target="_blank" rel="noopener noreferrer">Regularized Regression | 正規化迴歸 &#8211; Ridge, Lasso, Elastic Net | R語言</a></li>
<li><a href="/logistic-regression-part1-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part1 &#8211; 資料探勘與處理 | 統計 R語言</a></li>
<li><a href="/logistic-regression-part2-%e7%be%85%e5%90%89%e6%96%af%e8%bf%b4%e6%ad%b8/" target="_blank" rel="noopener noreferrer">Logistic Regression 羅吉斯迴歸 | part2 &#8211; 模型建置、診斷與比較 | R語言</a></li>
<li><a href="/regression-tree-%e8%bf%b4%e6%ad%b8%e6%a8%b9-bagging-bootstrap-aggrgation-r%e8%aa%9e%e8%a8%80/" target="_blank" rel="noopener noreferrer">Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言</a></li>
<li><a href="/random-forests-%e9%9a%a8%e6%a9%9f%e6%a3%ae%e6%9e%97/" target="_blank" rel="noopener noreferrer">Random Forests 隨機森林 | randomForest, ranger, h2o | R語言</a></li>
<li><a href="/gradient-boosting-machines-gbm/" target="_blank" rel="noopener noreferrer">Gradient Boosting Machines GBM | gbm, xgboost, h2o | R語言</a></li>
<li><a href="/hierarchical-clustering-%e9%9a%8e%e5%b1%a4%e5%bc%8f%e5%88%86%e7%be%a4/" target="_blank" rel="noopener noreferrer">Hierarchical Clustering 階層式分群 | Clustering 資料分群 | R統計</a></li>
<li><a href="/partitional-clustering-kmeans-kmedoid/" target="_blank" rel="noopener noreferrer">Partitional Clustering | 切割式分群 | Kmeans, Kmedoid | Clustering 資料分群</a></li>
<li><a href="/principal-components-analysis-pca-%e4%b8%bb%e6%88%90%e4%bb%bd%e5%88%86%e6%9e%90/" target="_blank" rel="noopener noreferrer">Principal Components Analysis (PCA) | 主成份分析 | R 統計</a></li>
</ol>
<hr />
<p>學習筆記參考連結：</p>
<ol>
<li><a href="https://www.guru99.com/r-decision-trees.html" target="_blank" rel="noopener noreferrer">Decision Tree in R with Example </a></li>
<li><a href="https://www.statmethods.net/advstats/cart.html">Tree-Based Models</a></li>
<li><a href="https://rstudio-pubs-static.s3.amazonaws.com/275285_90aaf9a2a64d43a5846a86dbcde8eba9.html">R_programming &#8211; (8)決策樹(Decision Tree)</a></li>
<li><a href="https://dzone.com/articles/decision-trees-and-pruning-in-r">Decision Trees and Pruning in R</a></li>
<li><a href="https://statinfer.com/203-3-10-pruning-a-decision-tree-in-r/">Pruning a Decision Tree in R</a></li>
<li><a href="https://www.edureka.co/blog/decision-trees/">How To Create A Perfect Decision Tree</a></li>
<li><a href="https://medium.com/@yehjames/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda">[資料分析&amp;機器學習] 第3.5講 : 決策樹(Decision Tree)以及隨機森林(Random Forest)介紹</a></li>
</ol>
<p>這篇文章 <a rel="nofollow" href="/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/">Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest</a> 最早出現於 <a rel="nofollow" href="/">果醬珍珍•JamJam</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/decision-tree-cart-%e6%b1%ba%e7%ad%96%e6%a8%b9/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
	</channel>
</rss>
